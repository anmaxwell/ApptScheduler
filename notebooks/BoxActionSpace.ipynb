{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ale_py/roms/utils.py:90: DeprecationWarning: SelectableGroups dict interface is deprecated. Use select.\n",
      "  for external in metadata.entry_points().get(self.group, []):\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import ptan\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set parameters\n",
    "num_gps = 3\n",
    "num_slots = 4\n",
    "num_pre_booked = 3\n",
    "to_book = torch.tensor([1,2,3,1])\n",
    "num_to_book = len(to_book)\n",
    "entropy_beta = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert numpy array to tensor for input\n",
    "def tensor_convert(x):\n",
    "    return torch.from_numpy(x).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, diary_shape):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128) \n",
    "        )\n",
    "\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(128, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, diary_shape)\n",
    "        )\n",
    "\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(128, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        init_out = self.net(x)\n",
    "        return self.actor(init_out), self.critic(init_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SchedulerEnv(gym.Env):\n",
    "    \n",
    "    metadata = {'render.modes': ['human']}\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        #set parameters for the day\n",
    "        self.num_gps = num_gps\n",
    "        self.num_slots = num_slots\n",
    "        self.num_pre_booked = num_pre_booked\n",
    "        self.to_book = to_book\n",
    "        self.num_to_book = num_to_book\n",
    "        self.diary_slots = num_gps*num_slots\n",
    "\n",
    "        #set action space this format of the diary\n",
    "        self.action_space = gym.spaces.Box(low=0, high=1, shape=(self.num_slots, self.num_gps), dtype=np.int32)\n",
    "        \n",
    "        #set observation space \n",
    "        self.observation_space = gym.spaces.Box(low=0, high=1, shape=(self.num_slots, self.num_gps), dtype=np.int32)\n",
    "   \n",
    "    #creates daily diary for each gp and randomly populates prebooked appointments\n",
    "    def reset(self):\n",
    "\n",
    "        #creates zero filled dataframe with row per time slot and column per gp\n",
    "        self.state = np.zeros((self.num_slots, self.num_gps),dtype=float)\n",
    "\n",
    "        #randomly enters a 1 for each pre booked appointments\n",
    "        while self.num_pre_booked>0:\n",
    "            self.num_pre_booked -= 1\n",
    "            row_to_update = np.random.randint(self.num_slots, size=1)\n",
    "            col_to_update = np.random.randint(self.num_gps, size=1)\n",
    "            #self.state.at[row_to_update[0],col_to_update[0]]=1\n",
    "            self.state[row_to_update,col_to_update]=1\n",
    "\n",
    "        #resets parameters for new episode\n",
    "        self.done = False\n",
    "        self.reward = 0\n",
    "        self.num_to_book = num_to_book\n",
    "        self.num_pre_booked = num_pre_booked\n",
    "\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "    \n",
    "        tot_appts = self.num_pre_booked + self.num_to_book\n",
    "        #print('total appts to book = ', tot_appts)\n",
    "        final_diary = action\n",
    "        #print('final appts in diary = ', action.sum())\n",
    "        \n",
    "        #rewards if keeps original appointments in same place\n",
    "        pre_booked_position = np.transpose(np.nonzero(self.state))\n",
    "        for i in (pre_booked_position):\n",
    "            if action[i[0],i[1]]:\n",
    "                self.reward +=1\n",
    "                #print('plus 1 matching')\n",
    "            else:\n",
    "                self.reward -=1\n",
    "                #print('minus 1 appt lost')\n",
    "\n",
    "        #rewards if all new appts are booked\n",
    "        if tot_appts == action.sum():\n",
    "            self.reward +=5\n",
    "            #print('plus 1 all booked')\n",
    "        else:\n",
    "            self.reward -=5\n",
    "            #print('not all booked')\n",
    "            \n",
    "        #print('tot reward', self.reward)\n",
    "        \n",
    "        #rewards if all longer appts are booked together\n",
    "\n",
    "        self.done = True\n",
    "        info = {}\n",
    "\n",
    "        return action, self.reward, self.done, info\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(states, actions, rewards, critics, max_rewards, optimizer, batch_count):\n",
    "    \n",
    "    #unpack batches for training\n",
    "    for i in range(len(rewards)):\n",
    "        state = states[i]\n",
    "        action = actions[i]\n",
    "        reward = rewards[i]\n",
    "        max_reward = max_rewards[i]\n",
    "        critic = critics[i]\n",
    "\n",
    "        obs_v = torch.FloatTensor(state)\n",
    "        rewards_v = torch.tensor(reward)\n",
    "        critic = torch.FloatTensor(critic)\n",
    "        actions_t = torch.FloatTensor(action)\n",
    "        max_reward = torch.tensor(max_reward)\n",
    "\n",
    "        loss_value_v = F.mse_loss(critic, rewards_v)\n",
    "\n",
    "        log_prob_v = F.log_softmax(action)\n",
    "        adv_v = max_reward - critic\n",
    "        log_prob_actions_v = adv_v * log_prob_v[range(len(action))]\n",
    "        loss_policy_v = -log_prob_actions_v.mean()\n",
    "\n",
    "        prob_v = F.softmax(action)\n",
    "        #entropy_loss_v = (prob_v * log_prob_v).sum().mean()\n",
    "        #loss_v = (entropy_beta * entropy_loss_v + loss_value_v + loss_policy_v)\n",
    "        loss_v = (loss_value_v + loss_policy_v)\n",
    "        loss_v = torch.tensor(loss_v, requires_grad = True)\n",
    "        print('loss_v', loss_v)\n",
    "        writer.add_scalar(\"loss\", loss_v, i + (len(rewards)*batch_count))\n",
    "\n",
    "        loss_v.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    writer.close()\n",
    "    return obs_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run a number of episodes to gather values and rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:17: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:19: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:24: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_v tensor(36.6578, requires_grad=True)\n",
      "loss_v tensor(56.1652, requires_grad=True)\n",
      "loss_v tensor(84.0317, requires_grad=True)\n",
      "loss_v tensor(55.9152, requires_grad=True)\n",
      "loss_v tensor(56.0401, requires_grad=True)\n",
      "loss_v tensor(36.2927, requires_grad=True)\n",
      "loss_v tensor(36.2927, requires_grad=True)\n",
      "loss_v tensor(36.7034, requires_grad=True)\n",
      "loss_v tensor(36.4241, requires_grad=True)\n",
      "loss_v tensor(55.9428, requires_grad=True)\n",
      "loss_v tensor(83.6877, requires_grad=True)\n",
      "loss_v tensor(57.8201, requires_grad=True)\n",
      "loss_v tensor(24.7897, requires_grad=True)\n",
      "loss_v tensor(56.1148, requires_grad=True)\n",
      "loss_v tensor(56.1334, requires_grad=True)\n",
      "loss_v tensor(83.7591, requires_grad=True)\n",
      "loss_v tensor(36.6972, requires_grad=True)\n",
      "loss_v tensor(55.9428, requires_grad=True)\n",
      "loss_v tensor(55.5834, requires_grad=True)\n",
      "loss_v tensor(56.0964, requires_grad=True)\n",
      "loss_v tensor(55.9791, requires_grad=True)\n",
      "loss_v tensor(36.4444, requires_grad=True)\n",
      "loss_v tensor(83.3195, requires_grad=True)\n",
      "loss_v tensor(24.7737, requires_grad=True)\n",
      "loss_v tensor(83.6954, requires_grad=True)\n",
      "loss_v tensor(56.1180, requires_grad=True)\n",
      "loss_v tensor(55.7087, requires_grad=True)\n",
      "loss_v tensor(36.4609, requires_grad=True)\n",
      "loss_v tensor(57.9952, requires_grad=True)\n",
      "loss_v tensor(36.4350, requires_grad=True)\n",
      "loss_v tensor(36.4700, requires_grad=True)\n",
      "loss_v tensor(56.2208, requires_grad=True)\n",
      "loss_v tensor(55.6472, requires_grad=True)\n",
      "loss_v tensor(36.5033, requires_grad=True)\n",
      "loss_v tensor(36.3671, requires_grad=True)\n",
      "loss_v tensor(36.4926, requires_grad=True)\n",
      "loss_v tensor(36.4173, requires_grad=True)\n",
      "loss_v tensor(56.4482, requires_grad=True)\n",
      "loss_v tensor(36.4615, requires_grad=True)\n",
      "loss_v tensor(45.2250, requires_grad=True)\n",
      "loss_v tensor(84.0317, requires_grad=True)\n",
      "loss_v tensor(36.4909, requires_grad=True)\n",
      "loss_v tensor(36.6579, requires_grad=True)\n",
      "loss_v tensor(83.6782, requires_grad=True)\n",
      "loss_v tensor(36.4700, requires_grad=True)\n",
      "loss_v tensor(57.4557, requires_grad=True)\n",
      "loss_v tensor(24.7737, requires_grad=True)\n",
      "loss_v tensor(45.2250, requires_grad=True)\n",
      "loss_v tensor(86.0421, requires_grad=True)\n",
      "loss_v tensor(56.0038, requires_grad=True)\n",
      "loss_v tensor(36.3341, requires_grad=True)\n",
      "loss_v tensor(55.8848, requires_grad=True)\n",
      "loss_v tensor(36.1933, requires_grad=True)\n",
      "loss_v tensor(86.0421, requires_grad=True)\n",
      "loss_v tensor(56.2425, requires_grad=True)\n",
      "loss_v tensor(56.1671, requires_grad=True)\n",
      "loss_v tensor(83.4440, requires_grad=True)\n",
      "loss_v tensor(44.9654, requires_grad=True)\n",
      "loss_v tensor(83.3195, requires_grad=True)\n",
      "loss_v tensor(56.0148, requires_grad=True)\n",
      "loss_v tensor(24.7589, requires_grad=True)\n",
      "loss_v tensor(85.5546, requires_grad=True)\n",
      "loss_v tensor(36.1906, requires_grad=True)\n",
      "loss_v tensor(24.7589, requires_grad=True)\n",
      "loss_v tensor(56.1249, requires_grad=True)\n",
      "loss_v tensor(55.9578, requires_grad=True)\n",
      "loss_v tensor(29.6498, requires_grad=True)\n",
      "loss_v tensor(56.1424, requires_grad=True)\n",
      "loss_v tensor(36.5392, requires_grad=True)\n",
      "loss_v tensor(36.4171, requires_grad=True)\n",
      "loss_v tensor(83.8850, requires_grad=True)\n",
      "loss_v tensor(36.2838, requires_grad=True)\n",
      "loss_v tensor(69.0847, requires_grad=True)\n",
      "loss_v tensor(83.6782, requires_grad=True)\n",
      "loss_v tensor(56.5426, requires_grad=True)\n",
      "loss_v tensor(83.6782, requires_grad=True)\n",
      "loss_v tensor(57.2590, requires_grad=True)\n",
      "loss_v tensor(55.9038, requires_grad=True)\n",
      "loss_v tensor(36.5993, requires_grad=True)\n",
      "loss_v tensor(45.1424, requires_grad=True)\n",
      "loss_v tensor(36.5837, requires_grad=True)\n",
      "loss_v tensor(57.9952, requires_grad=True)\n",
      "loss_v tensor(56.1148, requires_grad=True)\n",
      "loss_v tensor(84.0317, requires_grad=True)\n",
      "loss_v tensor(36.4997, requires_grad=True)\n",
      "loss_v tensor(36.3977, requires_grad=True)\n",
      "loss_v tensor(36.3240, requires_grad=True)\n",
      "loss_v tensor(45.2977, requires_grad=True)\n",
      "loss_v tensor(56.0148, requires_grad=True)\n",
      "loss_v tensor(83.6877, requires_grad=True)\n",
      "loss_v tensor(24.7538, requires_grad=True)\n",
      "loss_v tensor(84.0543, requires_grad=True)\n",
      "loss_v tensor(68.7394, requires_grad=True)\n",
      "loss_v tensor(55.8528, requires_grad=True)\n",
      "loss_v tensor(46.4442, requires_grad=True)\n",
      "loss_v tensor(56.3867, requires_grad=True)\n",
      "loss_v tensor(29.6459, requires_grad=True)\n",
      "loss_v tensor(56.2252, requires_grad=True)\n",
      "loss_v tensor(36.2927, requires_grad=True)\n",
      "loss_v tensor(24.7212, requires_grad=True)\n",
      "loss_v tensor(69.4001, requires_grad=True)\n",
      "loss_v tensor(83.6954, requires_grad=True)\n",
      "loss_v tensor(70.8007, requires_grad=True)\n",
      "loss_v tensor(83.5277, requires_grad=True)\n",
      "loss_v tensor(36.5033, requires_grad=True)\n",
      "loss_v tensor(36.7034, requires_grad=True)\n",
      "loss_v tensor(24.7538, requires_grad=True)\n",
      "loss_v tensor(55.7087, requires_grad=True)\n",
      "loss_v tensor(56.1424, requires_grad=True)\n",
      "loss_v tensor(36.3040, requires_grad=True)\n",
      "loss_v tensor(84.1677, requires_grad=True)\n",
      "loss_v tensor(84.2189, requires_grad=True)\n",
      "loss_v tensor(83.9053, requires_grad=True)\n",
      "loss_v tensor(36.4340, requires_grad=True)\n",
      "loss_v tensor(24.7347, requires_grad=True)\n",
      "loss_v tensor(36.4069, requires_grad=True)\n",
      "loss_v tensor(68.7394, requires_grad=True)\n",
      "loss_v tensor(24.7589, requires_grad=True)\n",
      "loss_v tensor(56.1920, requires_grad=True)\n",
      "loss_v tensor(45.2977, requires_grad=True)\n",
      "loss_v tensor(36.2456, requires_grad=True)\n",
      "loss_v tensor(36.5000, requires_grad=True)\n",
      "loss_v tensor(68.9385, requires_grad=True)\n",
      "loss_v tensor(86.5822, requires_grad=True)\n",
      "loss_v tensor(24.7335, requires_grad=True)\n",
      "loss_v tensor(56.0517, requires_grad=True)\n",
      "loss_v tensor(45.5534, requires_grad=True)\n",
      "loss_v tensor(83.6835, requires_grad=True)\n",
      "loss_v tensor(45.6672, requires_grad=True)\n",
      "loss_v tensor(56.4482, requires_grad=True)\n",
      "loss_v tensor(56.0382, requires_grad=True)\n",
      "loss_v tensor(24.7335, requires_grad=True)\n",
      "loss_v tensor(56.1671, requires_grad=True)\n",
      "loss_v tensor(55.5834, requires_grad=True)\n",
      "loss_v tensor(36.4340, requires_grad=True)\n",
      "loss_v tensor(29.6978, requires_grad=True)\n",
      "loss_v tensor(56.3501, requires_grad=True)\n",
      "loss_v tensor(69.3967, requires_grad=True)\n",
      "loss_v tensor(36.4639, requires_grad=True)\n",
      "loss_v tensor(36.3587, requires_grad=True)\n",
      "loss_v tensor(55.9992, requires_grad=True)\n",
      "loss_v tensor(57.8400, requires_grad=True)\n",
      "loss_v tensor(36.4171, requires_grad=True)\n",
      "loss_v tensor(36.6578, requires_grad=True)\n",
      "loss_v tensor(58.0375, requires_grad=True)\n",
      "loss_v tensor(83.4452, requires_grad=True)\n",
      "loss_v tensor(56.2425, requires_grad=True)\n",
      "loss_v tensor(36.7232, requires_grad=True)\n",
      "loss_v tensor(85.5546, requires_grad=True)\n",
      "loss_v tensor(56.5151, requires_grad=True)\n",
      "loss_v tensor(36.3289, requires_grad=True)\n",
      "loss_v tensor(36.6572, requires_grad=True)\n",
      "loss_v tensor(83.4440, requires_grad=True)\n",
      "loss_v tensor(69.0694, requires_grad=True)\n",
      "loss_v tensor(69.3839, requires_grad=True)\n",
      "loss_v tensor(36.4516, requires_grad=True)\n",
      "loss_v tensor(36.5993, requires_grad=True)\n",
      "loss_v tensor(36.5837, requires_grad=True)\n",
      "loss_v tensor(29.6459, requires_grad=True)\n",
      "loss_v tensor(83.4452, requires_grad=True)\n",
      "loss_v tensor(83.5277, requires_grad=True)\n",
      "loss_v tensor(56.0148, requires_grad=True)\n",
      "loss_v tensor(36.7397, requires_grad=True)\n",
      "loss_v tensor(24.7335, requires_grad=True)\n",
      "loss_v tensor(56.1182, requires_grad=True)\n",
      "loss_v tensor(36.7034, requires_grad=True)\n",
      "loss_v tensor(44.9654, requires_grad=True)\n",
      "loss_v tensor(56.4482, requires_grad=True)\n",
      "loss_v tensor(36.6572, requires_grad=True)\n",
      "loss_v tensor(29.5784, requires_grad=True)\n",
      "loss_v tensor(83.8237, requires_grad=True)\n",
      "loss_v tensor(36.5837, requires_grad=True)\n",
      "loss_v tensor(36.5135, requires_grad=True)\n",
      "loss_v tensor(36.5528, requires_grad=True)\n",
      "loss_v tensor(36.3689, requires_grad=True)\n",
      "loss_v tensor(36.4553, requires_grad=True)\n",
      "loss_v tensor(55.8643, requires_grad=True)\n",
      "loss_v tensor(56.2482, requires_grad=True)\n",
      "loss_v tensor(36.3041, requires_grad=True)\n",
      "loss_v tensor(56.0321, requires_grad=True)\n",
      "loss_v tensor(57.9952, requires_grad=True)\n",
      "loss_v tensor(24.7646, requires_grad=True)\n",
      "loss_v tensor(55.8156, requires_grad=True)\n",
      "loss_v tensor(83.5552, requires_grad=True)\n",
      "loss_v tensor(84.0317, requires_grad=True)\n",
      "loss_v tensor(45.2273, requires_grad=True)\n",
      "loss_v tensor(45.2124, requires_grad=True)\n",
      "loss_v tensor(36.3587, requires_grad=True)\n",
      "loss_v tensor(45.2273, requires_grad=True)\n",
      "loss_v tensor(36.6073, requires_grad=True)\n",
      "loss_v tensor(45.2711, requires_grad=True)\n",
      "loss_v tensor(36.5304, requires_grad=True)\n",
      "loss_v tensor(36.4149, requires_grad=True)\n",
      "loss_v tensor(45.2711, requires_grad=True)\n",
      "loss_v tensor(83.6691, requires_grad=True)\n",
      "loss_v tensor(83.6691, requires_grad=True)\n",
      "loss_v tensor(36.7034, requires_grad=True)\n",
      "loss_v tensor(56.1180, requires_grad=True)\n",
      "loss_v tensor(56.0316, requires_grad=True)\n",
      "loss_v tensor(36.4350, requires_grad=True)\n",
      "loss_v tensor(29.5758, requires_grad=True)\n",
      "loss_v tensor(84.0543, requires_grad=True)\n",
      "loss_v tensor(56.2848, requires_grad=True)\n",
      "loss_v tensor(55.6099, requires_grad=True)\n",
      "loss_v tensor(36.3267, requires_grad=True)\n",
      "loss_v tensor(24.7189, requires_grad=True)\n",
      "loss_v tensor(83.6691, requires_grad=True)\n",
      "loss_v tensor(84.3304, requires_grad=True)\n",
      "loss_v tensor(55.9152, requires_grad=True)\n",
      "loss_v tensor(36.4553, requires_grad=True)\n",
      "loss_v tensor(36.4139, requires_grad=True)\n",
      "loss_v tensor(84.3304, requires_grad=True)\n",
      "loss_v tensor(45.4111, requires_grad=True)\n",
      "loss_v tensor(56.2265, requires_grad=True)\n",
      "loss_v tensor(56.1920, requires_grad=True)\n",
      "loss_v tensor(68.9340, requires_grad=True)\n",
      "loss_v tensor(36.5392, requires_grad=True)\n",
      "loss_v tensor(84.2051, requires_grad=True)\n",
      "loss_v tensor(36.6579, requires_grad=True)\n",
      "loss_v tensor(36.2410, requires_grad=True)\n",
      "loss_v tensor(83.3195, requires_grad=True)\n",
      "loss_v tensor(83.7591, requires_grad=True)\n",
      "loss_v tensor(29.6978, requires_grad=True)\n",
      "loss_v tensor(56.2211, requires_grad=True)\n",
      "loss_v tensor(55.5834, requires_grad=True)\n",
      "loss_v tensor(56.1410, requires_grad=True)\n",
      "loss_v tensor(36.5993, requires_grad=True)\n",
      "loss_v tensor(83.9238, requires_grad=True)\n",
      "loss_v tensor(36.3240, requires_grad=True)\n",
      "loss_v tensor(56.5518, requires_grad=True)\n",
      "loss_v tensor(45.4496, requires_grad=True)\n",
      "loss_v tensor(45.2124, requires_grad=True)\n",
      "loss_v tensor(36.4340, requires_grad=True)\n",
      "loss_v tensor(36.5391, requires_grad=True)\n",
      "loss_v tensor(55.8156, requires_grad=True)\n",
      "loss_v tensor(36.6578, requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_v tensor(36.5135, requires_grad=True)\n",
      "loss_v tensor(24.7646, requires_grad=True)\n",
      "loss_v tensor(68.9385, requires_grad=True)\n",
      "loss_v tensor(36.2456, requires_grad=True)\n",
      "loss_v tensor(36.5392, requires_grad=True)\n",
      "loss_v tensor(24.7589, requires_grad=True)\n",
      "loss_v tensor(45.4231, requires_grad=True)\n",
      "loss_v tensor(56.2211, requires_grad=True)\n",
      "loss_v tensor(29.6008, requires_grad=True)\n",
      "loss_v tensor(55.8825, requires_grad=True)\n",
      "loss_v tensor(68.7235, requires_grad=True)\n",
      "loss_v tensor(36.3289, requires_grad=True)\n",
      "loss_v tensor(36.4069, requires_grad=True)\n",
      "loss_v tensor(56.2819, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "#initialise environment, model and optimiser\n",
    "env = SchedulerEnv()\n",
    "model = Model((env.diary_slots+env.num_to_book), env.diary_slots)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, \n",
    "                             amsgrad=False)\n",
    "optimizer.zero_grad()\n",
    "writer = SummaryWriter()\n",
    "\n",
    "\n",
    "#create batches to train model. only use top 25% for training\n",
    "\n",
    "for i in range(25):\n",
    "    #create empty lists to store batches\n",
    "    batch_inputs = []\n",
    "    batch_actions = []\n",
    "    batch_rewards = []\n",
    "    batch_critic = []\n",
    "    batch_max_rewards = []\n",
    "    \n",
    "    for j in range(10):\n",
    "\n",
    "        state = env.reset()\n",
    "\n",
    "        flat_state = torch.flatten(tensor_convert(state))\n",
    "        nn_input = torch.cat((flat_state, env.to_book))\n",
    "        action, critic = model(nn_input)\n",
    "        \n",
    "        #convert output to be between -1 and 1\n",
    "        action = F.tanh(action)\n",
    "\n",
    "        #convert output to match action space\n",
    "        end_diary = action.reshape(env.observation_space.shape)\n",
    "        end_diary[end_diary>0] = 1 \n",
    "        end_diary[end_diary<0] = 0 \n",
    "        _, rewards, done, _ = env.step(end_diary)\n",
    "\n",
    "        #calc max rewards\n",
    "        max_rewards = num_pre_booked + 5\n",
    "        \n",
    "        batch_inputs.append(nn_input)\n",
    "        batch_actions.append(action)\n",
    "        batch_rewards.append(rewards)\n",
    "        batch_critic.append(critic)\n",
    "        batch_max_rewards.append(max_rewards)\n",
    "        #print(batch_rewards)\n",
    "\n",
    "    train(batch_inputs, batch_actions, batch_rewards, batch_critic, batch_max_rewards, optimizer, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['GRID_SAMPLE_INTERPOLATION_MODES',\n",
       " 'GRID_SAMPLE_PADDING_MODES',\n",
       " 'List',\n",
       " 'Optional',\n",
       " 'Tensor',\n",
       " 'Tuple',\n",
       " '_Reduction',\n",
       " '_VF',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " '__warningregistry__',\n",
       " '_adaptive_max_pool1d',\n",
       " '_adaptive_max_pool2d',\n",
       " '_adaptive_max_pool3d',\n",
       " '_add_docstr',\n",
       " '_fractional_max_pool2d',\n",
       " '_fractional_max_pool3d',\n",
       " '_get_softmax_dim',\n",
       " '_infer_size',\n",
       " '_list_with_default',\n",
       " '_max_pool1d',\n",
       " '_max_pool2d',\n",
       " '_max_pool3d',\n",
       " '_no_grad_embedding_renorm_',\n",
       " '_overload',\n",
       " '_pad',\n",
       " '_pad_circular',\n",
       " '_pair',\n",
       " '_single',\n",
       " '_threshold',\n",
       " '_triple',\n",
       " '_unpool_output_size',\n",
       " '_verify_batch_size',\n",
       " 'adaptive_avg_pool1d',\n",
       " 'adaptive_avg_pool2d',\n",
       " 'adaptive_avg_pool3d',\n",
       " 'adaptive_max_pool1d',\n",
       " 'adaptive_max_pool1d_with_indices',\n",
       " 'adaptive_max_pool2d',\n",
       " 'adaptive_max_pool2d_with_indices',\n",
       " 'adaptive_max_pool3d',\n",
       " 'adaptive_max_pool3d_with_indices',\n",
       " 'affine_grid',\n",
       " 'alpha_dropout',\n",
       " 'assert_int_or_pair',\n",
       " 'avg_pool1d',\n",
       " 'avg_pool2d',\n",
       " 'avg_pool3d',\n",
       " 'batch_norm',\n",
       " 'bilinear',\n",
       " 'binary_cross_entropy',\n",
       " 'binary_cross_entropy_with_logits',\n",
       " 'boolean_dispatch',\n",
       " 'celu',\n",
       " 'celu_',\n",
       " 'channel_shuffle',\n",
       " 'conv1d',\n",
       " 'conv2d',\n",
       " 'conv3d',\n",
       " 'conv_tbc',\n",
       " 'conv_transpose1d',\n",
       " 'conv_transpose2d',\n",
       " 'conv_transpose3d',\n",
       " 'cosine_embedding_loss',\n",
       " 'cosine_similarity',\n",
       " 'cross_entropy',\n",
       " 'ctc_loss',\n",
       " 'dropout',\n",
       " 'dropout2d',\n",
       " 'dropout3d',\n",
       " 'elu',\n",
       " 'elu_',\n",
       " 'embedding',\n",
       " 'embedding_bag',\n",
       " 'feature_alpha_dropout',\n",
       " 'fold',\n",
       " 'fractional_max_pool2d',\n",
       " 'fractional_max_pool2d_with_indices',\n",
       " 'fractional_max_pool3d',\n",
       " 'fractional_max_pool3d_with_indices',\n",
       " 'gelu',\n",
       " 'glu',\n",
       " 'grad',\n",
       " 'grid_sample',\n",
       " 'group_norm',\n",
       " 'gumbel_softmax',\n",
       " 'handle_torch_function',\n",
       " 'hardshrink',\n",
       " 'hardsigmoid',\n",
       " 'hardswish',\n",
       " 'hardtanh',\n",
       " 'hardtanh_',\n",
       " 'has_torch_function',\n",
       " 'hinge_embedding_loss',\n",
       " 'instance_norm',\n",
       " 'interpolate',\n",
       " 'kl_div',\n",
       " 'l1_loss',\n",
       " 'layer_norm',\n",
       " 'leaky_relu',\n",
       " 'leaky_relu_',\n",
       " 'linear',\n",
       " 'local_response_norm',\n",
       " 'log_softmax',\n",
       " 'logsigmoid',\n",
       " 'lp_pool1d',\n",
       " 'lp_pool2d',\n",
       " 'margin_ranking_loss',\n",
       " 'math',\n",
       " 'max_pool1d',\n",
       " 'max_pool1d_with_indices',\n",
       " 'max_pool2d',\n",
       " 'max_pool2d_with_indices',\n",
       " 'max_pool3d',\n",
       " 'max_pool3d_with_indices',\n",
       " 'max_unpool1d',\n",
       " 'max_unpool2d',\n",
       " 'max_unpool3d',\n",
       " 'mse_loss',\n",
       " 'multi_head_attention_forward',\n",
       " 'multi_margin_loss',\n",
       " 'multilabel_margin_loss',\n",
       " 'multilabel_soft_margin_loss',\n",
       " 'nll_loss',\n",
       " 'normalize',\n",
       " 'one_hot',\n",
       " 'pad',\n",
       " 'pairwise_distance',\n",
       " 'pdist',\n",
       " 'pixel_shuffle',\n",
       " 'poisson_nll_loss',\n",
       " 'prelu',\n",
       " 'relu',\n",
       " 'relu6',\n",
       " 'relu_',\n",
       " 'rrelu',\n",
       " 'rrelu_',\n",
       " 'selu',\n",
       " 'selu_',\n",
       " 'sigmoid',\n",
       " 'silu',\n",
       " 'smooth_l1_loss',\n",
       " 'soft_margin_loss',\n",
       " 'softmax',\n",
       " 'softmin',\n",
       " 'softplus',\n",
       " 'softshrink',\n",
       " 'softsign',\n",
       " 'tanh',\n",
       " 'tanhshrink',\n",
       " 'threshold',\n",
       " 'threshold_',\n",
       " 'torch',\n",
       " 'triplet_margin_loss',\n",
       " 'triplet_margin_with_distance_loss',\n",
       " 'unfold',\n",
       " 'upsample',\n",
       " 'upsample_bilinear',\n",
       " 'upsample_nearest',\n",
       " 'utils',\n",
       " 'warnings']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(torch.nn.functional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Becca\n"
     ]
    }
   ],
   "source": [
    "print('Hello Becca')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 2., 3., 1.]),\n",
       " tensor([0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 2., 3., 1.]),\n",
       " tensor([0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 2., 3., 1.]),\n",
       " tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 2., 3., 1.]),\n",
       " tensor([0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 2., 3., 1.]),\n",
       " tensor([0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 2., 3., 1.]),\n",
       " tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 2., 3., 1.]),\n",
       " tensor([0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 2., 3., 1.]),\n",
       " tensor([0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 2., 3., 1.]),\n",
       " tensor([0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 2., 3., 1.])]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
