{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import ptan\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set parameters\n",
    "num_gps = 3\n",
    "num_slots = 4\n",
    "num_pre_booked = 3\n",
    "to_book = torch.tensor([1,2,3,1])\n",
    "num_to_book = len(to_book)\n",
    "entropy_beta = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert numpy array to tensor for input\n",
    "def tensor_convert(x):\n",
    "    return torch.from_numpy(x).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, diary_shape):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128) \n",
    "        )\n",
    "\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(128, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, diary_shape)\n",
    "        )\n",
    "\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(128, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        init_out = self.net(x)\n",
    "        return self.actor(init_out), self.critic(init_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SchedulerEnv(gym.Env):\n",
    "    \n",
    "    metadata = {'render.modes': ['human']}\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        #set parameters for the day\n",
    "        self.num_gps = num_gps\n",
    "        self.num_slots = num_slots\n",
    "        self.num_pre_booked = num_pre_booked\n",
    "        self.to_book = to_book\n",
    "        self.num_to_book = num_to_book\n",
    "        self.diary_slots = num_gps*num_slots\n",
    "\n",
    "        #set action space this format of the diary\n",
    "        self.action_space = gym.spaces.Box(low=0, high=1, shape=(self.num_slots, self.num_gps), dtype=np.int32)\n",
    "        \n",
    "        #set observation space \n",
    "        self.observation_space = gym.spaces.Box(low=0, high=1, shape=(self.num_slots, self.num_gps), dtype=np.int32)\n",
    "   \n",
    "    #creates daily diary for each gp and randomly populates prebooked appointments\n",
    "    def reset(self):\n",
    "\n",
    "        #creates zero filled dataframe with row per time slot and column per gp\n",
    "        self.state = np.zeros((self.num_slots, self.num_gps),dtype=float)\n",
    "\n",
    "        #randomly enters a 1 for each pre booked appointments\n",
    "        while self.num_pre_booked>0:\n",
    "            self.num_pre_booked -= 1\n",
    "            row_to_update = np.random.randint(self.num_slots, size=1)\n",
    "            col_to_update = np.random.randint(self.num_gps, size=1)\n",
    "            #self.state.at[row_to_update[0],col_to_update[0]]=1\n",
    "            self.state[row_to_update,col_to_update]=1\n",
    "\n",
    "        #resets parameters for new episode\n",
    "        self.done = False\n",
    "        self.reward = 0\n",
    "        self.num_to_book = num_to_book\n",
    "        self.num_pre_booked = num_pre_booked\n",
    "\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "    \n",
    "        tot_appts = self.num_pre_booked + self.num_to_book\n",
    "        #print('total appts to book = ', tot_appts)\n",
    "        final_diary = action\n",
    "        #print('final appts in diary = ', action.sum())\n",
    "        \n",
    "        #rewards if keeps original appointments in same place\n",
    "        pre_booked_position = np.transpose(np.nonzero(self.state))\n",
    "        for i in (pre_booked_position):\n",
    "            if action[i[0],i[1]]:\n",
    "                self.reward +=1\n",
    "                #print('plus 1 matching')\n",
    "            else:\n",
    "                self.reward -=1\n",
    "                #print('minus 1 appt lost')\n",
    "\n",
    "        #rewards if all new appts are booked\n",
    "        if tot_appts == action.sum():\n",
    "            self.reward +=5\n",
    "            #print('plus 1 all booked')\n",
    "        else:\n",
    "            self.reward -=5\n",
    "            #print('not all booked')\n",
    "            \n",
    "        #print('tot reward', self.reward)\n",
    "        \n",
    "        #rewards if all longer appts are booked together\n",
    "\n",
    "        self.done = True\n",
    "        info = {}\n",
    "\n",
    "        return action, self.reward, self.done, info\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(states, actions, rewards, critics, max_rewards, optimizer, batch_count):\n",
    "    \n",
    "    #unpack batches for training\n",
    "    for i in range(len(rewards)):\n",
    "        state = states[i]\n",
    "        action = actions[i]\n",
    "        reward = rewards[i]\n",
    "        max_reward = max_rewards[i]\n",
    "        critic = critics[i]\n",
    "\n",
    "        obs_v = torch.FloatTensor(state)\n",
    "        rewards_v = torch.tensor(reward)\n",
    "        critic = torch.FloatTensor(critic)\n",
    "        actions_t = torch.FloatTensor(action)\n",
    "        max_reward = torch.tensor(max_reward)\n",
    "\n",
    "        loss_value_v = F.mse_loss(critic, rewards_v)\n",
    "\n",
    "        log_prob_v = F.log_softmax(action)\n",
    "        adv_v = max_reward - critic\n",
    "        log_prob_actions_v = adv_v * log_prob_v[range(len(action))]\n",
    "        loss_policy_v = -log_prob_actions_v.mean()\n",
    "\n",
    "        prob_v = F.softmax(action)\n",
    "        #entropy_loss_v = (prob_v * log_prob_v).sum().mean()\n",
    "        #loss_v = (entropy_beta * entropy_loss_v + loss_value_v + loss_policy_v)\n",
    "        loss_v = (loss_value_v + loss_policy_v)\n",
    "        loss_v = torch.tensor(loss_v, requires_grad = True)\n",
    "        print('loss_v', loss_v)\n",
    "        writer.add_scalar(\"loss\", loss_v, i + (len(rewards)*batch_count))\n",
    "\n",
    "        loss_v.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    writer.close()\n",
    "    return obs_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run a number of episodes to gather values and rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialise environment, model and optimiser\n",
    "env = SchedulerEnv()\n",
    "model = Model((env.diary_slots+env.num_to_book), env.diary_slots)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, \n",
    "                             amsgrad=False)\n",
    "optimizer.zero_grad()\n",
    "writer = SummaryWriter()\n",
    "\n",
    "\n",
    "#create batches to train model. only use top 25% for training\n",
    "\n",
    "for i in range(25):\n",
    "    #create empty lists to store batches\n",
    "    batch_inputs = []\n",
    "    batch_actions = []\n",
    "    batch_rewards = []\n",
    "    batch_critic = []\n",
    "    batch_max_rewards = []\n",
    "    \n",
    "    for j in range(10):\n",
    "\n",
    "        state = env.reset()\n",
    "\n",
    "        flat_state = torch.flatten(tensor_convert(state))\n",
    "        nn_input = torch.cat((flat_state, env.to_book))\n",
    "        action, critic = model(nn_input)\n",
    "        \n",
    "        #convert output to be between -1 and 1\n",
    "        action = F.tanh(action)\n",
    "\n",
    "        #convert output to match action space\n",
    "        end_diary = action.reshape(env.observation_space.shape)\n",
    "        end_diary[end_diary>0] = 1 \n",
    "        end_diary[end_diary<0] = 0 \n",
    "        _, rewards, done, _ = env.step(end_diary)\n",
    "\n",
    "        #calc max rewards\n",
    "        max_rewards = num_pre_booked + 5\n",
    "        \n",
    "        batch_inputs.append(nn_input)\n",
    "        batch_actions.append(action)\n",
    "        batch_rewards.append(rewards)\n",
    "        batch_critic.append(critic)\n",
    "        batch_max_rewards.append(max_rewards)\n",
    "        #print(batch_rewards)\n",
    "\n",
    "    train(batch_inputs, batch_actions, batch_rewards, batch_critic, batch_max_rewards, optimizer, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(torch.nn.functional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
