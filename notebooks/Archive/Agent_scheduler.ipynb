{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SchedulerEnv(gym.Env):\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        #starting parameters\n",
    "        num_gps = 100\n",
    "        num_slots = 32\n",
    "        num_pre_booked = 15\n",
    "        to_book = [2,1,1,1,1,1,1]\n",
    "        num_to_book = len(to_book)\n",
    "        agent_pos = [0,0]\n",
    "        reward_decay = 0.95\n",
    "        \n",
    "        #set parameters for the day\n",
    "        self.num_gps = num_gps\n",
    "        self.num_slots = num_slots\n",
    "        self.num_pre_booked = num_pre_booked\n",
    "        self.to_book = to_book\n",
    "        self.num_to_book = num_to_book\n",
    "        self.diary_slots = num_gps*num_slots\n",
    "        self.agent_pos = agent_pos\n",
    "        self.reward_decay = reward_decay\n",
    "\n",
    "        #set action space to move around the grid\n",
    "        self.action_space = gym.spaces.Discrete(4) #up, down, left, right\n",
    "        \n",
    "        #set observation space \n",
    "        self.observation_space = gym.spaces.Box(low=0, high=1, shape=(self.num_slots, self.num_gps), dtype=np.int32)\n",
    "   \n",
    "    #creates daily diary for each gp, randomly populates prebooked appointments and resets parameters\n",
    "    def reset(self):\n",
    "\n",
    "        #creates zero filled dataframe with row per time slot and column per gp\n",
    "        self.state = np.zeros((self.num_slots, self.num_gps),dtype=float)\n",
    "\n",
    "        #randomly enters a 1 for each pre booked appointments\n",
    "        pre_booked = self.num_pre_booked\n",
    "        while pre_booked>0:\n",
    "            pre_booked -= 1\n",
    "            self.state[np.random.randint(self.num_slots), np.random.randint(self.num_gps)] = 1\n",
    "            \n",
    "        #randomly sets the agent start space\n",
    "        self.agent_pos = [np.random.randint(self.num_slots), np.random.randint(self.num_gps)]\n",
    "\n",
    "        #resets parameters for new episode\n",
    "        self.done = False\n",
    "        self.reward = 0\n",
    "        self.appt_idx = 0\n",
    "        self.decay_steps = 1\n",
    "        \n",
    "        #print('starting state', self.state.sum(), self.state)\n",
    "\n",
    "        return self.state\n",
    "    \n",
    "    #calculates new position of the agent based on the action\n",
    "    def move_agent(self, action):\n",
    "\n",
    "        #set boundaries for the grid\n",
    "        max_row = self.num_slots - 1\n",
    "        max_col = self.num_gps - 1\n",
    "\n",
    "        #setting new co-ordinates for the agent\n",
    "        new_row = self.agent_pos[0]\n",
    "        new_col = self.agent_pos[1]\n",
    "\n",
    "        #calculate what the new position may be based on the action without going out the grid\n",
    "        if action == 0:\n",
    "            #print('up')\n",
    "            new_row = max(self.agent_pos[0] - 1, 0)\n",
    "        if action == 1:\n",
    "            #print('down')\n",
    "            new_row = min(self.agent_pos[0] + 1, max_row)\n",
    "        if action == 2:\n",
    "            #print('left')\n",
    "            new_col = max(self.agent_pos[1] - 1, 0)\n",
    "        if action == 3:\n",
    "            #print('right')\n",
    "            new_col = min(self.agent_pos[1] + 1, max_col)\n",
    "\n",
    "        new_pos = [new_row, new_col]\n",
    "        #print('new pos', new_pos)\n",
    "\n",
    "        return new_pos\n",
    "\n",
    "    #checks if we can look to book appointment starting here\n",
    "    def check_bookable(self):\n",
    "        return self.state[self.agent_pos[0], self.agent_pos[1]] == 0.0\n",
    "    \n",
    "    #action if we can't book the appointment\n",
    "    def invalid_booking(self):\n",
    "        #print('cant book')\n",
    "        self.decay_steps += 1\n",
    "        self.reward = -1\n",
    "        \n",
    "    #action if we can book the appointment\n",
    "    def valid_booking(self):\n",
    "        #print('go ahead and book')\n",
    "        self.appt_idx += 1\n",
    "        self.decay_steps = 1\n",
    "        self.reward = 1\n",
    "    \n",
    "    #checks if the appointment fits\n",
    "    def check_and_book(self):\n",
    "        \n",
    "        max_row = self.num_slots - 1\n",
    "        cells_to_check = self.to_book[self.appt_idx]\n",
    "        \n",
    "        if cells_to_check==1:\n",
    "            #print('good to check for single')\n",
    "            if self.state[self.agent_pos[0], self.agent_pos[1]] == 0:\n",
    "                self.state[self.agent_pos[0], self.agent_pos[1]] = 1\n",
    "                self.valid_booking()\n",
    "            else:\n",
    "                #print('single taken')\n",
    "                self.invalid_booking()\n",
    "\n",
    "        if cells_to_check==2:\n",
    "            #check we're not at the bottom of the grid\n",
    "            if self.agent_pos[0]<max_row:\n",
    "                #check the next cells is also 0.0\n",
    "                #print('good to check for double')\n",
    "                if self.state[self.agent_pos[0], self.agent_pos[1]] == 0 and \\\n",
    "                self.state[(self.agent_pos[0]+1), self.agent_pos[1]] == 0:\n",
    "                    self.state[self.agent_pos[0], self.agent_pos[1]] = 1\n",
    "                    self.state[(self.agent_pos[0]+1), self.agent_pos[1]] = 1\n",
    "                    self.valid_booking()\n",
    "                    self.agent_pos = [(self.agent_pos[0]+1), self.agent_pos[1]]\n",
    "                    #print('after booking', self.agent_pos)\n",
    "                else:\n",
    "                    #print('double taken')\n",
    "                    self.invalid_booking()\n",
    "            else:\n",
    "                #print('not for double')\n",
    "                self.invalid_booking()\n",
    "                \n",
    "        if cells_to_check==3:\n",
    "            #check we're not at the bottom of the grid\n",
    "            if self.agent_pos[0]+1<max_row:\n",
    "                #print('good to check for treble')\n",
    "                if self.state[self.agent_pos[0], self.agent_pos[1]] == 0 and \\\n",
    "                self.state[(self.agent_pos[0]+1), self.agent_pos[1]] == 0 \\\n",
    "                 and self.state[(self.agent_pos[0]+2), self.agent_pos[1]] == 0:\n",
    "                    self.state[self.agent_pos[0], self.agent_pos[1]] = 1\n",
    "                    self.state[(self.agent_pos[0]+1), self.agent_pos[1]] = 1\n",
    "                    self.state[(self.agent_pos[0]+2), self.agent_pos[1]] = 1\n",
    "                    self.valid_booking()\n",
    "                    self.agent_pos = [(self.agent_pos[0]+2), self.agent_pos[1]]\n",
    "                else:\n",
    "                    #print('treble taken')\n",
    "                    self.invalid_booking()\n",
    "            else:\n",
    "                #print('not for treble')\n",
    "                self.invalid_booking()\n",
    "                \n",
    "        if cells_to_check==4:\n",
    "            #check we're not at the bottom of the grid\n",
    "            if self.agent_pos[0]+2<max_row:\n",
    "                #check the next cells is also 0.0\n",
    "                #print('good for quad')\n",
    "                if self.state[self.agent_pos[0], self.agent_pos[1]] == 0 and \\\n",
    "                self.state[(self.agent_pos[0]+1), self.agent_pos[1]] == 0 \\\n",
    "                 and self.state[(self.agent_pos[0]+2), self.agent_pos[1]] == 0 and \\\n",
    "                self.state[(self.agent_pos[0]+3), self.agent_pos[1]] == 0:\n",
    "                    self.state[self.agent_pos[0], self.agent_pos[1]] = 1\n",
    "                    self.state[(self.agent_pos[0]+1), self.agent_pos[1]] = 1\n",
    "                    self.state[(self.agent_pos[0]+2), self.agent_pos[1]] = 1\n",
    "                    self.state[(self.agent_pos[0]+3), self.agent_pos[1]] = 1\n",
    "                    self.valid_booking()\n",
    "                    self.agent_pos = [(self.agent_pos[0]+3), self.agent_pos[1]]\n",
    "                else:\n",
    "                    #print('quad taken')\n",
    "                    self.invalid_booking()\n",
    "            else:\n",
    "                #print('not for quad')\n",
    "                self.invalid_booking()\n",
    "\n",
    "        next_state = self.state\n",
    "\n",
    "        return next_state\n",
    "\n",
    "    def step(self, action):\n",
    "\n",
    "        #print('start step' , self.decay_steps)\n",
    "        #get new position of agent based on action\n",
    "        new_agent_pos = self.move_agent(action)\n",
    "        #print('new and old pos', new_agent_pos, self.agent_pos)\n",
    "        \n",
    "        #if the agent is stuck on an edge then move to a new position\n",
    "        if new_agent_pos == self.agent_pos:\n",
    "            self.agent_pos = [np.random.randint(self.num_slots), np.random.randint(self.num_gps)]\n",
    "            #print('here1', self.agent_pos)\n",
    "        else:\n",
    "            self.agent_pos = new_agent_pos\n",
    "            #print('here2', self.agent_pos)\n",
    "        \n",
    "        #print('trying to book', self.to_book, self.appt_idx)\n",
    "        \n",
    "        #check if it's possible to book then book\n",
    "        if self.check_bookable():\n",
    "            #print('checked here')\n",
    "            self.state = self.check_and_book()\n",
    "        else:\n",
    "            #print('not bookable')\n",
    "            self.invalid_booking()\n",
    "        \n",
    "        #work out if episode complete\n",
    "        if self.appt_idx == len(self.to_book):\n",
    "            #print('all booked')\n",
    "            self.done = True\n",
    "  \n",
    "        #work out rewards\n",
    "        #self.reward = (1 - (self.reward_decay**self.decay_steps))\n",
    "        \n",
    "        #print('step', self.decay_steps, self.reward)\n",
    "        #print('end step')\n",
    "\n",
    "        info = {}\n",
    "\n",
    "        return self.state, self.reward, self.done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, action_size):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128) \n",
    "        )\n",
    "\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(128, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, action_size)\n",
    "        )\n",
    "\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(128, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        init_out = self.net(x)\n",
    "        return self.actor(init_out), self.critic(init_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert numpy array to tensor for input\n",
    "def tensor_convert(x):\n",
    "    return torch.from_numpy(x).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def train(b_states, b_actions, b_rewards, b_actor, b_critic, eps_steps, optimizer, model):\n",
    "\n",
    "    #unpack batches for training\n",
    "    for i in range(eps_steps):\n",
    "    \n",
    "        state = b_states[i]\n",
    "        action = b_actions[i]\n",
    "        reward = b_rewards[i]\n",
    "        actor = b_actor[i]\n",
    "        critic = b_critic[i]\n",
    "        #print('actions', action, 'reward', reward, 'actor', actor, 'critic', critic)\n",
    "\n",
    "        obs_v = torch.FloatTensor(state)\n",
    "        rewards_v = torch.tensor(reward)\n",
    "        critic = torch.FloatTensor(critic)\n",
    "        actions_t = torch.tensor(action)\n",
    "        #print('obv_s', obs_v, 'rewards_v', rewards_v, 'critic', critic, 'action_t', actions_t)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        logits_v, value_v = model(state)\n",
    "        test = value_v.detach()\n",
    "        print('a', logits_v, 'b', test)\n",
    "        loss_value_v = F.mse_loss(value_v, reward)\n",
    "\n",
    "        log_prob_v = F.log_softmax(logits_v, dim=1)\n",
    "        adv_v = reward - value_v.detach()\n",
    "        log_prob_actions_v = adv_v * log_prob_v[1, action]\n",
    "        loss_policy_v = -log_prob_actions_v.mean()\n",
    "\n",
    "        prob_v = F.softmax(logits_v, dim=1)\n",
    "        entropy_loss_v = 0.01 * (prob_v * log_prob_v).sum(dim=1).mean()\n",
    "\n",
    "        # calculate policy gradients only\n",
    "        loss_policy_v.backward(retain_graph=True)\n",
    "        grads = np.concatenate([p.grad.data.cpu().numpy().flatten()\n",
    "                                for p in net.parameters()\n",
    "                                if p.grad is not None])\n",
    "\n",
    "        # apply entropy and value gradients\n",
    "        loss_v = entropy_loss_v + loss_value_v\n",
    "        loss_v.backward()\n",
    "        nn_utils.clip_grad_norm_(net.parameters(), CLIP_GRAD)\n",
    "        optimizer.step()\n",
    "    \n",
    "    return loss_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(b_states, b_actions, b_rewards, b_actor, b_critic, eps_steps, optimizer):\n",
    "    \n",
    "    #unpack batches for training\n",
    "    for i in range(eps_steps):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        state = b_states[i]\n",
    "        action = b_actions[i]\n",
    "        reward = b_rewards[i]\n",
    "        actor = b_actor[i]\n",
    "        critic = b_critic[i]\n",
    "        #print('actions', action, 'reward', reward, 'actor', actor, 'critic', critic)\n",
    "\n",
    "        obs_v = torch.FloatTensor(state)\n",
    "        rewards_v = torch.tensor(reward)\n",
    "        critic = torch.FloatTensor(critic)\n",
    "        actions_t = torch.tensor(action)\n",
    "        #print('obv_s', obs_v, 'rewards_v', rewards_v, 'critic', critic, 'action_t', actions_t)\n",
    "\n",
    "        loss_value_v = F.mse_loss(critic, rewards_v)\n",
    "        #print('loss_value_v', loss_value_v)\n",
    "\n",
    "        log_prob_v = F.log_softmax(actor)\n",
    "        adv_v = rewards_v - critic\n",
    "        log_prob_actions_v = adv_v * log_prob_v[range(1), action]\n",
    "        loss_policy_v = -log_prob_actions_v.mean()\n",
    "\n",
    "        prob_v = F.softmax(actor)\n",
    "        #entropy_loss_v = (prob_v * log_prob_v).sum().mean()\n",
    "        #loss_v = (entropy_beta * entropy_loss_v + loss_value_v + loss_policy_v)\n",
    "        loss_v = (loss_value_v + loss_policy_v)\n",
    "        loss_v = torch.tensor(loss_v, requires_grad = True)\n",
    "        #print('loss_v', loss_v)\n",
    "\n",
    "        loss_v.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    return loss_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:21: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:29: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards for episode 0 7\n",
      "rewards for episode 100 -1\n",
      "rewards for episode 200 7\n",
      "rewards for episode 300 -1\n",
      "rewards for episode 400 7\n",
      "rewards for episode 500 7\n",
      "rewards for episode 600 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-ce72b721343b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;31m#print('end of episode', eps_reward, eps_steps)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;31m#loss = train(b_states, b_actions, b_rewards, b_actor, b_critic, eps_steps, optimizer, model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_rewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_actor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_critic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"eps_reward\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps_reward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-52-92d17e2b69ee>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(b_states, b_actions, b_rewards, b_actor, b_critic, eps_steps, optimizer)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m#print('loss_value_v', loss_value_v)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mlog_prob_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0madv_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrewards_v\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcritic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mlog_prob_actions_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madv_v\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlog_prob_v\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#initialise environment, model and optimiser\n",
    "env = SchedulerEnv()\n",
    "model = Model((env.observation_space.shape[0]*env.observation_space.shape[1]), env.action_space.n)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, \n",
    "                             amsgrad=False)\n",
    "optimizer.zero_grad()\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "for i in range(10000):\n",
    "\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    b_states = []\n",
    "    b_actions = []\n",
    "    b_rewards = []\n",
    "    b_new_state = []\n",
    "    b_actor = []\n",
    "    b_critic = []\n",
    "    eps_reward = 0\n",
    "    eps_steps = 0\n",
    "\n",
    "    while not done:\n",
    "        \n",
    "        #save value of agent pos to be able to show model where agent is\n",
    "        #print('start state and posn', env.agent_pos)\n",
    "        orig_value = state[env.agent_pos[0], env.agent_pos[1]]\n",
    "        #print('orig', orig_value)\n",
    "        state[env.agent_pos[0], env.agent_pos[1]] = 5\n",
    "        \n",
    "        #create model input from flattened grid\n",
    "        nn_input = torch.flatten(tensor_convert(state))\n",
    "        nn_input = nn_input.unsqueeze(0)\n",
    "        #print('to model', nn_input)\n",
    "        actor, critic = model(nn_input)\n",
    "        b_actor.append(actor)\n",
    "        b_critic.append(critic)\n",
    "\n",
    "        #print('get new action')\n",
    "        action = torch.argmax(actor)\n",
    "\n",
    "        #print('action', action)\n",
    "        #print('middle state and posn', state, env.agent_pos)\n",
    "        #print('orig', orig_value)\n",
    "        state[env.agent_pos[0], env.agent_pos[1]] = orig_value\n",
    "\n",
    "        #run through step to book appointment\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        eps_reward += reward\n",
    "        eps_steps += 1\n",
    "        if eps_steps > 10:\n",
    "            reward -= 50\n",
    "            env.agent_pos = [np.random.randint(env.num_slots), np.random.randint(env.num_gps)]\n",
    "\n",
    "        \n",
    "        b_states.append(nn_input)\n",
    "        b_actions.append(action)\n",
    "        b_rewards.append(reward)\n",
    "        b_new_state.append(new_state)\n",
    "\n",
    "\n",
    "        #print('done', done)\n",
    "        state = new_state\n",
    "        \n",
    "        #print('end state and posn', env.agent_pos, eps_steps)\n",
    "\n",
    "    #print(\"train with this\", b_rewards, b_actions)\n",
    "    #print('end of episode', eps_reward, eps_steps)\n",
    "    #loss = train(b_states, b_actions, b_rewards, b_actor, b_critic, eps_steps, optimizer, model)\n",
    "    loss = train(b_states, b_actions, b_rewards, b_actor, b_critic, eps_steps, optimizer)\n",
    "    writer.add_scalar(\"eps_reward\", eps_reward, i )\n",
    "    writer.add_scalar(\"loss\", loss, i )\n",
    "    if i%100 == 0:\n",
    "        print('rewards for episode', i, eps_reward)\n",
    "    #print('rewards for episode', i, eps_reward)\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first = torch.tensor([[-0.0708]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first - reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 100)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_input.unsqueeze(0).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
