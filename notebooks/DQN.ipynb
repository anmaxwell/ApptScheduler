{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "from random import choices\n",
    "from collections import Counter, deque\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils as nn_utils\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 32\n",
    "\n",
    "gamma = 0.99\n",
    "\n",
    "eps_start=1.0\n",
    "eps_decay = 0.995\n",
    "eps_min = 0.1      # Minimal exploration rate (epsilon-greedy)\n",
    "\n",
    "num_rounds = 50000\n",
    "#num_episodes = 500\n",
    "learning_limit = 100\n",
    "replay_limit = 1000  # Number of steps until starting replay\n",
    "#weight_update = 1000 # Number of steps until updating the target weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create model\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_shape[0]*input_shape[1], 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_actions)\n",
    "            \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # flatten the observation space Box to linear tensor\n",
    "        tensor_array = torch.from_numpy(state)\n",
    "        x_flat = torch.flatten(tensor_array).to(torch.float32)\n",
    "        return self.net(x_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SchedulerEnv(gym.Env):\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        #starting parameters\n",
    "        num_gps = 100\n",
    "        num_slots = 40\n",
    "#         num_pre_booked = 750\n",
    "#         #to_book = [2,1,2,2,1,1,1,3,3,1,2,1,3,2,1,1,2,1,3,2,3,2]\n",
    "#         to_book = [2,1,1,1,1]\n",
    "#         num_to_book = len(to_book)\n",
    "        \n",
    "        num_pre_booked = random.randint(6*num_gps, 12*num_gps)\n",
    "        num_to_book = random.randint(6*num_gps, 14*num_gps)\n",
    "        to_book = []\n",
    "        for j in range(num_to_book):\n",
    "            to_book.append(*choices([1,2,3],[.7, .25, .05]))\n",
    "        agent_pos = [0,0]\n",
    "        \n",
    "        #set parameters for the day\n",
    "        self.num_gps = num_gps\n",
    "        self.num_slots = num_slots\n",
    "        self.num_pre_booked = num_pre_booked\n",
    "        self.to_book = to_book\n",
    "        self.num_to_book = num_to_book\n",
    "        self.diary_slots = num_gps*num_slots\n",
    "        self.agent_pos = agent_pos\n",
    "\n",
    "        #set action space to move around the grid\n",
    "        self.action_space = gym.spaces.Discrete(4) #up, down, left, right\n",
    "        \n",
    "        #set observation space \n",
    "        self.observation_space = gym.spaces.Box(low=0, high=1, shape=(self.num_slots, self.num_gps), dtype=np.int32)\n",
    "   \n",
    "    #creates daily diary for each gp, randomly populates prebooked appointments and resets parameters\n",
    "    def reset(self):\n",
    "\n",
    "        #creates zero filled dataframe with row per time slot and column per gp\n",
    "        self.state = np.zeros((self.num_slots, self.num_gps),dtype=float)\n",
    "\n",
    "        #randomly enters a 1 for each pre booked appointments\n",
    "        pre_booked = self.num_pre_booked\n",
    "        while pre_booked>0:\n",
    "            pre_booked -= 1\n",
    "            self.state[np.random.randint(self.num_slots), np.random.randint(self.num_gps)] = 1\n",
    "            \n",
    "        #randomly sets the agent start space\n",
    "        self.agent_pos = [np.random.randint(self.num_slots), np.random.randint(self.num_gps)]\n",
    "\n",
    "        #resets parameters for new episode\n",
    "        self.done = False\n",
    "        self.reward = 0\n",
    "        self.appt_idx = 0\n",
    "        \n",
    "        #print('starting state', self.state.sum(), self.state)\n",
    "\n",
    "        return self.state\n",
    "    \n",
    "    #calculates new position of the agent based on the action\n",
    "    def move_agent(self, action):\n",
    "\n",
    "        #set boundaries for the grid\n",
    "        max_row = self.num_slots - 1\n",
    "        max_col = self.num_gps - 1\n",
    "\n",
    "        #setting new co-ordinates for the agent\n",
    "        new_row = self.agent_pos[0]\n",
    "        new_col = self.agent_pos[1]\n",
    "\n",
    "        #calculate what the new position may be based on the action without going out the grid\n",
    "        if action == 0:\n",
    "            #print('up')\n",
    "            new_row = max(self.agent_pos[0] - 1, 0)\n",
    "        if action == 1:\n",
    "            #print('down')\n",
    "            new_row = min(self.agent_pos[0] + 1, max_row)\n",
    "        if action == 2:\n",
    "            #print('left')\n",
    "            new_col = max(self.agent_pos[1] - 1, 0)\n",
    "        if action == 3:\n",
    "            #print('right')\n",
    "            new_col = min(self.agent_pos[1] + 1, max_col)\n",
    "\n",
    "        new_pos = [new_row, new_col]\n",
    "        #print('new pos', new_pos)\n",
    "\n",
    "        return new_pos\n",
    "\n",
    "    #checks if we can look to book appointment starting here\n",
    "    def check_bookable(self):\n",
    "        return self.state[self.agent_pos[0], self.agent_pos[1]] == 0.0\n",
    "    \n",
    "    #action if we can't book the appointment\n",
    "    def invalid_booking(self):\n",
    "        #print('cant book')\n",
    "        self.reward = -1\n",
    "        \n",
    "    #action if we can book the appointment\n",
    "    def valid_booking(self):\n",
    "        #print('go ahead and book')\n",
    "        self.appt_idx += 1\n",
    "        self.reward = 1\n",
    "    \n",
    "    #checks if the appointment fits\n",
    "    def check_and_book(self):\n",
    "        \n",
    "        max_row = self.num_slots - 1\n",
    "        cells_to_check = self.to_book[self.appt_idx]\n",
    "        \n",
    "        if cells_to_check==1:\n",
    "            #print('good to check for single')\n",
    "            if self.state[self.agent_pos[0], self.agent_pos[1]] == 0:\n",
    "                self.state[self.agent_pos[0], self.agent_pos[1]] = 1\n",
    "                self.valid_booking()\n",
    "            else:\n",
    "                #print('single taken')\n",
    "                self.invalid_booking()\n",
    "\n",
    "        if cells_to_check==2:\n",
    "            #check we're not at the bottom of the grid\n",
    "            if self.agent_pos[0]<max_row:\n",
    "                #check the next cells is also 0.0\n",
    "                #print('good to check for double')\n",
    "                if self.state[self.agent_pos[0], self.agent_pos[1]] == 0 and \\\n",
    "                self.state[(self.agent_pos[0]+1), self.agent_pos[1]] == 0:\n",
    "                    self.state[self.agent_pos[0], self.agent_pos[1]] = 1\n",
    "                    self.state[(self.agent_pos[0]+1), self.agent_pos[1]] = 1\n",
    "                    self.valid_booking()\n",
    "                    self.agent_pos = [(self.agent_pos[0]+1), self.agent_pos[1]]\n",
    "                    #print('after booking', self.agent_pos)\n",
    "                else:\n",
    "                    #print('double taken')\n",
    "                    self.invalid_booking()\n",
    "            else:\n",
    "                #print('not for double')\n",
    "                self.invalid_booking()\n",
    "                \n",
    "        if cells_to_check==3:\n",
    "            #check we're not at the bottom of the grid\n",
    "            if self.agent_pos[0]+1<max_row:\n",
    "                #print('good to check for treble')\n",
    "                if self.state[self.agent_pos[0], self.agent_pos[1]] == 0 and \\\n",
    "                self.state[(self.agent_pos[0]+1), self.agent_pos[1]] == 0 \\\n",
    "                 and self.state[(self.agent_pos[0]+2), self.agent_pos[1]] == 0:\n",
    "                    self.state[self.agent_pos[0], self.agent_pos[1]] = 1\n",
    "                    self.state[(self.agent_pos[0]+1), self.agent_pos[1]] = 1\n",
    "                    self.state[(self.agent_pos[0]+2), self.agent_pos[1]] = 1\n",
    "                    self.valid_booking()\n",
    "                    self.agent_pos = [(self.agent_pos[0]+2), self.agent_pos[1]]\n",
    "                else:\n",
    "                    #print('treble taken')\n",
    "                    self.invalid_booking()\n",
    "            else:\n",
    "                #print('not for treble')\n",
    "                self.invalid_booking()\n",
    "                \n",
    "        if cells_to_check==4:\n",
    "            #check we're not at the bottom of the grid\n",
    "            if self.agent_pos[0]+2<max_row:\n",
    "                #check the next cells is also 0.0\n",
    "                #print('good for quad')\n",
    "                if self.state[self.agent_pos[0], self.agent_pos[1]] == 0 and \\\n",
    "                self.state[(self.agent_pos[0]+1), self.agent_pos[1]] == 0 \\\n",
    "                 and self.state[(self.agent_pos[0]+2), self.agent_pos[1]] == 0 and \\\n",
    "                self.state[(self.agent_pos[0]+3), self.agent_pos[1]] == 0:\n",
    "                    self.state[self.agent_pos[0], self.agent_pos[1]] = 1\n",
    "                    self.state[(self.agent_pos[0]+1), self.agent_pos[1]] = 1\n",
    "                    self.state[(self.agent_pos[0]+2), self.agent_pos[1]] = 1\n",
    "                    self.state[(self.agent_pos[0]+3), self.agent_pos[1]] = 1\n",
    "                    self.valid_booking()\n",
    "                    self.agent_pos = [(self.agent_pos[0]+3), self.agent_pos[1]]\n",
    "                else:\n",
    "                    #print('quad taken')\n",
    "                    self.invalid_booking()\n",
    "            else:\n",
    "                #print('not for quad')\n",
    "                self.invalid_booking()\n",
    "\n",
    "        next_state = self.state\n",
    "\n",
    "        return next_state\n",
    "\n",
    "    def step(self, action):\n",
    "\n",
    "        #get new position of agent based on action\n",
    "        new_agent_pos = self.move_agent(action)\n",
    "        #print('new and old pos', new_agent_pos, self.agent_pos)\n",
    "        \n",
    "        #if the agent is stuck on an edge then move to a new position\n",
    "        if new_agent_pos == self.agent_pos:\n",
    "            self.agent_pos = [np.random.randint(self.num_slots), np.random.randint(self.num_gps)]\n",
    "            #print('here1', self.agent_pos)\n",
    "        else:\n",
    "            self.agent_pos = new_agent_pos\n",
    "            #print('here2', self.agent_pos)\n",
    "            \n",
    "        #print('trying to book', self.to_book, self.appt_idx)\n",
    "        \n",
    "        #check if it's possible to book then book\n",
    "        if self.check_bookable():\n",
    "            #print('checked here')\n",
    "            self.state = self.check_and_book()\n",
    "        else:\n",
    "            #print('not bookable')\n",
    "            self.invalid_booking()\n",
    "        \n",
    "        #work out if episode complete\n",
    "        if self.appt_idx == len(self.to_book):\n",
    "            self.done = True\n",
    "            \n",
    "        #print(self.state, self.agent_pos)\n",
    "        agent_state = self.state.copy()\n",
    "        agent_state[self.agent_pos[0], self.agent_pos[1]] = 5\n",
    "        #print('agent', agent_state)\n",
    "\n",
    "        info = {}\n",
    "        return agent_state, self.reward, self.done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = \"cuda\"\n",
    "device = \"cpu\"\n",
    "\n",
    "env = SchedulerEnv()\n",
    "\n",
    "#start writing to tensorboard\n",
    "writer = SummaryWriter(comment=\"Scheduler DQN\")\n",
    "\n",
    "#create the current network and target network\n",
    "policy_model = Model(env.observation_space.shape, env.action_space.n).to(device)\n",
    "\n",
    "target_model = Model(env.observation_space.shape, env.action_space.n).to(device)\n",
    "target_model.load_state_dict(policy_model.state_dict())\n",
    "\n",
    "optimizer = optim.Adam(policy_model.parameters(), lr=0.001, eps=1e-3)\n",
    "\n",
    "          # Exploration rate    \n",
    "replay_buffer = deque(maxlen=1000)\n",
    "\n",
    "step_idx = 0\n",
    "epsilon = eps_start\n",
    "\n",
    "for i in range(num_rounds):\n",
    "    #change this for while not true once it works\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    #print('reset here')\n",
    "\n",
    "    for j in range(50):\n",
    "#    while not done:\n",
    "        \n",
    "        step_idx += 1\n",
    "        #print(i,j,step_idx)\n",
    "\n",
    "\n",
    "        #epsilon for epsilon greedy strategy  \n",
    "        if epsilon > eps_min:\n",
    "            epsilon *= eps_decay\n",
    "            \n",
    "        #print('epsilon', epsilon)   \n",
    "        #check = policy_model(state)\n",
    "        #print(check)\n",
    "\n",
    "        # Select and perform an action\n",
    "        if step_idx > learning_limit:\n",
    "            if np.random.rand() > epsilon:\n",
    "                action = torch.argmax(policy_model(state))\n",
    "        else:\n",
    "            action = np.random.randint(env.action_space.n)\n",
    "\n",
    "        #step through an episode\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        # Store all info in replay buffer\n",
    "        replay_buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "        #increase the episode reward\n",
    "        episode_reward += reward\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    #print('stopped episode', j, episode_reward)\n",
    "\n",
    "    writer.add_scalar('episode_reward', episode_reward, i)\n",
    "        \n",
    "    #print('step', step_idx, 'i', i, 'j', j, episode_reward)\n",
    "\n",
    "    #once replay_buffer hits preset limit start training with mini batches\n",
    "    if len(replay_buffer) == replay_limit:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #take a random sample from the buffer\n",
    "        minibatch = random.sample(replay_buffer, batch_size)\n",
    "\n",
    "        #for each step in the mini batch\n",
    "        for state, action, reward, next_state, done in minibatch:    \n",
    "            #pass state to main policy to get qval\n",
    "            pred_qval = max(policy_model(state)) \n",
    "\n",
    "        #pass next state to target policy to get next set of qvals (future gains)\n",
    "        if not done:\n",
    "            next_qval = (reward + (gamma * max(target_model(next_state).detach())))\n",
    "        else:\n",
    "            next_qval = reward   \n",
    "        \n",
    "        pred_qval = pred_qval.to(torch.float32).unsqueeze(-1)\n",
    "        #print('pred_qval', pred_qval, pred_qval.size())\n",
    "        next_qval = next_qval.to(torch.float32)\n",
    "        #print('next_qval', next_qval, next_qval.size())\n",
    "\n",
    "        loss = F.mse_loss(pred_qval, next_qval)\n",
    "        #print('loss', loss)\n",
    "        writer.add_scalar('loss', loss, i)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        #print('step', step_idx, 'i', i, 'j', j)\n",
    "\n",
    "\n",
    "        # Periodically update the target network\n",
    "        if i % 2000 == 0:\n",
    "            target_model.load_state_dict(policy_model.state_dict())\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'final_dqn_model.sav'\n",
    "pickle.dump(target_model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
