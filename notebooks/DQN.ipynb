{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import Counter\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils as nn_utils\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 32\n",
    "\n",
    "episodes = 500\n",
    "gamma = 0.99\n",
    "\n",
    "self.epsilon = 1.0          # Exploration rate\n",
    "self.epsilon_decay = 0.995\n",
    "self.epsilon_min = 0.1      # Minimal exploration rate (epsilon-greedy)\n",
    "\n",
    "self.learning_rate = 0.001\n",
    "replay_buffer = deque(maxlen=2000)\n",
    "    \n",
    "target_update = 1000 # Number of steps until updating the target network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create model\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_shape[0]*input_shape[1], 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # flatten the observation space Box to linear tensor\n",
    "        x_flat = torch.flatten(x, 1,2).to(torch.float32)\n",
    "        return self.net(x_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replay once hit learning size\n",
    "def replay(self, batch_size):\n",
    "    minibatch = random.sample(self.memory, batch_size)\n",
    "\n",
    "    for state, action, reward, next_state, done in minibatch:\n",
    "\n",
    "        if not done:\n",
    "            target = (reward + self.gamma * np.amax(self.target_model.predict(next_state)))\n",
    "        else:\n",
    "            target = reward\n",
    "\n",
    "        # Construct the target vector as follows:\n",
    "        # 1. Use the current model to output the Q-value predictions\n",
    "        target_f = self.model.predict(state)\n",
    "        # 2. Rewrite the chosen action value with the computed target\n",
    "        target_f[0][action] = target\n",
    "        # 3. Use vectors in the objective computation\n",
    "        self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "\n",
    "\n",
    "#use mini batches to learn once size is correct\n",
    "def replay(self, batch_size):\n",
    "    minibatch = random.sample(self.memory, batch_size)\n",
    "    for state, action, reward, next_state, done in minibatch:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        target = reward\n",
    "        if not done:\n",
    "            target = (reward + self.gamma *\n",
    "                      np.amax(self.model.predict(next_state)[0]))\n",
    "        target_f = self.model.predict(state)\n",
    "        target_f[0][action] = target\n",
    "        \n",
    "\n",
    "        loss = F.mse_loss(state, target_f)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        self.model.fit(state, target_f, epochs=1, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#epsilon for epsilon greedy strategy\n",
    "def set_epsilon(self, epsilon):\n",
    "    self.epsilon = epsilon        \n",
    "    if self.epsilon > self.epsilon_min:\n",
    "        self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(self, epochs=10, batch_size=128, verbose=False):\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        #reads in mini batches\n",
    "        states, actions, rewards, dones, next_states = self.sample(batch_size)\n",
    "\n",
    "        #converts to tensors\n",
    "        states = tf.convert_to_tensor(states, dtype=tf.float64)\n",
    "        done_masks = tf.convert_to_tensor((~dones.astype(bool)).astype(int), dtype=tf.float64)\n",
    "        next_states = tf.convert_to_tensor(next_states, dtype=tf.float64)\n",
    "        #gets target values by taking action taken from output of all actions\n",
    "        action_masks = tf.one_hot(actions, self.action_space.n, dtype=tf.float64)\n",
    "        target_values = tf.expand_dims( rewards + self.gamma * np.max( self.target_model( next_states ) ) * done_masks, axis = 1 )\n",
    "        target_values *= action_masks\n",
    "\n",
    "        #updates qvalues\n",
    "        with tf.GradientTape() as tape:\n",
    "            q_values = self.model(states) * action_masks\n",
    "            if verbose:\n",
    "                print(target_values, q_values)\n",
    "            loss = tf.reduce_mean((target_values - q_values)**2)\n",
    "\n",
    "        losses.append(loss)\n",
    "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "        optimizer = tf.train.AdamOptimizer(self.lr)\n",
    "        optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "    return np.mean(losses)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SchedulerEnv(gym.Env):\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        #starting parameters\n",
    "        num_gps = 100\n",
    "        num_slots = 32\n",
    "        num_pre_booked = 750\n",
    "        to_book = [2,1,2,2,1,1,1,3,3,1,2,1,3,2,1,1,2,1,3,2,3,2]\n",
    "        num_to_book = len(to_book)\n",
    "        agent_pos = [0,0]\n",
    "        reward_decay = 0.95\n",
    "        \n",
    "        #set parameters for the day\n",
    "        self.num_gps = num_gps\n",
    "        self.num_slots = num_slots\n",
    "        self.num_pre_booked = num_pre_booked\n",
    "        self.to_book = to_book\n",
    "        self.num_to_book = num_to_book\n",
    "        self.diary_slots = num_gps*num_slots\n",
    "        self.agent_pos = agent_pos\n",
    "        self.reward_decay = reward_decay\n",
    "\n",
    "        #set action space to move around the grid\n",
    "        self.action_space = gym.spaces.Discrete(4) #up, down, left, right\n",
    "        \n",
    "        #set observation space \n",
    "        self.observation_space = gym.spaces.Box(low=0, high=1, shape=(self.num_slots, self.num_gps), dtype=np.int32)\n",
    "   \n",
    "    #creates daily diary for each gp, randomly populates prebooked appointments and resets parameters\n",
    "    def reset(self):\n",
    "\n",
    "        #creates zero filled dataframe with row per time slot and column per gp\n",
    "        self.state = np.zeros((self.num_slots, self.num_gps),dtype=float)\n",
    "\n",
    "        #randomly enters a 1 for each pre booked appointments\n",
    "        pre_booked = self.num_pre_booked\n",
    "        while pre_booked>0:\n",
    "            pre_booked -= 1\n",
    "            self.state[np.random.randint(self.num_slots), np.random.randint(self.num_gps)] = 1\n",
    "            \n",
    "        #randomly sets the agent start space\n",
    "        self.agent_pos = [np.random.randint(self.num_slots), np.random.randint(self.num_gps)]\n",
    "\n",
    "        #resets parameters for new episode\n",
    "        self.done = False\n",
    "        self.reward = 0\n",
    "        self.appt_idx = 0\n",
    "        self.decay_steps = 1\n",
    "        \n",
    "        #print('starting state', self.state.sum(), self.state)\n",
    "\n",
    "        return self.state\n",
    "    \n",
    "    #calculates new position of the agent based on the action\n",
    "    def move_agent(self, action):\n",
    "\n",
    "        #set boundaries for the grid\n",
    "        max_row = self.num_slots - 1\n",
    "        max_col = self.num_gps - 1\n",
    "\n",
    "        #setting new co-ordinates for the agent\n",
    "        new_row = self.agent_pos[0]\n",
    "        new_col = self.agent_pos[1]\n",
    "\n",
    "        #calculate what the new position may be based on the action without going out the grid\n",
    "        if action == 0:\n",
    "            #print('up')\n",
    "            new_row = max(self.agent_pos[0] - 1, 0)\n",
    "        if action == 1:\n",
    "            #print('down')\n",
    "            new_row = min(self.agent_pos[0] + 1, max_row)\n",
    "        if action == 2:\n",
    "            #print('left')\n",
    "            new_col = max(self.agent_pos[1] - 1, 0)\n",
    "        if action == 3:\n",
    "            #print('right')\n",
    "            new_col = min(self.agent_pos[1] + 1, max_col)\n",
    "\n",
    "        new_pos = [new_row, new_col]\n",
    "        #print('new pos', new_pos)\n",
    "\n",
    "        return new_pos\n",
    "\n",
    "    #checks if we can look to book appointment starting here\n",
    "    def check_bookable(self):\n",
    "        return self.state[self.agent_pos[0], self.agent_pos[1]] == 0.0\n",
    "    \n",
    "    #action if we can't book the appointment\n",
    "    def invalid_booking(self):\n",
    "        #print('cant book')\n",
    "        self.decay_steps += 1\n",
    "        self.reward = -1\n",
    "        \n",
    "    #action if we can book the appointment\n",
    "    def valid_booking(self):\n",
    "        #print('go ahead and book')\n",
    "        self.appt_idx += 1\n",
    "        self.decay_steps = 1\n",
    "        self.reward = 1\n",
    "    \n",
    "    #checks if the appointment fits\n",
    "    def check_and_book(self):\n",
    "        \n",
    "        max_row = self.num_slots - 1\n",
    "        cells_to_check = self.to_book[self.appt_idx]\n",
    "        \n",
    "        if cells_to_check==1:\n",
    "            #print('good to check for single')\n",
    "            if self.state[self.agent_pos[0], self.agent_pos[1]] == 0:\n",
    "                self.state[self.agent_pos[0], self.agent_pos[1]] = 1\n",
    "                self.valid_booking()\n",
    "            else:\n",
    "                #print('single taken')\n",
    "                self.invalid_booking()\n",
    "\n",
    "        if cells_to_check==2:\n",
    "            #check we're not at the bottom of the grid\n",
    "            if self.agent_pos[0]<max_row:\n",
    "                #check the next cells is also 0.0\n",
    "                #print('good to check for double')\n",
    "                if self.state[self.agent_pos[0], self.agent_pos[1]] == 0 and \\\n",
    "                self.state[(self.agent_pos[0]+1), self.agent_pos[1]] == 0:\n",
    "                    self.state[self.agent_pos[0], self.agent_pos[1]] = 1\n",
    "                    self.state[(self.agent_pos[0]+1), self.agent_pos[1]] = 1\n",
    "                    self.valid_booking()\n",
    "                    self.agent_pos = [(self.agent_pos[0]+1), self.agent_pos[1]]\n",
    "                    #print('after booking', self.agent_pos)\n",
    "                else:\n",
    "                    #print('double taken')\n",
    "                    self.invalid_booking()\n",
    "            else:\n",
    "                #print('not for double')\n",
    "                self.invalid_booking()\n",
    "                \n",
    "        if cells_to_check==3:\n",
    "            #check we're not at the bottom of the grid\n",
    "            if self.agent_pos[0]+1<max_row:\n",
    "                #print('good to check for treble')\n",
    "                if self.state[self.agent_pos[0], self.agent_pos[1]] == 0 and \\\n",
    "                self.state[(self.agent_pos[0]+1), self.agent_pos[1]] == 0 \\\n",
    "                 and self.state[(self.agent_pos[0]+2), self.agent_pos[1]] == 0:\n",
    "                    self.state[self.agent_pos[0], self.agent_pos[1]] = 1\n",
    "                    self.state[(self.agent_pos[0]+1), self.agent_pos[1]] = 1\n",
    "                    self.state[(self.agent_pos[0]+2), self.agent_pos[1]] = 1\n",
    "                    self.valid_booking()\n",
    "                    self.agent_pos = [(self.agent_pos[0]+2), self.agent_pos[1]]\n",
    "                else:\n",
    "                    #print('treble taken')\n",
    "                    self.invalid_booking()\n",
    "            else:\n",
    "                #print('not for treble')\n",
    "                self.invalid_booking()\n",
    "                \n",
    "        if cells_to_check==4:\n",
    "            #check we're not at the bottom of the grid\n",
    "            if self.agent_pos[0]+2<max_row:\n",
    "                #check the next cells is also 0.0\n",
    "                #print('good for quad')\n",
    "                if self.state[self.agent_pos[0], self.agent_pos[1]] == 0 and \\\n",
    "                self.state[(self.agent_pos[0]+1), self.agent_pos[1]] == 0 \\\n",
    "                 and self.state[(self.agent_pos[0]+2), self.agent_pos[1]] == 0 and \\\n",
    "                self.state[(self.agent_pos[0]+3), self.agent_pos[1]] == 0:\n",
    "                    self.state[self.agent_pos[0], self.agent_pos[1]] = 1\n",
    "                    self.state[(self.agent_pos[0]+1), self.agent_pos[1]] = 1\n",
    "                    self.state[(self.agent_pos[0]+2), self.agent_pos[1]] = 1\n",
    "                    self.state[(self.agent_pos[0]+3), self.agent_pos[1]] = 1\n",
    "                    self.valid_booking()\n",
    "                    self.agent_pos = [(self.agent_pos[0]+3), self.agent_pos[1]]\n",
    "                else:\n",
    "                    #print('quad taken')\n",
    "                    self.invalid_booking()\n",
    "            else:\n",
    "                #print('not for quad')\n",
    "                self.invalid_booking()\n",
    "\n",
    "        next_state = self.state\n",
    "\n",
    "        return next_state\n",
    "\n",
    "    def step(self, action):\n",
    "\n",
    "        #get new position of agent based on action\n",
    "        new_agent_pos = self.move_agent(action)\n",
    "        #print('new and old pos', new_agent_pos, self.agent_pos)\n",
    "        \n",
    "        #if the agent is stuck on an edge then move to a new position\n",
    "        if new_agent_pos == self.agent_pos:\n",
    "            self.agent_pos = [np.random.randint(self.num_slots), np.random.randint(self.num_gps)]\n",
    "            #print('here1', self.agent_pos)\n",
    "        else:\n",
    "            self.agent_pos = new_agent_pos\n",
    "            #print('here2', self.agent_pos)\n",
    "        \n",
    "        #print('trying to book', self.to_book, self.appt_idx)\n",
    "        \n",
    "        #check if it's possible to book then book\n",
    "        if self.check_bookable():\n",
    "            #print('checked here')\n",
    "            self.state = self.check_and_book()\n",
    "        else:\n",
    "            #print('not bookable')\n",
    "            self.invalid_booking()\n",
    "        \n",
    "        #work out if episode complete\n",
    "        if self.appt_idx == len(self.to_book):\n",
    "            #print('all booked')\n",
    "            self.done = True\n",
    "  \n",
    "        #work out rewards\n",
    "        #self.reward = (1 - (self.reward_decay**self.decay_steps))\n",
    "        \n",
    "        #print('step', self.decay_steps, self.reward)\n",
    "        #print('end step')\n",
    "\n",
    "        info = {}\n",
    "        return self.state, self.reward, self.done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SchedulerEnv()\n",
    "\n",
    "#create the current network and target network\n",
    "policy_model = Model(envs[0].observation_space.shape, envs[0].action_space.n).to(device)\n",
    "#self.model = MLP(model_input_dim, self.action_space.n)\n",
    "#self.model(tf.convert_to_tensor([np.random.normal(size=model_input_shape)], dtype=tf.float64))\n",
    "\n",
    "\n",
    "target_model = Model(envs[0].observation_space.shape, envs[0].action_space.n).to(device)\n",
    "#self.target_model = MLP(model_input_dim, self.action_space.n)\n",
    "#self.target_model(tf.convert_to_tensor([np.random.normal(size=model_input_shape)], dtype=tf.float64))\n",
    "\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "\n",
    "\n",
    "num_episodes = 50\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "    #set initial parameters\n",
    "    total_reward = 0\n",
    "\n",
    "    replay_buffer = deque(maxlen=max_len)  \n",
    "\n",
    "    state = env.reset()\n",
    "    \n",
    "    \n",
    "    for t in count():\n",
    "        # Select and perform an action\n",
    "        action = select_action(state)\n",
    "        if t > learning_starts:\n",
    "            if np.random.rand() => self.epsilon:\n",
    "                action = policy_net(state).max(1)[1].view(1, 1)\n",
    "        else:\n",
    "            action = random.randrange(num_actions)\n",
    "\n",
    "        \n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #take a step and save the info to the replay buffer\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        # Store other info in replay memory\n",
    "        remember(state, action, reward, next_state, done)\n",
    "        # Append experience to replay buffer\n",
    "        replay_buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "        \n",
    "        \n",
    "        reward -= 1  # Punish behavior which does not accumulate reward\n",
    "        total_reward += reward\n",
    "        \n",
    "        if done:\n",
    "            all_rewards += game_score\n",
    "\n",
    "            \n",
    "            break\n",
    "            \n",
    "        #once we're ready to learn then start learning with mini batches\n",
    "        if len(replay_buffer) > batch_size:\n",
    "            replay(batch_size)\n",
    "        \n",
    "        \n",
    "\n",
    "        # Update the target network, copying all weights and biases in DQN\n",
    "        # Periodically update the target network by Q network to target Q network\n",
    "        if num_param_updates % target_update_freq == 0:\n",
    "            # Update weights of target\n",
    "            #self.target_model.set_weights(self.model.get_weights())\n",
    "            target_model.load_state_dict(model.state_dict())\n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
