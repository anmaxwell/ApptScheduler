{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import ptan\n",
    "import numpy as np\n",
    "import argparse\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils as nn_utils\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import random\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from agents.Base_Agent import Base_Agent\n",
    "from exploration_strategies.Epsilon_Greedy_Exploration import Epsilon_Greedy_Exploration\n",
    "from utilities.data_structures.Replay_Buffer import Replay_Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "\n",
    "gamma = 0.99\n",
    "batch_size = 11\n",
    "num_envs = 6\n",
    "reward_steps = 4\n",
    "\n",
    "#more parameters\n",
    "input_size = 84\n",
    "frame_history = 4\n",
    "batch_size = 32\n",
    "input_shape = (input_size, input_size, frame_history)\n",
    "mem_size = 1000000\n",
    "gamma = 0.99\n",
    "target_update_freq = 10000\n",
    "num_burn_in = 20000\n",
    "train_freq = 4\n",
    "\n",
    "episodes = 500\n",
    "batch_size = 8\n",
    "skip_start = 90  # MsPacman-v0 waits for 90 actions before the episode begins\n",
    "total_time = 0   # Counter for total number of steps taken\n",
    "all_rewards = 0  # Used to compute avg reward over time\n",
    "blend = 4        # Number of images to blend\n",
    "done = False\n",
    "\n",
    "self.env = env\n",
    "self.observation_space = env.observation_space\n",
    "self.action_space = env.action_space\n",
    "self.epsilon = epsilon\n",
    "self.epsilon_min = epsilon_min\n",
    "self.lr = lr\n",
    "self.gamma = gamma\n",
    "self.memory = collections.deque([], memory_limit)\n",
    "self.memory_limit = memory_limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_shape[0]*input_shape[1], 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # flatten the observation space Box to linear tensor\n",
    "        x_flat = torch.flatten(x, 1,2).to(torch.float32)\n",
    "        return self.net(x_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updates the target model parameters with the current model parameters\n",
    "# typically after so many iterations as defined in hyper parameters\n",
    "def update_target_model(self):\n",
    "    self.target_model.set_weights(self.model.get_weights())\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replay once hit learning size\n",
    "def replay(self, batch_size):\n",
    "    minibatch = random.sample(self.memory, batch_size)\n",
    "\n",
    "    for state, action, reward, next_state, done in minibatch:\n",
    "\n",
    "        if not done:\n",
    "            target = (reward + self.gamma * np.amax(self.target_model.predict(next_state)))\n",
    "        else:\n",
    "            target = reward\n",
    "\n",
    "        # Construct the target vector as follows:\n",
    "        # 1. Use the current model to output the Q-value predictions\n",
    "        target_f = self.model.predict(state)\n",
    "\n",
    "        # 2. Rewrite the chosen action value with the computed target\n",
    "        target_f[0][action] = target\n",
    "\n",
    "        # 3. Use vectors in the objective computation\n",
    "        self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "\n",
    "#taking minibatches from the replay buffer \n",
    "def sample(self, size):\n",
    "    sample_index = np.random.choice(len(self.memory), size=size, replace=True)\n",
    "\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    dones = []\n",
    "    next_states = []\n",
    "    for idx in sample_index:\n",
    "        unit = self.memory[idx]\n",
    "        states.append(unit[0])\n",
    "        actions.append(unit[1])\n",
    "        rewards.append(unit[2])\n",
    "        dones.append(unit[3])\n",
    "        next_states.append(unit[4])\n",
    "    return np.array(states), np.array(actions), np.array(rewards), np.array(dones), np.array(next_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stores experience in replay memory\n",
    "#\n",
    "def remember(self, state, action, reward, next_state, done):\n",
    "    self.replay_buffer.append((state, action, reward, next_state, done))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose action\n",
    "#\n",
    "def select_action(self, state):\n",
    "\n",
    "    if t > learning_starts:\n",
    "        if np.random.rand() => self.epsilon:\n",
    "            action = policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        action = random.randrange(num_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#epsilon for epsilon greedy strategy\n",
    "def set_epsilon(self, epsilon):\n",
    "    self.epsilon = epsilon        \n",
    "    if self.epsilon > self.epsilon_min:\n",
    "        self.epsilon *= self.epsilon_decay\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the current network and target network\n",
    "self.model = MLP(model_input_dim, self.action_space.n)\n",
    "self.model(tf.convert_to_tensor([np.random.normal(size=model_input_shape)], dtype=tf.float64))\n",
    "\n",
    "self.target_model = MLP(model_input_dim, self.action_space.n)\n",
    "self.target_model(tf.convert_to_tensor([np.random.normal(size=model_input_shape)], dtype=tf.float64))\n",
    "\n",
    "num_episodes = 50\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "    #set initial parameters\n",
    "    total_reward = 0\n",
    "\n",
    "    replay_buffer = deque(maxlen=max_len)  \n",
    "\n",
    "    state = env.reset()\n",
    "    \n",
    "    \n",
    "    for t in count():\n",
    "        # Select and perform an action\n",
    "        action = select_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #take a step and save the info to the replay buffer\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        # Store other info in replay memory\n",
    "        remember(state, action, reward, next_state, done)\n",
    "\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "        \n",
    "        \n",
    "        reward -= 1  # Punish behavior which does not accumulate reward\n",
    "        total_reward += reward\n",
    "        \n",
    "        if done:\n",
    "            all_rewards += game_score\n",
    "\n",
    "            \n",
    "            break\n",
    "            \n",
    "        #once we're ready to learn then start learning with mini batches\n",
    "        if len(replay_buffer) > batch_size:\n",
    "            replay(batch_size)\n",
    "        \n",
    "        \n",
    "\n",
    "        # Update the target network, copying all weights and biases in DQN\n",
    "        # Periodically update the target network by Q network to target Q network\n",
    "        if num_param_updates % target_update_freq == 0:\n",
    "            update_target_model()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(self, epochs=10, batch_size=128, verbose=False):\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        #reads in mini batches\n",
    "        states, actions, rewards, dones, next_states = self.sample(batch_size)\n",
    "\n",
    "        #converts to tensors\n",
    "        states = tf.convert_to_tensor(states, dtype=tf.float64)\n",
    "        done_masks = tf.convert_to_tensor((~dones.astype(bool)).astype(int), dtype=tf.float64)\n",
    "        next_states = tf.convert_to_tensor(next_states, dtype=tf.float64)\n",
    "        #gets target values by taking action taken from output of all actions\n",
    "        action_masks = tf.one_hot(actions, self.action_space.n, dtype=tf.float64)\n",
    "        target_values = tf.expand_dims( rewards + self.gamma * np.max( self.target_model( next_states ) ) * done_masks, axis = 1 )\n",
    "        target_values *= action_masks\n",
    "\n",
    "        #updates qvalues\n",
    "        with tf.GradientTape() as tape:\n",
    "            q_values = self.model(states) * action_masks\n",
    "            if verbose:\n",
    "                print(target_values, q_values)\n",
    "            loss = tf.reduce_mean((target_values - q_values)**2)\n",
    "\n",
    "        losses.append(loss)\n",
    "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "        optimizer = tf.train.AdamOptimizer(self.lr)\n",
    "        optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "    return np.mean(losses)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#initialise agent\n",
    "class DQN(Base_Agent):\n",
    "    \"\"\"A deep Q learning agent\"\"\"\n",
    "    agent_name = \"DQN\"\n",
    "    def __init__(self, config):\n",
    "        Base_Agent.__init__(self, config)\n",
    "\n",
    "        #create replay buffer, current model and optimser but n target model??\n",
    "        self.memory = Replay_Buffer(self.hyperparameters[\"buffer_size\"], self.hyperparameters[\"batch_size\"], config.seed, self.device)\n",
    "        self.q_network_local = self.create_NN(input_dim=self.state_size, output_dim=self.action_size)\n",
    "        self.q_network_optimizer = optim.Adam(self.q_network_local.parameters(),\n",
    "                                              lr=self.hyperparameters[\"learning_rate\"], eps=1e-4)\n",
    "        self.exploration_strategy = Epsilon_Greedy_Exploration(config)\n",
    "\n",
    "    def reset_game(self):\n",
    "        super(DQN, self).reset_game()\n",
    "        self.update_learning_rate(self.hyperparameters[\"learning_rate\"], self.q_network_optimizer)\n",
    "\n",
    "        \n",
    "    #run thrugh creating episodes and adding to replay buffer, learn if buffer hits limit\n",
    "    def step(self):\n",
    "        \"\"\"Runs a step within a game including a learning step if required\"\"\"\n",
    "        while not self.done:\n",
    "            self.action = self.pick_action()\n",
    "            self.conduct_action(self.action)\n",
    "            if self.time_for_q_network_to_learn():\n",
    "                for _ in range(self.hyperparameters[\"learning_iterations\"]):\n",
    "                    self.learn()\n",
    "            self.save_experience()\n",
    "            self.state = self.next_state #this is to set the state for the next iteration\n",
    "            self.global_step_number += 1\n",
    "        self.episode_number += 1\n",
    "\n",
    "    #does someting to pick an action??\n",
    "    def pick_action(self, state=None):\n",
    "        \"\"\"Uses the local Q network and an epsilon greedy policy to pick an action\"\"\"\n",
    "        # PyTorch only accepts mini-batches and not single observations so we have to use unsqueeze to add\n",
    "        # a \"fake\" dimension to make it a mini-batch rather than a single observation\n",
    "        if state is None: state = self.state\n",
    "        if isinstance(state, np.int64) or isinstance(state, int): state = np.array([state])\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n",
    "        if len(state.shape) < 2: state = state.unsqueeze(0)\n",
    "        self.q_network_local.eval() #puts network in evaluation mode\n",
    "        with torch.no_grad():\n",
    "            action_values = self.q_network_local(state)\n",
    "        self.q_network_local.train() #puts network back in training mode\n",
    "        action = self.exploration_strategy.perturb_action_for_exploration_purposes({\"action_values\": action_values,\n",
    "                                                                                    \"turn_off_exploration\": self.turn_off_exploration,\n",
    "                                                                                    \"episode_number\": self.episode_number})\n",
    "        self.logger.info(\"Q values {} -- Action chosen {}\".format(action_values, action))\n",
    "        return action\n",
    "\n",
    "    \n",
    "    def learn(self, experiences=None):\n",
    "        \"\"\"Runs a learning iteration for the Q network\"\"\"\n",
    "        if experiences is None: states, actions, rewards, next_states, dones = self.sample_experiences() #Sample experiences\n",
    "        else: states, actions, rewards, next_states, dones = experiences\n",
    "        loss = self.compute_loss(states, next_states, rewards, actions, dones)\n",
    "\n",
    "        actions_list = [action_X.item() for action_X in actions ]\n",
    "\n",
    "        self.logger.info(\"Action counts {}\".format(Counter(actions_list)))\n",
    "        self.take_optimisation_step(self.q_network_optimizer, self.q_network_local, loss, self.hyperparameters[\"gradient_clipping_norm\"])\n",
    "\n",
    "    def sample_experiences(self):\n",
    "        \"\"\"Draws a random sample of experience from the memory buffer\"\"\"\n",
    "        experiences = self.memory.sample()\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        return states, actions, rewards, next_states, dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialise agent\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        \n",
    "        #set parameters \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95    # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        # Neural Net for Deep-Q learning Model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse',\n",
    "                      optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    #append experience to buffer\n",
    "    def memorize(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    #get an action using epsilon greedy strategy\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])  # returns action\n",
    "\n",
    "    #use mini batches to learn once size is correct\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = (reward + self.gamma *\n",
    "                          np.amax(self.model.predict(next_state)[0]))\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    #set everything up\n",
    "    env = gym.make('CartPole-v1')\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "    agent = DQNAgent(state_size, action_size)\n",
    "\n",
    "    \n",
    "    done = False\n",
    "    batch_size = 32\n",
    "\n",
    "    #run thrugh the episodes\n",
    "    for e in range(EPISODES):\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "        for time in range(500):\n",
    "            # get the action\n",
    "            action = agent.act(state)\n",
    "            #take a step\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            #sort reward\n",
    "            reward = reward if not done else -10\n",
    "            #reshape\n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "            #add to replay buffer\n",
    "            agent.memorize(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            #once episode complete then stop\n",
    "            if done:\n",
    "                print(\"episode: {}/{}, score: {}, e: {:.2}\"\n",
    "                      .format(e, EPISODES, time, agent.epsilon))\n",
    "                break\n",
    "            #if we've hit amount to learn then learn\n",
    "            if len(agent.memory) > batch_size:\n",
    "                agent.replay(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialising hyper parameters\n",
    "env = gym.make('MsPacman-v0')\n",
    "state_size = (88, 80, 1)\n",
    "action_size = env.action_space.n\n",
    "agent = DQN_Agent(state_size, action_size)\n",
    "\n",
    "\n",
    "policy_net = Model(envs[0].observation_space.shape, envs[0].action_space.n).to(device)\n",
    "target_net = Model(envs[0].observation_space.shape, envs[0].action_space.n).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "\n",
    "\n",
    "#initial set up\n",
    "def __init__(self, state_size, action_size):\n",
    "    self.state_size = state_size\n",
    "    self.action_size = action_size\n",
    "    self.memory = deque(maxlen=5000)\n",
    "\n",
    "    # Hyperparameters\n",
    "    self.gamma = 1.0            # Discount rate\n",
    "    self.epsilon = 1.0          # Exploration rate\n",
    "    self.epsilon_min = 0.1      # Minimal exploration rate (epsilon-greedy)\n",
    "    self.epsilon_decay = 0.995  # Decay rate for epsilon\n",
    "    self.update_rate = 1000     # Number of steps until updating the target network\n",
    "\n",
    "    # Construct DQN models\n",
    "    self.model = self._build_model()\n",
    "    self.target_model = self._build_model()\n",
    "    self.target_model.set_weights(self.model.get_weights())\n",
    "    self.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SchedulerEnv(gym.Env):\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        #starting parameters\n",
    "        num_gps = 100\n",
    "        num_slots = 32\n",
    "        num_pre_booked = 750\n",
    "        to_book = [2,1,2,2,1,1,1,3,3,1,2,1,3,2,1,1,2,1,3,2,3,2]\n",
    "        num_to_book = len(to_book)\n",
    "        agent_pos = [0,0]\n",
    "        reward_decay = 0.95\n",
    "        \n",
    "        #set parameters for the day\n",
    "        self.num_gps = num_gps\n",
    "        self.num_slots = num_slots\n",
    "        self.num_pre_booked = num_pre_booked\n",
    "        self.to_book = to_book\n",
    "        self.num_to_book = num_to_book\n",
    "        self.diary_slots = num_gps*num_slots\n",
    "        self.agent_pos = agent_pos\n",
    "        self.reward_decay = reward_decay\n",
    "\n",
    "        #set action space to move around the grid\n",
    "        self.action_space = gym.spaces.Discrete(4) #up, down, left, right\n",
    "        \n",
    "        #set observation space \n",
    "        self.observation_space = gym.spaces.Box(low=0, high=1, shape=(self.num_slots, self.num_gps), dtype=np.int32)\n",
    "   \n",
    "    #creates daily diary for each gp, randomly populates prebooked appointments and resets parameters\n",
    "    def reset(self):\n",
    "\n",
    "        #creates zero filled dataframe with row per time slot and column per gp\n",
    "        self.state = np.zeros((self.num_slots, self.num_gps),dtype=float)\n",
    "\n",
    "        #randomly enters a 1 for each pre booked appointments\n",
    "        pre_booked = self.num_pre_booked\n",
    "        while pre_booked>0:\n",
    "            pre_booked -= 1\n",
    "            self.state[np.random.randint(self.num_slots), np.random.randint(self.num_gps)] = 1\n",
    "            \n",
    "        #randomly sets the agent start space\n",
    "        self.agent_pos = [np.random.randint(self.num_slots), np.random.randint(self.num_gps)]\n",
    "\n",
    "        #resets parameters for new episode\n",
    "        self.done = False\n",
    "        self.reward = 0\n",
    "        self.appt_idx = 0\n",
    "        self.decay_steps = 1\n",
    "        \n",
    "        #print('starting state', self.state.sum(), self.state)\n",
    "\n",
    "        return self.state\n",
    "    \n",
    "    #calculates new position of the agent based on the action\n",
    "    def move_agent(self, action):\n",
    "\n",
    "        #set boundaries for the grid\n",
    "        max_row = self.num_slots - 1\n",
    "        max_col = self.num_gps - 1\n",
    "\n",
    "        #setting new co-ordinates for the agent\n",
    "        new_row = self.agent_pos[0]\n",
    "        new_col = self.agent_pos[1]\n",
    "\n",
    "        #calculate what the new position may be based on the action without going out the grid\n",
    "        if action == 0:\n",
    "            #print('up')\n",
    "            new_row = max(self.agent_pos[0] - 1, 0)\n",
    "        if action == 1:\n",
    "            #print('down')\n",
    "            new_row = min(self.agent_pos[0] + 1, max_row)\n",
    "        if action == 2:\n",
    "            #print('left')\n",
    "            new_col = max(self.agent_pos[1] - 1, 0)\n",
    "        if action == 3:\n",
    "            #print('right')\n",
    "            new_col = min(self.agent_pos[1] + 1, max_col)\n",
    "\n",
    "        new_pos = [new_row, new_col]\n",
    "        #print('new pos', new_pos)\n",
    "\n",
    "        return new_pos\n",
    "\n",
    "    #checks if we can look to book appointment starting here\n",
    "    def check_bookable(self):\n",
    "        return self.state[self.agent_pos[0], self.agent_pos[1]] == 0.0\n",
    "    \n",
    "    #action if we can't book the appointment\n",
    "    def invalid_booking(self):\n",
    "        #print('cant book')\n",
    "        self.decay_steps += 1\n",
    "        self.reward = -1\n",
    "        \n",
    "    #action if we can book the appointment\n",
    "    def valid_booking(self):\n",
    "        #print('go ahead and book')\n",
    "        self.appt_idx += 1\n",
    "        self.decay_steps = 1\n",
    "        self.reward = 1\n",
    "    \n",
    "    #checks if the appointment fits\n",
    "    def check_and_book(self):\n",
    "        \n",
    "        max_row = self.num_slots - 1\n",
    "        cells_to_check = self.to_book[self.appt_idx]\n",
    "        \n",
    "        if cells_to_check==1:\n",
    "            #print('good to check for single')\n",
    "            if self.state[self.agent_pos[0], self.agent_pos[1]] == 0:\n",
    "                self.state[self.agent_pos[0], self.agent_pos[1]] = 1\n",
    "                self.valid_booking()\n",
    "            else:\n",
    "                #print('single taken')\n",
    "                self.invalid_booking()\n",
    "\n",
    "        if cells_to_check==2:\n",
    "            #check we're not at the bottom of the grid\n",
    "            if self.agent_pos[0]<max_row:\n",
    "                #check the next cells is also 0.0\n",
    "                #print('good to check for double')\n",
    "                if self.state[self.agent_pos[0], self.agent_pos[1]] == 0 and \\\n",
    "                self.state[(self.agent_pos[0]+1), self.agent_pos[1]] == 0:\n",
    "                    self.state[self.agent_pos[0], self.agent_pos[1]] = 1\n",
    "                    self.state[(self.agent_pos[0]+1), self.agent_pos[1]] = 1\n",
    "                    self.valid_booking()\n",
    "                    self.agent_pos = [(self.agent_pos[0]+1), self.agent_pos[1]]\n",
    "                    #print('after booking', self.agent_pos)\n",
    "                else:\n",
    "                    #print('double taken')\n",
    "                    self.invalid_booking()\n",
    "            else:\n",
    "                #print('not for double')\n",
    "                self.invalid_booking()\n",
    "                \n",
    "        if cells_to_check==3:\n",
    "            #check we're not at the bottom of the grid\n",
    "            if self.agent_pos[0]+1<max_row:\n",
    "                #print('good to check for treble')\n",
    "                if self.state[self.agent_pos[0], self.agent_pos[1]] == 0 and \\\n",
    "                self.state[(self.agent_pos[0]+1), self.agent_pos[1]] == 0 \\\n",
    "                 and self.state[(self.agent_pos[0]+2), self.agent_pos[1]] == 0:\n",
    "                    self.state[self.agent_pos[0], self.agent_pos[1]] = 1\n",
    "                    self.state[(self.agent_pos[0]+1), self.agent_pos[1]] = 1\n",
    "                    self.state[(self.agent_pos[0]+2), self.agent_pos[1]] = 1\n",
    "                    self.valid_booking()\n",
    "                    self.agent_pos = [(self.agent_pos[0]+2), self.agent_pos[1]]\n",
    "                else:\n",
    "                    #print('treble taken')\n",
    "                    self.invalid_booking()\n",
    "            else:\n",
    "                #print('not for treble')\n",
    "                self.invalid_booking()\n",
    "                \n",
    "        if cells_to_check==4:\n",
    "            #check we're not at the bottom of the grid\n",
    "            if self.agent_pos[0]+2<max_row:\n",
    "                #check the next cells is also 0.0\n",
    "                #print('good for quad')\n",
    "                if self.state[self.agent_pos[0], self.agent_pos[1]] == 0 and \\\n",
    "                self.state[(self.agent_pos[0]+1), self.agent_pos[1]] == 0 \\\n",
    "                 and self.state[(self.agent_pos[0]+2), self.agent_pos[1]] == 0 and \\\n",
    "                self.state[(self.agent_pos[0]+3), self.agent_pos[1]] == 0:\n",
    "                    self.state[self.agent_pos[0], self.agent_pos[1]] = 1\n",
    "                    self.state[(self.agent_pos[0]+1), self.agent_pos[1]] = 1\n",
    "                    self.state[(self.agent_pos[0]+2), self.agent_pos[1]] = 1\n",
    "                    self.state[(self.agent_pos[0]+3), self.agent_pos[1]] = 1\n",
    "                    self.valid_booking()\n",
    "                    self.agent_pos = [(self.agent_pos[0]+3), self.agent_pos[1]]\n",
    "                else:\n",
    "                    #print('quad taken')\n",
    "                    self.invalid_booking()\n",
    "            else:\n",
    "                #print('not for quad')\n",
    "                self.invalid_booking()\n",
    "\n",
    "        next_state = self.state\n",
    "\n",
    "        return next_state\n",
    "\n",
    "    def step(self, action):\n",
    "\n",
    "        #print('start step' , self.decay_steps)\n",
    "        #get new position of agent based on action\n",
    "        new_agent_pos = self.move_agent(action)\n",
    "        #print('new and old pos', new_agent_pos, self.agent_pos)\n",
    "        \n",
    "        #if the agent is stuck on an edge then move to a new position\n",
    "        if new_agent_pos == self.agent_pos:\n",
    "            self.agent_pos = [np.random.randint(self.num_slots), np.random.randint(self.num_gps)]\n",
    "            #print('here1', self.agent_pos)\n",
    "        else:\n",
    "            self.agent_pos = new_agent_pos\n",
    "            #print('here2', self.agent_pos)\n",
    "        \n",
    "        #print('trying to book', self.to_book, self.appt_idx)\n",
    "        \n",
    "        #check if it's possible to book then book\n",
    "        if self.check_bookable():\n",
    "            #print('checked here')\n",
    "            self.state = self.check_and_book()\n",
    "        else:\n",
    "            #print('not bookable')\n",
    "            self.invalid_booking()\n",
    "        \n",
    "        #work out if episode complete\n",
    "        if self.appt_idx == len(self.to_book):\n",
    "            #print('all booked')\n",
    "            self.done = True\n",
    "  \n",
    "        #work out rewards\n",
    "        #self.reward = (1 - (self.reward_decay**self.decay_steps))\n",
    "        \n",
    "        #print('step', self.decay_steps, self.reward)\n",
    "        #print('end step')\n",
    "\n",
    "        info = {}\n",
    "        return self.state, self.reward, self.done, info"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
