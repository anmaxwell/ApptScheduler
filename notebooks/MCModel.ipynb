{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ale_py/roms/utils.py:90: DeprecationWarning: SelectableGroups dict interface is deprecated. Use select.\n",
      "  for external in metadata.entry_points().get(self.group, []):\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "from random import choices\n",
    "import pickle\n",
    "from pickle import load\n",
    "from collections import Counter, deque\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils as nn_utils\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 32\n",
    "\n",
    "gamma = 0.99\n",
    "\n",
    "epsilon = 0.1\n",
    "\n",
    "num_rounds = 50000\n",
    "#num_episodes = 500\n",
    "learning_limit = 100\n",
    "replay_limit = 1000  # Number of steps until starting replay\n",
    "#weight_update = 1000 # Number of steps until updating the target weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create model\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_shape[0]*input_shape[1], 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_actions)\n",
    "            \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # flatten the observation space Box to linear tensor\n",
    "        tensor_array = torch.from_numpy(state)\n",
    "        x_flat = torch.flatten(tensor_array).to(torch.float32)\n",
    "        return self.net(x_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SchedulerEnv(gym.Env):\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        #starting parameters\n",
    "        num_gps = 100\n",
    "        num_slots = 40\n",
    "        \n",
    "#         num_pre_booked = 250\n",
    "#         to_book = [2,1,2,2,1,1,1,3,3,1,2,1,3,2,1,1,2,1,3,2,3,2]\n",
    "# #        to_book = [2,1,1,1,1]\n",
    "#         num_to_book = len(to_book)\n",
    "               \n",
    "        num_pre_booked = random.randint(6*num_gps, 14*num_gps)\n",
    "        num_to_book = random.randint(6*num_gps, 12*num_gps)\n",
    "        to_book = []\n",
    "        for j in range(num_to_book):\n",
    "            to_book.append(*choices([1,2,3],[.7, .25, .05]))\n",
    "                \n",
    "        agent_pos = [0,0]\n",
    "        \n",
    "        #set parameters for the day\n",
    "        self.num_gps = num_gps\n",
    "        self.num_slots = num_slots\n",
    "        self.num_pre_booked = num_pre_booked\n",
    "        self.to_book = to_book\n",
    "        self.num_to_book = num_to_book\n",
    "        self.diary_slots = num_gps*num_slots\n",
    "        self.agent_pos = agent_pos\n",
    "\n",
    "        #set action space to move around the grid\n",
    "        self.action_space = gym.spaces.Discrete(4) #up, down, left, right\n",
    "        \n",
    "        #set observation space \n",
    "        self.observation_space = gym.spaces.Box(low=0, high=1, shape=(self.num_slots, self.num_gps), dtype=np.int32)\n",
    "   \n",
    "    #creates daily diary for each gp, randomly populates prebooked appointments and resets parameters\n",
    "    def reset(self):\n",
    "\n",
    "        #creates zero filled dataframe with row per time slot and column per gp\n",
    "        self.state = np.zeros((self.num_slots, self.num_gps),dtype=float)\n",
    "\n",
    "        #randomly enters a 1 for each pre booked appointments\n",
    "        pre_booked = self.num_pre_booked\n",
    "        while pre_booked>0:\n",
    "            pre_booked -= 1\n",
    "            self.state[np.random.randint(self.num_slots), np.random.randint(self.num_gps)] = 1\n",
    "            \n",
    "        #randomly sets the agent start space\n",
    "        self.agent_pos = [np.random.randint(self.num_slots), np.random.randint(self.num_gps)]\n",
    "\n",
    "        #resets parameters for new episode\n",
    "        self.done = False\n",
    "        self.reward = 0\n",
    "        self.appt_idx = 0\n",
    "        \n",
    "        #print('starting state', self.state.sum(), self.state)\n",
    "\n",
    "        return self.state\n",
    "    \n",
    "    #calculates new position of the agent based on the action\n",
    "    def move_agent(self, action):\n",
    "\n",
    "        #set boundaries for the grid\n",
    "        max_row = self.num_slots - 1\n",
    "        max_col = self.num_gps - 1\n",
    "\n",
    "        #setting new co-ordinates for the agent\n",
    "        new_row = self.agent_pos[0]\n",
    "        new_col = self.agent_pos[1]\n",
    "\n",
    "        #calculate what the new position may be based on the action without going out the grid\n",
    "        if action == 0:\n",
    "            #print('up')\n",
    "            new_row = max(self.agent_pos[0] - 1, 0)\n",
    "        if action == 1:\n",
    "            #print('down')\n",
    "            new_row = min(self.agent_pos[0] + 1, max_row)\n",
    "        if action == 2:\n",
    "            #print('left')\n",
    "            new_col = max(self.agent_pos[1] - 1, 0)\n",
    "        if action == 3:\n",
    "            #print('right')\n",
    "            new_col = min(self.agent_pos[1] + 1, max_col)\n",
    "\n",
    "        new_pos = [new_row, new_col]\n",
    "        #print('new pos', new_pos)\n",
    "\n",
    "        return new_pos\n",
    "\n",
    "    #checks if we can look to book appointment starting here\n",
    "    def check_bookable(self):\n",
    "        return self.state[self.agent_pos[0], self.agent_pos[1]] == 0.0\n",
    "    \n",
    "    #action if we can't book the appointment\n",
    "    def invalid_booking(self):\n",
    "        #print('cant book')\n",
    "        self.reward = -1\n",
    "        \n",
    "    #action if we can book the appointment\n",
    "    def valid_booking(self):\n",
    "        #print('go ahead and book')\n",
    "        self.appt_idx += 1\n",
    "        self.reward = 1\n",
    "    \n",
    "    #checks if the appointment fits\n",
    "    def check_and_book(self):\n",
    "        \n",
    "        max_row = self.num_slots - 1\n",
    "        cells_to_check = self.to_book[self.appt_idx]\n",
    "        \n",
    "        if cells_to_check==1:\n",
    "            #print('good to check for single')\n",
    "            if self.state[self.agent_pos[0], self.agent_pos[1]] == 0:\n",
    "                self.state[self.agent_pos[0], self.agent_pos[1]] = 1\n",
    "                self.valid_booking()\n",
    "            else:\n",
    "                #print('single taken')\n",
    "                self.invalid_booking()\n",
    "\n",
    "        if cells_to_check==2:\n",
    "            #check we're not at the bottom of the grid\n",
    "            if self.agent_pos[0]<max_row:\n",
    "                #check the next cells is also 0.0\n",
    "                #print('good to check for double')\n",
    "                if self.state[self.agent_pos[0], self.agent_pos[1]] == 0 and \\\n",
    "                self.state[(self.agent_pos[0]+1), self.agent_pos[1]] == 0:\n",
    "                    self.state[self.agent_pos[0], self.agent_pos[1]] = 1\n",
    "                    self.state[(self.agent_pos[0]+1), self.agent_pos[1]] = 1\n",
    "                    self.valid_booking()\n",
    "                    self.agent_pos = [(self.agent_pos[0]+1), self.agent_pos[1]]\n",
    "                    #print('after booking', self.agent_pos)\n",
    "                else:\n",
    "                    #print('double taken')\n",
    "                    self.invalid_booking()\n",
    "            else:\n",
    "                #print('not for double')\n",
    "                self.invalid_booking()\n",
    "                \n",
    "        if cells_to_check==3:\n",
    "            #check we're not at the bottom of the grid\n",
    "            if self.agent_pos[0]+1<max_row:\n",
    "                #print('good to check for treble')\n",
    "                if self.state[self.agent_pos[0], self.agent_pos[1]] == 0 and \\\n",
    "                self.state[(self.agent_pos[0]+1), self.agent_pos[1]] == 0 \\\n",
    "                 and self.state[(self.agent_pos[0]+2), self.agent_pos[1]] == 0:\n",
    "                    self.state[self.agent_pos[0], self.agent_pos[1]] = 1\n",
    "                    self.state[(self.agent_pos[0]+1), self.agent_pos[1]] = 1\n",
    "                    self.state[(self.agent_pos[0]+2), self.agent_pos[1]] = 1\n",
    "                    self.valid_booking()\n",
    "                    self.agent_pos = [(self.agent_pos[0]+2), self.agent_pos[1]]\n",
    "                else:\n",
    "                    #print('treble taken')\n",
    "                    self.invalid_booking()\n",
    "            else:\n",
    "                #print('not for treble')\n",
    "                self.invalid_booking()\n",
    "                \n",
    "        if cells_to_check==4:\n",
    "            #check we're not at the bottom of the grid\n",
    "            if self.agent_pos[0]+2<max_row:\n",
    "                #check the next cells is also 0.0\n",
    "                #print('good for quad')\n",
    "                if self.state[self.agent_pos[0], self.agent_pos[1]] == 0 and \\\n",
    "                self.state[(self.agent_pos[0]+1), self.agent_pos[1]] == 0 \\\n",
    "                 and self.state[(self.agent_pos[0]+2), self.agent_pos[1]] == 0 and \\\n",
    "                self.state[(self.agent_pos[0]+3), self.agent_pos[1]] == 0:\n",
    "                    self.state[self.agent_pos[0], self.agent_pos[1]] = 1\n",
    "                    self.state[(self.agent_pos[0]+1), self.agent_pos[1]] = 1\n",
    "                    self.state[(self.agent_pos[0]+2), self.agent_pos[1]] = 1\n",
    "                    self.state[(self.agent_pos[0]+3), self.agent_pos[1]] = 1\n",
    "                    self.valid_booking()\n",
    "                    self.agent_pos = [(self.agent_pos[0]+3), self.agent_pos[1]]\n",
    "                else:\n",
    "                    #print('quad taken')\n",
    "                    self.invalid_booking()\n",
    "            else:\n",
    "                #print('not for quad')\n",
    "                self.invalid_booking()\n",
    "\n",
    "        next_state = self.state\n",
    "\n",
    "        return next_state\n",
    "\n",
    "    def step(self, action):\n",
    "\n",
    "        #get new position of agent based on action\n",
    "        new_agent_pos = self.move_agent(action)\n",
    "        #print('new and old pos', new_agent_pos, self.agent_pos)\n",
    "        \n",
    "        #if the agent is stuck on an edge then move to a new position\n",
    "        if new_agent_pos == self.agent_pos:\n",
    "            self.agent_pos = [np.random.randint(self.num_slots), np.random.randint(self.num_gps)]\n",
    "            #print('here1', self.agent_pos)\n",
    "        else:\n",
    "            self.agent_pos = new_agent_pos\n",
    "            #print('here2', self.agent_pos)\n",
    "            \n",
    "        #print('trying to book', self.to_book, self.appt_idx)\n",
    "        \n",
    "        #check if it's possible to book then book\n",
    "        if self.check_bookable():\n",
    "            #print('checked here')\n",
    "            self.state = self.check_and_book()\n",
    "        else:\n",
    "            #print('not bookable')\n",
    "            self.invalid_booking()\n",
    "        \n",
    "        #work out if episode complete\n",
    "        if self.appt_idx == len(self.to_book):\n",
    "            self.done = True\n",
    "            \n",
    "        #print(self.state, self.agent_pos)\n",
    "        agent_state = self.state.copy()\n",
    "        agent_state[self.agent_pos[0], self.agent_pos[1]] = 5\n",
    "        #print('agent', agent_state)\n",
    "\n",
    "        info = {}\n",
    "        return agent_state, self.reward, self.done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SchedulerEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = \"cuda\"\n",
    "device = \"cpu\"\n",
    "\n",
    "env = SchedulerEnv()\n",
    "\n",
    "#start writing to tensorboard\n",
    "writer = SummaryWriter(comment=\"Scheduler MC\")\n",
    "\n",
    "#create the current network and target network\n",
    "policy_model = Model(env.observation_space.shape, env.action_space.n).to(device)\n",
    "\n",
    "optimizer = optim.Adam(policy_model.parameters(), lr=0.001, eps=1e-3)\n",
    "stp_idx = 0\n",
    "\n",
    "#epsilon = eps_start\n",
    "for a in range(num_rounds):\n",
    "    #print(a)\n",
    "    episode_list =[]\n",
    "\n",
    "    for i in range(10):\n",
    "        #change this for while not true once it works\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        #print('reset here')\n",
    "\n",
    "        for j in range(50):\n",
    "    #    while not done:\n",
    "\n",
    "            # Select and perform an action\n",
    "            if np.random.rand() > epsilon:\n",
    "                action = torch.argmax(policy_model(state))\n",
    "            else:\n",
    "                action = np.random.randint(env.action_space.n)\n",
    "\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            episode_reward += reward\n",
    "\n",
    "            episode_list.append([state, action, reward, done])\n",
    "\n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "            \n",
    "            stp_idx +=1\n",
    "            if stp_idx%100000 == 0:\n",
    "                print('stp_idx', stp_idx)\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        #print('stopped episode', j, episode_reward)\n",
    "\n",
    "        writer.add_scalar('episode_reward', episode_reward, stp_idx)\n",
    "\n",
    "        #print('step', step_idx, 'i', i, 'j', j, episode_reward)\n",
    "\n",
    "    #create list of state, action pairs with values\n",
    "    #set list to capture actual rewards per step\n",
    "    tot_reward = []\n",
    "    #set sum to zero\n",
    "    reward_to_add = 0\n",
    "    #work backwards through each step\n",
    "    for item in reversed(episode_list):\n",
    "        #if the step is a final step\n",
    "        if item[3]:\n",
    "            #append the step and actual reward to the list\n",
    "            tot_reward.append([item[0], int(item[1]), item[2]])\n",
    "            #set the start for the cumulative reward\n",
    "            reward_to_add = 1\n",
    "        #if the step is not a final step\n",
    "        else:\n",
    "            #add the reward for this step to the cumulative total\n",
    "            reward_to_add += item[2]\n",
    "            #append the step with the cumulative total to the list\n",
    "            tot_reward.append([item[0], int(item[1]), reward_to_add])\n",
    "\n",
    "    # create dictionary to be able to average action values to use for training\n",
    "    sa_dict = {}\n",
    "    #loop through all the steps\n",
    "    for item in tot_reward:\n",
    "        #flatten each state into a tuple\n",
    "        flat_state = tuple(item[0].flatten())\n",
    "        #set the action to an integer\n",
    "        int_act = int(item[1])\n",
    "        k = (flat_state, int_act)\n",
    "        #if the state action pair is in the dictionary\n",
    "        if k in sa_dict.keys():\n",
    "            #append the reward\n",
    "            sa_dict[k].append(item[2])\n",
    "        else:\n",
    "            #create a new entry for the state action pair\n",
    "            sa_dict[k] = [item[2]] \n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    #create empty lists to record the states, actions and rewards\n",
    "    state_list = []\n",
    "    action_list = []\n",
    "    state_value = []\n",
    "    #loop through each key in the dictionary\n",
    "    for key in sa_dict.keys():\n",
    "        #append ths state and action to the respective list\n",
    "        state_list.append(key[0])\n",
    "        action_list.append(key[1])\n",
    "        v = sa_dict[key]\n",
    "        #if the total sum of the rewards is zero, append to the reward list\n",
    "        if sum(v) == 0:\n",
    "            state_value.append(sum(v))\n",
    "        #else work out the average and append this to the reward list\n",
    "        else:\n",
    "            state_value.append(sum(v)/ float(len(v))) \n",
    "\n",
    "    predicted = []\n",
    "    target = []\n",
    "    for i in range(len(state_list)):\n",
    "        predicted.append(policy_model(state_list[i])[action_list[i]])\n",
    "        target.append(state_value[i])\n",
    "    \n",
    "    #print('this stp_idx', stp_idx)\n",
    "\n",
    "    loss = F.mse_loss(torch.Tensor(predicted), torch.Tensor(target))\n",
    "    loss.requires_grad = True\n",
    "    loss.backward()\n",
    "    writer.add_scalar('loss', loss, stp_idx)\n",
    "\n",
    "    optimizer.step()      \n",
    "\n",
    "    if stp_idx>3000000:\n",
    "        break\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'finalized_model.sav'\n",
    "#pickle.dump(policy_model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#policy_model(my_state[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0037, -0.0202, -0.0544,  0.0982], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_out = torch.argmax(loaded_model(state))\n",
    "test_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the data from the futuresNHS and select the day to schedule\n",
    "appt_figs = pd.read_csv('JanData.csv')\n",
    "day_to_schedule = appt_figs[appt_figs['Date'].isin(['11-Jan-22'])].to_numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the national appointment split for the day\n",
    "tot_appt = float((day_to_schedule[1]).replace(',',''))\n",
    "to_book = float((day_to_schedule[7]).replace(',',''))\n",
    "pre_book = tot_appt-to_book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the percentage split of appointment type\n",
    "small_app = (float((day_to_schedule[2]).replace(',',''))+float((day_to_schedule[5]).replace(',','')))/tot_appt\n",
    "med_app = float((day_to_schedule[4]).replace(',',''))/tot_appt\n",
    "large_app = 1-small_app-med_app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the appointments due for 100 gps\n",
    "surgery_to_book = round(to_book/float((day_to_schedule[8]).replace(',',''))*100)\n",
    "surgery_pre_booked = round(pre_book/float((day_to_schedule[8]).replace(',',''))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the scheduler with pre booked appointments\n",
    "def create_starting_schedule(num_gps, num_slots, num_pre_booked): \n",
    "    \n",
    "    #creates zero filled dataframe with row per time slot and column per gp\n",
    "    state = np.zeros((num_slots, num_gps),dtype=float)\n",
    "\n",
    "    #randomly enters a 1 for each pre booked appointments\n",
    "    while num_pre_booked>0:\n",
    "        num_pre_booked -= 1\n",
    "        state[np.random.randint(num_slots), np.random.randint(num_gps)] = 1\n",
    "    \n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create list of appointments to book based on split\n",
    "def list_to_book(num_to_book, small_split, med_split, large_split):\n",
    "    to_book = []\n",
    "    for i in range(num_to_book):\n",
    "        to_book.append(*choices([1,2,3],[small_split, med_split, large_split]))\n",
    "    return to_book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculates new position of the agent based on the action\n",
    "def move_agent(agent_pos, action):\n",
    "\n",
    "    #set boundaries for the grid\n",
    "    max_row = num_slots - 1\n",
    "    max_col = num_gps - 1\n",
    "    \n",
    "    #setting new co-ordinates for the agent\n",
    "    new_row = agent_pos[0]\n",
    "    new_col = agent_pos[1]\n",
    "\n",
    "    #calculate what the new position may be based on the action without going out the grid\n",
    "    if action == 0:\n",
    "        new_row = max(agent_pos[0] - 1, 0)\n",
    "    if action == 1:\n",
    "        new_row = min(agent_pos[0] + 1, max_row)\n",
    "    if action == 2:\n",
    "        new_col = max(agent_pos[1] - 1, 0)\n",
    "    if action == 3:\n",
    "        new_col = min(agent_pos[1] + 1, max_col)\n",
    "\n",
    "    new_pos = [new_row, new_col]\n",
    "\n",
    "    return new_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checks if the appointment fits\n",
    "def check_and_book(state, cells_to_check, agent_pos, appt_idx):\n",
    "\n",
    "    max_row = num_slots - 1\n",
    "\n",
    "    if cells_to_check==1:\n",
    "        if state[agent_pos[0], agent_pos[1]] == 0:\n",
    "            state[agent_pos[0], agent_pos[1]] = 1\n",
    "            appt_idx+=1\n",
    "\n",
    "    if cells_to_check==2:\n",
    "        #check we're not at the bottom of the grid\n",
    "        if agent_pos[0]<max_row:\n",
    "            if state[agent_pos[0], agent_pos[1]] == 0 and \\\n",
    "            state[(agent_pos[0]+1), agent_pos[1]] == 0:\n",
    "                state[agent_pos[0], agent_pos[1]] = 1\n",
    "                state[(agent_pos[0]+1), agent_pos[1]] = 1\n",
    "\n",
    "                agent_pos = [(agent_pos[0]+1), agent_pos[1]]\n",
    "                appt_idx+=1\n",
    "\n",
    "    if cells_to_check==3:\n",
    "        #check we're not at the bottom of the grid\n",
    "        if agent_pos[0]+1<max_row:\n",
    "            if state[agent_pos[0], agent_pos[1]] == 0 and \\\n",
    "            state[(agent_pos[0]+1), agent_pos[1]] == 0 \\\n",
    "             and state[(agent_pos[0]+2), agent_pos[1]] == 0:\n",
    "                state[agent_pos[0], agent_pos[1]] = 1\n",
    "                state[(agent_pos[0]+1), agent_pos[1]] = 1\n",
    "                state[(agent_pos[0]+2), agent_pos[1]] = 1\n",
    "\n",
    "                agent_pos = [(agent_pos[0]+2), agent_pos[1]]\n",
    "                appt_idx+=1\n",
    "\n",
    "    next_state = state\n",
    "\n",
    "    return next_state, agent_pos, appt_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_slots = 40\n",
    "num_gps = 100\n",
    "state = create_starting_schedule(num_gps, num_slots, surgery_pre_booked)\n",
    "to_book = list_to_book(surgery_to_book, small_app, med_app, large_app)\n",
    "agent_pos = (0,0)\n",
    "appt_idx=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 tensor(3) [[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]] [1, 1]\n",
      "2 tensor(0) [[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 1. 0. ... 0. 0. 0.]\n",
      " [0. 1. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]] [1, 1]\n",
      "2 tensor(0) [[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 1. 0. ... 0. 0. 0.]\n",
      " [0. 1. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]] [0, 1]\n",
      "3 tensor(3) [[1. 1. 0. ... 0. 0. 0.]\n",
      " [1. 1. 0. ... 0. 0. 0.]\n",
      " [0. 1. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]] [0, 2]\n",
      "4 tensor(0) [[1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 0. ... 0. 0. 0.]\n",
      " [0. 1. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]] [0, 2]\n"
     ]
    }
   ],
   "source": [
    "#while appt_idx < len(test_to_book):\n",
    "for i in range(5):\n",
    "    cells_to_check = to_book[appt_idx]\n",
    "    state, agent_pos, appt_idx = check_and_book(state, cells_to_check, agent_pos, appt_idx) \n",
    "    agent_state = state.copy()\n",
    "    agent_state[agent_pos[0], agent_pos[1]] = 5\n",
    "    action = torch.argmax(loaded_model(agent_state))\n",
    "    agent_pos = move_agent(agent_pos, action)\n",
    "    print(appt_idx, action, state, agent_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
