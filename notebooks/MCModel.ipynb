{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "from random import choices\n",
    "from collections import Counter, deque\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils as nn_utils\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 32\n",
    "\n",
    "gamma = 0.99\n",
    "\n",
    "epsilon = 0.1\n",
    "\n",
    "num_rounds = 50\n",
    "#num_episodes = 500\n",
    "learning_limit = 100\n",
    "replay_limit = 1000  # Number of steps until starting replay\n",
    "#weight_update = 1000 # Number of steps until updating the target weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create model\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_shape[0]*input_shape[1], 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_actions)\n",
    "            \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # flatten the observation space Box to linear tensor\n",
    "        tensor_array = torch.from_numpy(state)\n",
    "        x_flat = torch.flatten(tensor_array).to(torch.float32)\n",
    "        return self.net(x_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SchedulerEnv(gym.Env):\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        #starting parameters\n",
    "        num_gps = 10\n",
    "        num_slots = 10\n",
    "        \n",
    "        num_pre_booked = 25\n",
    "        #to_book = [2,1,2,2,1,1,1,3,3,1,2,1,3,2,1,1,2,1,3,2,3,2]\n",
    "        to_book = [2,1,1,1,1]\n",
    "        num_to_book = len(to_book)\n",
    "        \n",
    "        \n",
    "#         num_pre_booked = random.randint(6*num_gps, 14*num_gps)\n",
    "#         num_to_book = random.randint(6*num_gps, 12*num_gps)\n",
    "#         to_book = []\n",
    "#         for j in range(num_to_book):\n",
    "#             to_book.append(*choices([1,2,3],[.7, .25, .05]))\n",
    "            \n",
    "            \n",
    "        agent_pos = [0,0]\n",
    "        \n",
    "        #set parameters for the day\n",
    "        self.num_gps = num_gps\n",
    "        self.num_slots = num_slots\n",
    "        self.num_pre_booked = num_pre_booked\n",
    "        self.to_book = to_book\n",
    "        self.num_to_book = num_to_book\n",
    "        self.diary_slots = num_gps*num_slots\n",
    "        self.agent_pos = agent_pos\n",
    "\n",
    "        #set action space to move around the grid\n",
    "        self.action_space = gym.spaces.Discrete(4) #up, down, left, right\n",
    "        \n",
    "        #set observation space \n",
    "        self.observation_space = gym.spaces.Box(low=0, high=1, shape=(self.num_slots, self.num_gps), dtype=np.int32)\n",
    "   \n",
    "    #creates daily diary for each gp, randomly populates prebooked appointments and resets parameters\n",
    "    def reset(self):\n",
    "\n",
    "        #creates zero filled dataframe with row per time slot and column per gp\n",
    "        self.state = np.zeros((self.num_slots, self.num_gps),dtype=float)\n",
    "\n",
    "        #randomly enters a 1 for each pre booked appointments\n",
    "        pre_booked = self.num_pre_booked\n",
    "        while pre_booked>0:\n",
    "            pre_booked -= 1\n",
    "            self.state[np.random.randint(self.num_slots), np.random.randint(self.num_gps)] = 1\n",
    "            \n",
    "        #randomly sets the agent start space\n",
    "        self.agent_pos = [np.random.randint(self.num_slots), np.random.randint(self.num_gps)]\n",
    "\n",
    "        #resets parameters for new episode\n",
    "        self.done = False\n",
    "        self.reward = 0\n",
    "        self.appt_idx = 0\n",
    "        \n",
    "        #print('starting state', self.state.sum(), self.state)\n",
    "\n",
    "        return self.state\n",
    "    \n",
    "    #calculates new position of the agent based on the action\n",
    "    def move_agent(self, action):\n",
    "\n",
    "        #set boundaries for the grid\n",
    "        max_row = self.num_slots - 1\n",
    "        max_col = self.num_gps - 1\n",
    "\n",
    "        #setting new co-ordinates for the agent\n",
    "        new_row = self.agent_pos[0]\n",
    "        new_col = self.agent_pos[1]\n",
    "\n",
    "        #calculate what the new position may be based on the action without going out the grid\n",
    "        if action == 0:\n",
    "            #print('up')\n",
    "            new_row = max(self.agent_pos[0] - 1, 0)\n",
    "        if action == 1:\n",
    "            #print('down')\n",
    "            new_row = min(self.agent_pos[0] + 1, max_row)\n",
    "        if action == 2:\n",
    "            #print('left')\n",
    "            new_col = max(self.agent_pos[1] - 1, 0)\n",
    "        if action == 3:\n",
    "            #print('right')\n",
    "            new_col = min(self.agent_pos[1] + 1, max_col)\n",
    "\n",
    "        new_pos = [new_row, new_col]\n",
    "        #print('new pos', new_pos)\n",
    "\n",
    "        return new_pos\n",
    "\n",
    "    #checks if we can look to book appointment starting here\n",
    "    def check_bookable(self):\n",
    "        return self.state[self.agent_pos[0], self.agent_pos[1]] == 0.0\n",
    "    \n",
    "    #action if we can't book the appointment\n",
    "    def invalid_booking(self):\n",
    "        #print('cant book')\n",
    "        self.reward = -1\n",
    "        \n",
    "    #action if we can book the appointment\n",
    "    def valid_booking(self):\n",
    "        #print('go ahead and book')\n",
    "        self.appt_idx += 1\n",
    "        self.reward = 1\n",
    "    \n",
    "    #checks if the appointment fits\n",
    "    def check_and_book(self):\n",
    "        \n",
    "        max_row = self.num_slots - 1\n",
    "        cells_to_check = self.to_book[self.appt_idx]\n",
    "        \n",
    "        if cells_to_check==1:\n",
    "            #print('good to check for single')\n",
    "            if self.state[self.agent_pos[0], self.agent_pos[1]] == 0:\n",
    "                self.state[self.agent_pos[0], self.agent_pos[1]] = 1\n",
    "                self.valid_booking()\n",
    "            else:\n",
    "                #print('single taken')\n",
    "                self.invalid_booking()\n",
    "\n",
    "        if cells_to_check==2:\n",
    "            #check we're not at the bottom of the grid\n",
    "            if self.agent_pos[0]<max_row:\n",
    "                #check the next cells is also 0.0\n",
    "                #print('good to check for double')\n",
    "                if self.state[self.agent_pos[0], self.agent_pos[1]] == 0 and \\\n",
    "                self.state[(self.agent_pos[0]+1), self.agent_pos[1]] == 0:\n",
    "                    self.state[self.agent_pos[0], self.agent_pos[1]] = 1\n",
    "                    self.state[(self.agent_pos[0]+1), self.agent_pos[1]] = 1\n",
    "                    self.valid_booking()\n",
    "                    self.agent_pos = [(self.agent_pos[0]+1), self.agent_pos[1]]\n",
    "                    #print('after booking', self.agent_pos)\n",
    "                else:\n",
    "                    #print('double taken')\n",
    "                    self.invalid_booking()\n",
    "            else:\n",
    "                #print('not for double')\n",
    "                self.invalid_booking()\n",
    "                \n",
    "        if cells_to_check==3:\n",
    "            #check we're not at the bottom of the grid\n",
    "            if self.agent_pos[0]+1<max_row:\n",
    "                #print('good to check for treble')\n",
    "                if self.state[self.agent_pos[0], self.agent_pos[1]] == 0 and \\\n",
    "                self.state[(self.agent_pos[0]+1), self.agent_pos[1]] == 0 \\\n",
    "                 and self.state[(self.agent_pos[0]+2), self.agent_pos[1]] == 0:\n",
    "                    self.state[self.agent_pos[0], self.agent_pos[1]] = 1\n",
    "                    self.state[(self.agent_pos[0]+1), self.agent_pos[1]] = 1\n",
    "                    self.state[(self.agent_pos[0]+2), self.agent_pos[1]] = 1\n",
    "                    self.valid_booking()\n",
    "                    self.agent_pos = [(self.agent_pos[0]+2), self.agent_pos[1]]\n",
    "                else:\n",
    "                    #print('treble taken')\n",
    "                    self.invalid_booking()\n",
    "            else:\n",
    "                #print('not for treble')\n",
    "                self.invalid_booking()\n",
    "                \n",
    "        if cells_to_check==4:\n",
    "            #check we're not at the bottom of the grid\n",
    "            if self.agent_pos[0]+2<max_row:\n",
    "                #check the next cells is also 0.0\n",
    "                #print('good for quad')\n",
    "                if self.state[self.agent_pos[0], self.agent_pos[1]] == 0 and \\\n",
    "                self.state[(self.agent_pos[0]+1), self.agent_pos[1]] == 0 \\\n",
    "                 and self.state[(self.agent_pos[0]+2), self.agent_pos[1]] == 0 and \\\n",
    "                self.state[(self.agent_pos[0]+3), self.agent_pos[1]] == 0:\n",
    "                    self.state[self.agent_pos[0], self.agent_pos[1]] = 1\n",
    "                    self.state[(self.agent_pos[0]+1), self.agent_pos[1]] = 1\n",
    "                    self.state[(self.agent_pos[0]+2), self.agent_pos[1]] = 1\n",
    "                    self.state[(self.agent_pos[0]+3), self.agent_pos[1]] = 1\n",
    "                    self.valid_booking()\n",
    "                    self.agent_pos = [(self.agent_pos[0]+3), self.agent_pos[1]]\n",
    "                else:\n",
    "                    #print('quad taken')\n",
    "                    self.invalid_booking()\n",
    "            else:\n",
    "                #print('not for quad')\n",
    "                self.invalid_booking()\n",
    "\n",
    "        next_state = self.state\n",
    "\n",
    "        return next_state\n",
    "\n",
    "    def step(self, action):\n",
    "\n",
    "        #get new position of agent based on action\n",
    "        new_agent_pos = self.move_agent(action)\n",
    "        #print('new and old pos', new_agent_pos, self.agent_pos)\n",
    "        \n",
    "        #if the agent is stuck on an edge then move to a new position\n",
    "        if new_agent_pos == self.agent_pos:\n",
    "            self.agent_pos = [np.random.randint(self.num_slots), np.random.randint(self.num_gps)]\n",
    "            #print('here1', self.agent_pos)\n",
    "        else:\n",
    "            self.agent_pos = new_agent_pos\n",
    "            #print('here2', self.agent_pos)\n",
    "            \n",
    "        #print('trying to book', self.to_book, self.appt_idx)\n",
    "        \n",
    "        #check if it's possible to book then book\n",
    "        if self.check_bookable():\n",
    "            #print('checked here')\n",
    "            self.state = self.check_and_book()\n",
    "        else:\n",
    "            #print('not bookable')\n",
    "            self.invalid_booking()\n",
    "        \n",
    "        #work out if episode complete\n",
    "        if self.appt_idx == len(self.to_book):\n",
    "            self.done = True\n",
    "            \n",
    "        #print(self.state, self.agent_pos)\n",
    "        agent_state = self.state.copy()\n",
    "        agent_state[self.agent_pos[0], self.agent_pos[1]] = 5\n",
    "        #print('agent', agent_state)\n",
    "\n",
    "        info = {}\n",
    "        return agent_state, self.reward, self.done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = \"cuda\"\n",
    "device = \"cpu\"\n",
    "\n",
    "env = SchedulerEnv()\n",
    "\n",
    "#start writing to tensorboard\n",
    "writer = SummaryWriter(comment=\"Scheduler MC\")\n",
    "\n",
    "#create the current network and target network\n",
    "policy_model = Model(env.observation_space.shape, env.action_space.n).to(device)\n",
    "\n",
    "optimizer = optim.Adam(policy_model.parameters(), lr=0.001, eps=1e-3)\n",
    "stp_idx = 0\n",
    "\n",
    "#epsilon = eps_start\n",
    "for a in range(7):\n",
    "    print(a)\n",
    "    episode_list =[]\n",
    "\n",
    "    for i in range(num_rounds):\n",
    "        #change this for while not true once it works\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        #print('reset here')\n",
    "\n",
    "        for j in range(50):\n",
    "    #    while not done:\n",
    "\n",
    "            # Select and perform an action\n",
    "            if np.random.rand() > epsilon:\n",
    "                action = torch.argmax(policy_model(state))\n",
    "            else:\n",
    "                action = np.random.randint(env.action_space.n)\n",
    "\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            episode_reward += reward\n",
    "\n",
    "            episode_list.append([state, action, reward, done])\n",
    "\n",
    "            #print('here rewards', episode_reward, reward, step_idx)\n",
    "\n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "            \n",
    "            stp_idx +=1\n",
    "            print(stp_idx)\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        #print('stopped episode', j, episode_reward)\n",
    "\n",
    "        writer.add_scalar('episode_reward', episode_reward, i)\n",
    "\n",
    "        #print('step', step_idx, 'i', i, 'j', j, episode_reward)\n",
    "\n",
    "    #create list of state, action pairs with action values\n",
    "    tot_reward = []\n",
    "    reward_to_add = 0\n",
    "    for item in reversed(episode_list):\n",
    "        if item[3]:\n",
    "            tot_reward.append([item[0], int(item[1]), item[2]])\n",
    "            #print([item[0], int(item[1]), item[2]])\n",
    "            reward_to_add = 1\n",
    "        else:\n",
    "            reward_to_add += item[2]\n",
    "            tot_reward.append([item[0], int(item[1]), reward_to_add])\n",
    "\n",
    "    # create dictionary to be able to average action values to use for training\n",
    "    test_dict = {}\n",
    "    for item in tot_reward:\n",
    "        flat_state = tuple(item[0].flatten())\n",
    "        int_act = int(item[1])\n",
    "        k = (flat_state, int_act)\n",
    "        if k in test_dict.keys():\n",
    "            test_dict[k].append(item[2])\n",
    "        else:\n",
    "            test_dict[k] = [item[2]] \n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    state_list = []\n",
    "    action_list = []\n",
    "    state_value = []\n",
    "    for key in test_dict.keys():\n",
    "        #print(len(key))\n",
    "        state_list.append(key[0])\n",
    "        action_list.append(key[1])\n",
    "        v = test_dict[key]\n",
    "        if sum(v) == 0:\n",
    "            state_value.append(sum(v))\n",
    "        else:\n",
    "            state_value.append(sum(v)/ float(len(v))) \n",
    "\n",
    "    predicted = []\n",
    "    target = []\n",
    "    for i in range(len(state_list)):\n",
    "        predicted.append(policy_model(state_list[i])[action_list[i]])\n",
    "        target.append(state_value[i])\n",
    "\n",
    "    loss = F.mse_loss(torch.Tensor(predicted), torch.Tensor(target))\n",
    "    loss.requires_grad = True\n",
    "    loss.backward()\n",
    "    writer.add_scalar('loss', loss, stp_idx)\n",
    "\n",
    "    optimizer.step()      \n",
    "\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_example = episode_list.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_example[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_reward = []\n",
    "reward_to_add = 0\n",
    "for item in reversed(test_example):\n",
    "    if item[3]:\n",
    "        tot_reward.append([item[0], int(item[1]), item[2]])\n",
    "        #print([item[0], int(item[1]), item[2]])\n",
    "        reward_to_add = 1\n",
    "    else:\n",
    "        reward_to_add += item[2]\n",
    "        tot_reward.append([item[0], int(item[1]), reward_to_add])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in tot_reward:\n",
    "    flat_state = tuple(item[0].flatten())\n",
    "    int_act = int(item[1])\n",
    "    k = (flat_state, int_act)\n",
    "    #print(k)\n",
    "    if k in test_dict.keys():\n",
    "        test_dict[k].append(item[2])\n",
    "    else:\n",
    "        test_dict[k] = [item[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = policy_model(k[0])\n",
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans[k[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_model(k[0])[k[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Train\n",
    "optimizer.zero_grad()\n",
    "\n",
    "state_list = []\n",
    "action_list = []\n",
    "state_value = []\n",
    "for key in test_dict.keys():\n",
    "    #print(len(key))\n",
    "    state_list.append(key[0])\n",
    "    action_list.append(key[1])\n",
    "    v = test_dict[key]\n",
    "    if sum(v) == 0:\n",
    "        state_value.append(sum(v))\n",
    "    else:\n",
    "        state_value.append(sum(v)/ float(len(v))) \n",
    "        \n",
    "predicted = []\n",
    "target = []\n",
    "for i in range(len(state_list)):\n",
    "    predicted.append(policy_model(state_list[i])[action_list[i]])\n",
    "    target.append(state_value[i])\n",
    "    \n",
    "#pred = torch.Tensor(predicted)\n",
    "#pred = (predicted, requires_grad=True)\n",
    "#tar = torch.Tensor(target)\n",
    "\n",
    "loss = F.mse_loss(torch.Tensor(predicted), torch.Tensor(target))\n",
    "loss.requires_grad = True\n",
    "loss.backward()\n",
    "\n",
    "optimizer.step()\n",
    "# #print('step', step_idx, 'i', i, 'j', j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict1 = {('abd',45):[1,2,3], ('acd',54):[1,3,3], ('a3d',52):[0,0,0]}\n",
    "\n",
    "state_list = []\n",
    "action_list = []\n",
    "state_value = []\n",
    "for key in test_dict1.keys():\n",
    "    print(len(key))\n",
    "    state_list.append(key[0])\n",
    "    action_list.append(key[1])\n",
    "    v = test_dict1[key]\n",
    "    if sum(v) == 0:\n",
    "        state_value.append(sum(v))\n",
    "    else:\n",
    "        state_value.append(sum(v)/ float(len(v)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
