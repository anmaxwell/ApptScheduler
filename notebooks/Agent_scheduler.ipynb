{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SchedulerEnv(gym.Env):\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        #starting parameters\n",
    "        num_gps = 10\n",
    "        num_slots = 15\n",
    "        num_pre_booked = 75\n",
    "        to_book = [2,3,1]\n",
    "        num_to_book = len(to_book)\n",
    "        agent_pos = [0,0]\n",
    "        reward_decay = 0.95\n",
    "        \n",
    "        #set parameters for the day\n",
    "        self.num_gps = num_gps\n",
    "        self.num_slots = num_slots\n",
    "        self.num_pre_booked = num_pre_booked\n",
    "        self.to_book = to_book\n",
    "        self.num_to_book = num_to_book\n",
    "        self.diary_slots = num_gps*num_slots\n",
    "        self.agent_pos = agent_pos\n",
    "        self.reward_decay = reward_decay\n",
    "\n",
    "        #set action space to move around the grid\n",
    "        self.action_space = gym.spaces.Discrete(4) #up, down, left, right\n",
    "        \n",
    "        #set observation space \n",
    "        self.observation_space = gym.spaces.Box(low=0, high=1, shape=(self.num_slots, self.num_gps), dtype=np.int32)\n",
    "   \n",
    "    #creates daily diary for each gp, randomly populates prebooked appointments and resets parameters\n",
    "    def reset(self):\n",
    "\n",
    "        #creates zero filled dataframe with row per time slot and column per gp\n",
    "        self.state = np.zeros((self.num_slots, self.num_gps),dtype=float)\n",
    "\n",
    "        #randomly enters a 1 for each pre booked appointments\n",
    "        pre_booked = self.num_pre_booked\n",
    "        while pre_booked>0:\n",
    "            pre_booked -= 1\n",
    "            self.state[np.random.randint(self.num_slots), np.random.randint(self.num_gps)] = 1\n",
    "            \n",
    "        #randomly sets the agent start space\n",
    "        self.agent_pos = [np.random.randint(self.num_slots), np.random.randint(self.num_gps)]\n",
    "\n",
    "        #resets parameters for new episode\n",
    "        self.done = False\n",
    "        self.reward = 0\n",
    "        self.appt_idx = 0\n",
    "        self.decay_steps = 1\n",
    "        \n",
    "        #print('starting state', self.state.sum(), self.state)\n",
    "\n",
    "        return self.state\n",
    "    \n",
    "    #calculates new position of the agent based on the action\n",
    "    def move_agent(self, action):\n",
    "\n",
    "        #set boundaries for the grid\n",
    "        max_row = env.num_slots - 1\n",
    "        max_col = env.num_gps - 1\n",
    "\n",
    "        #set new co-ordinates for the agent\n",
    "        new_row = self.agent_pos[0]\n",
    "        new_col = self.agent_pos[1]\n",
    "\n",
    "        #calculate what the new position may be based on the action without going out the griid\n",
    "        if action == 0:\n",
    "            #print('up')\n",
    "            new_row = max(self.agent_pos[0] - 1, 0)\n",
    "        if action == 1:\n",
    "            #print('down')\n",
    "            new_row = min(self.agent_pos[0] + 1, max_row)\n",
    "        if action == 2:\n",
    "            #print('left')\n",
    "            new_col = max(self.agent_pos[1] - 1, 0)\n",
    "        if action == 3:\n",
    "            #print('right')\n",
    "            new_col = min(self.agent_pos[1] + 1, max_col)\n",
    "\n",
    "        new_pos = [new_row, new_col]\n",
    "        #print('new pos', new_pos)\n",
    "\n",
    "        return new_pos\n",
    "\n",
    "    #checks if we can look to book appointment starting here\n",
    "    def check_bookable(self):\n",
    "        return self.state[self.agent_pos[0], self.agent_pos[1]] == 0.0\n",
    "    \n",
    "    #checks if the appointment fits\n",
    "    def check_and_book(self):\n",
    "        max_row = env.num_slots - 1\n",
    "\n",
    "        #checks if the appointment fits\n",
    "        cells_to_check = self.to_book[self.appt_idx]\n",
    "        if cells_to_check==1:\n",
    "            #print('good to check for single')\n",
    "            if self.state[self.agent_pos[0], self.agent_pos[1]] == 0:\n",
    "                self.state[self.agent_pos[0], self.agent_pos[1]] = 1\n",
    "                self.appt_idx += 1\n",
    "                #print('go ahead and book')\n",
    "                self.decay_steps = 1\n",
    "                self.reward = 1\n",
    "            else:\n",
    "                #print('already taken')\n",
    "                self.decay_steps += 1\n",
    "                self.reward = -1\n",
    "        if cells_to_check==2:\n",
    "            #check we're not at the bottom of the grid\n",
    "            if self.agent_pos[0]<max_row:\n",
    "                #check the next cells is also 0.0\n",
    "                #print('good to check for double')\n",
    "                if self.state[self.agent_pos[0], self.agent_pos[1]] == 0 and \\\n",
    "                self.state[(self.agent_pos[0]+1), self.agent_pos[1]] == 0:\n",
    "                    self.state[self.agent_pos[0], self.agent_pos[1]] = 1\n",
    "                    self.state[(self.agent_pos[0]+1), self.agent_pos[1]] = 1\n",
    "                    self.appt_idx += 1\n",
    "                    #print('go ahead and book')\n",
    "                    self.decay_steps = 1\n",
    "                    self.reward = 1\n",
    "                else:\n",
    "                    #print('already taken')\n",
    "                    self.decay_steps += 1\n",
    "                    self.reward = -1\n",
    "            else:\n",
    "                #print('not for double')\n",
    "                self.decay_steps += 1\n",
    "                self.reward = -1\n",
    "        if cells_to_check==3:\n",
    "            #check we're not at the bottom of the grid\n",
    "            if self.agent_pos[0]+1<max_row:\n",
    "                #print('good to check for treble')\n",
    "                if self.state[self.agent_pos[0], self.agent_pos[1]] == 0 and \\\n",
    "                self.state[(self.agent_pos[0]+1), self.agent_pos[1]] == 0 \\\n",
    "                 and self.state[(self.agent_pos[0]+2), self.agent_pos[1]] == 0:\n",
    "                    self.state[self.agent_pos[0], self.agent_pos[1]] = 1\n",
    "                    self.state[(self.agent_pos[0]+1), self.agent_pos[1]] = 1\n",
    "                    self.state[(self.agent_pos[0]+2), self.agent_pos[1]] = 1\n",
    "                    self.appt_idx += 1\n",
    "                    #print('go ahead and book')\n",
    "                    self.decay_steps = 1\n",
    "                    self.reward = 1\n",
    "                else:\n",
    "                    #print('already taken')\n",
    "                    self.decay_steps += 1\n",
    "                    self.reward = -1\n",
    "            else:\n",
    "                #print('not for treble')\n",
    "                self.decay_steps += 1\n",
    "                self.reward = -1\n",
    "        if cells_to_check==4:\n",
    "            #check we're not at the bottom of the grid\n",
    "            if self.agent_pos[0]+2<max_row:\n",
    "                #check the next cells is also 0.0\n",
    "                #print('good for quad')\n",
    "                if self.state[self.agent_pos[0], self.agent_pos[1]] == 0 and \\\n",
    "                self.state[(self.agent_pos[0]+1), self.agent_pos[1]] == 0 \\\n",
    "                 and self.state[(self.agent_pos[0]+2), self.agent_pos[1]] == 0 and \\\n",
    "                self.state[(self.agent_pos[0]+3), self.agent_pos[1]] == 0:\n",
    "                    self.state[self.agent_pos[0], self.agent_pos[1]] = 1\n",
    "                    self.state[(self.agent_pos[0]+1), self.agent_pos[1]] = 1\n",
    "                    self.state[(self.agent_pos[0]+2), self.agent_pos[1]] = 1\n",
    "                    self.state[(self.agent_pos[0]+3), self.agent_pos[1]] = 1\n",
    "                    self.appt_idx += 1\n",
    "                    #print('go ahead and book')\n",
    "                    self.decay_steps = 1\n",
    "                    self.reward = 1\n",
    "                else:\n",
    "                    #print('already taken')\n",
    "                    self.decay_steps += 1\n",
    "                    self.reward = -1\n",
    "            else:\n",
    "                #print('not for quad')\n",
    "                self.decay_steps += 1\n",
    "                self.reward = -1\n",
    "\n",
    "        next_state = self.state\n",
    "\n",
    "        return next_state\n",
    "\n",
    "    def step(self, action):\n",
    "\n",
    "        #print('start step' , self.decay_steps)\n",
    "        #get new positioin of agent based on action\n",
    "        self.agent_pos = self.move_agent(action)\n",
    "        #print('trying to book', self.to_book, self.appt_idx)\n",
    "        \n",
    "        #check if it's possible to book then book\n",
    "        if self.check_bookable():\n",
    "            self.state = self.check_and_book()\n",
    "            #print('checked here')\n",
    "        else:\n",
    "            #print('not bookable')\n",
    "            self.decay_steps += 1\n",
    "            self.reward = -1\n",
    "        \n",
    "        #work out if episode complete\n",
    "        if self.appt_idx == len(self.to_book):\n",
    "            #print('all booked')\n",
    "            self.done = True\n",
    "  \n",
    "            \n",
    "        #work out rewards\n",
    "        #self.reward = (1 - (self.reward_decay**self.decay_steps))\n",
    "        \n",
    "        #print('step', self.decay_steps, self.reward)\n",
    "        #print('end step')\n",
    "\n",
    "        info = {}\n",
    "\n",
    "        return self.state, self.reward, self.done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, action_size):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128) \n",
    "        )\n",
    "\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(128, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, action_size)\n",
    "        )\n",
    "\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(128, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        init_out = self.net(x)\n",
    "        return self.actor(init_out), self.critic(init_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert numpy array to tensor for input\n",
    "def tensor_convert(x):\n",
    "    return torch.from_numpy(x).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def train(b_states, b_actions, b_rewards, b_actor, b_critic, eps_steps, optimizer):\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    #logits_v = b_actor\n",
    "    #value_v = b_critic\n",
    "\n",
    "    b_rewards = torch.Tensor(b_rewards)\n",
    "    b_critic = torch.FloatTensor(b_critic)\n",
    "    \n",
    "    loss_value_v = F.mse_loss(b_critic, b_rewards)\n",
    "\n",
    "    log_prob_v = F.log_softmax(b_actor, dim=1)\n",
    "    adv_v = b_rewards - b_critic.detach()\n",
    "    log_prob_actions_v = adv_v * log_prob_v[eps_steps, b_actions]\n",
    "    loss_policy_v = -log_prob_actions_v.mean()\n",
    "\n",
    "    prob_v = F.softmax(b_actor, dim=1)\n",
    "    entropy_loss_v = 0.01 * (prob_v * log_prob_v).sum(dim=1).mean()\n",
    "\n",
    "    # calculate policy gradients only\n",
    "    loss_policy_v.backward(retain_graph=True)\n",
    "    grads = np.concatenate([p.grad.data.cpu().numpy().flatten()\n",
    "                            for p in net.parameters()\n",
    "                            if p.grad is not None])\n",
    "\n",
    "    # apply entropy and value gradients\n",
    "    loss_v = entropy_loss_v + loss_value_v\n",
    "    loss_v.backward()\n",
    "    nn_utils.clip_grad_norm_(net.parameters(), CLIP_GRAD)\n",
    "    optimizer.step()\n",
    "    # get full loss\n",
    "    loss_v += loss_policy_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(b_states, b_actions, b_rewards, b_actor, b_critic, eps_steps, optimizer):\n",
    "    \n",
    "    #unpack batches for training\n",
    "    for i in range(eps_steps):\n",
    "        state = b_states[i]\n",
    "        action = b_actions[i]\n",
    "        reward = b_rewards[i]\n",
    "        actor = b_actor[i]\n",
    "        critic = b_critic[i]\n",
    "\n",
    "        obs_v = torch.FloatTensor(state)\n",
    "        rewards_v = torch.tensor(reward)\n",
    "        critic = torch.FloatTensor(critic)\n",
    "        actions_t = torch.FloatTensor(action)\n",
    "\n",
    "        loss_value_v = F.mse_loss(critic, rewards_v)\n",
    "\n",
    "        log_prob_v = F.log_softmax(actor)\n",
    "        adv_v = rewards_v - critic\n",
    "        log_prob_actions_v = adv_v * log_prob_v[range(len(action))]\n",
    "        loss_policy_v = -log_prob_actions_v.mean()\n",
    "\n",
    "        prob_v = F.softmax(action)\n",
    "        #entropy_loss_v = (prob_v * log_prob_v).sum().mean()\n",
    "        #loss_v = (entropy_beta * entropy_loss_v + loss_value_v + loss_policy_v)\n",
    "        loss_v = (loss_value_v + loss_policy_v)\n",
    "        loss_v = torch.tensor(loss_v, requires_grad = True)\n",
    "        print('loss_v', loss_v)\n",
    "        writer.add_scalar(\"loss\", loss_v, i + (len(rewards)*batch_count))\n",
    "\n",
    "        loss_v.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialise environment, model and optimiser\n",
    "env = SchedulerEnv()\n",
    "model = Model((env.observation_space.shape[0]*env.observation_space.shape[1]), env.action_space.n)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, \n",
    "                             amsgrad=False)\n",
    "optimizer.zero_grad()\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "for i in range(3):\n",
    "\n",
    "    state = env.reset()\n",
    "    #print('start pos', env.agent_pos)\n",
    "    done = False\n",
    "    b_states = []\n",
    "    b_actions = []\n",
    "    b_rewards = []\n",
    "    b_new_state = []\n",
    "    b_actor = []\n",
    "    b_critic = []\n",
    "    eps_reward = 0\n",
    "    eps_steps = 0\n",
    "\n",
    "    while not done:\n",
    "\n",
    "        #create model input from flattened grid\n",
    "        nn_input = torch.flatten(tensor_convert(state))\n",
    "        actor, critic = model(nn_input)\n",
    "        b_actor.append(actor)\n",
    "        b_critic.append(critic)\n",
    "\n",
    "        #print('get new action')\n",
    "        #action = torch.argmax(actor)\n",
    "        action = env.action_space.sample()\n",
    "\n",
    "        #print(F.softmax(actor), action)\n",
    "\n",
    "        #run through step to book appointment\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        b_states.append(nn_input)\n",
    "        b_actions.append(action)\n",
    "        b_rewards.append(reward)\n",
    "        b_new_state.append(new_state)\n",
    "        eps_reward += reward\n",
    "        eps_steps += 1\n",
    "\n",
    "        #print('done', done)\n",
    "        state = new_state\n",
    "\n",
    "    #print(\"train with this\", b_rewards, b_actions)\n",
    "    #print('end of episode', eps_reward, eps_steps)\n",
    "    #print(rewards_np[0], b_rewards[0], b_critic[0], tensor_rewards[0])\n",
    "    train(b_states, b_actions, b_rewards, b_actor, b_critic, eps_steps, optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
