{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class RewardTracker:\n",
    "    def __init__(self, writer, stop_reward):\n",
    "        self.writer = writer\n",
    "        self.stop_reward = stop_reward\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.ts = time.time()\n",
    "        self.ts_frame = 0\n",
    "        self.total_rewards = []\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *args):\n",
    "        self.writer.close()\n",
    "\n",
    "    def reward(self, reward, frame, epsilon=None):\n",
    "        self.total_rewards.append(reward)\n",
    "        speed = (frame - self.ts_frame) / (time.time() - self.ts)\n",
    "        self.ts_frame = frame\n",
    "        self.ts = time.time()\n",
    "        mean_reward = np.mean(self.total_rewards[-100:])\n",
    "        epsilon_str = \"\" if epsilon is None else \", eps %.2f\" % epsilon\n",
    "        print(\"%d: done %d games, mean reward %.3f, speed %.2f f/s%s\" % (\n",
    "            frame, len(self.total_rewards), mean_reward, speed, epsilon_str\n",
    "        ))\n",
    "        sys.stdout.flush()\n",
    "        if epsilon is not None:\n",
    "            self.writer.add_scalar(\"epsilon\", epsilon, frame)\n",
    "        self.writer.add_scalar(\"speed\", speed, frame)\n",
    "        self.writer.add_scalar(\"reward_100\", mean_reward, frame)\n",
    "        self.writer.add_scalar(\"reward\", reward, frame)\n",
    "        if mean_reward > self.stop_reward:\n",
    "            print(\"Solved in %d frames!\" % frame)\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "\n",
    "class AtariPGN(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(AtariPGN, self).__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "\n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        fx = x.float() / 256\n",
    "        conv_out = self.conv(fx).view(fx.size()[0], -1)\n",
    "        return self.fc(conv_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batching here\n",
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(1.2038e-06, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.3535, -1.3975, -1.4256, -1.3701],\n",
      "        [-1.3534, -1.3976, -1.4256, -1.3702],\n",
      "        [-1.3535, -1.3972, -1.4257, -1.3702],\n",
      "        [-1.3535, -1.3975, -1.4255, -1.3702],\n",
      "        [-1.3537, -1.3974, -1.4256, -1.3699],\n",
      "        [-1.3535, -1.3975, -1.4256, -1.3701],\n",
      "        [-1.3536, -1.3972, -1.4258, -1.3701],\n",
      "        [-1.3535, -1.3972, -1.4258, -1.3701],\n",
      "        [-1.3535, -1.3975, -1.4256, -1.3701],\n",
      "        [-1.3534, -1.3976, -1.4256, -1.3702],\n",
      "        [-1.3535, -1.3972, -1.4257, -1.3702],\n",
      "        [-1.3536, -1.3975, -1.4255, -1.3701],\n",
      "        [-1.3537, -1.3974, -1.4256, -1.3699],\n",
      "        [-1.3537, -1.3975, -1.4257, -1.3699],\n",
      "        [-1.3536, -1.3974, -1.4256, -1.3701],\n",
      "        [-1.3537, -1.3974, -1.4258, -1.3699],\n",
      "        [-1.3537, -1.3974, -1.4256, -1.3699],\n",
      "        [-1.3535, -1.3974, -1.4257, -1.3700],\n",
      "        [-1.3535, -1.3972, -1.4259, -1.3700],\n",
      "        [-1.3535, -1.3972, -1.4258, -1.3702],\n",
      "        [-1.3538, -1.3973, -1.4256, -1.3699],\n",
      "        [-1.3537, -1.3973, -1.4256, -1.3700],\n",
      "        [-1.3537, -1.3973, -1.4256, -1.3702],\n",
      "        [-1.3539, -1.3972, -1.4256, -1.3701],\n",
      "        [-1.3537, -1.3974, -1.4256, -1.3699],\n",
      "        [-1.3537, -1.3975, -1.4257, -1.3699],\n",
      "        [-1.3535, -1.3973, -1.4256, -1.3701],\n",
      "        [-1.3537, -1.3974, -1.4257, -1.3699],\n",
      "        [-1.3535, -1.3975, -1.4256, -1.3701],\n",
      "        [-1.3534, -1.3976, -1.4256, -1.3702],\n",
      "        [-1.3534, -1.3973, -1.4258, -1.3702],\n",
      "        [-1.3535, -1.3975, -1.4256, -1.3701],\n",
      "        [-1.3537, -1.3974, -1.4256, -1.3699],\n",
      "        [-1.3535, -1.3974, -1.4257, -1.3700],\n",
      "        [-1.3535, -1.3972, -1.4259, -1.3700],\n",
      "        [-1.3534, -1.3972, -1.4259, -1.3701],\n",
      "        [-1.3535, -1.3975, -1.4256, -1.3701],\n",
      "        [-1.3533, -1.3975, -1.4257, -1.3702],\n",
      "        [-1.3535, -1.3973, -1.4257, -1.3702],\n",
      "        [-1.3534, -1.3975, -1.4256, -1.3701],\n",
      "        [-1.3537, -1.3974, -1.4256, -1.3699],\n",
      "        [-1.3535, -1.3974, -1.4257, -1.3700],\n",
      "        [-1.3536, -1.3972, -1.4258, -1.3700],\n",
      "        [-1.3535, -1.3972, -1.4258, -1.3702],\n",
      "        [-1.3537, -1.3974, -1.4256, -1.3699],\n",
      "        [-1.3537, -1.3975, -1.4257, -1.3699],\n",
      "        [-1.3535, -1.3973, -1.4257, -1.3701],\n",
      "        [-1.3536, -1.3973, -1.4258, -1.3699],\n",
      "        [-1.3537, -1.3974, -1.4256, -1.3699],\n",
      "        [-1.3535, -1.3974, -1.4257, -1.3700],\n",
      "        [-1.3535, -1.3972, -1.4259, -1.3700],\n",
      "        [-1.3534, -1.3972, -1.4259, -1.3701],\n",
      "        [-1.3537, -1.3974, -1.4256, -1.3699],\n",
      "        [-1.3537, -1.3975, -1.4256, -1.3699],\n",
      "        [-1.3536, -1.3973, -1.4256, -1.3702],\n",
      "        [-1.3537, -1.3974, -1.4257, -1.3699],\n",
      "        [-1.3538, -1.3973, -1.4256, -1.3699],\n",
      "        [-1.3537, -1.3973, -1.4256, -1.3700],\n",
      "        [-1.3537, -1.3973, -1.4256, -1.3702],\n",
      "        [-1.3538, -1.3971, -1.4256, -1.3701],\n",
      "        [-1.3537, -1.3974, -1.4256, -1.3699],\n",
      "        [-1.3535, -1.3974, -1.4257, -1.3700],\n",
      "        [-1.3536, -1.3972, -1.4258, -1.3700],\n",
      "        [-1.3534, -1.3973, -1.4259, -1.3701]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([-1.5881e-03, -9.3021e-04, -5.4031e-04, -3.0556e-04, -2.2982e-03,\n",
      "        -1.6827e-03, -1.2603e-03, -7.6407e-04, -1.5422e-03, -8.5499e-04,\n",
      "        -5.2038e-04, -2.0405e-04, -2.8176e-03, -2.1719e-03, -1.6584e-03,\n",
      "        -7.5169e-04, -2.2673e-03, -1.6310e-03, -9.9465e-04, -6.6073e-04,\n",
      "        -2.0990e-03, -1.4476e-03, -1.2167e-03, -8.5652e-04, -2.5307e-03,\n",
      "        -2.0006e-03, -1.3951e-03, -5.6485e-04, -1.3210e-03, -6.4218e-04,\n",
      "        -2.8081e-04,  2.6155e-05, -2.2516e-03, -1.6355e-03, -1.0726e-03,\n",
      "        -7.0253e-04, -1.8807e-03, -1.2132e-03, -7.1630e-04, -4.6143e-04,\n",
      "        -2.4407e-03, -1.7706e-03, -1.2056e-03, -9.6453e-04, -2.5268e-03,\n",
      "        -1.9570e-03, -1.2816e-03, -5.8567e-04, -2.2994e-03, -1.6111e-03,\n",
      "        -1.0471e-03, -6.6375e-04, -2.5071e-03, -1.9683e-03, -1.5465e-03,\n",
      "        -6.6249e-04, -2.1589e-03, -1.5094e-03, -1.2158e-03, -9.9503e-04,\n",
      "        -2.3339e-03, -1.7395e-03, -1.1506e-03, -9.2033e-04],\n",
      "       grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(0.0014, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2583, 0.2472, 0.2404, 0.2541],\n",
      "        [0.2584, 0.2472, 0.2404, 0.2541],\n",
      "        [0.2583, 0.2473, 0.2403, 0.2540],\n",
      "        [0.2583, 0.2472, 0.2404, 0.2541],\n",
      "        [0.2583, 0.2472, 0.2404, 0.2541],\n",
      "        [0.2583, 0.2472, 0.2404, 0.2541],\n",
      "        [0.2583, 0.2473, 0.2403, 0.2541],\n",
      "        [0.2583, 0.2473, 0.2403, 0.2541],\n",
      "        [0.2583, 0.2472, 0.2404, 0.2541],\n",
      "        [0.2584, 0.2472, 0.2404, 0.2541],\n",
      "        [0.2583, 0.2473, 0.2403, 0.2540],\n",
      "        [0.2583, 0.2472, 0.2404, 0.2541],\n",
      "        [0.2583, 0.2472, 0.2404, 0.2541],\n",
      "        [0.2583, 0.2472, 0.2403, 0.2541],\n",
      "        [0.2583, 0.2472, 0.2404, 0.2541],\n",
      "        [0.2583, 0.2472, 0.2403, 0.2541],\n",
      "        [0.2583, 0.2472, 0.2404, 0.2541],\n",
      "        [0.2583, 0.2472, 0.2403, 0.2541],\n",
      "        [0.2583, 0.2473, 0.2403, 0.2541],\n",
      "        [0.2583, 0.2473, 0.2403, 0.2541],\n",
      "        [0.2583, 0.2473, 0.2404, 0.2541],\n",
      "        [0.2583, 0.2473, 0.2404, 0.2541],\n",
      "        [0.2583, 0.2473, 0.2404, 0.2541],\n",
      "        [0.2582, 0.2473, 0.2404, 0.2541],\n",
      "        [0.2583, 0.2472, 0.2404, 0.2541],\n",
      "        [0.2583, 0.2472, 0.2403, 0.2541],\n",
      "        [0.2583, 0.2473, 0.2404, 0.2541],\n",
      "        [0.2583, 0.2472, 0.2403, 0.2541],\n",
      "        [0.2583, 0.2472, 0.2404, 0.2541],\n",
      "        [0.2584, 0.2472, 0.2404, 0.2541],\n",
      "        [0.2583, 0.2473, 0.2403, 0.2541],\n",
      "        [0.2583, 0.2472, 0.2404, 0.2541],\n",
      "        [0.2583, 0.2472, 0.2404, 0.2541],\n",
      "        [0.2583, 0.2472, 0.2403, 0.2541],\n",
      "        [0.2583, 0.2473, 0.2403, 0.2541],\n",
      "        [0.2583, 0.2473, 0.2403, 0.2541],\n",
      "        [0.2583, 0.2472, 0.2404, 0.2541],\n",
      "        [0.2584, 0.2472, 0.2403, 0.2541],\n",
      "        [0.2583, 0.2473, 0.2403, 0.2541],\n",
      "        [0.2584, 0.2472, 0.2404, 0.2541],\n",
      "        [0.2583, 0.2472, 0.2404, 0.2541],\n",
      "        [0.2583, 0.2472, 0.2403, 0.2541],\n",
      "        [0.2583, 0.2473, 0.2403, 0.2541],\n",
      "        [0.2583, 0.2473, 0.2403, 0.2541],\n",
      "        [0.2583, 0.2472, 0.2404, 0.2541],\n",
      "        [0.2583, 0.2472, 0.2403, 0.2541],\n",
      "        [0.2583, 0.2473, 0.2403, 0.2541],\n",
      "        [0.2583, 0.2473, 0.2403, 0.2541],\n",
      "        [0.2583, 0.2472, 0.2404, 0.2541],\n",
      "        [0.2583, 0.2472, 0.2403, 0.2541],\n",
      "        [0.2583, 0.2473, 0.2403, 0.2541],\n",
      "        [0.2583, 0.2473, 0.2403, 0.2541],\n",
      "        [0.2583, 0.2472, 0.2404, 0.2541],\n",
      "        [0.2583, 0.2472, 0.2404, 0.2541],\n",
      "        [0.2583, 0.2473, 0.2404, 0.2541],\n",
      "        [0.2583, 0.2472, 0.2403, 0.2541],\n",
      "        [0.2583, 0.2473, 0.2404, 0.2541],\n",
      "        [0.2583, 0.2473, 0.2404, 0.2541],\n",
      "        [0.2583, 0.2473, 0.2404, 0.2541],\n",
      "        [0.2583, 0.2473, 0.2404, 0.2541],\n",
      "        [0.2583, 0.2472, 0.2404, 0.2541],\n",
      "        [0.2583, 0.2472, 0.2403, 0.2541],\n",
      "        [0.2583, 0.2473, 0.2403, 0.2541],\n",
      "        [0.2584, 0.2473, 0.2403, 0.2541]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3859, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(-0.0125, grad_fn=<AddBackward0>)\n",
      "batching here\n",
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(5.7650e-07, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.3608, -1.3901, -1.4285, -1.3672],\n",
      "        [-1.3608, -1.3903, -1.4284, -1.3670],\n",
      "        [-1.3606, -1.3903, -1.4285, -1.3671],\n",
      "        [-1.3606, -1.3903, -1.4285, -1.3671],\n",
      "        [-1.3608, -1.3905, -1.4283, -1.3669],\n",
      "        [-1.3606, -1.3903, -1.4285, -1.3671],\n",
      "        [-1.3605, -1.3906, -1.4285, -1.3670],\n",
      "        [-1.3607, -1.3902, -1.4285, -1.3672],\n",
      "        [-1.3607, -1.3902, -1.4285, -1.3673],\n",
      "        [-1.3607, -1.3904, -1.4286, -1.3670],\n",
      "        [-1.3605, -1.3902, -1.4286, -1.3672],\n",
      "        [-1.3607, -1.3902, -1.4285, -1.3672],\n",
      "        [-1.3608, -1.3905, -1.4284, -1.3669],\n",
      "        [-1.3608, -1.3906, -1.4286, -1.3666],\n",
      "        [-1.3609, -1.3899, -1.4286, -1.3672],\n",
      "        [-1.3608, -1.3905, -1.4285, -1.3668],\n",
      "        [-1.3607, -1.3905, -1.4284, -1.3669],\n",
      "        [-1.3606, -1.3902, -1.4287, -1.3671],\n",
      "        [-1.3604, -1.3907, -1.4285, -1.3670],\n",
      "        [-1.3605, -1.3905, -1.4285, -1.3671],\n",
      "        [-1.3606, -1.3904, -1.4285, -1.3671],\n",
      "        [-1.3606, -1.3902, -1.4287, -1.3671],\n",
      "        [-1.3607, -1.3906, -1.4282, -1.3671],\n",
      "        [-1.3606, -1.3903, -1.4285, -1.3671],\n",
      "        [-1.3609, -1.3905, -1.4283, -1.3669],\n",
      "        [-1.3609, -1.3906, -1.4285, -1.3665],\n",
      "        [-1.3607, -1.3901, -1.4286, -1.3671],\n",
      "        [-1.3608, -1.3904, -1.4287, -1.3667],\n",
      "        [-1.3607, -1.3903, -1.4284, -1.3672],\n",
      "        [-1.3606, -1.3904, -1.4287, -1.3669],\n",
      "        [-1.3604, -1.3904, -1.4287, -1.3670],\n",
      "        [-1.3605, -1.3903, -1.4286, -1.3671],\n",
      "        [-1.3607, -1.3905, -1.4285, -1.3669],\n",
      "        [-1.3605, -1.3903, -1.4287, -1.3672],\n",
      "        [-1.3606, -1.3904, -1.4286, -1.3670],\n",
      "        [-1.3608, -1.3902, -1.4285, -1.3671],\n",
      "        [-1.3607, -1.3902, -1.4286, -1.3672],\n",
      "        [-1.3607, -1.3905, -1.4285, -1.3668],\n",
      "        [-1.3606, -1.3903, -1.4286, -1.3670],\n",
      "        [-1.3607, -1.3904, -1.4286, -1.3670],\n",
      "        [-1.3608, -1.3905, -1.4283, -1.3669],\n",
      "        [-1.3605, -1.3903, -1.4286, -1.3672],\n",
      "        [-1.3605, -1.3905, -1.4286, -1.3670],\n",
      "        [-1.3606, -1.3902, -1.4286, -1.3672],\n",
      "        [-1.3608, -1.3905, -1.4284, -1.3669],\n",
      "        [-1.3608, -1.3905, -1.4286, -1.3667],\n",
      "        [-1.3607, -1.3900, -1.4286, -1.3673],\n",
      "        [-1.3609, -1.3902, -1.4286, -1.3668],\n",
      "        [-1.3607, -1.3905, -1.4284, -1.3670],\n",
      "        [-1.3605, -1.3903, -1.4286, -1.3672],\n",
      "        [-1.3605, -1.3905, -1.4286, -1.3669],\n",
      "        [-1.3606, -1.3903, -1.4286, -1.3671],\n",
      "        [-1.3609, -1.3904, -1.4284, -1.3669],\n",
      "        [-1.3607, -1.3908, -1.4285, -1.3666],\n",
      "        [-1.3606, -1.3903, -1.4286, -1.3671],\n",
      "        [-1.3608, -1.3903, -1.4288, -1.3667],\n",
      "        [-1.3607, -1.3902, -1.4285, -1.3671],\n",
      "        [-1.3606, -1.3903, -1.4286, -1.3670],\n",
      "        [-1.3608, -1.3906, -1.4282, -1.3670],\n",
      "        [-1.3607, -1.3904, -1.4284, -1.3671],\n",
      "        [-1.3608, -1.3905, -1.4284, -1.3669],\n",
      "        [-1.3604, -1.3905, -1.4286, -1.3672],\n",
      "        [-1.3605, -1.3907, -1.4285, -1.3669],\n",
      "        [-1.3607, -1.3904, -1.4284, -1.3671]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([1.1003e-03, 1.5212e-03, 8.6785e-04, 5.9298e-04, 1.5626e-03, 1.3386e-03,\n",
      "        4.0818e-04, 3.5923e-04, 1.1903e-03, 1.6036e-03, 9.0904e-04, 6.6008e-04,\n",
      "        2.0406e-03, 8.6433e-04, 9.4585e-04, 1.5570e-04, 1.2261e-03, 1.1613e-03,\n",
      "        1.4947e-04, 4.2196e-04, 1.4756e-03, 1.2631e-03, 7.8117e-04, 8.5084e-05,\n",
      "        1.6209e-03, 7.4135e-04, 7.1358e-04, 1.7718e-04, 9.5441e-04, 1.4134e-03,\n",
      "        9.6639e-04, 6.1415e-04, 1.2476e-03, 1.0958e-03, 2.3501e-04, 2.5052e-04,\n",
      "        1.3893e-03, 1.4730e-03, 9.7023e-04, 7.7991e-04, 1.4500e-03, 1.3790e-03,\n",
      "        5.9825e-04, 3.2128e-04, 1.6495e-03, 7.0894e-04, 7.7307e-04, 1.3607e-04,\n",
      "        1.4389e-03, 1.2910e-03, 3.9738e-04, 4.9501e-04, 2.0942e-03, 1.0513e-03,\n",
      "        1.0026e-03, 2.1867e-04, 1.5795e-03, 1.4218e-03, 9.1424e-04, 2.3896e-04,\n",
      "        1.0302e-03, 1.0497e-03, 1.6936e-04, 4.6742e-05],\n",
      "       grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(-0.0009, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2565, 0.2491, 0.2397, 0.2548],\n",
      "        [0.2565, 0.2490, 0.2397, 0.2549],\n",
      "        [0.2565, 0.2490, 0.2397, 0.2549],\n",
      "        [0.2565, 0.2490, 0.2397, 0.2548],\n",
      "        [0.2564, 0.2489, 0.2397, 0.2549],\n",
      "        [0.2565, 0.2490, 0.2397, 0.2548],\n",
      "        [0.2565, 0.2489, 0.2397, 0.2549],\n",
      "        [0.2565, 0.2490, 0.2397, 0.2548],\n",
      "        [0.2565, 0.2490, 0.2397, 0.2548],\n",
      "        [0.2565, 0.2490, 0.2396, 0.2549],\n",
      "        [0.2565, 0.2490, 0.2396, 0.2548],\n",
      "        [0.2565, 0.2490, 0.2397, 0.2548],\n",
      "        [0.2565, 0.2489, 0.2397, 0.2549],\n",
      "        [0.2565, 0.2489, 0.2396, 0.2550],\n",
      "        [0.2564, 0.2491, 0.2396, 0.2548],\n",
      "        [0.2565, 0.2489, 0.2397, 0.2549],\n",
      "        [0.2565, 0.2489, 0.2397, 0.2549],\n",
      "        [0.2565, 0.2490, 0.2396, 0.2549],\n",
      "        [0.2566, 0.2489, 0.2397, 0.2549],\n",
      "        [0.2565, 0.2490, 0.2397, 0.2549],\n",
      "        [0.2565, 0.2490, 0.2397, 0.2548],\n",
      "        [0.2565, 0.2490, 0.2396, 0.2549],\n",
      "        [0.2565, 0.2489, 0.2397, 0.2548],\n",
      "        [0.2565, 0.2490, 0.2397, 0.2548],\n",
      "        [0.2564, 0.2490, 0.2397, 0.2549],\n",
      "        [0.2564, 0.2489, 0.2397, 0.2550],\n",
      "        [0.2565, 0.2490, 0.2396, 0.2548],\n",
      "        [0.2564, 0.2490, 0.2396, 0.2550],\n",
      "        [0.2565, 0.2490, 0.2397, 0.2548],\n",
      "        [0.2565, 0.2490, 0.2396, 0.2549],\n",
      "        [0.2566, 0.2490, 0.2396, 0.2549],\n",
      "        [0.2565, 0.2490, 0.2396, 0.2548],\n",
      "        [0.2565, 0.2489, 0.2397, 0.2549],\n",
      "        [0.2565, 0.2490, 0.2396, 0.2548],\n",
      "        [0.2565, 0.2490, 0.2396, 0.2549],\n",
      "        [0.2565, 0.2490, 0.2397, 0.2548],\n",
      "        [0.2565, 0.2490, 0.2396, 0.2548],\n",
      "        [0.2565, 0.2490, 0.2397, 0.2549],\n",
      "        [0.2565, 0.2490, 0.2396, 0.2549],\n",
      "        [0.2565, 0.2490, 0.2397, 0.2549],\n",
      "        [0.2565, 0.2490, 0.2397, 0.2549],\n",
      "        [0.2565, 0.2490, 0.2396, 0.2548],\n",
      "        [0.2565, 0.2490, 0.2396, 0.2549],\n",
      "        [0.2565, 0.2490, 0.2396, 0.2548],\n",
      "        [0.2565, 0.2490, 0.2397, 0.2549],\n",
      "        [0.2565, 0.2489, 0.2396, 0.2550],\n",
      "        [0.2565, 0.2491, 0.2397, 0.2548],\n",
      "        [0.2564, 0.2490, 0.2396, 0.2549],\n",
      "        [0.2565, 0.2489, 0.2397, 0.2549],\n",
      "        [0.2565, 0.2490, 0.2396, 0.2548],\n",
      "        [0.2565, 0.2489, 0.2396, 0.2549],\n",
      "        [0.2565, 0.2490, 0.2397, 0.2548],\n",
      "        [0.2564, 0.2490, 0.2397, 0.2549],\n",
      "        [0.2565, 0.2489, 0.2397, 0.2550],\n",
      "        [0.2565, 0.2490, 0.2396, 0.2548],\n",
      "        [0.2565, 0.2490, 0.2396, 0.2549],\n",
      "        [0.2565, 0.2490, 0.2397, 0.2549],\n",
      "        [0.2565, 0.2490, 0.2396, 0.2549],\n",
      "        [0.2565, 0.2489, 0.2398, 0.2549],\n",
      "        [0.2565, 0.2490, 0.2397, 0.2549],\n",
      "        [0.2565, 0.2490, 0.2397, 0.2549],\n",
      "        [0.2566, 0.2490, 0.2397, 0.2548],\n",
      "        [0.2565, 0.2489, 0.2397, 0.2549],\n",
      "        [0.2565, 0.2490, 0.2397, 0.2549]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3859, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(-0.0148, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batching here\n",
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(3.1505e-07, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.3606, -1.3925, -1.4202, -1.3729],\n",
      "        [-1.3605, -1.3924, -1.4202, -1.3731],\n",
      "        [-1.3606, -1.3924, -1.4201, -1.3730],\n",
      "        [-1.3606, -1.3924, -1.4201, -1.3731],\n",
      "        [-1.3607, -1.3923, -1.4201, -1.3731],\n",
      "        [-1.3607, -1.3925, -1.4200, -1.3730],\n",
      "        [-1.3605, -1.3927, -1.4201, -1.3728],\n",
      "        [-1.3606, -1.3924, -1.4202, -1.3731],\n",
      "        [-1.3607, -1.3924, -1.4201, -1.3729],\n",
      "        [-1.3607, -1.3924, -1.4201, -1.3730],\n",
      "        [-1.3607, -1.3924, -1.4201, -1.3730],\n",
      "        [-1.3604, -1.3926, -1.4201, -1.3731],\n",
      "        [-1.3604, -1.3927, -1.4203, -1.3728],\n",
      "        [-1.3607, -1.3923, -1.4205, -1.3728],\n",
      "        [-1.3607, -1.3922, -1.4204, -1.3730],\n",
      "        [-1.3603, -1.3925, -1.4204, -1.3730],\n",
      "        [-1.3606, -1.3925, -1.4201, -1.3729],\n",
      "        [-1.3606, -1.3925, -1.4200, -1.3731],\n",
      "        [-1.3606, -1.3926, -1.4200, -1.3730],\n",
      "        [-1.3606, -1.3923, -1.4202, -1.3731],\n",
      "        [-1.3605, -1.3926, -1.4202, -1.3729],\n",
      "        [-1.3608, -1.3923, -1.4203, -1.3727],\n",
      "        [-1.3606, -1.3927, -1.4201, -1.3728],\n",
      "        [-1.3604, -1.3926, -1.4202, -1.3730],\n",
      "        [-1.3604, -1.3925, -1.4203, -1.3730],\n",
      "        [-1.3607, -1.3924, -1.4203, -1.3729],\n",
      "        [-1.3608, -1.3921, -1.4203, -1.3730],\n",
      "        [-1.3606, -1.3924, -1.4203, -1.3729],\n",
      "        [-1.3606, -1.3924, -1.4201, -1.3730],\n",
      "        [-1.3607, -1.3924, -1.4201, -1.3730],\n",
      "        [-1.3606, -1.3925, -1.4202, -1.3729],\n",
      "        [-1.3606, -1.3924, -1.4202, -1.3731],\n",
      "        [-1.3607, -1.3923, -1.4201, -1.3730],\n",
      "        [-1.3608, -1.3924, -1.4200, -1.3730],\n",
      "        [-1.3606, -1.3924, -1.4202, -1.3730],\n",
      "        [-1.3606, -1.3922, -1.4203, -1.3732],\n",
      "        [-1.3606, -1.3925, -1.4202, -1.3729],\n",
      "        [-1.3606, -1.3925, -1.4201, -1.3730],\n",
      "        [-1.3606, -1.3925, -1.4201, -1.3730],\n",
      "        [-1.3605, -1.3924, -1.4201, -1.3731],\n",
      "        [-1.3607, -1.3923, -1.4202, -1.3730],\n",
      "        [-1.3607, -1.3925, -1.4200, -1.3729],\n",
      "        [-1.3607, -1.3926, -1.4202, -1.3727],\n",
      "        [-1.3606, -1.3923, -1.4203, -1.3730],\n",
      "        [-1.3606, -1.3925, -1.4202, -1.3730],\n",
      "        [-1.3607, -1.3924, -1.4202, -1.3729],\n",
      "        [-1.3607, -1.3921, -1.4204, -1.3730],\n",
      "        [-1.3606, -1.3924, -1.4202, -1.3729],\n",
      "        [-1.3606, -1.3923, -1.4202, -1.3731],\n",
      "        [-1.3607, -1.3925, -1.4200, -1.3729],\n",
      "        [-1.3606, -1.3927, -1.4202, -1.3727],\n",
      "        [-1.3606, -1.3923, -1.4202, -1.3732],\n",
      "        [-1.3605, -1.3925, -1.4203, -1.3729],\n",
      "        [-1.3607, -1.3921, -1.4205, -1.3729],\n",
      "        [-1.3607, -1.3921, -1.4204, -1.3730],\n",
      "        [-1.3606, -1.3924, -1.4203, -1.3729],\n",
      "        [-1.3606, -1.3926, -1.4201, -1.3729],\n",
      "        [-1.3608, -1.3922, -1.4202, -1.3729],\n",
      "        [-1.3605, -1.3925, -1.4202, -1.3729],\n",
      "        [-1.3604, -1.3925, -1.4203, -1.3731],\n",
      "        [-1.3606, -1.3925, -1.4201, -1.3730],\n",
      "        [-1.3607, -1.3926, -1.4200, -1.3729],\n",
      "        [-1.3606, -1.3927, -1.4200, -1.3729],\n",
      "        [-1.3606, -1.3924, -1.4201, -1.3731]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([-1.1810e-03, -8.9290e-04, -5.0118e-04, -5.3931e-04, -8.1842e-04,\n",
      "        -3.5416e-04, -3.6300e-04, -3.5762e-04, -1.2661e-03, -7.2004e-04,\n",
      "        -2.5861e-04, -4.5487e-04, -1.3557e-03, -1.1101e-03, -9.9806e-04,\n",
      "        -7.0154e-04, -1.0450e-03, -5.1296e-04, -3.0392e-04, -4.1299e-04,\n",
      "        -1.0003e-03, -4.6161e-04, -3.1430e-04, -1.6564e-04, -1.0620e-03,\n",
      "        -7.4453e-04, -8.3300e-04, -6.3966e-04, -1.3859e-03, -9.5109e-04,\n",
      "        -3.5129e-04, -4.2825e-04, -8.8695e-04, -4.9956e-04, -3.5343e-04,\n",
      "        -2.1611e-04, -1.2374e-03, -7.6629e-04, -3.5135e-04, -4.1213e-04,\n",
      "        -1.0203e-03, -6.3529e-04, -4.8529e-04, -4.4479e-04, -1.1853e-03,\n",
      "        -7.5999e-04, -7.9356e-04, -6.0309e-04, -1.0669e-03, -7.1774e-04,\n",
      "        -4.1586e-04, -3.2749e-04, -1.5777e-03, -1.0368e-03, -1.0267e-03,\n",
      "        -7.4278e-04, -1.1406e-03, -5.1638e-04, -1.8820e-04,  6.5024e-06,\n",
      "        -9.7011e-04, -4.6283e-04, -5.9029e-04, -3.0628e-04],\n",
      "       grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(0.0007, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2565, 0.2485, 0.2417, 0.2534],\n",
      "        [0.2565, 0.2485, 0.2417, 0.2533],\n",
      "        [0.2565, 0.2485, 0.2417, 0.2533],\n",
      "        [0.2565, 0.2485, 0.2417, 0.2533],\n",
      "        [0.2565, 0.2485, 0.2417, 0.2533],\n",
      "        [0.2565, 0.2485, 0.2417, 0.2534],\n",
      "        [0.2565, 0.2484, 0.2417, 0.2534],\n",
      "        [0.2565, 0.2485, 0.2417, 0.2533],\n",
      "        [0.2565, 0.2485, 0.2417, 0.2534],\n",
      "        [0.2565, 0.2485, 0.2417, 0.2533],\n",
      "        [0.2565, 0.2485, 0.2417, 0.2533],\n",
      "        [0.2566, 0.2484, 0.2417, 0.2533],\n",
      "        [0.2566, 0.2484, 0.2416, 0.2534],\n",
      "        [0.2565, 0.2485, 0.2416, 0.2534],\n",
      "        [0.2565, 0.2485, 0.2416, 0.2534],\n",
      "        [0.2566, 0.2484, 0.2416, 0.2534],\n",
      "        [0.2565, 0.2485, 0.2417, 0.2534],\n",
      "        [0.2565, 0.2485, 0.2417, 0.2533],\n",
      "        [0.2565, 0.2484, 0.2417, 0.2534],\n",
      "        [0.2565, 0.2485, 0.2417, 0.2533],\n",
      "        [0.2565, 0.2484, 0.2417, 0.2534],\n",
      "        [0.2564, 0.2485, 0.2416, 0.2534],\n",
      "        [0.2565, 0.2484, 0.2417, 0.2534],\n",
      "        [0.2566, 0.2484, 0.2417, 0.2533],\n",
      "        [0.2566, 0.2484, 0.2417, 0.2533],\n",
      "        [0.2565, 0.2485, 0.2416, 0.2534],\n",
      "        [0.2565, 0.2486, 0.2416, 0.2533],\n",
      "        [0.2565, 0.2485, 0.2416, 0.2534],\n",
      "        [0.2565, 0.2485, 0.2417, 0.2534],\n",
      "        [0.2565, 0.2485, 0.2417, 0.2534],\n",
      "        [0.2565, 0.2485, 0.2417, 0.2534],\n",
      "        [0.2565, 0.2485, 0.2417, 0.2533],\n",
      "        [0.2565, 0.2485, 0.2417, 0.2533],\n",
      "        [0.2565, 0.2485, 0.2417, 0.2533],\n",
      "        [0.2565, 0.2485, 0.2417, 0.2534],\n",
      "        [0.2565, 0.2485, 0.2416, 0.2533],\n",
      "        [0.2565, 0.2484, 0.2417, 0.2534],\n",
      "        [0.2565, 0.2485, 0.2417, 0.2534],\n",
      "        [0.2565, 0.2485, 0.2417, 0.2534],\n",
      "        [0.2565, 0.2485, 0.2417, 0.2533],\n",
      "        [0.2565, 0.2485, 0.2417, 0.2533],\n",
      "        [0.2565, 0.2484, 0.2417, 0.2534],\n",
      "        [0.2565, 0.2484, 0.2417, 0.2534],\n",
      "        [0.2565, 0.2485, 0.2417, 0.2533],\n",
      "        [0.2565, 0.2485, 0.2417, 0.2534],\n",
      "        [0.2565, 0.2485, 0.2417, 0.2534],\n",
      "        [0.2565, 0.2486, 0.2416, 0.2533],\n",
      "        [0.2565, 0.2485, 0.2417, 0.2534],\n",
      "        [0.2565, 0.2485, 0.2417, 0.2533],\n",
      "        [0.2565, 0.2485, 0.2417, 0.2534],\n",
      "        [0.2565, 0.2484, 0.2417, 0.2534],\n",
      "        [0.2565, 0.2485, 0.2417, 0.2533],\n",
      "        [0.2565, 0.2485, 0.2416, 0.2534],\n",
      "        [0.2565, 0.2485, 0.2416, 0.2534],\n",
      "        [0.2565, 0.2486, 0.2416, 0.2533],\n",
      "        [0.2565, 0.2485, 0.2416, 0.2534],\n",
      "        [0.2565, 0.2484, 0.2417, 0.2534],\n",
      "        [0.2565, 0.2485, 0.2417, 0.2534],\n",
      "        [0.2565, 0.2484, 0.2417, 0.2534],\n",
      "        [0.2566, 0.2485, 0.2416, 0.2533],\n",
      "        [0.2565, 0.2485, 0.2417, 0.2533],\n",
      "        [0.2565, 0.2484, 0.2417, 0.2534],\n",
      "        [0.2565, 0.2484, 0.2417, 0.2534],\n",
      "        [0.2565, 0.2485, 0.2417, 0.2533]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3860, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(-0.0132, grad_fn=<AddBackward0>)\n",
      "batching here\n",
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(3.3839e-08, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.3634, -1.3933, -1.4189, -1.3705],\n",
      "        [-1.3634, -1.3932, -1.4189, -1.3707],\n",
      "        [-1.3634, -1.3933, -1.4188, -1.3706],\n",
      "        [-1.3634, -1.3933, -1.4188, -1.3706],\n",
      "        [-1.3633, -1.3936, -1.4188, -1.3704],\n",
      "        [-1.3634, -1.3934, -1.4188, -1.3705],\n",
      "        [-1.3635, -1.3933, -1.4187, -1.3706],\n",
      "        [-1.3634, -1.3933, -1.4188, -1.3707],\n",
      "        [-1.3633, -1.3933, -1.4190, -1.3706],\n",
      "        [-1.3633, -1.3933, -1.4189, -1.3706],\n",
      "        [-1.3635, -1.3934, -1.4187, -1.3706],\n",
      "        [-1.3634, -1.3932, -1.4189, -1.3707],\n",
      "        [-1.3634, -1.3932, -1.4187, -1.3708],\n",
      "        [-1.3633, -1.3932, -1.4189, -1.3707],\n",
      "        [-1.3634, -1.3934, -1.4188, -1.3706],\n",
      "        [-1.3634, -1.3933, -1.4189, -1.3705],\n",
      "        [-1.3633, -1.3935, -1.4189, -1.3705],\n",
      "        [-1.3634, -1.3933, -1.4189, -1.3705],\n",
      "        [-1.3634, -1.3934, -1.4187, -1.3706],\n",
      "        [-1.3634, -1.3932, -1.4189, -1.3706],\n",
      "        [-1.3634, -1.3931, -1.4189, -1.3707],\n",
      "        [-1.3633, -1.3933, -1.4188, -1.3707],\n",
      "        [-1.3634, -1.3932, -1.4188, -1.3706],\n",
      "        [-1.3634, -1.3934, -1.4188, -1.3705],\n",
      "        [-1.3635, -1.3930, -1.4188, -1.3708],\n",
      "        [-1.3633, -1.3931, -1.4190, -1.3707],\n",
      "        [-1.3633, -1.3932, -1.4189, -1.3706],\n",
      "        [-1.3634, -1.3933, -1.4188, -1.3706],\n",
      "        [-1.3634, -1.3933, -1.4189, -1.3706],\n",
      "        [-1.3633, -1.3931, -1.4189, -1.3707],\n",
      "        [-1.3634, -1.3931, -1.4189, -1.3707],\n",
      "        [-1.3634, -1.3932, -1.4188, -1.3707],\n",
      "        [-1.3634, -1.3933, -1.4188, -1.3706],\n",
      "        [-1.3632, -1.3933, -1.4190, -1.3706],\n",
      "        [-1.3634, -1.3933, -1.4188, -1.3706],\n",
      "        [-1.3634, -1.3933, -1.4189, -1.3706],\n",
      "        [-1.3634, -1.3933, -1.4189, -1.3705],\n",
      "        [-1.3634, -1.3932, -1.4189, -1.3707],\n",
      "        [-1.3634, -1.3933, -1.4188, -1.3706],\n",
      "        [-1.3634, -1.3932, -1.4189, -1.3706],\n",
      "        [-1.3634, -1.3934, -1.4189, -1.3705],\n",
      "        [-1.3635, -1.3932, -1.4189, -1.3704],\n",
      "        [-1.3635, -1.3933, -1.4188, -1.3705],\n",
      "        [-1.3633, -1.3932, -1.4190, -1.3705],\n",
      "        [-1.3633, -1.3933, -1.4188, -1.3708],\n",
      "        [-1.3633, -1.3934, -1.4189, -1.3705],\n",
      "        [-1.3633, -1.3935, -1.4188, -1.3705],\n",
      "        [-1.3633, -1.3934, -1.4189, -1.3705],\n",
      "        [-1.3634, -1.3933, -1.4189, -1.3705],\n",
      "        [-1.3636, -1.3931, -1.4189, -1.3705],\n",
      "        [-1.3634, -1.3934, -1.4187, -1.3706],\n",
      "        [-1.3633, -1.3933, -1.4189, -1.3706],\n",
      "        [-1.3635, -1.3931, -1.4187, -1.3708],\n",
      "        [-1.3632, -1.3934, -1.4189, -1.3706],\n",
      "        [-1.3633, -1.3935, -1.4189, -1.3705],\n",
      "        [-1.3634, -1.3935, -1.4188, -1.3704],\n",
      "        [-1.3635, -1.3931, -1.4189, -1.3707],\n",
      "        [-1.3633, -1.3933, -1.4188, -1.3707],\n",
      "        [-1.3635, -1.3932, -1.4189, -1.3705],\n",
      "        [-1.3633, -1.3934, -1.4188, -1.3705],\n",
      "        [-1.3633, -1.3933, -1.4190, -1.3705],\n",
      "        [-1.3633, -1.3932, -1.4190, -1.3706],\n",
      "        [-1.3635, -1.3933, -1.4186, -1.3707],\n",
      "        [-1.3634, -1.3932, -1.4189, -1.3705]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([ 2.5466e-04,  2.0993e-04,  1.4134e-04,  8.7819e-05, -1.9505e-04,\n",
      "        -1.7086e-04, -2.1500e-04, -2.9072e-05, -9.4422e-05, -1.0413e-04,\n",
      "        -5.5354e-05, -6.5452e-05,  2.4927e-04, -1.6364e-04, -7.0516e-05,\n",
      "        -3.7831e-05,  2.5190e-04,  1.6216e-05,  9.6117e-05,  6.7381e-05,\n",
      "        -3.3999e-05,  4.6303e-04,  6.1114e-05,  1.4833e-04,  1.8874e-04,\n",
      "         9.5951e-05, -3.2882e-05, -1.3779e-04,  2.7689e-04,  3.1950e-04,\n",
      "         1.5796e-04,  3.1030e-05,  3.1847e-04,  3.6236e-04,  3.7220e-04,\n",
      "        -1.0272e-04,  1.2929e-04,  1.6629e-04,  1.0425e-04,  4.3453e-05,\n",
      "         2.8874e-04,  1.9150e-04,  2.1371e-04,  1.7769e-04,  2.5842e-04,\n",
      "         2.5875e-05, -1.1874e-04, -4.9422e-05,  4.8921e-04,  3.8678e-04,\n",
      "         3.0305e-04,  1.7159e-04,  5.8499e-04,  1.9871e-04,  1.0011e-04,\n",
      "        -2.3788e-05,  7.4540e-05,  4.4101e-04, -3.1700e-05,  1.4000e-04,\n",
      "         7.9330e-04,  5.3467e-04,  6.7357e-04,  7.0733e-06],\n",
      "       grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(-0.0001, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2558, 0.2482, 0.2420, 0.2540],\n",
      "        [0.2558, 0.2483, 0.2420, 0.2539],\n",
      "        [0.2558, 0.2483, 0.2420, 0.2540],\n",
      "        [0.2558, 0.2483, 0.2420, 0.2539],\n",
      "        [0.2558, 0.2482, 0.2420, 0.2540],\n",
      "        [0.2558, 0.2482, 0.2420, 0.2540],\n",
      "        [0.2558, 0.2482, 0.2420, 0.2540],\n",
      "        [0.2558, 0.2483, 0.2420, 0.2539],\n",
      "        [0.2558, 0.2482, 0.2420, 0.2540],\n",
      "        [0.2558, 0.2483, 0.2420, 0.2539],\n",
      "        [0.2558, 0.2482, 0.2420, 0.2540],\n",
      "        [0.2558, 0.2483, 0.2420, 0.2539],\n",
      "        [0.2558, 0.2483, 0.2420, 0.2539],\n",
      "        [0.2558, 0.2483, 0.2420, 0.2539],\n",
      "        [0.2558, 0.2482, 0.2420, 0.2540],\n",
      "        [0.2558, 0.2482, 0.2420, 0.2540],\n",
      "        [0.2558, 0.2482, 0.2420, 0.2540],\n",
      "        [0.2558, 0.2483, 0.2420, 0.2540],\n",
      "        [0.2558, 0.2482, 0.2420, 0.2540],\n",
      "        [0.2558, 0.2483, 0.2420, 0.2540],\n",
      "        [0.2558, 0.2483, 0.2420, 0.2539],\n",
      "        [0.2558, 0.2482, 0.2420, 0.2539],\n",
      "        [0.2558, 0.2483, 0.2420, 0.2540],\n",
      "        [0.2558, 0.2482, 0.2420, 0.2540],\n",
      "        [0.2558, 0.2483, 0.2420, 0.2539],\n",
      "        [0.2558, 0.2483, 0.2420, 0.2539],\n",
      "        [0.2558, 0.2483, 0.2420, 0.2539],\n",
      "        [0.2558, 0.2482, 0.2420, 0.2539],\n",
      "        [0.2558, 0.2483, 0.2420, 0.2540],\n",
      "        [0.2558, 0.2483, 0.2420, 0.2539],\n",
      "        [0.2558, 0.2483, 0.2420, 0.2539],\n",
      "        [0.2558, 0.2483, 0.2420, 0.2539],\n",
      "        [0.2558, 0.2482, 0.2420, 0.2540],\n",
      "        [0.2558, 0.2482, 0.2420, 0.2540],\n",
      "        [0.2558, 0.2483, 0.2420, 0.2540],\n",
      "        [0.2558, 0.2483, 0.2420, 0.2540],\n",
      "        [0.2558, 0.2483, 0.2420, 0.2540],\n",
      "        [0.2558, 0.2483, 0.2420, 0.2539],\n",
      "        [0.2558, 0.2483, 0.2420, 0.2540],\n",
      "        [0.2558, 0.2483, 0.2420, 0.2540],\n",
      "        [0.2558, 0.2482, 0.2420, 0.2540],\n",
      "        [0.2558, 0.2483, 0.2420, 0.2540],\n",
      "        [0.2558, 0.2482, 0.2420, 0.2540],\n",
      "        [0.2558, 0.2483, 0.2419, 0.2540],\n",
      "        [0.2558, 0.2483, 0.2420, 0.2539],\n",
      "        [0.2558, 0.2482, 0.2420, 0.2540],\n",
      "        [0.2558, 0.2482, 0.2420, 0.2540],\n",
      "        [0.2558, 0.2482, 0.2420, 0.2540],\n",
      "        [0.2558, 0.2483, 0.2420, 0.2540],\n",
      "        [0.2557, 0.2483, 0.2420, 0.2540],\n",
      "        [0.2558, 0.2482, 0.2420, 0.2540],\n",
      "        [0.2558, 0.2482, 0.2420, 0.2540],\n",
      "        [0.2558, 0.2483, 0.2420, 0.2539],\n",
      "        [0.2558, 0.2482, 0.2420, 0.2539],\n",
      "        [0.2558, 0.2482, 0.2420, 0.2540],\n",
      "        [0.2558, 0.2482, 0.2420, 0.2540],\n",
      "        [0.2558, 0.2483, 0.2420, 0.2539],\n",
      "        [0.2558, 0.2482, 0.2420, 0.2539],\n",
      "        [0.2558, 0.2483, 0.2420, 0.2540],\n",
      "        [0.2558, 0.2482, 0.2420, 0.2540],\n",
      "        [0.2558, 0.2482, 0.2420, 0.2540],\n",
      "        [0.2558, 0.2483, 0.2420, 0.2540],\n",
      "        [0.2558, 0.2482, 0.2420, 0.2539],\n",
      "        [0.2558, 0.2483, 0.2420, 0.2540]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3861, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(-0.0140, grad_fn=<AddBackward0>)\n",
      "batching here\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(1.4474e-08, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.3661, -1.3930, -1.4146, -1.3721],\n",
      "        [-1.3661, -1.3931, -1.4146, -1.3722],\n",
      "        [-1.3661, -1.3930, -1.4146, -1.3721],\n",
      "        [-1.3661, -1.3930, -1.4146, -1.3722],\n",
      "        [-1.3660, -1.3932, -1.4146, -1.3721],\n",
      "        [-1.3661, -1.3932, -1.4144, -1.3722],\n",
      "        [-1.3661, -1.3930, -1.4146, -1.3722],\n",
      "        [-1.3661, -1.3930, -1.4145, -1.3723],\n",
      "        [-1.3660, -1.3930, -1.4145, -1.3723],\n",
      "        [-1.3662, -1.3931, -1.4146, -1.3721],\n",
      "        [-1.3661, -1.3930, -1.4147, -1.3722],\n",
      "        [-1.3661, -1.3930, -1.4146, -1.3722],\n",
      "        [-1.3661, -1.3933, -1.4145, -1.3720],\n",
      "        [-1.3661, -1.3931, -1.4145, -1.3722],\n",
      "        [-1.3661, -1.3930, -1.4145, -1.3723],\n",
      "        [-1.3661, -1.3931, -1.4145, -1.3723],\n",
      "        [-1.3660, -1.3932, -1.4145, -1.3721],\n",
      "        [-1.3661, -1.3933, -1.4145, -1.3721],\n",
      "        [-1.3661, -1.3930, -1.4145, -1.3723],\n",
      "        [-1.3661, -1.3930, -1.4147, -1.3721],\n",
      "        [-1.3661, -1.3931, -1.4145, -1.3722],\n",
      "        [-1.3661, -1.3930, -1.4147, -1.3720],\n",
      "        [-1.3662, -1.3930, -1.4146, -1.3722],\n",
      "        [-1.3661, -1.3931, -1.4146, -1.3722],\n",
      "        [-1.3661, -1.3930, -1.4146, -1.3722],\n",
      "        [-1.3662, -1.3929, -1.4145, -1.3722],\n",
      "        [-1.3660, -1.3932, -1.4145, -1.3722],\n",
      "        [-1.3660, -1.3932, -1.4146, -1.3722],\n",
      "        [-1.3661, -1.3930, -1.4146, -1.3722],\n",
      "        [-1.3662, -1.3930, -1.4145, -1.3721],\n",
      "        [-1.3662, -1.3931, -1.4146, -1.3721],\n",
      "        [-1.3661, -1.3931, -1.4146, -1.3721],\n",
      "        [-1.3661, -1.3932, -1.4145, -1.3722],\n",
      "        [-1.3662, -1.3933, -1.4145, -1.3720],\n",
      "        [-1.3662, -1.3931, -1.4145, -1.3720],\n",
      "        [-1.3662, -1.3931, -1.4145, -1.3721],\n",
      "        [-1.3660, -1.3931, -1.4146, -1.3722],\n",
      "        [-1.3661, -1.3930, -1.4146, -1.3721],\n",
      "        [-1.3663, -1.3929, -1.4146, -1.3722],\n",
      "        [-1.3661, -1.3931, -1.4145, -1.3722],\n",
      "        [-1.3660, -1.3932, -1.4146, -1.3722],\n",
      "        [-1.3661, -1.3932, -1.4145, -1.3721],\n",
      "        [-1.3661, -1.3930, -1.4146, -1.3721],\n",
      "        [-1.3662, -1.3931, -1.4146, -1.3721],\n",
      "        [-1.3661, -1.3931, -1.4146, -1.3721],\n",
      "        [-1.3662, -1.3930, -1.4146, -1.3721],\n",
      "        [-1.3661, -1.3932, -1.4146, -1.3721],\n",
      "        [-1.3661, -1.3932, -1.4146, -1.3721],\n",
      "        [-1.3660, -1.3931, -1.4146, -1.3722],\n",
      "        [-1.3661, -1.3934, -1.4145, -1.3720],\n",
      "        [-1.3662, -1.3931, -1.4145, -1.3721],\n",
      "        [-1.3662, -1.3929, -1.4145, -1.3723],\n",
      "        [-1.3661, -1.3933, -1.4145, -1.3720],\n",
      "        [-1.3660, -1.3932, -1.4146, -1.3721],\n",
      "        [-1.3661, -1.3932, -1.4145, -1.3721],\n",
      "        [-1.3662, -1.3930, -1.4145, -1.3722],\n",
      "        [-1.3661, -1.3931, -1.4145, -1.3722],\n",
      "        [-1.3661, -1.3930, -1.4147, -1.3721],\n",
      "        [-1.3661, -1.3931, -1.4146, -1.3721],\n",
      "        [-1.3661, -1.3929, -1.4146, -1.3723],\n",
      "        [-1.3660, -1.3930, -1.4146, -1.3722],\n",
      "        [-1.3662, -1.3932, -1.4145, -1.3720],\n",
      "        [-1.3662, -1.3930, -1.4147, -1.3721],\n",
      "        [-1.3661, -1.3929, -1.4145, -1.3723]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([-8.0108e-05, -1.1576e-04, -9.0856e-06, -2.1944e-05, -3.4232e-05,\n",
      "        -1.4734e-04, -3.3395e-05, -2.4903e-04,  3.2156e-04, -7.5322e-05,\n",
      "        -8.1530e-05, -2.9309e-05, -1.1028e-04, -3.3732e-04, -2.3894e-04,\n",
      "         1.5703e-05, -8.7101e-05, -8.3213e-05, -8.3103e-05, -2.2338e-04,\n",
      "         6.5196e-06, -9.7295e-06, -7.9822e-05, -1.2496e-04,  1.3343e-04,\n",
      "         1.5150e-05, -6.5250e-05,  1.3985e-04, -4.3154e-05, -1.0805e-04,\n",
      "         6.6358e-05, -1.6635e-05, -1.3064e-04, -1.6457e-04,  3.7619e-05,\n",
      "         3.4664e-04,  1.3613e-04,  2.4027e-06,  1.7101e-04, -1.4137e-04,\n",
      "        -1.7360e-04, -1.8093e-04, -9.8677e-05, -9.1757e-06, -1.3329e-04,\n",
      "        -1.1432e-04, -2.7249e-05, -2.4030e-05, -4.9048e-04, -3.8415e-04,\n",
      "        -3.0569e-04, -5.0900e-05, -1.6870e-04, -1.5352e-04, -3.5639e-05,\n",
      "        -1.0836e-04, -3.4211e-05, -4.3792e-05, -7.5842e-05, -2.6158e-05,\n",
      "        -3.8411e-04, -3.7073e-04, -1.7357e-04,  7.6255e-05],\n",
      "       grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(7.8486e-05, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2551, 0.2483, 0.2430, 0.2536],\n",
      "        [0.2551, 0.2483, 0.2430, 0.2536],\n",
      "        [0.2551, 0.2483, 0.2430, 0.2536],\n",
      "        [0.2551, 0.2483, 0.2430, 0.2536],\n",
      "        [0.2551, 0.2483, 0.2430, 0.2536],\n",
      "        [0.2551, 0.2483, 0.2431, 0.2536],\n",
      "        [0.2551, 0.2483, 0.2430, 0.2536],\n",
      "        [0.2551, 0.2483, 0.2430, 0.2535],\n",
      "        [0.2551, 0.2483, 0.2430, 0.2535],\n",
      "        [0.2551, 0.2483, 0.2430, 0.2536],\n",
      "        [0.2551, 0.2483, 0.2430, 0.2536],\n",
      "        [0.2551, 0.2483, 0.2430, 0.2535],\n",
      "        [0.2551, 0.2483, 0.2430, 0.2536],\n",
      "        [0.2551, 0.2483, 0.2430, 0.2536],\n",
      "        [0.2551, 0.2483, 0.2431, 0.2535],\n",
      "        [0.2551, 0.2483, 0.2431, 0.2535],\n",
      "        [0.2551, 0.2483, 0.2430, 0.2536],\n",
      "        [0.2551, 0.2483, 0.2430, 0.2536],\n",
      "        [0.2551, 0.2483, 0.2430, 0.2535],\n",
      "        [0.2551, 0.2483, 0.2430, 0.2536],\n",
      "        [0.2551, 0.2483, 0.2430, 0.2535],\n",
      "        [0.2551, 0.2483, 0.2430, 0.2536],\n",
      "        [0.2551, 0.2483, 0.2430, 0.2536],\n",
      "        [0.2551, 0.2483, 0.2430, 0.2535],\n",
      "        [0.2551, 0.2483, 0.2430, 0.2535],\n",
      "        [0.2551, 0.2483, 0.2430, 0.2535],\n",
      "        [0.2551, 0.2483, 0.2431, 0.2535],\n",
      "        [0.2551, 0.2483, 0.2430, 0.2536],\n",
      "        [0.2551, 0.2483, 0.2430, 0.2536],\n",
      "        [0.2551, 0.2483, 0.2430, 0.2536],\n",
      "        [0.2551, 0.2483, 0.2430, 0.2536],\n",
      "        [0.2551, 0.2483, 0.2430, 0.2536],\n",
      "        [0.2551, 0.2483, 0.2431, 0.2536],\n",
      "        [0.2551, 0.2483, 0.2430, 0.2536],\n",
      "        [0.2551, 0.2483, 0.2430, 0.2536],\n",
      "        [0.2551, 0.2483, 0.2430, 0.2536],\n",
      "        [0.2551, 0.2483, 0.2430, 0.2536],\n",
      "        [0.2551, 0.2483, 0.2430, 0.2536],\n",
      "        [0.2551, 0.2483, 0.2430, 0.2536],\n",
      "        [0.2551, 0.2483, 0.2430, 0.2536],\n",
      "        [0.2551, 0.2483, 0.2430, 0.2536],\n",
      "        [0.2551, 0.2483, 0.2430, 0.2536],\n",
      "        [0.2551, 0.2483, 0.2430, 0.2536],\n",
      "        [0.2551, 0.2483, 0.2430, 0.2536],\n",
      "        [0.2551, 0.2483, 0.2430, 0.2536],\n",
      "        [0.2551, 0.2483, 0.2430, 0.2536],\n",
      "        [0.2551, 0.2483, 0.2430, 0.2536],\n",
      "        [0.2551, 0.2483, 0.2430, 0.2536],\n",
      "        [0.2551, 0.2483, 0.2430, 0.2535],\n",
      "        [0.2551, 0.2482, 0.2431, 0.2536],\n",
      "        [0.2551, 0.2483, 0.2430, 0.2536],\n",
      "        [0.2551, 0.2484, 0.2431, 0.2535],\n",
      "        [0.2551, 0.2483, 0.2430, 0.2536],\n",
      "        [0.2551, 0.2483, 0.2430, 0.2536],\n",
      "        [0.2551, 0.2483, 0.2430, 0.2536],\n",
      "        [0.2551, 0.2483, 0.2430, 0.2535],\n",
      "        [0.2551, 0.2483, 0.2430, 0.2536],\n",
      "        [0.2551, 0.2483, 0.2430, 0.2536],\n",
      "        [0.2551, 0.2483, 0.2430, 0.2536],\n",
      "        [0.2551, 0.2483, 0.2430, 0.2535],\n",
      "        [0.2551, 0.2483, 0.2430, 0.2535],\n",
      "        [0.2551, 0.2483, 0.2430, 0.2536],\n",
      "        [0.2551, 0.2483, 0.2430, 0.2536],\n",
      "        [0.2551, 0.2483, 0.2430, 0.2535]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3861, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(-0.0138, grad_fn=<AddBackward0>)\n",
      "batching here\n",
      "5: done 13 episodes, mean_reward=0.00, best_reward=0.00, speed=164.23\n",
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(1.7629e-06, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.3682, -1.3931, -1.4112, -1.3732],\n",
      "        [-1.3682, -1.3930, -1.4113, -1.3732],\n",
      "        [-1.3683, -1.3931, -1.4112, -1.3732],\n",
      "        [-1.3681, -1.3933, -1.4112, -1.3732],\n",
      "        [-1.3683, -1.3931, -1.4111, -1.3732],\n",
      "        [-1.3683, -1.3931, -1.4112, -1.3732],\n",
      "        [-1.3683, -1.3931, -1.4112, -1.3732],\n",
      "        [-1.3684, -1.3932, -1.4111, -1.3730],\n",
      "        [-1.3683, -1.3930, -1.4112, -1.3732],\n",
      "        [-1.3683, -1.3930, -1.4112, -1.3733],\n",
      "        [-1.3683, -1.3930, -1.4112, -1.3732],\n",
      "        [-1.3682, -1.3932, -1.4113, -1.3731],\n",
      "        [-1.3683, -1.3931, -1.4112, -1.3732],\n",
      "        [-1.3683, -1.3931, -1.4112, -1.3731],\n",
      "        [-1.3682, -1.3932, -1.4111, -1.3731],\n",
      "        [-1.3680, -1.3934, -1.4112, -1.3732],\n",
      "        [-1.3682, -1.3931, -1.4112, -1.3732],\n",
      "        [-1.3681, -1.3931, -1.4113, -1.3733],\n",
      "        [-1.3683, -1.3930, -1.4113, -1.3732],\n",
      "        [-1.3681, -1.3933, -1.4111, -1.3733],\n",
      "        [-1.3683, -1.3932, -1.4112, -1.3731],\n",
      "        [-1.3683, -1.3931, -1.4112, -1.3731],\n",
      "        [-1.3682, -1.3932, -1.4112, -1.3732],\n",
      "        [-1.3684, -1.3931, -1.4111, -1.3731],\n",
      "        [-1.3682, -1.3931, -1.4113, -1.3731],\n",
      "        [-1.3683, -1.3931, -1.4112, -1.3731],\n",
      "        [-1.3683, -1.3930, -1.4113, -1.3732],\n",
      "        [-1.3681, -1.3933, -1.4111, -1.3733],\n",
      "        [-1.3683, -1.3931, -1.4112, -1.3731],\n",
      "        [-1.3683, -1.3931, -1.4112, -1.3731],\n",
      "        [-1.3683, -1.3931, -1.4112, -1.3731],\n",
      "        [-1.3682, -1.3932, -1.4113, -1.3731],\n",
      "        [-1.3681, -1.3931, -1.4112, -1.3734],\n",
      "        [-1.3682, -1.3931, -1.4111, -1.3733],\n",
      "        [-1.3681, -1.3934, -1.4110, -1.3732],\n",
      "        [-1.3681, -1.3931, -1.4112, -1.3734],\n",
      "        [-1.3682, -1.3930, -1.4114, -1.3731],\n",
      "        [-1.3682, -1.3931, -1.4113, -1.3731],\n",
      "        [-1.3683, -1.3931, -1.4113, -1.3732],\n",
      "        [-1.3680, -1.3933, -1.4112, -1.3732],\n",
      "        [-1.3683, -1.3931, -1.4113, -1.3731],\n",
      "        [-1.3683, -1.3932, -1.4112, -1.3730],\n",
      "        [-1.3683, -1.3931, -1.4112, -1.3731],\n",
      "        [-1.3684, -1.3932, -1.4112, -1.3730],\n",
      "        [-1.3682, -1.3932, -1.4112, -1.3731],\n",
      "        [-1.3682, -1.3932, -1.4112, -1.3731],\n",
      "        [-1.3682, -1.3932, -1.4112, -1.3731],\n",
      "        [-1.3683, -1.3932, -1.4112, -1.3731],\n",
      "        [-1.3681, -1.3929, -1.4113, -1.3734],\n",
      "        [-1.3683, -1.3930, -1.4111, -1.3733],\n",
      "        [-1.3682, -1.3933, -1.4110, -1.3732],\n",
      "        [-1.3681, -1.3930, -1.4113, -1.3733],\n",
      "        [-1.3682, -1.3930, -1.4112, -1.3733],\n",
      "        [-1.3683, -1.3932, -1.4112, -1.3731],\n",
      "        [-1.3683, -1.3931, -1.4112, -1.3732],\n",
      "        [-1.3682, -1.3933, -1.4112, -1.3731],\n",
      "        [-1.3683, -1.3932, -1.4112, -1.3731],\n",
      "        [-1.3683, -1.3931, -1.4112, -1.3731],\n",
      "        [-1.3683, -1.3932, -1.4111, -1.3732],\n",
      "        [-1.3683, -1.3932, -1.4112, -1.3732],\n",
      "        [-1.3679, -1.3930, -1.4114, -1.3735],\n",
      "        [-1.3683, -1.3929, -1.4112, -1.3733],\n",
      "        [-1.3682, -1.3931, -1.4112, -1.3732],\n",
      "        [-1.3681, -1.3931, -1.4113, -1.3733]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([ 2.4130e-03,  2.2474e-03,  2.2196e-03, -1.3995e-05,  2.5027e-03,\n",
      "         2.4030e-03,  2.2897e-03, -1.1600e-04,  2.2454e-03,  2.5300e-03,\n",
      "         2.4057e-03, -1.3438e-04,  2.4600e-03,  2.4420e-03,  2.6249e-03,\n",
      "        -9.1515e-05,  2.3619e-03,  2.3327e-03,  2.3208e-03, -4.0110e-05,\n",
      "         2.3063e-03,  2.2583e-03,  2.2883e-03, -1.9456e-04,  2.2717e-03,\n",
      "         1.9343e-03,  2.1818e-03, -2.0476e-05,  2.3066e-03,  2.3365e-03,\n",
      "         2.3365e-03, -1.3219e-04,  9.3728e-05,  3.3444e-04,  3.2174e-04,\n",
      "         6.2523e-04,  2.2300e-03,  2.2771e-03,  2.2165e-03,  1.4484e-05,\n",
      "         2.3781e-03,  2.3665e-03,  2.2738e-03,  1.0123e-04,  2.3750e-03,\n",
      "         2.3323e-03,  2.3750e-03, -1.6684e-04,  3.5436e-04,  3.9022e-04,\n",
      "         1.2673e-04,  5.5944e-04,  2.3207e-03,  2.5376e-03,  2.6038e-03,\n",
      "        -1.1570e-04,  2.4038e-03,  2.3513e-03,  2.5145e-03, -2.1180e-05,\n",
      "         4.6432e-04,  6.0621e-04,  2.3850e-04,  8.4581e-04],\n",
      "       grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(-0.0015, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2546, 0.2483, 0.2438, 0.2533],\n",
      "        [0.2546, 0.2483, 0.2438, 0.2533],\n",
      "        [0.2545, 0.2483, 0.2439, 0.2533],\n",
      "        [0.2546, 0.2482, 0.2439, 0.2533],\n",
      "        [0.2545, 0.2483, 0.2439, 0.2533],\n",
      "        [0.2545, 0.2483, 0.2438, 0.2533],\n",
      "        [0.2545, 0.2483, 0.2439, 0.2533],\n",
      "        [0.2545, 0.2483, 0.2439, 0.2534],\n",
      "        [0.2545, 0.2483, 0.2438, 0.2533],\n",
      "        [0.2545, 0.2483, 0.2439, 0.2533],\n",
      "        [0.2545, 0.2483, 0.2439, 0.2533],\n",
      "        [0.2546, 0.2483, 0.2438, 0.2533],\n",
      "        [0.2545, 0.2483, 0.2438, 0.2533],\n",
      "        [0.2545, 0.2483, 0.2438, 0.2533],\n",
      "        [0.2546, 0.2483, 0.2439, 0.2533],\n",
      "        [0.2546, 0.2482, 0.2439, 0.2533],\n",
      "        [0.2546, 0.2483, 0.2438, 0.2533],\n",
      "        [0.2546, 0.2483, 0.2438, 0.2533],\n",
      "        [0.2545, 0.2483, 0.2438, 0.2533],\n",
      "        [0.2546, 0.2483, 0.2439, 0.2533],\n",
      "        [0.2545, 0.2483, 0.2439, 0.2533],\n",
      "        [0.2545, 0.2483, 0.2438, 0.2533],\n",
      "        [0.2546, 0.2483, 0.2438, 0.2533],\n",
      "        [0.2545, 0.2483, 0.2439, 0.2533],\n",
      "        [0.2546, 0.2483, 0.2438, 0.2533],\n",
      "        [0.2545, 0.2483, 0.2438, 0.2533],\n",
      "        [0.2546, 0.2483, 0.2438, 0.2533],\n",
      "        [0.2546, 0.2483, 0.2439, 0.2533],\n",
      "        [0.2545, 0.2483, 0.2438, 0.2533],\n",
      "        [0.2545, 0.2483, 0.2438, 0.2533],\n",
      "        [0.2545, 0.2483, 0.2438, 0.2533],\n",
      "        [0.2546, 0.2483, 0.2438, 0.2533],\n",
      "        [0.2546, 0.2483, 0.2438, 0.2532],\n",
      "        [0.2546, 0.2483, 0.2439, 0.2533],\n",
      "        [0.2546, 0.2482, 0.2439, 0.2533],\n",
      "        [0.2546, 0.2483, 0.2438, 0.2532],\n",
      "        [0.2546, 0.2483, 0.2438, 0.2533],\n",
      "        [0.2546, 0.2483, 0.2438, 0.2533],\n",
      "        [0.2545, 0.2483, 0.2438, 0.2533],\n",
      "        [0.2546, 0.2482, 0.2438, 0.2533],\n",
      "        [0.2545, 0.2483, 0.2438, 0.2533],\n",
      "        [0.2545, 0.2483, 0.2438, 0.2533],\n",
      "        [0.2545, 0.2483, 0.2438, 0.2533],\n",
      "        [0.2545, 0.2483, 0.2439, 0.2534],\n",
      "        [0.2546, 0.2483, 0.2438, 0.2533],\n",
      "        [0.2546, 0.2483, 0.2438, 0.2533],\n",
      "        [0.2546, 0.2483, 0.2438, 0.2533],\n",
      "        [0.2545, 0.2483, 0.2438, 0.2533],\n",
      "        [0.2546, 0.2483, 0.2438, 0.2532],\n",
      "        [0.2545, 0.2483, 0.2439, 0.2533],\n",
      "        [0.2546, 0.2483, 0.2439, 0.2533],\n",
      "        [0.2546, 0.2483, 0.2438, 0.2533],\n",
      "        [0.2546, 0.2483, 0.2439, 0.2533],\n",
      "        [0.2545, 0.2483, 0.2438, 0.2533],\n",
      "        [0.2545, 0.2483, 0.2439, 0.2533],\n",
      "        [0.2546, 0.2483, 0.2438, 0.2533],\n",
      "        [0.2545, 0.2483, 0.2439, 0.2533],\n",
      "        [0.2545, 0.2483, 0.2439, 0.2533],\n",
      "        [0.2546, 0.2483, 0.2439, 0.2533],\n",
      "        [0.2545, 0.2483, 0.2439, 0.2533],\n",
      "        [0.2546, 0.2483, 0.2438, 0.2532],\n",
      "        [0.2545, 0.2483, 0.2439, 0.2533],\n",
      "        [0.2546, 0.2483, 0.2438, 0.2533],\n",
      "        [0.2546, 0.2483, 0.2438, 0.2533]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3861, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(-0.0154, grad_fn=<AddBackward0>)\n",
      "batching here\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(1.3824e-06, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.3666, -1.3901, -1.4151, -1.3741],\n",
      "        [-1.3667, -1.3898, -1.4152, -1.3742],\n",
      "        [-1.3666, -1.3900, -1.4151, -1.3741],\n",
      "        [-1.3669, -1.3896, -1.4153, -1.3741],\n",
      "        [-1.3668, -1.3901, -1.4151, -1.3738],\n",
      "        [-1.3669, -1.3899, -1.4150, -1.3740],\n",
      "        [-1.3669, -1.3898, -1.4153, -1.3739],\n",
      "        [-1.3670, -1.3898, -1.4151, -1.3739],\n",
      "        [-1.3668, -1.3898, -1.4152, -1.3740],\n",
      "        [-1.3668, -1.3895, -1.4156, -1.3740],\n",
      "        [-1.3666, -1.3899, -1.4152, -1.3741],\n",
      "        [-1.3668, -1.3899, -1.4153, -1.3739],\n",
      "        [-1.3665, -1.3900, -1.4152, -1.3741],\n",
      "        [-1.3667, -1.3897, -1.4153, -1.3742],\n",
      "        [-1.3666, -1.3900, -1.4152, -1.3741],\n",
      "        [-1.3669, -1.3896, -1.4153, -1.3741],\n",
      "        [-1.3665, -1.3901, -1.4151, -1.3741],\n",
      "        [-1.3667, -1.3898, -1.4152, -1.3742],\n",
      "        [-1.3667, -1.3899, -1.4150, -1.3743],\n",
      "        [-1.3670, -1.3895, -1.4153, -1.3741],\n",
      "        [-1.3669, -1.3897, -1.4151, -1.3741],\n",
      "        [-1.3669, -1.3897, -1.4152, -1.3741],\n",
      "        [-1.3670, -1.3898, -1.4150, -1.3741],\n",
      "        [-1.3668, -1.3900, -1.4152, -1.3740],\n",
      "        [-1.3665, -1.3900, -1.4151, -1.3742],\n",
      "        [-1.3667, -1.3898, -1.4152, -1.3742],\n",
      "        [-1.3667, -1.3900, -1.4151, -1.3741],\n",
      "        [-1.3670, -1.3896, -1.4153, -1.3740],\n",
      "        [-1.3668, -1.3899, -1.4152, -1.3740],\n",
      "        [-1.3668, -1.3898, -1.4154, -1.3739],\n",
      "        [-1.3667, -1.3898, -1.4153, -1.3740],\n",
      "        [-1.3668, -1.3900, -1.4152, -1.3738],\n",
      "        [-1.3667, -1.3897, -1.4155, -1.3740],\n",
      "        [-1.3670, -1.3897, -1.4152, -1.3740],\n",
      "        [-1.3666, -1.3898, -1.4154, -1.3740],\n",
      "        [-1.3669, -1.3899, -1.4152, -1.3739],\n",
      "        [-1.3666, -1.3901, -1.4152, -1.3740],\n",
      "        [-1.3667, -1.3898, -1.4153, -1.3741],\n",
      "        [-1.3667, -1.3901, -1.4151, -1.3740],\n",
      "        [-1.3669, -1.3896, -1.4153, -1.3740],\n",
      "        [-1.3669, -1.3900, -1.4152, -1.3737],\n",
      "        [-1.3669, -1.3899, -1.4151, -1.3740],\n",
      "        [-1.3669, -1.3900, -1.4153, -1.3737],\n",
      "        [-1.3670, -1.3900, -1.4149, -1.3739],\n",
      "        [-1.3668, -1.3898, -1.4153, -1.3740],\n",
      "        [-1.3668, -1.3898, -1.4152, -1.3740],\n",
      "        [-1.3669, -1.3899, -1.4151, -1.3739],\n",
      "        [-1.3667, -1.3899, -1.4153, -1.3739],\n",
      "        [-1.3666, -1.3897, -1.4155, -1.3740],\n",
      "        [-1.3670, -1.3897, -1.4152, -1.3739],\n",
      "        [-1.3667, -1.3898, -1.4153, -1.3740],\n",
      "        [-1.3668, -1.3899, -1.4152, -1.3739],\n",
      "        [-1.3668, -1.3899, -1.4152, -1.3741],\n",
      "        [-1.3668, -1.3897, -1.4154, -1.3739],\n",
      "        [-1.3667, -1.3898, -1.4153, -1.3741],\n",
      "        [-1.3668, -1.3899, -1.4153, -1.3739],\n",
      "        [-1.3668, -1.3898, -1.4152, -1.3741],\n",
      "        [-1.3668, -1.3897, -1.4152, -1.3742],\n",
      "        [-1.3669, -1.3898, -1.4152, -1.3739],\n",
      "        [-1.3666, -1.3899, -1.4153, -1.3740],\n",
      "        [-1.3665, -1.3898, -1.4155, -1.3740],\n",
      "        [-1.3669, -1.3898, -1.4153, -1.3739],\n",
      "        [-1.3666, -1.3897, -1.4156, -1.3740],\n",
      "        [-1.3668, -1.3898, -1.4153, -1.3739]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([-2.4491e-03, -1.9173e-03, -1.5599e-03, -1.0905e-03, -1.7677e-03,\n",
      "        -1.3282e-03, -6.8237e-04, -1.1125e-04, -2.1021e-03, -1.7998e-03,\n",
      "        -1.0935e-03, -6.0859e-04, -2.4373e-03, -2.1627e-03, -1.7938e-03,\n",
      "        -1.1588e-03, -2.4674e-03, -1.8987e-03, -1.6857e-03, -1.1951e-03,\n",
      "        -2.0617e-03, -1.5954e-03, -1.1948e-03, -4.1186e-04, -2.4530e-03,\n",
      "        -1.9684e-03, -1.6566e-03, -1.0356e-03, -2.1545e-03, -1.7794e-03,\n",
      "        -1.2202e-03, -5.5762e-04, -2.5365e-03, -1.5659e-03, -1.1866e-03,\n",
      "        -8.4811e-04, -2.3772e-03, -1.8573e-03, -1.5086e-03, -1.1015e-03,\n",
      "        -1.8934e-03, -1.5283e-03, -6.8657e-04, -2.0160e-05, -1.9168e-03,\n",
      "        -1.7393e-03, -1.2093e-03, -6.1374e-04, -2.5571e-03, -1.5085e-03,\n",
      "        -1.1446e-03, -8.6965e-04, -2.0688e-03, -1.7769e-03, -1.2975e-03,\n",
      "        -7.2100e-04, -2.1637e-03, -1.6297e-03, -1.6642e-03, -5.4832e-04,\n",
      "        -2.6693e-03, -1.7206e-03, -1.3830e-03, -8.7429e-04],\n",
      "       grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(0.0015, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2550, 0.2491, 0.2429, 0.2531],\n",
      "        [0.2550, 0.2491, 0.2429, 0.2530],\n",
      "        [0.2550, 0.2491, 0.2429, 0.2531],\n",
      "        [0.2549, 0.2492, 0.2429, 0.2531],\n",
      "        [0.2549, 0.2490, 0.2429, 0.2531],\n",
      "        [0.2549, 0.2491, 0.2429, 0.2531],\n",
      "        [0.2549, 0.2491, 0.2429, 0.2531],\n",
      "        [0.2549, 0.2491, 0.2429, 0.2531],\n",
      "        [0.2549, 0.2491, 0.2429, 0.2531],\n",
      "        [0.2549, 0.2492, 0.2428, 0.2531],\n",
      "        [0.2550, 0.2491, 0.2429, 0.2531],\n",
      "        [0.2549, 0.2491, 0.2428, 0.2531],\n",
      "        [0.2550, 0.2491, 0.2429, 0.2531],\n",
      "        [0.2549, 0.2491, 0.2429, 0.2531],\n",
      "        [0.2550, 0.2491, 0.2429, 0.2531],\n",
      "        [0.2549, 0.2492, 0.2429, 0.2531],\n",
      "        [0.2550, 0.2491, 0.2429, 0.2531],\n",
      "        [0.2550, 0.2491, 0.2429, 0.2530],\n",
      "        [0.2550, 0.2491, 0.2429, 0.2530],\n",
      "        [0.2549, 0.2492, 0.2428, 0.2531],\n",
      "        [0.2549, 0.2491, 0.2429, 0.2531],\n",
      "        [0.2549, 0.2492, 0.2429, 0.2531],\n",
      "        [0.2549, 0.2491, 0.2429, 0.2531],\n",
      "        [0.2549, 0.2491, 0.2429, 0.2531],\n",
      "        [0.2550, 0.2491, 0.2429, 0.2531],\n",
      "        [0.2550, 0.2491, 0.2429, 0.2530],\n",
      "        [0.2550, 0.2491, 0.2429, 0.2531],\n",
      "        [0.2549, 0.2492, 0.2429, 0.2531],\n",
      "        [0.2549, 0.2491, 0.2429, 0.2531],\n",
      "        [0.2549, 0.2491, 0.2428, 0.2531],\n",
      "        [0.2549, 0.2491, 0.2429, 0.2531],\n",
      "        [0.2549, 0.2491, 0.2429, 0.2531],\n",
      "        [0.2550, 0.2491, 0.2428, 0.2531],\n",
      "        [0.2549, 0.2492, 0.2429, 0.2531],\n",
      "        [0.2550, 0.2491, 0.2428, 0.2531],\n",
      "        [0.2549, 0.2491, 0.2429, 0.2531],\n",
      "        [0.2550, 0.2491, 0.2429, 0.2531],\n",
      "        [0.2550, 0.2491, 0.2429, 0.2531],\n",
      "        [0.2549, 0.2491, 0.2429, 0.2531],\n",
      "        [0.2549, 0.2492, 0.2429, 0.2531],\n",
      "        [0.2549, 0.2491, 0.2429, 0.2532],\n",
      "        [0.2549, 0.2491, 0.2429, 0.2531],\n",
      "        [0.2549, 0.2491, 0.2429, 0.2532],\n",
      "        [0.2549, 0.2491, 0.2429, 0.2531],\n",
      "        [0.2549, 0.2491, 0.2429, 0.2531],\n",
      "        [0.2549, 0.2491, 0.2429, 0.2531],\n",
      "        [0.2549, 0.2491, 0.2429, 0.2531],\n",
      "        [0.2549, 0.2491, 0.2428, 0.2531],\n",
      "        [0.2550, 0.2491, 0.2428, 0.2531],\n",
      "        [0.2549, 0.2491, 0.2429, 0.2531],\n",
      "        [0.2550, 0.2491, 0.2428, 0.2531],\n",
      "        [0.2549, 0.2491, 0.2429, 0.2531],\n",
      "        [0.2549, 0.2491, 0.2429, 0.2531],\n",
      "        [0.2549, 0.2492, 0.2428, 0.2531],\n",
      "        [0.2549, 0.2491, 0.2429, 0.2531],\n",
      "        [0.2549, 0.2491, 0.2429, 0.2531],\n",
      "        [0.2549, 0.2491, 0.2429, 0.2531],\n",
      "        [0.2549, 0.2491, 0.2429, 0.2531],\n",
      "        [0.2549, 0.2491, 0.2429, 0.2531],\n",
      "        [0.2550, 0.2491, 0.2428, 0.2531],\n",
      "        [0.2550, 0.2491, 0.2428, 0.2531],\n",
      "        [0.2549, 0.2491, 0.2429, 0.2531],\n",
      "        [0.2550, 0.2492, 0.2428, 0.2531],\n",
      "        [0.2549, 0.2491, 0.2428, 0.2531]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3861, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(-0.0124, grad_fn=<AddBackward0>)\n",
      "batching here\n",
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(0.0929, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.3725, -1.3933, -1.4090, -1.3708],\n",
      "        [-1.3724, -1.3931, -1.4092, -1.3709],\n",
      "        [-1.3724, -1.3932, -1.4093, -1.3708],\n",
      "        [-1.3724, -1.3934, -1.4091, -1.3708],\n",
      "        [-1.3725, -1.3936, -1.4091, -1.3705],\n",
      "        [-1.3725, -1.3928, -1.4093, -1.3711],\n",
      "        [-1.3727, -1.3930, -1.4092, -1.3707],\n",
      "        [-1.3723, -1.3933, -1.4093, -1.3708],\n",
      "        [-1.3723, -1.3931, -1.4092, -1.3710],\n",
      "        [-1.3723, -1.3933, -1.4092, -1.3708],\n",
      "        [-1.3725, -1.3932, -1.4091, -1.3709],\n",
      "        [-1.3725, -1.3933, -1.4092, -1.3707],\n",
      "        [-1.3723, -1.3933, -1.4092, -1.3709],\n",
      "        [-1.3723, -1.3931, -1.4093, -1.3710],\n",
      "        [-1.3724, -1.3932, -1.4093, -1.3708],\n",
      "        [-1.3724, -1.3935, -1.4091, -1.3707],\n",
      "        [-1.3726, -1.3932, -1.4090, -1.3708],\n",
      "        [-1.3724, -1.3931, -1.4092, -1.3709],\n",
      "        [-1.3724, -1.3930, -1.4095, -1.3709],\n",
      "        [-1.3725, -1.3933, -1.4092, -1.3708],\n",
      "        [-1.3724, -1.3930, -1.4093, -1.3710],\n",
      "        [-1.3724, -1.3934, -1.4088, -1.3710],\n",
      "        [-1.3726, -1.3932, -1.4091, -1.3708],\n",
      "        [-1.3724, -1.3934, -1.4091, -1.3708],\n",
      "        [-1.3725, -1.3933, -1.4090, -1.3708],\n",
      "        [-1.3724, -1.3932, -1.4092, -1.3709],\n",
      "        [-1.3724, -1.3932, -1.4093, -1.3708],\n",
      "        [-1.3724, -1.3934, -1.4091, -1.3708],\n",
      "        [-1.3723, -1.3932, -1.4092, -1.3709],\n",
      "        [-1.3723, -1.3934, -1.4092, -1.3708],\n",
      "        [-1.3724, -1.3932, -1.4091, -1.3710],\n",
      "        [-1.3725, -1.3933, -1.4092, -1.3708],\n",
      "        [-1.3725, -1.3933, -1.4090, -1.3708],\n",
      "        [-1.3724, -1.3932, -1.4091, -1.3710],\n",
      "        [-1.3723, -1.3933, -1.4090, -1.3710],\n",
      "        [-1.3725, -1.3935, -1.4089, -1.3708],\n",
      "        [-1.3725, -1.3933, -1.4091, -1.3708],\n",
      "        [-1.3724, -1.3932, -1.4092, -1.3709],\n",
      "        [-1.3724, -1.3932, -1.4093, -1.3708],\n",
      "        [-1.3724, -1.3934, -1.4091, -1.3708],\n",
      "        [-1.3726, -1.3935, -1.4092, -1.3704],\n",
      "        [-1.3725, -1.3929, -1.4092, -1.3711],\n",
      "        [-1.3726, -1.3931, -1.4093, -1.3706],\n",
      "        [-1.3723, -1.3933, -1.4093, -1.3708],\n",
      "        [-1.3724, -1.3930, -1.4093, -1.3710],\n",
      "        [-1.3723, -1.3935, -1.4089, -1.3710],\n",
      "        [-1.3726, -1.3931, -1.4092, -1.3708],\n",
      "        [-1.3724, -1.3933, -1.4092, -1.3708],\n",
      "        [-1.3725, -1.3933, -1.4090, -1.3709],\n",
      "        [-1.3724, -1.3933, -1.4090, -1.3710],\n",
      "        [-1.3724, -1.3933, -1.4090, -1.3710],\n",
      "        [-1.3724, -1.3935, -1.4088, -1.3709],\n",
      "        [-1.3722, -1.3932, -1.4093, -1.3709],\n",
      "        [-1.3723, -1.3933, -1.4093, -1.3708],\n",
      "        [-1.3725, -1.3930, -1.4092, -1.3710],\n",
      "        [-1.3724, -1.3932, -1.4093, -1.3708],\n",
      "        [-1.3722, -1.3931, -1.4095, -1.3709],\n",
      "        [-1.3725, -1.3934, -1.4090, -1.3708],\n",
      "        [-1.3726, -1.3931, -1.4092, -1.3708],\n",
      "        [-1.3724, -1.3933, -1.4092, -1.3708],\n",
      "        [-1.3725, -1.3932, -1.4091, -1.3709],\n",
      "        [-1.3724, -1.3932, -1.4090, -1.3711],\n",
      "        [-1.3724, -1.3933, -1.4090, -1.3710],\n",
      "        [-1.3725, -1.3933, -1.4089, -1.3709]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([-3.3138e-04, -4.8321e-04, -3.3393e-04, -4.1259e-04, -6.3991e-04,\n",
      "        -5.2211e-04, -3.6857e-04, -1.9515e-04, -3.3562e-04, -8.2699e-04,\n",
      "        -3.9213e-04, -2.4167e-04, -3.0107e-05, -1.9282e-04, -3.0777e-04,\n",
      "        -1.9016e-04,  2.7076e-05, -3.7523e-04, -3.5127e-04, -3.8473e-04,\n",
      "        -5.4604e-04, -3.1392e-04, -5.2194e-04, -2.6070e-04, -2.3449e-04,\n",
      "        -4.0674e-04, -2.6460e-04, -4.0031e-04, -2.8409e-04, -7.2953e-04,\n",
      "        -4.1489e-04, -2.0710e-04, -1.3795e+00, -1.3725e+00, -1.9801e-04,\n",
      "        -6.2018e-05, -2.3264e-04, -4.0744e-04, -2.6796e-04, -4.0069e-04,\n",
      "        -6.0104e-04, -3.3642e-04, -3.5941e-04, -1.9019e-04, -7.5786e-04,\n",
      "        -5.1322e-04, -7.1705e-04, -4.7320e-04, -1.3953e+00, -1.3936e+00,\n",
      "        -3.8349e-04,  3.1245e-05, -3.6103e-04, -6.3036e-04, -3.3187e-04,\n",
      "        -3.3467e-04, -5.4088e-04, -8.1085e-04, -6.6395e-04, -5.0612e-04,\n",
      "        -1.3954e+00, -1.3725e+00, -3.9644e-04, -1.7534e-04],\n",
      "       grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(0.1302, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2535, 0.2482, 0.2444, 0.2539],\n",
      "        [0.2535, 0.2483, 0.2443, 0.2539],\n",
      "        [0.2535, 0.2483, 0.2443, 0.2539],\n",
      "        [0.2535, 0.2482, 0.2444, 0.2539],\n",
      "        [0.2535, 0.2482, 0.2444, 0.2540],\n",
      "        [0.2535, 0.2484, 0.2443, 0.2538],\n",
      "        [0.2534, 0.2483, 0.2443, 0.2539],\n",
      "        [0.2535, 0.2483, 0.2443, 0.2539],\n",
      "        [0.2535, 0.2483, 0.2443, 0.2538],\n",
      "        [0.2535, 0.2483, 0.2443, 0.2539],\n",
      "        [0.2535, 0.2483, 0.2444, 0.2539],\n",
      "        [0.2535, 0.2483, 0.2443, 0.2539],\n",
      "        [0.2535, 0.2483, 0.2443, 0.2539],\n",
      "        [0.2535, 0.2483, 0.2443, 0.2538],\n",
      "        [0.2535, 0.2483, 0.2443, 0.2539],\n",
      "        [0.2535, 0.2482, 0.2444, 0.2539],\n",
      "        [0.2534, 0.2483, 0.2444, 0.2539],\n",
      "        [0.2535, 0.2483, 0.2443, 0.2539],\n",
      "        [0.2535, 0.2483, 0.2443, 0.2539],\n",
      "        [0.2535, 0.2483, 0.2443, 0.2539],\n",
      "        [0.2535, 0.2483, 0.2443, 0.2539],\n",
      "        [0.2535, 0.2482, 0.2444, 0.2539],\n",
      "        [0.2534, 0.2483, 0.2444, 0.2539],\n",
      "        [0.2535, 0.2482, 0.2444, 0.2539],\n",
      "        [0.2535, 0.2482, 0.2444, 0.2539],\n",
      "        [0.2535, 0.2483, 0.2443, 0.2539],\n",
      "        [0.2535, 0.2483, 0.2443, 0.2539],\n",
      "        [0.2535, 0.2482, 0.2444, 0.2539],\n",
      "        [0.2535, 0.2483, 0.2443, 0.2539],\n",
      "        [0.2535, 0.2482, 0.2443, 0.2539],\n",
      "        [0.2535, 0.2483, 0.2444, 0.2539],\n",
      "        [0.2535, 0.2483, 0.2443, 0.2539],\n",
      "        [0.2535, 0.2483, 0.2444, 0.2539],\n",
      "        [0.2535, 0.2483, 0.2444, 0.2538],\n",
      "        [0.2535, 0.2483, 0.2444, 0.2538],\n",
      "        [0.2535, 0.2482, 0.2444, 0.2539],\n",
      "        [0.2535, 0.2482, 0.2444, 0.2539],\n",
      "        [0.2535, 0.2483, 0.2443, 0.2539],\n",
      "        [0.2535, 0.2483, 0.2443, 0.2539],\n",
      "        [0.2535, 0.2482, 0.2444, 0.2539],\n",
      "        [0.2534, 0.2482, 0.2443, 0.2540],\n",
      "        [0.2535, 0.2484, 0.2443, 0.2538],\n",
      "        [0.2534, 0.2483, 0.2443, 0.2539],\n",
      "        [0.2535, 0.2483, 0.2443, 0.2539],\n",
      "        [0.2535, 0.2483, 0.2443, 0.2538],\n",
      "        [0.2535, 0.2482, 0.2444, 0.2539],\n",
      "        [0.2535, 0.2483, 0.2443, 0.2539],\n",
      "        [0.2535, 0.2482, 0.2443, 0.2539],\n",
      "        [0.2535, 0.2483, 0.2444, 0.2539],\n",
      "        [0.2535, 0.2483, 0.2444, 0.2539],\n",
      "        [0.2535, 0.2483, 0.2444, 0.2539],\n",
      "        [0.2535, 0.2482, 0.2444, 0.2539],\n",
      "        [0.2535, 0.2483, 0.2443, 0.2539],\n",
      "        [0.2535, 0.2483, 0.2443, 0.2539],\n",
      "        [0.2535, 0.2483, 0.2443, 0.2539],\n",
      "        [0.2535, 0.2483, 0.2443, 0.2539],\n",
      "        [0.2536, 0.2483, 0.2443, 0.2539],\n",
      "        [0.2535, 0.2482, 0.2444, 0.2539],\n",
      "        [0.2535, 0.2483, 0.2443, 0.2539],\n",
      "        [0.2535, 0.2483, 0.2443, 0.2539],\n",
      "        [0.2535, 0.2483, 0.2444, 0.2539],\n",
      "        [0.2535, 0.2483, 0.2444, 0.2538],\n",
      "        [0.2535, 0.2483, 0.2444, 0.2539],\n",
      "        [0.2535, 0.2482, 0.2444, 0.2539]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3862, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(0.2092, grad_fn=<AddBackward0>)\n",
      "batching here\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.2955, -1.2911, -1.3609, -1.6356],\n",
      "        [-1.2954, -1.2910, -1.3612, -1.6355],\n",
      "        [-1.2957, -1.2911, -1.3610, -1.6352],\n",
      "        [-1.2954, -1.2910, -1.3611, -1.6355],\n",
      "        [-1.2955, -1.2912, -1.3611, -1.6351],\n",
      "        [-1.2955, -1.2912, -1.3610, -1.6353],\n",
      "        [-1.2954, -1.2912, -1.3612, -1.6352],\n",
      "        [-1.2956, -1.2910, -1.3608, -1.6357],\n",
      "        [-1.2955, -1.2911, -1.3610, -1.6354],\n",
      "        [-1.2956, -1.2913, -1.3607, -1.6354],\n",
      "        [-1.2955, -1.2909, -1.3610, -1.6356],\n",
      "        [-1.2956, -1.2911, -1.3610, -1.6354],\n",
      "        [-1.2954, -1.2914, -1.3608, -1.6355],\n",
      "        [-1.2953, -1.2912, -1.3610, -1.6356],\n",
      "        [-1.2954, -1.2914, -1.3609, -1.6353],\n",
      "        [-1.2954, -1.2910, -1.3612, -1.6354],\n",
      "        [-1.2957, -1.2912, -1.3607, -1.6355],\n",
      "        [-1.2956, -1.2909, -1.3610, -1.6355],\n",
      "        [-1.2957, -1.2912, -1.3608, -1.6354],\n",
      "        [-1.2956, -1.2911, -1.3609, -1.6354],\n",
      "        [-1.2954, -1.2912, -1.3611, -1.6353],\n",
      "        [-1.2956, -1.2910, -1.3610, -1.6355],\n",
      "        [-1.2952, -1.2912, -1.3611, -1.6357],\n",
      "        [-1.2956, -1.2911, -1.3609, -1.6354],\n",
      "        [-1.2956, -1.2913, -1.3608, -1.6354],\n",
      "        [-1.2955, -1.2910, -1.3610, -1.6355],\n",
      "        [-1.2956, -1.2912, -1.3609, -1.6353],\n",
      "        [-1.2956, -1.2911, -1.3610, -1.6353],\n",
      "        [-1.2955, -1.2911, -1.3610, -1.6353],\n",
      "        [-1.2958, -1.2912, -1.3607, -1.6353],\n",
      "        [-1.2956, -1.2909, -1.3611, -1.6355],\n",
      "        [-1.2956, -1.2913, -1.3609, -1.6352],\n",
      "        [-1.2957, -1.2912, -1.3606, -1.6354],\n",
      "        [-1.2958, -1.2913, -1.3608, -1.6351],\n",
      "        [-1.2958, -1.2914, -1.3607, -1.6350],\n",
      "        [-1.2958, -1.2915, -1.3607, -1.6349],\n",
      "        [-1.2956, -1.2913, -1.3608, -1.6354],\n",
      "        [-1.2955, -1.2910, -1.3610, -1.6355],\n",
      "        [-1.2956, -1.2912, -1.3609, -1.6353],\n",
      "        [-1.2956, -1.2911, -1.3610, -1.6353],\n",
      "        [-1.2956, -1.2911, -1.3611, -1.6351],\n",
      "        [-1.2957, -1.2911, -1.3609, -1.6353],\n",
      "        [-1.2955, -1.2912, -1.3611, -1.6351],\n",
      "        [-1.2956, -1.2910, -1.3608, -1.6356],\n",
      "        [-1.2954, -1.2913, -1.3612, -1.6350],\n",
      "        [-1.2955, -1.2912, -1.3610, -1.6353],\n",
      "        [-1.2951, -1.2912, -1.3612, -1.6356],\n",
      "        [-1.2955, -1.2912, -1.3610, -1.6355],\n",
      "        [-1.2957, -1.2914, -1.3607, -1.6352],\n",
      "        [-1.2956, -1.2914, -1.3608, -1.6352],\n",
      "        [-1.2957, -1.2913, -1.3608, -1.6352],\n",
      "        [-1.2955, -1.2916, -1.3608, -1.6351],\n",
      "        [-1.2954, -1.2910, -1.3611, -1.6356],\n",
      "        [-1.2956, -1.2912, -1.3607, -1.6355],\n",
      "        [-1.2955, -1.2908, -1.3612, -1.6356],\n",
      "        [-1.2954, -1.2913, -1.3610, -1.6353],\n",
      "        [-1.2952, -1.2913, -1.3612, -1.6352],\n",
      "        [-1.2954, -1.2911, -1.3610, -1.6355],\n",
      "        [-1.2952, -1.2911, -1.3611, -1.6357],\n",
      "        [-1.2957, -1.2909, -1.3609, -1.6355],\n",
      "        [-1.2958, -1.2913, -1.3607, -1.6352],\n",
      "        [-1.2958, -1.2912, -1.3608, -1.6351],\n",
      "        [-1.2957, -1.2914, -1.3608, -1.6350],\n",
      "        [-1.2957, -1.2915, -1.3607, -1.6350]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([0.0636, 0.0403, 0.0264, 0.0129, 0.0535, 0.0480, 0.0273, 0.0132, 0.0500,\n",
      "        0.0476, 0.0255, 0.0125, 0.0498, 0.0398, 0.0246, 0.0157, 0.0498, 0.0404,\n",
      "        0.0250, 0.0126, 0.0527, 0.0375, 0.0254, 0.0155, 0.0526, 0.0482, 0.0261,\n",
      "        0.0133, 0.0502, 0.0477, 0.0321, 0.0123, 0.0509, 0.0372, 0.0251, 0.0155,\n",
      "        0.0501, 0.0482, 0.0249, 0.0126, 0.0507, 0.0476, 0.0257, 0.0132, 0.0507,\n",
      "        0.0378, 0.0328, 0.0128, 0.0536, 0.0379, 0.0254, 0.0125, 0.0533, 0.0383,\n",
      "        0.0260, 0.0132, 0.0503, 0.0375, 0.0256, 0.0124, 0.0510, 0.0373, 0.0262,\n",
      "        0.0123], grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(-0.0335, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2738, 0.2750, 0.2564, 0.1948],\n",
      "        [0.2738, 0.2750, 0.2564, 0.1949],\n",
      "        [0.2737, 0.2750, 0.2564, 0.1949],\n",
      "        [0.2738, 0.2750, 0.2564, 0.1949],\n",
      "        [0.2738, 0.2750, 0.2564, 0.1949],\n",
      "        [0.2738, 0.2750, 0.2564, 0.1949],\n",
      "        [0.2738, 0.2749, 0.2564, 0.1949],\n",
      "        [0.2737, 0.2750, 0.2565, 0.1948],\n",
      "        [0.2738, 0.2750, 0.2564, 0.1949],\n",
      "        [0.2737, 0.2749, 0.2565, 0.1949],\n",
      "        [0.2738, 0.2750, 0.2564, 0.1948],\n",
      "        [0.2737, 0.2750, 0.2564, 0.1949],\n",
      "        [0.2738, 0.2749, 0.2565, 0.1949],\n",
      "        [0.2738, 0.2749, 0.2564, 0.1948],\n",
      "        [0.2738, 0.2749, 0.2564, 0.1949],\n",
      "        [0.2738, 0.2750, 0.2564, 0.1949],\n",
      "        [0.2737, 0.2750, 0.2565, 0.1949],\n",
      "        [0.2737, 0.2750, 0.2564, 0.1948],\n",
      "        [0.2737, 0.2749, 0.2565, 0.1949],\n",
      "        [0.2737, 0.2750, 0.2564, 0.1949],\n",
      "        [0.2738, 0.2749, 0.2564, 0.1949],\n",
      "        [0.2737, 0.2750, 0.2564, 0.1949],\n",
      "        [0.2738, 0.2749, 0.2564, 0.1948],\n",
      "        [0.2737, 0.2750, 0.2564, 0.1949],\n",
      "        [0.2737, 0.2749, 0.2565, 0.1949],\n",
      "        [0.2738, 0.2750, 0.2564, 0.1949],\n",
      "        [0.2737, 0.2749, 0.2564, 0.1949],\n",
      "        [0.2737, 0.2750, 0.2564, 0.1949],\n",
      "        [0.2738, 0.2750, 0.2564, 0.1949],\n",
      "        [0.2737, 0.2749, 0.2565, 0.1949],\n",
      "        [0.2737, 0.2750, 0.2564, 0.1949],\n",
      "        [0.2737, 0.2749, 0.2564, 0.1949],\n",
      "        [0.2737, 0.2749, 0.2565, 0.1949],\n",
      "        [0.2737, 0.2749, 0.2564, 0.1949],\n",
      "        [0.2737, 0.2749, 0.2565, 0.1950],\n",
      "        [0.2737, 0.2749, 0.2565, 0.1950],\n",
      "        [0.2737, 0.2749, 0.2565, 0.1949],\n",
      "        [0.2738, 0.2750, 0.2564, 0.1949],\n",
      "        [0.2737, 0.2749, 0.2564, 0.1949],\n",
      "        [0.2737, 0.2750, 0.2564, 0.1949],\n",
      "        [0.2737, 0.2750, 0.2564, 0.1949],\n",
      "        [0.2737, 0.2750, 0.2564, 0.1949],\n",
      "        [0.2738, 0.2749, 0.2564, 0.1949],\n",
      "        [0.2737, 0.2750, 0.2564, 0.1948],\n",
      "        [0.2738, 0.2749, 0.2564, 0.1949],\n",
      "        [0.2738, 0.2750, 0.2564, 0.1949],\n",
      "        [0.2739, 0.2749, 0.2564, 0.1948],\n",
      "        [0.2738, 0.2750, 0.2564, 0.1949],\n",
      "        [0.2737, 0.2749, 0.2565, 0.1949],\n",
      "        [0.2737, 0.2749, 0.2565, 0.1949],\n",
      "        [0.2737, 0.2749, 0.2565, 0.1949],\n",
      "        [0.2738, 0.2748, 0.2565, 0.1949],\n",
      "        [0.2738, 0.2750, 0.2564, 0.1948],\n",
      "        [0.2737, 0.2749, 0.2565, 0.1949],\n",
      "        [0.2738, 0.2751, 0.2563, 0.1948],\n",
      "        [0.2738, 0.2749, 0.2564, 0.1949],\n",
      "        [0.2738, 0.2749, 0.2564, 0.1949],\n",
      "        [0.2738, 0.2750, 0.2564, 0.1949],\n",
      "        [0.2738, 0.2750, 0.2564, 0.1948],\n",
      "        [0.2737, 0.2750, 0.2564, 0.1948],\n",
      "        [0.2737, 0.2749, 0.2565, 0.1949],\n",
      "        [0.2737, 0.2749, 0.2564, 0.1949],\n",
      "        [0.2737, 0.2749, 0.2565, 0.1950],\n",
      "        [0.2737, 0.2749, 0.2565, 0.1950]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3773, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(-0.0465, grad_fn=<AddBackward0>)\n",
      "batching here\n",
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(5.1353e-07, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.3576, -1.3685, -1.3882, -1.4325],\n",
      "        [-1.3576, -1.3686, -1.3882, -1.4324],\n",
      "        [-1.3576, -1.3684, -1.3882, -1.4326],\n",
      "        [-1.3577, -1.3684, -1.3882, -1.4325],\n",
      "        [-1.3576, -1.3685, -1.3882, -1.4325],\n",
      "        [-1.3576, -1.3685, -1.3882, -1.4325],\n",
      "        [-1.3576, -1.3685, -1.3882, -1.4325],\n",
      "        [-1.3576, -1.3685, -1.3882, -1.4325],\n",
      "        [-1.3578, -1.3684, -1.3882, -1.4324],\n",
      "        [-1.3577, -1.3685, -1.3880, -1.4326],\n",
      "        [-1.3576, -1.3685, -1.3883, -1.4324],\n",
      "        [-1.3576, -1.3686, -1.3881, -1.4324],\n",
      "        [-1.3577, -1.3684, -1.3882, -1.4325],\n",
      "        [-1.3577, -1.3685, -1.3882, -1.4324],\n",
      "        [-1.3576, -1.3685, -1.3882, -1.4325],\n",
      "        [-1.3575, -1.3686, -1.3883, -1.4324],\n",
      "        [-1.3577, -1.3685, -1.3882, -1.4325],\n",
      "        [-1.3576, -1.3685, -1.3881, -1.4325],\n",
      "        [-1.3576, -1.3685, -1.3881, -1.4326],\n",
      "        [-1.3576, -1.3686, -1.3882, -1.4325],\n",
      "        [-1.3576, -1.3686, -1.3882, -1.4324],\n",
      "        [-1.3576, -1.3685, -1.3882, -1.4325],\n",
      "        [-1.3576, -1.3687, -1.3881, -1.4324],\n",
      "        [-1.3576, -1.3685, -1.3882, -1.4325],\n",
      "        [-1.3577, -1.3684, -1.3882, -1.4326],\n",
      "        [-1.3576, -1.3685, -1.3882, -1.4325],\n",
      "        [-1.3576, -1.3685, -1.3882, -1.4325],\n",
      "        [-1.3576, -1.3685, -1.3882, -1.4325],\n",
      "        [-1.3577, -1.3684, -1.3882, -1.4325],\n",
      "        [-1.3576, -1.3685, -1.3881, -1.4326],\n",
      "        [-1.3576, -1.3685, -1.3883, -1.4325],\n",
      "        [-1.3576, -1.3685, -1.3882, -1.4325],\n",
      "        [-1.3576, -1.3686, -1.3881, -1.4325],\n",
      "        [-1.3576, -1.3686, -1.3882, -1.4325],\n",
      "        [-1.3577, -1.3685, -1.3882, -1.4324],\n",
      "        [-1.3576, -1.3686, -1.3882, -1.4324],\n",
      "        [-1.3577, -1.3684, -1.3882, -1.4326],\n",
      "        [-1.3576, -1.3685, -1.3882, -1.4325],\n",
      "        [-1.3576, -1.3685, -1.3882, -1.4325],\n",
      "        [-1.3576, -1.3685, -1.3882, -1.4325],\n",
      "        [-1.3576, -1.3685, -1.3882, -1.4325],\n",
      "        [-1.3576, -1.3685, -1.3882, -1.4325],\n",
      "        [-1.3576, -1.3684, -1.3883, -1.4325],\n",
      "        [-1.3576, -1.3685, -1.3882, -1.4325],\n",
      "        [-1.3576, -1.3687, -1.3881, -1.4323],\n",
      "        [-1.3576, -1.3686, -1.3882, -1.4324],\n",
      "        [-1.3577, -1.3687, -1.3880, -1.4324],\n",
      "        [-1.3577, -1.3686, -1.3882, -1.4324],\n",
      "        [-1.3576, -1.3687, -1.3881, -1.4324],\n",
      "        [-1.3577, -1.3686, -1.3881, -1.4324],\n",
      "        [-1.3578, -1.3685, -1.3882, -1.4323],\n",
      "        [-1.3577, -1.3687, -1.3881, -1.4323],\n",
      "        [-1.3577, -1.3684, -1.3882, -1.4325],\n",
      "        [-1.3576, -1.3685, -1.3881, -1.4326],\n",
      "        [-1.3576, -1.3684, -1.3882, -1.4325],\n",
      "        [-1.3576, -1.3685, -1.3882, -1.4325],\n",
      "        [-1.3575, -1.3686, -1.3882, -1.4324],\n",
      "        [-1.3576, -1.3685, -1.3882, -1.4325],\n",
      "        [-1.3576, -1.3687, -1.3881, -1.4324],\n",
      "        [-1.3576, -1.3685, -1.3882, -1.4325],\n",
      "        [-1.3576, -1.3686, -1.3881, -1.4325],\n",
      "        [-1.3576, -1.3685, -1.3881, -1.4325],\n",
      "        [-1.3577, -1.3685, -1.3882, -1.4325],\n",
      "        [-1.3577, -1.3686, -1.3881, -1.4324]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([0.0015, 0.0011, 0.0008, 0.0004, 0.0015, 0.0011, 0.0008, 0.0005, 0.0013,\n",
      "        0.0011, 0.0007, 0.0003, 0.0016, 0.0011, 0.0010, 0.0004, 0.0016, 0.0011,\n",
      "        0.0008, 0.0003, 0.0011, 0.0011, 0.0007, 0.0004, 0.0015, 0.0011, 0.0008,\n",
      "        0.0003, 0.0013, 0.0012, 0.0008, 0.0003, 0.0014, 0.0010, 0.0007, 0.0004,\n",
      "        0.0015, 0.0011, 0.0008, 0.0003, 0.0014, 0.0011, 0.0007, 0.0004, 0.0011,\n",
      "        0.0013, 0.0008, 0.0003, 0.0015, 0.0009, 0.0008, 0.0004, 0.0013, 0.0012,\n",
      "        0.0008, 0.0003, 0.0012, 0.0012, 0.0008, 0.0004, 0.0016, 0.0010, 0.0008,\n",
      "        0.0005], grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(-0.0009, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2573, 0.2545, 0.2495, 0.2387],\n",
      "        [0.2573, 0.2545, 0.2495, 0.2387],\n",
      "        [0.2573, 0.2545, 0.2495, 0.2387],\n",
      "        [0.2573, 0.2545, 0.2495, 0.2387],\n",
      "        [0.2573, 0.2545, 0.2495, 0.2387],\n",
      "        [0.2573, 0.2545, 0.2495, 0.2387],\n",
      "        [0.2573, 0.2545, 0.2495, 0.2387],\n",
      "        [0.2573, 0.2545, 0.2495, 0.2387],\n",
      "        [0.2572, 0.2545, 0.2495, 0.2387],\n",
      "        [0.2573, 0.2545, 0.2496, 0.2387],\n",
      "        [0.2573, 0.2545, 0.2495, 0.2387],\n",
      "        [0.2573, 0.2545, 0.2495, 0.2387],\n",
      "        [0.2572, 0.2545, 0.2495, 0.2387],\n",
      "        [0.2572, 0.2545, 0.2495, 0.2387],\n",
      "        [0.2573, 0.2545, 0.2495, 0.2387],\n",
      "        [0.2573, 0.2545, 0.2495, 0.2387],\n",
      "        [0.2573, 0.2545, 0.2495, 0.2387],\n",
      "        [0.2573, 0.2545, 0.2495, 0.2387],\n",
      "        [0.2573, 0.2545, 0.2495, 0.2387],\n",
      "        [0.2573, 0.2545, 0.2495, 0.2387],\n",
      "        [0.2573, 0.2545, 0.2495, 0.2387],\n",
      "        [0.2573, 0.2545, 0.2495, 0.2387],\n",
      "        [0.2573, 0.2544, 0.2495, 0.2387],\n",
      "        [0.2573, 0.2545, 0.2495, 0.2387],\n",
      "        [0.2573, 0.2545, 0.2495, 0.2387],\n",
      "        [0.2573, 0.2545, 0.2495, 0.2387],\n",
      "        [0.2573, 0.2545, 0.2495, 0.2387],\n",
      "        [0.2573, 0.2545, 0.2495, 0.2387],\n",
      "        [0.2573, 0.2545, 0.2495, 0.2387],\n",
      "        [0.2573, 0.2545, 0.2495, 0.2387],\n",
      "        [0.2573, 0.2545, 0.2495, 0.2387],\n",
      "        [0.2573, 0.2545, 0.2495, 0.2387],\n",
      "        [0.2573, 0.2545, 0.2495, 0.2387],\n",
      "        [0.2573, 0.2545, 0.2495, 0.2387],\n",
      "        [0.2573, 0.2545, 0.2495, 0.2387],\n",
      "        [0.2573, 0.2545, 0.2495, 0.2387],\n",
      "        [0.2573, 0.2545, 0.2495, 0.2387],\n",
      "        [0.2573, 0.2545, 0.2495, 0.2387],\n",
      "        [0.2573, 0.2545, 0.2495, 0.2387],\n",
      "        [0.2573, 0.2545, 0.2495, 0.2387],\n",
      "        [0.2573, 0.2545, 0.2495, 0.2387],\n",
      "        [0.2573, 0.2545, 0.2495, 0.2387],\n",
      "        [0.2573, 0.2545, 0.2495, 0.2387],\n",
      "        [0.2573, 0.2545, 0.2495, 0.2387],\n",
      "        [0.2573, 0.2544, 0.2496, 0.2388],\n",
      "        [0.2573, 0.2545, 0.2495, 0.2387],\n",
      "        [0.2573, 0.2544, 0.2496, 0.2387],\n",
      "        [0.2573, 0.2545, 0.2495, 0.2387],\n",
      "        [0.2573, 0.2544, 0.2495, 0.2387],\n",
      "        [0.2573, 0.2545, 0.2495, 0.2387],\n",
      "        [0.2572, 0.2545, 0.2495, 0.2388],\n",
      "        [0.2573, 0.2544, 0.2495, 0.2388],\n",
      "        [0.2573, 0.2545, 0.2495, 0.2387],\n",
      "        [0.2573, 0.2545, 0.2496, 0.2387],\n",
      "        [0.2573, 0.2545, 0.2495, 0.2387],\n",
      "        [0.2573, 0.2545, 0.2495, 0.2387],\n",
      "        [0.2573, 0.2545, 0.2495, 0.2387],\n",
      "        [0.2573, 0.2545, 0.2495, 0.2387],\n",
      "        [0.2573, 0.2544, 0.2495, 0.2387],\n",
      "        [0.2573, 0.2545, 0.2495, 0.2387],\n",
      "        [0.2573, 0.2545, 0.2496, 0.2387],\n",
      "        [0.2573, 0.2545, 0.2495, 0.2387],\n",
      "        [0.2572, 0.2545, 0.2495, 0.2387],\n",
      "        [0.2573, 0.2545, 0.2495, 0.2387]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3859, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(-0.0148, grad_fn=<AddBackward0>)\n",
      "batching here\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(1.6317e-07, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.3586, -1.3695, -1.3878, -1.4308],\n",
      "        [-1.3585, -1.3695, -1.3879, -1.4308],\n",
      "        [-1.3586, -1.3695, -1.3879, -1.4308],\n",
      "        [-1.3585, -1.3695, -1.3879, -1.4308],\n",
      "        [-1.3585, -1.3695, -1.3879, -1.4309],\n",
      "        [-1.3585, -1.3694, -1.3879, -1.4308],\n",
      "        [-1.3585, -1.3695, -1.3879, -1.4308],\n",
      "        [-1.3586, -1.3694, -1.3878, -1.4308],\n",
      "        [-1.3585, -1.3695, -1.3878, -1.4308],\n",
      "        [-1.3585, -1.3694, -1.3879, -1.4309],\n",
      "        [-1.3586, -1.3695, -1.3878, -1.4308],\n",
      "        [-1.3586, -1.3694, -1.3879, -1.4308],\n",
      "        [-1.3586, -1.3695, -1.3878, -1.4308],\n",
      "        [-1.3585, -1.3695, -1.3879, -1.4308],\n",
      "        [-1.3586, -1.3695, -1.3879, -1.4308],\n",
      "        [-1.3585, -1.3695, -1.3878, -1.4309],\n",
      "        [-1.3586, -1.3695, -1.3878, -1.4308],\n",
      "        [-1.3585, -1.3695, -1.3879, -1.4308],\n",
      "        [-1.3585, -1.3695, -1.3879, -1.4308],\n",
      "        [-1.3585, -1.3695, -1.3878, -1.4309],\n",
      "        [-1.3586, -1.3695, -1.3879, -1.4307],\n",
      "        [-1.3586, -1.3695, -1.3879, -1.4307],\n",
      "        [-1.3586, -1.3694, -1.3879, -1.4308],\n",
      "        [-1.3586, -1.3694, -1.3879, -1.4308],\n",
      "        [-1.3585, -1.3694, -1.3879, -1.4308],\n",
      "        [-1.3586, -1.3694, -1.3879, -1.4308],\n",
      "        [-1.3586, -1.3694, -1.3879, -1.4308],\n",
      "        [-1.3585, -1.3694, -1.3879, -1.4308],\n",
      "        [-1.3586, -1.3695, -1.3878, -1.4308],\n",
      "        [-1.3586, -1.3694, -1.3879, -1.4308],\n",
      "        [-1.3586, -1.3694, -1.3879, -1.4308],\n",
      "        [-1.3585, -1.3694, -1.3879, -1.4308],\n",
      "        [-1.3585, -1.3695, -1.3878, -1.4308],\n",
      "        [-1.3586, -1.3695, -1.3878, -1.4309],\n",
      "        [-1.3584, -1.3694, -1.3880, -1.4309],\n",
      "        [-1.3585, -1.3695, -1.3879, -1.4308],\n",
      "        [-1.3585, -1.3694, -1.3879, -1.4308],\n",
      "        [-1.3586, -1.3694, -1.3879, -1.4308],\n",
      "        [-1.3586, -1.3694, -1.3879, -1.4308],\n",
      "        [-1.3585, -1.3695, -1.3879, -1.4308],\n",
      "        [-1.3585, -1.3694, -1.3879, -1.4309],\n",
      "        [-1.3585, -1.3695, -1.3879, -1.4308],\n",
      "        [-1.3585, -1.3695, -1.3878, -1.4308],\n",
      "        [-1.3585, -1.3694, -1.3880, -1.4308],\n",
      "        [-1.3586, -1.3694, -1.3879, -1.4307],\n",
      "        [-1.3585, -1.3695, -1.3879, -1.4307],\n",
      "        [-1.3586, -1.3694, -1.3879, -1.4308],\n",
      "        [-1.3585, -1.3695, -1.3879, -1.4308],\n",
      "        [-1.3586, -1.3696, -1.3877, -1.4308],\n",
      "        [-1.3586, -1.3695, -1.3878, -1.4308],\n",
      "        [-1.3584, -1.3694, -1.3879, -1.4309],\n",
      "        [-1.3586, -1.3695, -1.3878, -1.4308],\n",
      "        [-1.3586, -1.3695, -1.3878, -1.4308],\n",
      "        [-1.3586, -1.3694, -1.3879, -1.4308],\n",
      "        [-1.3586, -1.3694, -1.3879, -1.4308],\n",
      "        [-1.3585, -1.3694, -1.3879, -1.4308],\n",
      "        [-1.3585, -1.3694, -1.3880, -1.4307],\n",
      "        [-1.3585, -1.3695, -1.3879, -1.4308],\n",
      "        [-1.3586, -1.3694, -1.3879, -1.4308],\n",
      "        [-1.3585, -1.3694, -1.3879, -1.4308],\n",
      "        [-1.3586, -1.3695, -1.3878, -1.4308],\n",
      "        [-1.3585, -1.3695, -1.3878, -1.4309],\n",
      "        [-1.3584, -1.3695, -1.3879, -1.4309],\n",
      "        [-1.3586, -1.3694, -1.3879, -1.4308]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([6.5358e-04, 4.4925e-04, 2.4929e-04, 8.1877e-05, 6.7416e-04, 4.8152e-04,\n",
      "        2.9843e-04, 1.5181e-04, 7.6863e-04, 6.9460e-04, 4.2884e-04, 1.3450e-04,\n",
      "        7.2939e-04, 5.0821e-04, 4.1689e-04, 1.3989e-04, 7.5157e-04, 5.8116e-04,\n",
      "        3.8308e-04, 1.3942e-04, 9.1350e-04, 6.1099e-04, 3.3408e-04, 2.3998e-04,\n",
      "        8.6236e-04, 6.6105e-04, 4.3240e-04, 2.2158e-04, 8.0266e-04, 6.9013e-04,\n",
      "        4.1501e-04, 2.2844e-04, 7.9023e-04, 5.9364e-04, 3.4708e-04, 2.3650e-04,\n",
      "        7.7027e-04, 5.5861e-04, 2.7336e-04, 1.0890e-04, 1.0515e-03, 8.9167e-04,\n",
      "        5.6297e-04, 5.2083e-04, 7.2095e-04, 5.6222e-04, 4.5723e-04, 2.1425e-04,\n",
      "        8.1888e-04, 6.3655e-04, 4.0867e-04, 2.4805e-04, 8.1993e-04, 7.1149e-04,\n",
      "        4.1501e-04, 2.1690e-04, 8.6344e-04, 6.4566e-04, 4.6263e-04, 2.2158e-04,\n",
      "        7.8262e-04, 6.8975e-04, 4.2611e-04, 2.6108e-04],\n",
      "       grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(-0.0005, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2570, 0.2542, 0.2496, 0.2391],\n",
      "        [0.2570, 0.2542, 0.2496, 0.2391],\n",
      "        [0.2570, 0.2542, 0.2496, 0.2391],\n",
      "        [0.2571, 0.2542, 0.2496, 0.2391],\n",
      "        [0.2570, 0.2542, 0.2496, 0.2391],\n",
      "        [0.2570, 0.2542, 0.2496, 0.2391],\n",
      "        [0.2571, 0.2542, 0.2496, 0.2391],\n",
      "        [0.2570, 0.2543, 0.2496, 0.2391],\n",
      "        [0.2570, 0.2542, 0.2496, 0.2391],\n",
      "        [0.2570, 0.2542, 0.2496, 0.2391],\n",
      "        [0.2570, 0.2542, 0.2496, 0.2391],\n",
      "        [0.2570, 0.2543, 0.2496, 0.2391],\n",
      "        [0.2570, 0.2542, 0.2496, 0.2391],\n",
      "        [0.2570, 0.2542, 0.2496, 0.2391],\n",
      "        [0.2570, 0.2542, 0.2496, 0.2391],\n",
      "        [0.2570, 0.2542, 0.2496, 0.2391],\n",
      "        [0.2570, 0.2542, 0.2496, 0.2391],\n",
      "        [0.2570, 0.2542, 0.2496, 0.2391],\n",
      "        [0.2570, 0.2542, 0.2496, 0.2391],\n",
      "        [0.2570, 0.2542, 0.2496, 0.2391],\n",
      "        [0.2570, 0.2542, 0.2496, 0.2391],\n",
      "        [0.2570, 0.2542, 0.2496, 0.2391],\n",
      "        [0.2570, 0.2543, 0.2496, 0.2391],\n",
      "        [0.2570, 0.2543, 0.2496, 0.2391],\n",
      "        [0.2570, 0.2543, 0.2496, 0.2391],\n",
      "        [0.2570, 0.2543, 0.2496, 0.2391],\n",
      "        [0.2570, 0.2543, 0.2496, 0.2391],\n",
      "        [0.2570, 0.2543, 0.2496, 0.2391],\n",
      "        [0.2570, 0.2542, 0.2496, 0.2391],\n",
      "        [0.2570, 0.2543, 0.2496, 0.2391],\n",
      "        [0.2570, 0.2543, 0.2496, 0.2391],\n",
      "        [0.2570, 0.2543, 0.2496, 0.2391],\n",
      "        [0.2570, 0.2542, 0.2496, 0.2391],\n",
      "        [0.2570, 0.2542, 0.2496, 0.2391],\n",
      "        [0.2571, 0.2543, 0.2496, 0.2391],\n",
      "        [0.2570, 0.2542, 0.2496, 0.2391],\n",
      "        [0.2570, 0.2543, 0.2496, 0.2391],\n",
      "        [0.2570, 0.2543, 0.2496, 0.2391],\n",
      "        [0.2570, 0.2543, 0.2496, 0.2391],\n",
      "        [0.2570, 0.2542, 0.2496, 0.2391],\n",
      "        [0.2570, 0.2543, 0.2496, 0.2391],\n",
      "        [0.2570, 0.2542, 0.2496, 0.2391],\n",
      "        [0.2570, 0.2542, 0.2496, 0.2391],\n",
      "        [0.2571, 0.2543, 0.2496, 0.2391],\n",
      "        [0.2570, 0.2543, 0.2496, 0.2391],\n",
      "        [0.2570, 0.2542, 0.2496, 0.2391],\n",
      "        [0.2570, 0.2543, 0.2496, 0.2391],\n",
      "        [0.2570, 0.2542, 0.2496, 0.2391],\n",
      "        [0.2570, 0.2542, 0.2496, 0.2391],\n",
      "        [0.2570, 0.2542, 0.2496, 0.2391],\n",
      "        [0.2571, 0.2543, 0.2496, 0.2391],\n",
      "        [0.2570, 0.2542, 0.2496, 0.2391],\n",
      "        [0.2570, 0.2542, 0.2496, 0.2391],\n",
      "        [0.2570, 0.2543, 0.2496, 0.2391],\n",
      "        [0.2570, 0.2543, 0.2496, 0.2391],\n",
      "        [0.2570, 0.2543, 0.2496, 0.2391],\n",
      "        [0.2570, 0.2543, 0.2496, 0.2391],\n",
      "        [0.2570, 0.2542, 0.2496, 0.2391],\n",
      "        [0.2570, 0.2543, 0.2496, 0.2391],\n",
      "        [0.2570, 0.2543, 0.2496, 0.2391],\n",
      "        [0.2570, 0.2542, 0.2496, 0.2391],\n",
      "        [0.2570, 0.2542, 0.2496, 0.2391],\n",
      "        [0.2571, 0.2542, 0.2496, 0.2391],\n",
      "        [0.2570, 0.2542, 0.2496, 0.2391]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3859, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(-0.0144, grad_fn=<AddBackward0>)\n",
      "batching here\n",
      "11: done 11 episodes, mean_reward=0.00, best_reward=0.00, speed=159.17\n",
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(4.3058e-05, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.3593, -1.3703, -1.3874, -1.4297],\n",
      "        [-1.3593, -1.3702, -1.3874, -1.4297],\n",
      "        [-1.3593, -1.3702, -1.3875, -1.4296],\n",
      "        [-1.3593, -1.3702, -1.3875, -1.4297],\n",
      "        [-1.3593, -1.3700, -1.3874, -1.4299],\n",
      "        [-1.3593, -1.3701, -1.3874, -1.4298],\n",
      "        [-1.3594, -1.3700, -1.3873, -1.4299],\n",
      "        [-1.3593, -1.3703, -1.3873, -1.4297],\n",
      "        [-1.3594, -1.3702, -1.3873, -1.4297],\n",
      "        [-1.3594, -1.3702, -1.3873, -1.4297],\n",
      "        [-1.3594, -1.3703, -1.3873, -1.4296],\n",
      "        [-1.3593, -1.3702, -1.3875, -1.4296],\n",
      "        [-1.3593, -1.3701, -1.3874, -1.4298],\n",
      "        [-1.3594, -1.3702, -1.3873, -1.4297],\n",
      "        [-1.3592, -1.3702, -1.3875, -1.4296],\n",
      "        [-1.3592, -1.3702, -1.3875, -1.4297],\n",
      "        [-1.3593, -1.3701, -1.3874, -1.4298],\n",
      "        [-1.3594, -1.3702, -1.3874, -1.4296],\n",
      "        [-1.3593, -1.3703, -1.3875, -1.4296],\n",
      "        [-1.3593, -1.3703, -1.3874, -1.4297],\n",
      "        [-1.3593, -1.3702, -1.3874, -1.4297],\n",
      "        [-1.3593, -1.3702, -1.3874, -1.4297],\n",
      "        [-1.3593, -1.3703, -1.3874, -1.4295],\n",
      "        [-1.3594, -1.3703, -1.3874, -1.4295],\n",
      "        [-1.3593, -1.3702, -1.3874, -1.4297],\n",
      "        [-1.3593, -1.3702, -1.3874, -1.4297],\n",
      "        [-1.3593, -1.3703, -1.3874, -1.4296],\n",
      "        [-1.3593, -1.3703, -1.3873, -1.4297],\n",
      "        [-1.3593, -1.3702, -1.3874, -1.4297],\n",
      "        [-1.3593, -1.3702, -1.3874, -1.4297],\n",
      "        [-1.3593, -1.3702, -1.3874, -1.4297],\n",
      "        [-1.3593, -1.3702, -1.3874, -1.4297],\n",
      "        [-1.3594, -1.3703, -1.3872, -1.4297],\n",
      "        [-1.3593, -1.3702, -1.3873, -1.4297],\n",
      "        [-1.3593, -1.3703, -1.3873, -1.4297],\n",
      "        [-1.3593, -1.3703, -1.3873, -1.4297],\n",
      "        [-1.3592, -1.3702, -1.3874, -1.4297],\n",
      "        [-1.3592, -1.3702, -1.3874, -1.4298],\n",
      "        [-1.3593, -1.3702, -1.3874, -1.4296],\n",
      "        [-1.3593, -1.3702, -1.3875, -1.4296],\n",
      "        [-1.3594, -1.3702, -1.3873, -1.4297],\n",
      "        [-1.3595, -1.3700, -1.3876, -1.4295],\n",
      "        [-1.3594, -1.3700, -1.3874, -1.4299],\n",
      "        [-1.3593, -1.3702, -1.3874, -1.4297],\n",
      "        [-1.3593, -1.3703, -1.3873, -1.4297],\n",
      "        [-1.3593, -1.3703, -1.3874, -1.4296],\n",
      "        [-1.3593, -1.3704, -1.3874, -1.4295],\n",
      "        [-1.3593, -1.3703, -1.3875, -1.4295],\n",
      "        [-1.3594, -1.3704, -1.3872, -1.4296],\n",
      "        [-1.3593, -1.3703, -1.3874, -1.4296],\n",
      "        [-1.3593, -1.3703, -1.3873, -1.4297],\n",
      "        [-1.3593, -1.3703, -1.3872, -1.4298],\n",
      "        [-1.3593, -1.3702, -1.3874, -1.4297],\n",
      "        [-1.3593, -1.3702, -1.3874, -1.4297],\n",
      "        [-1.3593, -1.3702, -1.3874, -1.4297],\n",
      "        [-1.3593, -1.3702, -1.3874, -1.4297],\n",
      "        [-1.3593, -1.3702, -1.3874, -1.4297],\n",
      "        [-1.3593, -1.3702, -1.3874, -1.4297],\n",
      "        [-1.3593, -1.3702, -1.3875, -1.4296],\n",
      "        [-1.3593, -1.3702, -1.3875, -1.4297],\n",
      "        [-1.3595, -1.3703, -1.3872, -1.4296],\n",
      "        [-1.3593, -1.3702, -1.3873, -1.4297],\n",
      "        [-1.3593, -1.3703, -1.3873, -1.4296],\n",
      "        [-1.3592, -1.3704, -1.3873, -1.4297]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([ 1.5626e-02,  1.5265e-02,  2.8458e-04,  1.0761e-04,  7.8542e-04,\n",
      "         2.3842e-04,  5.1076e-04,  3.2268e-04,  1.5504e-02,  1.5239e-02,\n",
      "        -3.6659e-05, -4.3618e-06,  1.5532e-02,  1.5421e-02,  2.6734e-04,\n",
      "         1.2321e-04,  1.6007e-02,  1.5369e-02,  6.2803e-04,  2.5505e-04,\n",
      "         1.5541e-02,  1.5356e-02,  5.7407e-04,  1.1659e-04,  1.5299e-02,\n",
      "         1.5178e-02,  4.9803e-04,  1.5934e-04,  1.5964e-02,  1.5299e-02,\n",
      "         3.5335e-04,  9.3101e-05,  7.4795e-04,  6.1631e-04,  3.0535e-04,\n",
      "         2.3497e-04,  1.5440e-02,  1.5411e-02, -5.1866e-05, -6.4479e-05,\n",
      "         8.5665e-04,  7.8829e-04,  4.7503e-04,  6.8542e-04,  1.5233e-02,\n",
      "         1.5556e-02,  3.4597e-04,  9.0604e-05,  6.0708e-04,  5.7341e-04,\n",
      "         6.3198e-05,  2.3320e-05,  1.5299e-02,  1.5964e-02,  3.1063e-04,\n",
      "         5.7031e-05,  1.5299e-02,  1.5299e-02,  2.7880e-04,  1.0542e-04,\n",
      "         7.0006e-04,  5.1590e-04,  1.7401e-04,  2.4723e-04],\n",
      "       grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(-0.0055, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2569, 0.2540, 0.2497, 0.2394],\n",
      "        [0.2568, 0.2541, 0.2497, 0.2394],\n",
      "        [0.2568, 0.2541, 0.2497, 0.2394],\n",
      "        [0.2569, 0.2541, 0.2497, 0.2394],\n",
      "        [0.2568, 0.2541, 0.2497, 0.2393],\n",
      "        [0.2568, 0.2541, 0.2497, 0.2394],\n",
      "        [0.2568, 0.2541, 0.2497, 0.2393],\n",
      "        [0.2568, 0.2540, 0.2498, 0.2394],\n",
      "        [0.2568, 0.2541, 0.2497, 0.2394],\n",
      "        [0.2568, 0.2541, 0.2497, 0.2394],\n",
      "        [0.2568, 0.2540, 0.2497, 0.2394],\n",
      "        [0.2568, 0.2540, 0.2497, 0.2394],\n",
      "        [0.2568, 0.2541, 0.2497, 0.2394],\n",
      "        [0.2568, 0.2540, 0.2498, 0.2394],\n",
      "        [0.2569, 0.2540, 0.2497, 0.2394],\n",
      "        [0.2569, 0.2541, 0.2497, 0.2394],\n",
      "        [0.2568, 0.2541, 0.2497, 0.2394],\n",
      "        [0.2568, 0.2541, 0.2497, 0.2394],\n",
      "        [0.2568, 0.2540, 0.2497, 0.2394],\n",
      "        [0.2568, 0.2540, 0.2497, 0.2394],\n",
      "        [0.2568, 0.2541, 0.2497, 0.2394],\n",
      "        [0.2568, 0.2540, 0.2497, 0.2394],\n",
      "        [0.2568, 0.2540, 0.2497, 0.2394],\n",
      "        [0.2568, 0.2540, 0.2497, 0.2394],\n",
      "        [0.2568, 0.2541, 0.2497, 0.2394],\n",
      "        [0.2568, 0.2541, 0.2497, 0.2394],\n",
      "        [0.2569, 0.2540, 0.2497, 0.2394],\n",
      "        [0.2568, 0.2540, 0.2497, 0.2394],\n",
      "        [0.2568, 0.2541, 0.2497, 0.2394],\n",
      "        [0.2568, 0.2541, 0.2497, 0.2394],\n",
      "        [0.2568, 0.2541, 0.2497, 0.2394],\n",
      "        [0.2568, 0.2540, 0.2497, 0.2394],\n",
      "        [0.2568, 0.2540, 0.2498, 0.2394],\n",
      "        [0.2568, 0.2540, 0.2497, 0.2394],\n",
      "        [0.2568, 0.2540, 0.2497, 0.2394],\n",
      "        [0.2568, 0.2540, 0.2497, 0.2394],\n",
      "        [0.2569, 0.2540, 0.2497, 0.2394],\n",
      "        [0.2569, 0.2540, 0.2497, 0.2394],\n",
      "        [0.2568, 0.2540, 0.2497, 0.2394],\n",
      "        [0.2568, 0.2541, 0.2497, 0.2394],\n",
      "        [0.2568, 0.2540, 0.2497, 0.2394],\n",
      "        [0.2568, 0.2541, 0.2497, 0.2394],\n",
      "        [0.2568, 0.2541, 0.2497, 0.2393],\n",
      "        [0.2568, 0.2540, 0.2497, 0.2394],\n",
      "        [0.2568, 0.2540, 0.2497, 0.2394],\n",
      "        [0.2568, 0.2540, 0.2497, 0.2394],\n",
      "        [0.2568, 0.2540, 0.2497, 0.2394],\n",
      "        [0.2568, 0.2540, 0.2497, 0.2394],\n",
      "        [0.2568, 0.2540, 0.2498, 0.2394],\n",
      "        [0.2568, 0.2540, 0.2497, 0.2394],\n",
      "        [0.2568, 0.2540, 0.2497, 0.2394],\n",
      "        [0.2568, 0.2540, 0.2498, 0.2394],\n",
      "        [0.2568, 0.2541, 0.2497, 0.2394],\n",
      "        [0.2568, 0.2541, 0.2497, 0.2394],\n",
      "        [0.2568, 0.2541, 0.2497, 0.2394],\n",
      "        [0.2568, 0.2540, 0.2497, 0.2394],\n",
      "        [0.2568, 0.2541, 0.2497, 0.2394],\n",
      "        [0.2568, 0.2541, 0.2497, 0.2394],\n",
      "        [0.2568, 0.2541, 0.2497, 0.2394],\n",
      "        [0.2569, 0.2541, 0.2497, 0.2394],\n",
      "        [0.2568, 0.2540, 0.2498, 0.2394],\n",
      "        [0.2568, 0.2541, 0.2497, 0.2394],\n",
      "        [0.2568, 0.2540, 0.2497, 0.2394],\n",
      "        [0.2569, 0.2540, 0.2498, 0.2394]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3859, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(-0.0193, grad_fn=<AddBackward0>)\n",
      "batching here\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12: done 2 episodes, mean_reward=1.00, best_reward=1.00, speed=174.61\n",
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.3568, -1.3778, -1.3902, -1.4214],\n",
      "        [-1.3569, -1.3778, -1.3901, -1.4215],\n",
      "        [-1.3570, -1.3781, -1.3898, -1.4214],\n",
      "        [-1.3568, -1.3778, -1.3900, -1.4216],\n",
      "        [-1.3569, -1.3779, -1.3900, -1.4215],\n",
      "        [-1.3567, -1.3780, -1.3900, -1.4215],\n",
      "        [-1.3568, -1.3779, -1.3902, -1.4214],\n",
      "        [-1.3568, -1.3778, -1.3900, -1.4216],\n",
      "        [-1.3570, -1.3778, -1.3900, -1.4214],\n",
      "        [-1.3569, -1.3778, -1.3900, -1.4216],\n",
      "        [-1.3569, -1.3778, -1.3902, -1.4214],\n",
      "        [-1.3568, -1.3780, -1.3901, -1.4214],\n",
      "        [-1.3568, -1.3778, -1.3902, -1.4214],\n",
      "        [-1.3569, -1.3778, -1.3901, -1.4214],\n",
      "        [-1.3570, -1.3781, -1.3898, -1.4214],\n",
      "        [-1.3568, -1.3778, -1.3900, -1.4216],\n",
      "        [-1.3567, -1.3778, -1.3901, -1.4216],\n",
      "        [-1.3568, -1.3779, -1.3902, -1.4214],\n",
      "        [-1.3568, -1.3779, -1.3901, -1.4215],\n",
      "        [-1.3569, -1.3780, -1.3901, -1.4212],\n",
      "        [-1.3568, -1.3779, -1.3901, -1.4214],\n",
      "        [-1.3570, -1.3776, -1.3901, -1.4215],\n",
      "        [-1.3568, -1.3779, -1.3901, -1.4215],\n",
      "        [-1.3569, -1.3777, -1.3901, -1.4216],\n",
      "        [-1.3568, -1.3778, -1.3901, -1.4216],\n",
      "        [-1.3569, -1.3779, -1.3902, -1.4214],\n",
      "        [-1.3568, -1.3779, -1.3900, -1.4215],\n",
      "        [-1.3569, -1.3780, -1.3901, -1.4212],\n",
      "        [-1.3568, -1.3779, -1.3901, -1.4215],\n",
      "        [-1.3569, -1.3777, -1.3901, -1.4215],\n",
      "        [-1.3568, -1.3778, -1.3902, -1.4215],\n",
      "        [-1.3569, -1.3777, -1.3901, -1.4216],\n",
      "        [-1.3569, -1.3779, -1.3900, -1.4215],\n",
      "        [-1.3569, -1.3778, -1.3900, -1.4216],\n",
      "        [-1.3568, -1.3779, -1.3900, -1.4216],\n",
      "        [-1.3568, -1.3779, -1.3901, -1.4214],\n",
      "        [-1.3570, -1.3778, -1.3900, -1.4214],\n",
      "        [-1.3569, -1.3778, -1.3900, -1.4215],\n",
      "        [-1.3569, -1.3778, -1.3901, -1.4214],\n",
      "        [-1.3569, -1.3780, -1.3900, -1.4214],\n",
      "        [-1.3568, -1.3778, -1.3901, -1.4216],\n",
      "        [-1.3569, -1.3777, -1.3901, -1.4215],\n",
      "        [-1.3569, -1.3779, -1.3900, -1.4214],\n",
      "        [-1.3570, -1.3781, -1.3900, -1.4212],\n",
      "        [-1.3569, -1.3779, -1.3902, -1.4213],\n",
      "        [-1.3569, -1.3779, -1.3901, -1.4214],\n",
      "        [-1.3571, -1.3781, -1.3897, -1.4213],\n",
      "        [-1.3569, -1.3779, -1.3900, -1.4216],\n",
      "        [-1.3569, -1.3779, -1.3900, -1.4215],\n",
      "        [-1.3567, -1.3780, -1.3899, -1.4216],\n",
      "        [-1.3569, -1.3779, -1.3900, -1.4215],\n",
      "        [-1.3568, -1.3781, -1.3899, -1.4215],\n",
      "        [-1.3568, -1.3779, -1.3901, -1.4215],\n",
      "        [-1.3570, -1.3777, -1.3900, -1.4215],\n",
      "        [-1.3569, -1.3777, -1.3901, -1.4215],\n",
      "        [-1.3569, -1.3777, -1.3901, -1.4216],\n",
      "        [-1.3568, -1.3778, -1.3902, -1.4214],\n",
      "        [-1.3569, -1.3778, -1.3901, -1.4215],\n",
      "        [-1.3570, -1.3781, -1.3898, -1.4214],\n",
      "        [-1.3568, -1.3778, -1.3900, -1.4216],\n",
      "        [-1.3569, -1.3779, -1.3899, -1.4215],\n",
      "        [-1.3568, -1.3779, -1.3900, -1.4215],\n",
      "        [-1.3570, -1.3779, -1.3899, -1.4215],\n",
      "        [-1.3569, -1.3780, -1.3899, -1.4214]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([-0.0017, -0.0010, -0.0005, -0.0005, -0.0019, -0.0019, -0.0011, -0.0003,\n",
      "        -0.0022, -0.0017, -0.0013, -0.0008, -0.0017, -0.0010, -0.0006, -0.0006,\n",
      "        -0.0021, -0.0010, -0.0012, -0.0007, -0.0022, -0.0015, -0.0008, -0.0004,\n",
      "        -0.0022, -0.0012, -0.0011, -0.0007, -0.0020, -0.0015, -0.0007, -0.0004,\n",
      "        -0.0493, -0.0508, -0.0507, -0.0005, -0.0020, -0.0017, -0.0013, -0.0008,\n",
      "        -0.0025, -0.0019, -0.0011, -0.0005, -0.0015, -0.0009, -0.0006, -0.0005,\n",
      "        -0.0019, -0.0014, -0.0008, -0.0007, -0.0020, -0.0015, -0.0008, -0.0004,\n",
      "        -0.0017, -0.0010, -0.0005, -0.0005, -0.0508, -0.0493, -0.0508, -0.0004],\n",
      "       grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(0.0058, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2575, 0.2521, 0.2490, 0.2414],\n",
      "        [0.2575, 0.2521, 0.2491, 0.2414],\n",
      "        [0.2574, 0.2521, 0.2491, 0.2414],\n",
      "        [0.2575, 0.2521, 0.2491, 0.2413],\n",
      "        [0.2575, 0.2521, 0.2491, 0.2414],\n",
      "        [0.2575, 0.2521, 0.2491, 0.2413],\n",
      "        [0.2575, 0.2521, 0.2490, 0.2414],\n",
      "        [0.2575, 0.2521, 0.2491, 0.2413],\n",
      "        [0.2574, 0.2521, 0.2491, 0.2414],\n",
      "        [0.2575, 0.2521, 0.2491, 0.2413],\n",
      "        [0.2575, 0.2521, 0.2490, 0.2414],\n",
      "        [0.2575, 0.2521, 0.2491, 0.2414],\n",
      "        [0.2575, 0.2521, 0.2490, 0.2414],\n",
      "        [0.2575, 0.2521, 0.2491, 0.2414],\n",
      "        [0.2574, 0.2521, 0.2491, 0.2414],\n",
      "        [0.2575, 0.2521, 0.2491, 0.2413],\n",
      "        [0.2575, 0.2521, 0.2490, 0.2413],\n",
      "        [0.2575, 0.2521, 0.2490, 0.2414],\n",
      "        [0.2575, 0.2521, 0.2491, 0.2413],\n",
      "        [0.2575, 0.2521, 0.2490, 0.2414],\n",
      "        [0.2575, 0.2521, 0.2490, 0.2414],\n",
      "        [0.2574, 0.2522, 0.2491, 0.2414],\n",
      "        [0.2575, 0.2521, 0.2491, 0.2414],\n",
      "        [0.2575, 0.2522, 0.2490, 0.2413],\n",
      "        [0.2575, 0.2521, 0.2490, 0.2413],\n",
      "        [0.2575, 0.2521, 0.2490, 0.2414],\n",
      "        [0.2575, 0.2521, 0.2491, 0.2414],\n",
      "        [0.2574, 0.2521, 0.2491, 0.2414],\n",
      "        [0.2575, 0.2521, 0.2490, 0.2414],\n",
      "        [0.2574, 0.2521, 0.2491, 0.2413],\n",
      "        [0.2575, 0.2521, 0.2490, 0.2413],\n",
      "        [0.2575, 0.2521, 0.2490, 0.2413],\n",
      "        [0.2575, 0.2521, 0.2491, 0.2413],\n",
      "        [0.2575, 0.2521, 0.2491, 0.2413],\n",
      "        [0.2575, 0.2521, 0.2491, 0.2413],\n",
      "        [0.2575, 0.2521, 0.2491, 0.2414],\n",
      "        [0.2574, 0.2521, 0.2491, 0.2414],\n",
      "        [0.2575, 0.2521, 0.2491, 0.2413],\n",
      "        [0.2575, 0.2521, 0.2490, 0.2414],\n",
      "        [0.2575, 0.2521, 0.2491, 0.2414],\n",
      "        [0.2575, 0.2521, 0.2491, 0.2413],\n",
      "        [0.2574, 0.2521, 0.2490, 0.2414],\n",
      "        [0.2574, 0.2521, 0.2491, 0.2414],\n",
      "        [0.2574, 0.2521, 0.2491, 0.2414],\n",
      "        [0.2575, 0.2521, 0.2490, 0.2414],\n",
      "        [0.2575, 0.2521, 0.2491, 0.2414],\n",
      "        [0.2574, 0.2521, 0.2491, 0.2414],\n",
      "        [0.2575, 0.2521, 0.2491, 0.2413],\n",
      "        [0.2575, 0.2521, 0.2491, 0.2413],\n",
      "        [0.2575, 0.2521, 0.2491, 0.2413],\n",
      "        [0.2574, 0.2521, 0.2491, 0.2414],\n",
      "        [0.2575, 0.2521, 0.2491, 0.2414],\n",
      "        [0.2575, 0.2521, 0.2491, 0.2414],\n",
      "        [0.2574, 0.2522, 0.2491, 0.2413],\n",
      "        [0.2575, 0.2521, 0.2490, 0.2413],\n",
      "        [0.2575, 0.2522, 0.2490, 0.2413],\n",
      "        [0.2575, 0.2521, 0.2490, 0.2414],\n",
      "        [0.2575, 0.2521, 0.2491, 0.2414],\n",
      "        [0.2574, 0.2521, 0.2491, 0.2414],\n",
      "        [0.2575, 0.2521, 0.2491, 0.2413],\n",
      "        [0.2575, 0.2521, 0.2491, 0.2413],\n",
      "        [0.2575, 0.2521, 0.2491, 0.2414],\n",
      "        [0.2574, 0.2521, 0.2491, 0.2414],\n",
      "        [0.2575, 0.2521, 0.2491, 0.2414]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3860, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(-0.0080, grad_fn=<AddBackward0>)\n",
      "batching here\n",
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(0.0156, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.3626, -1.3742, -1.3956, -1.4136],\n",
      "        [-1.3626, -1.3741, -1.3956, -1.4136],\n",
      "        [-1.3626, -1.3742, -1.3956, -1.4135],\n",
      "        [-1.3625, -1.3742, -1.3957, -1.4135],\n",
      "        [-1.3624, -1.3742, -1.3956, -1.4137],\n",
      "        [-1.3626, -1.3742, -1.3955, -1.4136],\n",
      "        [-1.3627, -1.3740, -1.3956, -1.4136],\n",
      "        [-1.3625, -1.3744, -1.3955, -1.4135],\n",
      "        [-1.3625, -1.3741, -1.3955, -1.4138],\n",
      "        [-1.3627, -1.3741, -1.3956, -1.4136],\n",
      "        [-1.3626, -1.3743, -1.3956, -1.4134],\n",
      "        [-1.3626, -1.3742, -1.3957, -1.4135],\n",
      "        [-1.3626, -1.3742, -1.3956, -1.4136],\n",
      "        [-1.3626, -1.3741, -1.3957, -1.4136],\n",
      "        [-1.3626, -1.3742, -1.3957, -1.4135],\n",
      "        [-1.3626, -1.3742, -1.3957, -1.4135],\n",
      "        [-1.3625, -1.3740, -1.3957, -1.4137],\n",
      "        [-1.3627, -1.3741, -1.3956, -1.4134],\n",
      "        [-1.3625, -1.3743, -1.3957, -1.4135],\n",
      "        [-1.3625, -1.3743, -1.3957, -1.4134],\n",
      "        [-1.3625, -1.3743, -1.3955, -1.4136],\n",
      "        [-1.3626, -1.3742, -1.3956, -1.4136],\n",
      "        [-1.3626, -1.3742, -1.3958, -1.4133],\n",
      "        [-1.3626, -1.3743, -1.3957, -1.4134],\n",
      "        [-1.3625, -1.3741, -1.3957, -1.4137],\n",
      "        [-1.3627, -1.3742, -1.3956, -1.4134],\n",
      "        [-1.3625, -1.3743, -1.3957, -1.4135],\n",
      "        [-1.3625, -1.3743, -1.3957, -1.4134],\n",
      "        [-1.3626, -1.3742, -1.3955, -1.4137],\n",
      "        [-1.3627, -1.3742, -1.3956, -1.4135],\n",
      "        [-1.3627, -1.3742, -1.3957, -1.4133],\n",
      "        [-1.3626, -1.3743, -1.3957, -1.4134],\n",
      "        [-1.3626, -1.3744, -1.3956, -1.4134],\n",
      "        [-1.3626, -1.3743, -1.3956, -1.4135],\n",
      "        [-1.3627, -1.3742, -1.3955, -1.4135],\n",
      "        [-1.3626, -1.3743, -1.3955, -1.4135],\n",
      "        [-1.3625, -1.3741, -1.3955, -1.4139],\n",
      "        [-1.3626, -1.3742, -1.3955, -1.4136],\n",
      "        [-1.3625, -1.3743, -1.3956, -1.4135],\n",
      "        [-1.3626, -1.3742, -1.3957, -1.4135],\n",
      "        [-1.3627, -1.3742, -1.3954, -1.4138],\n",
      "        [-1.3627, -1.3743, -1.3957, -1.4133],\n",
      "        [-1.3625, -1.3743, -1.3955, -1.4137],\n",
      "        [-1.3625, -1.3742, -1.3957, -1.4136],\n",
      "        [-1.3627, -1.3742, -1.3956, -1.4134],\n",
      "        [-1.3628, -1.3740, -1.3956, -1.4135],\n",
      "        [-1.3627, -1.3742, -1.3957, -1.4134],\n",
      "        [-1.3626, -1.3742, -1.3958, -1.4134],\n",
      "        [-1.3626, -1.3743, -1.3956, -1.4134],\n",
      "        [-1.3627, -1.3743, -1.3956, -1.4134],\n",
      "        [-1.3627, -1.3741, -1.3956, -1.4136],\n",
      "        [-1.3625, -1.3742, -1.3956, -1.4136],\n",
      "        [-1.3625, -1.3743, -1.3955, -1.4136],\n",
      "        [-1.3626, -1.3742, -1.3956, -1.4136],\n",
      "        [-1.3626, -1.3742, -1.3958, -1.4134],\n",
      "        [-1.3625, -1.3743, -1.3957, -1.4134],\n",
      "        [-1.3626, -1.3742, -1.3956, -1.4136],\n",
      "        [-1.3626, -1.3741, -1.3956, -1.4136],\n",
      "        [-1.3626, -1.3742, -1.3956, -1.4135],\n",
      "        [-1.3625, -1.3742, -1.3957, -1.4135],\n",
      "        [-1.3626, -1.3743, -1.3955, -1.4135],\n",
      "        [-1.3626, -1.3742, -1.3956, -1.4135],\n",
      "        [-1.3627, -1.3744, -1.3954, -1.4134],\n",
      "        [-1.3625, -1.3744, -1.3955, -1.4135]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([ 4.7363e-04,  4.3477e-04,  2.7693e-04, -4.2151e-04, -1.3737e+00,\n",
      "         1.0894e-04,  3.9942e-04, -7.9192e-05,  5.7835e-04,  4.5479e-04,\n",
      "         4.2316e-05, -2.8684e-04,  4.1162e-04,  4.5305e-04,  2.0675e-04,\n",
      "        -4.7748e-04,  6.7281e-04,  6.6705e-04,  2.9844e-04,  2.9369e-04,\n",
      "         6.7604e-04,  2.8197e-04,  5.1665e-04,  1.3239e-04,  7.4413e-04,\n",
      "         6.4049e-04,  2.5931e-04,  2.7796e-04,  7.6698e-04,  2.7216e-04,\n",
      "         4.6222e-04,  2.0509e-04,  3.6678e-04,  3.6056e-04,  4.1821e-05,\n",
      "         2.0859e-04,  6.3167e-04,  4.6374e-04,  1.1454e-04, -1.3823e-04,\n",
      "         8.2542e-04,  3.9103e-04,  5.5427e-04,  2.0242e-04,  4.3468e-04,\n",
      "         4.6082e-04,  1.9627e-04, -4.5462e-04,  3.0877e-04,  5.2589e-04,\n",
      "         2.0593e-04,  1.9276e-04,  7.2341e-04,  4.1150e-04,  5.2106e-04,\n",
      "         1.6022e-04,  4.6095e-04,  4.2978e-04,  2.6977e-04, -3.7986e-04,\n",
      "         4.8483e-04,  3.1484e-04,  8.3250e-04,  1.1620e-04],\n",
      "       grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(0.0212, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2560, 0.2531, 0.2477, 0.2433],\n",
      "        [0.2560, 0.2531, 0.2477, 0.2433],\n",
      "        [0.2560, 0.2530, 0.2477, 0.2433],\n",
      "        [0.2560, 0.2530, 0.2477, 0.2433],\n",
      "        [0.2560, 0.2530, 0.2477, 0.2432],\n",
      "        [0.2560, 0.2530, 0.2477, 0.2433],\n",
      "        [0.2560, 0.2531, 0.2477, 0.2433],\n",
      "        [0.2560, 0.2530, 0.2477, 0.2433],\n",
      "        [0.2560, 0.2531, 0.2477, 0.2432],\n",
      "        [0.2560, 0.2531, 0.2477, 0.2433],\n",
      "        [0.2560, 0.2530, 0.2477, 0.2433],\n",
      "        [0.2560, 0.2530, 0.2477, 0.2433],\n",
      "        [0.2560, 0.2530, 0.2477, 0.2433],\n",
      "        [0.2560, 0.2531, 0.2477, 0.2433],\n",
      "        [0.2560, 0.2530, 0.2477, 0.2433],\n",
      "        [0.2560, 0.2530, 0.2477, 0.2433],\n",
      "        [0.2560, 0.2531, 0.2477, 0.2432],\n",
      "        [0.2560, 0.2531, 0.2477, 0.2433],\n",
      "        [0.2560, 0.2530, 0.2477, 0.2433],\n",
      "        [0.2560, 0.2530, 0.2477, 0.2433],\n",
      "        [0.2560, 0.2530, 0.2477, 0.2433],\n",
      "        [0.2560, 0.2530, 0.2477, 0.2433],\n",
      "        [0.2560, 0.2530, 0.2476, 0.2433],\n",
      "        [0.2560, 0.2530, 0.2477, 0.2433],\n",
      "        [0.2560, 0.2531, 0.2476, 0.2432],\n",
      "        [0.2560, 0.2531, 0.2477, 0.2433],\n",
      "        [0.2560, 0.2530, 0.2477, 0.2433],\n",
      "        [0.2560, 0.2530, 0.2477, 0.2433],\n",
      "        [0.2560, 0.2530, 0.2477, 0.2433],\n",
      "        [0.2560, 0.2530, 0.2477, 0.2433],\n",
      "        [0.2560, 0.2530, 0.2477, 0.2433],\n",
      "        [0.2560, 0.2530, 0.2477, 0.2433],\n",
      "        [0.2560, 0.2530, 0.2477, 0.2433],\n",
      "        [0.2560, 0.2530, 0.2477, 0.2433],\n",
      "        [0.2560, 0.2530, 0.2477, 0.2433],\n",
      "        [0.2560, 0.2530, 0.2477, 0.2433],\n",
      "        [0.2560, 0.2531, 0.2477, 0.2432],\n",
      "        [0.2560, 0.2531, 0.2477, 0.2433],\n",
      "        [0.2560, 0.2530, 0.2477, 0.2433],\n",
      "        [0.2560, 0.2530, 0.2477, 0.2433],\n",
      "        [0.2560, 0.2531, 0.2477, 0.2432],\n",
      "        [0.2560, 0.2530, 0.2477, 0.2433],\n",
      "        [0.2560, 0.2530, 0.2477, 0.2433],\n",
      "        [0.2560, 0.2530, 0.2477, 0.2433],\n",
      "        [0.2560, 0.2530, 0.2477, 0.2433],\n",
      "        [0.2559, 0.2531, 0.2477, 0.2433],\n",
      "        [0.2560, 0.2530, 0.2477, 0.2433],\n",
      "        [0.2560, 0.2530, 0.2476, 0.2433],\n",
      "        [0.2560, 0.2530, 0.2477, 0.2433],\n",
      "        [0.2560, 0.2530, 0.2477, 0.2433],\n",
      "        [0.2560, 0.2531, 0.2477, 0.2433],\n",
      "        [0.2560, 0.2530, 0.2477, 0.2433],\n",
      "        [0.2560, 0.2530, 0.2477, 0.2433],\n",
      "        [0.2560, 0.2531, 0.2477, 0.2433],\n",
      "        [0.2560, 0.2530, 0.2476, 0.2433],\n",
      "        [0.2560, 0.2530, 0.2477, 0.2433],\n",
      "        [0.2560, 0.2531, 0.2477, 0.2433],\n",
      "        [0.2560, 0.2531, 0.2477, 0.2433],\n",
      "        [0.2560, 0.2530, 0.2477, 0.2433],\n",
      "        [0.2560, 0.2530, 0.2477, 0.2433],\n",
      "        [0.2560, 0.2530, 0.2477, 0.2433],\n",
      "        [0.2560, 0.2530, 0.2477, 0.2433],\n",
      "        [0.2560, 0.2530, 0.2477, 0.2433],\n",
      "        [0.2560, 0.2530, 0.2477, 0.2433]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3861, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(0.0229, grad_fn=<AddBackward0>)\n",
      "batching here\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(7.0718e-06, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.3800, -1.3056, -1.4198, -1.4454],\n",
      "        [-1.3800, -1.3055, -1.4198, -1.4455],\n",
      "        [-1.3800, -1.3057, -1.4199, -1.4453],\n",
      "        [-1.3801, -1.3056, -1.4197, -1.4454],\n",
      "        [-1.3800, -1.3054, -1.4200, -1.4454],\n",
      "        [-1.3800, -1.3055, -1.4199, -1.4453],\n",
      "        [-1.3799, -1.3055, -1.4200, -1.4454],\n",
      "        [-1.3800, -1.3054, -1.4200, -1.4454],\n",
      "        [-1.3799, -1.3056, -1.4199, -1.4454],\n",
      "        [-1.3799, -1.3057, -1.4198, -1.4455],\n",
      "        [-1.3800, -1.3056, -1.4198, -1.4454],\n",
      "        [-1.3801, -1.3054, -1.4198, -1.4455],\n",
      "        [-1.3800, -1.3056, -1.4198, -1.4454],\n",
      "        [-1.3800, -1.3055, -1.4198, -1.4455],\n",
      "        [-1.3800, -1.3057, -1.4198, -1.4453],\n",
      "        [-1.3801, -1.3056, -1.4197, -1.4455],\n",
      "        [-1.3800, -1.3055, -1.4199, -1.4454],\n",
      "        [-1.3799, -1.3056, -1.4199, -1.4454],\n",
      "        [-1.3800, -1.3055, -1.4198, -1.4455],\n",
      "        [-1.3800, -1.3055, -1.4199, -1.4455],\n",
      "        [-1.3800, -1.3056, -1.4198, -1.4454],\n",
      "        [-1.3799, -1.3055, -1.4199, -1.4455],\n",
      "        [-1.3800, -1.3055, -1.4198, -1.4455],\n",
      "        [-1.3799, -1.3057, -1.4198, -1.4454],\n",
      "        [-1.3799, -1.3055, -1.4199, -1.4454],\n",
      "        [-1.3799, -1.3055, -1.4199, -1.4454],\n",
      "        [-1.3800, -1.3055, -1.4198, -1.4455],\n",
      "        [-1.3800, -1.3055, -1.4198, -1.4455],\n",
      "        [-1.3799, -1.3057, -1.4198, -1.4454],\n",
      "        [-1.3799, -1.3055, -1.4199, -1.4455],\n",
      "        [-1.3799, -1.3055, -1.4199, -1.4456],\n",
      "        [-1.3799, -1.3056, -1.4198, -1.4455],\n",
      "        [-1.3801, -1.3057, -1.4198, -1.4453],\n",
      "        [-1.3800, -1.3057, -1.4197, -1.4454],\n",
      "        [-1.3800, -1.3058, -1.4196, -1.4453],\n",
      "        [-1.3799, -1.3059, -1.4198, -1.4452],\n",
      "        [-1.3799, -1.3056, -1.4198, -1.4455],\n",
      "        [-1.3799, -1.3056, -1.4198, -1.4455],\n",
      "        [-1.3800, -1.3055, -1.4199, -1.4454],\n",
      "        [-1.3800, -1.3054, -1.4198, -1.4456],\n",
      "        [-1.3800, -1.3057, -1.4198, -1.4454],\n",
      "        [-1.3799, -1.3058, -1.4197, -1.4454],\n",
      "        [-1.3800, -1.3056, -1.4196, -1.4455],\n",
      "        [-1.3800, -1.3056, -1.4198, -1.4455],\n",
      "        [-1.3800, -1.3056, -1.4197, -1.4454],\n",
      "        [-1.3800, -1.3055, -1.4198, -1.4455],\n",
      "        [-1.3800, -1.3057, -1.4198, -1.4453],\n",
      "        [-1.3801, -1.3056, -1.4197, -1.4454],\n",
      "        [-1.3800, -1.3056, -1.4197, -1.4455],\n",
      "        [-1.3801, -1.3056, -1.4197, -1.4454],\n",
      "        [-1.3800, -1.3056, -1.4198, -1.4455],\n",
      "        [-1.3800, -1.3057, -1.4197, -1.4454],\n",
      "        [-1.3800, -1.3056, -1.4198, -1.4455],\n",
      "        [-1.3799, -1.3055, -1.4199, -1.4455],\n",
      "        [-1.3799, -1.3055, -1.4199, -1.4456],\n",
      "        [-1.3799, -1.3056, -1.4198, -1.4455],\n",
      "        [-1.3800, -1.3055, -1.4198, -1.4455],\n",
      "        [-1.3800, -1.3055, -1.4198, -1.4455],\n",
      "        [-1.3800, -1.3056, -1.4198, -1.4453],\n",
      "        [-1.3800, -1.3056, -1.4197, -1.4455],\n",
      "        [-1.3801, -1.3056, -1.4198, -1.4452],\n",
      "        [-1.3800, -1.3056, -1.4198, -1.4455],\n",
      "        [-1.3801, -1.3056, -1.4198, -1.4453],\n",
      "        [-1.3799, -1.3057, -1.4198, -1.4454]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([0.0054, 0.0036, 0.0023, 0.0012, 0.0056, 0.0041, 0.0026, 0.0014, 0.0056,\n",
      "        0.0044, 0.0028, 0.0015, 0.0053, 0.0035, 0.0025, 0.0012, 0.0052, 0.0036,\n",
      "        0.0028, 0.0014, 0.0057, 0.0041, 0.0026, 0.0011, 0.0056, 0.0039, 0.0029,\n",
      "        0.0014, 0.0053, 0.0038, 0.0022, 0.0011, 0.0060, 0.0045, 0.0025, 0.0014,\n",
      "        0.0058, 0.0040, 0.0028, 0.0014, 0.0059, 0.0041, 0.0026, 0.0011, 0.0050,\n",
      "        0.0036, 0.0027, 0.0014, 0.0054, 0.0043, 0.0028, 0.0015, 0.0053, 0.0039,\n",
      "        0.0025, 0.0012, 0.0054, 0.0037, 0.0025, 0.0013, 0.0055, 0.0039, 0.0025,\n",
      "        0.0013], grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(-0.0033, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2516, 0.2710, 0.2418, 0.2356],\n",
      "        [0.2516, 0.2710, 0.2418, 0.2356],\n",
      "        [0.2516, 0.2710, 0.2417, 0.2357],\n",
      "        [0.2516, 0.2710, 0.2418, 0.2357],\n",
      "        [0.2516, 0.2711, 0.2417, 0.2357],\n",
      "        [0.2516, 0.2710, 0.2417, 0.2357],\n",
      "        [0.2516, 0.2710, 0.2417, 0.2357],\n",
      "        [0.2516, 0.2711, 0.2417, 0.2357],\n",
      "        [0.2516, 0.2710, 0.2417, 0.2357],\n",
      "        [0.2516, 0.2710, 0.2418, 0.2356],\n",
      "        [0.2516, 0.2710, 0.2418, 0.2356],\n",
      "        [0.2516, 0.2711, 0.2418, 0.2356],\n",
      "        [0.2516, 0.2710, 0.2418, 0.2356],\n",
      "        [0.2516, 0.2710, 0.2418, 0.2356],\n",
      "        [0.2516, 0.2710, 0.2418, 0.2357],\n",
      "        [0.2516, 0.2710, 0.2418, 0.2356],\n",
      "        [0.2516, 0.2710, 0.2417, 0.2356],\n",
      "        [0.2516, 0.2710, 0.2417, 0.2356],\n",
      "        [0.2516, 0.2710, 0.2418, 0.2356],\n",
      "        [0.2516, 0.2710, 0.2417, 0.2356],\n",
      "        [0.2516, 0.2710, 0.2418, 0.2356],\n",
      "        [0.2516, 0.2710, 0.2417, 0.2356],\n",
      "        [0.2516, 0.2710, 0.2418, 0.2356],\n",
      "        [0.2516, 0.2710, 0.2418, 0.2357],\n",
      "        [0.2516, 0.2710, 0.2417, 0.2356],\n",
      "        [0.2516, 0.2710, 0.2417, 0.2356],\n",
      "        [0.2516, 0.2710, 0.2418, 0.2356],\n",
      "        [0.2516, 0.2710, 0.2418, 0.2356],\n",
      "        [0.2516, 0.2710, 0.2418, 0.2356],\n",
      "        [0.2516, 0.2710, 0.2417, 0.2356],\n",
      "        [0.2516, 0.2710, 0.2417, 0.2356],\n",
      "        [0.2516, 0.2710, 0.2418, 0.2356],\n",
      "        [0.2516, 0.2710, 0.2418, 0.2357],\n",
      "        [0.2516, 0.2710, 0.2418, 0.2357],\n",
      "        [0.2516, 0.2710, 0.2418, 0.2357],\n",
      "        [0.2516, 0.2709, 0.2418, 0.2357],\n",
      "        [0.2516, 0.2710, 0.2418, 0.2356],\n",
      "        [0.2516, 0.2710, 0.2418, 0.2356],\n",
      "        [0.2516, 0.2710, 0.2417, 0.2356],\n",
      "        [0.2516, 0.2711, 0.2418, 0.2356],\n",
      "        [0.2516, 0.2710, 0.2418, 0.2357],\n",
      "        [0.2516, 0.2709, 0.2418, 0.2357],\n",
      "        [0.2516, 0.2710, 0.2418, 0.2356],\n",
      "        [0.2516, 0.2710, 0.2418, 0.2356],\n",
      "        [0.2516, 0.2710, 0.2418, 0.2357],\n",
      "        [0.2516, 0.2710, 0.2418, 0.2356],\n",
      "        [0.2516, 0.2710, 0.2418, 0.2357],\n",
      "        [0.2516, 0.2710, 0.2418, 0.2356],\n",
      "        [0.2516, 0.2710, 0.2418, 0.2356],\n",
      "        [0.2516, 0.2710, 0.2418, 0.2357],\n",
      "        [0.2516, 0.2710, 0.2418, 0.2356],\n",
      "        [0.2516, 0.2710, 0.2418, 0.2357],\n",
      "        [0.2516, 0.2710, 0.2418, 0.2356],\n",
      "        [0.2516, 0.2710, 0.2417, 0.2356],\n",
      "        [0.2516, 0.2710, 0.2418, 0.2356],\n",
      "        [0.2516, 0.2710, 0.2418, 0.2356],\n",
      "        [0.2516, 0.2710, 0.2418, 0.2356],\n",
      "        [0.2516, 0.2710, 0.2418, 0.2356],\n",
      "        [0.2516, 0.2710, 0.2418, 0.2357],\n",
      "        [0.2516, 0.2710, 0.2418, 0.2356],\n",
      "        [0.2516, 0.2710, 0.2418, 0.2357],\n",
      "        [0.2516, 0.2710, 0.2418, 0.2356],\n",
      "        [0.2516, 0.2710, 0.2418, 0.2357],\n",
      "        [0.2516, 0.2710, 0.2418, 0.2356]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3849, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(-0.0172, grad_fn=<AddBackward0>)\n",
      "batching here\n",
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(0.0155, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.3832, -1.3082, -1.4176, -1.4412],\n",
      "        [-1.3832, -1.3081, -1.4177, -1.4412],\n",
      "        [-1.3833, -1.3082, -1.4177, -1.4411],\n",
      "        [-1.3833, -1.3081, -1.4178, -1.4411],\n",
      "        [-1.3832, -1.3080, -1.4179, -1.4412],\n",
      "        [-1.3832, -1.3082, -1.4179, -1.4411],\n",
      "        [-1.3832, -1.3081, -1.4180, -1.4409],\n",
      "        [-1.3833, -1.3079, -1.4181, -1.4410],\n",
      "        [-1.3833, -1.3081, -1.4178, -1.4412],\n",
      "        [-1.3833, -1.3082, -1.4176, -1.4412],\n",
      "        [-1.3832, -1.3082, -1.4177, -1.4412],\n",
      "        [-1.3832, -1.3082, -1.4177, -1.4412],\n",
      "        [-1.3832, -1.3081, -1.4176, -1.4413],\n",
      "        [-1.3832, -1.3081, -1.4177, -1.4412],\n",
      "        [-1.3832, -1.3081, -1.4177, -1.4412],\n",
      "        [-1.3833, -1.3081, -1.4177, -1.4412],\n",
      "        [-1.3832, -1.3081, -1.4177, -1.4412],\n",
      "        [-1.3832, -1.3081, -1.4178, -1.4412],\n",
      "        [-1.3833, -1.3081, -1.4177, -1.4412],\n",
      "        [-1.3833, -1.3081, -1.4177, -1.4412],\n",
      "        [-1.3832, -1.3082, -1.4178, -1.4411],\n",
      "        [-1.3832, -1.3083, -1.4177, -1.4411],\n",
      "        [-1.3832, -1.3082, -1.4177, -1.4412],\n",
      "        [-1.3832, -1.3081, -1.4177, -1.4412],\n",
      "        [-1.3832, -1.3081, -1.4177, -1.4412],\n",
      "        [-1.3832, -1.3081, -1.4178, -1.4413],\n",
      "        [-1.3833, -1.3080, -1.4177, -1.4413],\n",
      "        [-1.3833, -1.3081, -1.4177, -1.4412],\n",
      "        [-1.3833, -1.3080, -1.4177, -1.4413],\n",
      "        [-1.3833, -1.3082, -1.4176, -1.4412],\n",
      "        [-1.3832, -1.3081, -1.4177, -1.4412],\n",
      "        [-1.3833, -1.3081, -1.4178, -1.4411],\n",
      "        [-1.3833, -1.3083, -1.4177, -1.4410],\n",
      "        [-1.3832, -1.3083, -1.4176, -1.4411],\n",
      "        [-1.3831, -1.3082, -1.4177, -1.4413],\n",
      "        [-1.3832, -1.3081, -1.4177, -1.4412],\n",
      "        [-1.3832, -1.3080, -1.4177, -1.4413],\n",
      "        [-1.3832, -1.3081, -1.4177, -1.4412],\n",
      "        [-1.3832, -1.3081, -1.4178, -1.4412],\n",
      "        [-1.3832, -1.3081, -1.4177, -1.4412],\n",
      "        [-1.3833, -1.3080, -1.4177, -1.4413],\n",
      "        [-1.3833, -1.3082, -1.4177, -1.4411],\n",
      "        [-1.3832, -1.3082, -1.4176, -1.4412],\n",
      "        [-1.3833, -1.3081, -1.4177, -1.4412],\n",
      "        [-1.3832, -1.3081, -1.4177, -1.4413],\n",
      "        [-1.3832, -1.3081, -1.4177, -1.4412],\n",
      "        [-1.3833, -1.3082, -1.4176, -1.4412],\n",
      "        [-1.3833, -1.3082, -1.4177, -1.4411],\n",
      "        [-1.3833, -1.3081, -1.4177, -1.4412],\n",
      "        [-1.3832, -1.3081, -1.4177, -1.4413],\n",
      "        [-1.3833, -1.3080, -1.4177, -1.4413],\n",
      "        [-1.3833, -1.3081, -1.4176, -1.4412],\n",
      "        [-1.3832, -1.3081, -1.4178, -1.4413],\n",
      "        [-1.3833, -1.3082, -1.4177, -1.4412],\n",
      "        [-1.3832, -1.3081, -1.4177, -1.4412],\n",
      "        [-1.3833, -1.3081, -1.4177, -1.4412],\n",
      "        [-1.3832, -1.3081, -1.4177, -1.4413],\n",
      "        [-1.3832, -1.3081, -1.4177, -1.4413],\n",
      "        [-1.3832, -1.3081, -1.4177, -1.4412],\n",
      "        [-1.3833, -1.3081, -1.4177, -1.4412],\n",
      "        [-1.3832, -1.3082, -1.4178, -1.4411],\n",
      "        [-1.3832, -1.3081, -1.4177, -1.4412],\n",
      "        [-1.3832, -1.3081, -1.4178, -1.4412],\n",
      "        [-1.3832, -1.3081, -1.4177, -1.4413]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([ 4.4328e-03,  3.4033e-03,  2.3423e-03,  1.2911e-03,  4.7544e-03,\n",
      "         3.1785e-03,  2.2428e-03,  9.2701e-04,  4.2678e-03,  3.2506e-03,\n",
      "         2.4914e-03,  1.3808e-03,  4.8363e-03,  3.4699e-03,  2.1748e-03,\n",
      "         1.0962e-03,  4.5992e-03,  3.3216e-03,  2.2248e-03,  9.9828e-04,\n",
      "         4.6952e-03,  3.2084e-03,  2.3613e-03,  1.2902e-03,  4.1647e-03,\n",
      "         3.4268e-03,  2.4077e-03,  1.1552e-03,  4.8913e-03,  3.6962e-03,\n",
      "         2.2084e-03,  1.1850e-03,  4.5420e-03,  3.5574e-03,  2.3538e-03,\n",
      "         1.1674e-03,  4.2745e-03,  3.3379e-03,  2.3343e-03,  1.1609e-03,\n",
      "        -1.3039e+00,  3.2509e-03,  2.2238e-03,  1.0926e-03,  4.4972e-03,\n",
      "         3.3187e-03,  2.2991e-03,  8.1825e-04,  4.0684e-03,  3.3800e-03,\n",
      "         2.0383e-03,  8.2101e-04,  4.7118e-03,  3.2670e-03,  2.2423e-03,\n",
      "         1.0882e-03,  4.7446e-03,  3.4885e-03,  2.0727e-03,  1.2077e-03,\n",
      "         4.4872e-03,  3.1047e-03,  2.1047e-03,  1.2996e-03],\n",
      "       grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(0.0176, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2508, 0.2703, 0.2423, 0.2366],\n",
      "        [0.2508, 0.2703, 0.2423, 0.2366],\n",
      "        [0.2508, 0.2703, 0.2423, 0.2367],\n",
      "        [0.2508, 0.2703, 0.2423, 0.2367],\n",
      "        [0.2508, 0.2704, 0.2422, 0.2366],\n",
      "        [0.2508, 0.2703, 0.2422, 0.2367],\n",
      "        [0.2508, 0.2703, 0.2422, 0.2367],\n",
      "        [0.2507, 0.2704, 0.2422, 0.2367],\n",
      "        [0.2508, 0.2703, 0.2423, 0.2366],\n",
      "        [0.2507, 0.2703, 0.2423, 0.2367],\n",
      "        [0.2508, 0.2703, 0.2423, 0.2366],\n",
      "        [0.2508, 0.2703, 0.2423, 0.2367],\n",
      "        [0.2508, 0.2703, 0.2423, 0.2366],\n",
      "        [0.2508, 0.2703, 0.2423, 0.2366],\n",
      "        [0.2508, 0.2703, 0.2423, 0.2366],\n",
      "        [0.2508, 0.2703, 0.2423, 0.2366],\n",
      "        [0.2508, 0.2703, 0.2423, 0.2366],\n",
      "        [0.2508, 0.2703, 0.2422, 0.2366],\n",
      "        [0.2508, 0.2703, 0.2423, 0.2366],\n",
      "        [0.2508, 0.2703, 0.2423, 0.2366],\n",
      "        [0.2508, 0.2703, 0.2423, 0.2367],\n",
      "        [0.2508, 0.2703, 0.2423, 0.2367],\n",
      "        [0.2508, 0.2703, 0.2423, 0.2366],\n",
      "        [0.2508, 0.2703, 0.2423, 0.2367],\n",
      "        [0.2508, 0.2703, 0.2423, 0.2366],\n",
      "        [0.2508, 0.2703, 0.2423, 0.2366],\n",
      "        [0.2508, 0.2704, 0.2423, 0.2366],\n",
      "        [0.2508, 0.2703, 0.2423, 0.2366],\n",
      "        [0.2508, 0.2703, 0.2423, 0.2366],\n",
      "        [0.2508, 0.2703, 0.2423, 0.2366],\n",
      "        [0.2508, 0.2703, 0.2423, 0.2366],\n",
      "        [0.2508, 0.2703, 0.2423, 0.2367],\n",
      "        [0.2508, 0.2703, 0.2423, 0.2367],\n",
      "        [0.2508, 0.2703, 0.2423, 0.2367],\n",
      "        [0.2508, 0.2703, 0.2423, 0.2366],\n",
      "        [0.2508, 0.2703, 0.2423, 0.2366],\n",
      "        [0.2508, 0.2703, 0.2423, 0.2366],\n",
      "        [0.2508, 0.2703, 0.2423, 0.2366],\n",
      "        [0.2508, 0.2703, 0.2423, 0.2366],\n",
      "        [0.2508, 0.2703, 0.2423, 0.2366],\n",
      "        [0.2508, 0.2703, 0.2423, 0.2366],\n",
      "        [0.2508, 0.2703, 0.2423, 0.2367],\n",
      "        [0.2508, 0.2703, 0.2423, 0.2366],\n",
      "        [0.2508, 0.2703, 0.2423, 0.2366],\n",
      "        [0.2508, 0.2703, 0.2423, 0.2366],\n",
      "        [0.2508, 0.2703, 0.2423, 0.2366],\n",
      "        [0.2508, 0.2703, 0.2423, 0.2366],\n",
      "        [0.2507, 0.2703, 0.2423, 0.2367],\n",
      "        [0.2508, 0.2703, 0.2423, 0.2366],\n",
      "        [0.2508, 0.2703, 0.2423, 0.2366],\n",
      "        [0.2508, 0.2703, 0.2423, 0.2366],\n",
      "        [0.2507, 0.2703, 0.2423, 0.2366],\n",
      "        [0.2508, 0.2703, 0.2423, 0.2366],\n",
      "        [0.2508, 0.2703, 0.2423, 0.2366],\n",
      "        [0.2508, 0.2703, 0.2423, 0.2366],\n",
      "        [0.2508, 0.2703, 0.2423, 0.2366],\n",
      "        [0.2508, 0.2703, 0.2423, 0.2366],\n",
      "        [0.2508, 0.2703, 0.2423, 0.2366],\n",
      "        [0.2508, 0.2703, 0.2423, 0.2366],\n",
      "        [0.2508, 0.2703, 0.2423, 0.2366],\n",
      "        [0.2508, 0.2703, 0.2423, 0.2367],\n",
      "        [0.2508, 0.2703, 0.2423, 0.2366],\n",
      "        [0.2508, 0.2703, 0.2423, 0.2366],\n",
      "        [0.2508, 0.2703, 0.2423, 0.2366]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3850, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(0.0193, grad_fn=<AddBackward0>)\n",
      "batching here\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(2.6492e-05, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.4134, -1.2116, -1.4599, -1.4841],\n",
      "        [-1.4134, -1.2114, -1.4601, -1.4841],\n",
      "        [-1.4134, -1.2112, -1.4601, -1.4843],\n",
      "        [-1.4135, -1.2114, -1.4601, -1.4841],\n",
      "        [-1.4134, -1.2115, -1.4601, -1.4841],\n",
      "        [-1.4134, -1.2115, -1.4601, -1.4841],\n",
      "        [-1.4134, -1.2114, -1.4602, -1.4841],\n",
      "        [-1.4134, -1.2113, -1.4602, -1.4841],\n",
      "        [-1.4134, -1.2114, -1.4601, -1.4842],\n",
      "        [-1.4134, -1.2114, -1.4601, -1.4842],\n",
      "        [-1.4134, -1.2113, -1.4602, -1.4842],\n",
      "        [-1.4135, -1.2113, -1.4601, -1.4843],\n",
      "        [-1.4134, -1.2113, -1.4601, -1.4843],\n",
      "        [-1.4134, -1.2112, -1.4601, -1.4843],\n",
      "        [-1.4134, -1.2112, -1.4601, -1.4843],\n",
      "        [-1.4134, -1.2112, -1.4601, -1.4843],\n",
      "        [-1.4134, -1.2114, -1.4601, -1.4842],\n",
      "        [-1.4134, -1.2114, -1.4601, -1.4842],\n",
      "        [-1.4134, -1.2113, -1.4601, -1.4843],\n",
      "        [-1.4134, -1.2114, -1.4601, -1.4842],\n",
      "        [-1.4134, -1.2114, -1.4601, -1.4841],\n",
      "        [-1.4134, -1.2114, -1.4601, -1.4842],\n",
      "        [-1.4134, -1.2114, -1.4601, -1.4842],\n",
      "        [-1.4134, -1.2114, -1.4601, -1.4841],\n",
      "        [-1.4134, -1.2113, -1.4601, -1.4842],\n",
      "        [-1.4134, -1.2113, -1.4602, -1.4843],\n",
      "        [-1.4134, -1.2113, -1.4601, -1.4842],\n",
      "        [-1.4134, -1.2114, -1.4601, -1.4842],\n",
      "        [-1.4134, -1.2113, -1.4601, -1.4842],\n",
      "        [-1.4134, -1.2113, -1.4601, -1.4842],\n",
      "        [-1.4134, -1.2113, -1.4601, -1.4842],\n",
      "        [-1.4134, -1.2113, -1.4601, -1.4843],\n",
      "        [-1.4133, -1.2117, -1.4600, -1.4840],\n",
      "        [-1.4134, -1.2115, -1.4601, -1.4840],\n",
      "        [-1.4134, -1.2117, -1.4599, -1.4840],\n",
      "        [-1.4134, -1.2116, -1.4600, -1.4841],\n",
      "        [-1.4134, -1.2112, -1.4601, -1.4843],\n",
      "        [-1.4134, -1.2112, -1.4601, -1.4843],\n",
      "        [-1.4134, -1.2112, -1.4601, -1.4843],\n",
      "        [-1.4134, -1.2112, -1.4601, -1.4843],\n",
      "        [-1.4135, -1.2116, -1.4599, -1.4841],\n",
      "        [-1.4135, -1.2114, -1.4600, -1.4842],\n",
      "        [-1.4135, -1.2115, -1.4599, -1.4841],\n",
      "        [-1.4134, -1.2115, -1.4599, -1.4842],\n",
      "        [-1.4134, -1.2116, -1.4600, -1.4841],\n",
      "        [-1.4132, -1.2116, -1.4602, -1.4840],\n",
      "        [-1.4135, -1.2114, -1.4601, -1.4841],\n",
      "        [-1.4134, -1.2114, -1.4600, -1.4842],\n",
      "        [-1.4133, -1.2115, -1.4600, -1.4841],\n",
      "        [-1.4134, -1.2116, -1.4600, -1.4840],\n",
      "        [-1.4133, -1.2117, -1.4600, -1.4840],\n",
      "        [-1.4133, -1.2116, -1.4599, -1.4841],\n",
      "        [-1.4134, -1.2113, -1.4601, -1.4843],\n",
      "        [-1.4135, -1.2112, -1.4601, -1.4843],\n",
      "        [-1.4134, -1.2112, -1.4601, -1.4843],\n",
      "        [-1.4134, -1.2112, -1.4601, -1.4843],\n",
      "        [-1.4134, -1.2113, -1.4601, -1.4843],\n",
      "        [-1.4134, -1.2112, -1.4601, -1.4843],\n",
      "        [-1.4135, -1.2112, -1.4601, -1.4843],\n",
      "        [-1.4134, -1.2113, -1.4601, -1.4843],\n",
      "        [-1.4134, -1.2115, -1.4600, -1.4842],\n",
      "        [-1.4134, -1.2115, -1.4600, -1.4841],\n",
      "        [-1.4134, -1.2114, -1.4601, -1.4842],\n",
      "        [-1.4134, -1.2114, -1.4601, -1.4841]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([0.0112, 0.0088, 0.0065, 0.0028, 0.0106, 0.0079, 0.0043, 0.0021, 0.0106,\n",
      "        0.0069, 0.0059, 0.0031, 0.0104, 0.0079, 0.0045, 0.0027, 0.0113, 0.0082,\n",
      "        0.0057, 0.0029, 0.0090, 0.0083, 0.0047, 0.0028, 0.0092, 0.0072, 0.0059,\n",
      "        0.0031, 0.0110, 0.0082, 0.0053, 0.0028, 0.0101, 0.0084, 0.0054, 0.0026,\n",
      "        0.0108, 0.0081, 0.0055, 0.0028, 0.0091, 0.0085, 0.0045, 0.0026, 0.0111,\n",
      "        0.0082, 0.0057, 0.0024, 0.0112, 0.0085, 0.0057, 0.0024, 0.0104, 0.0079,\n",
      "        0.0045, 0.0023, 0.0109, 0.0083, 0.0053, 0.0027, 0.0103, 0.0081, 0.0056,\n",
      "        0.0028], grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(-0.0066, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2433, 0.2977, 0.2323, 0.2267],\n",
      "        [0.2433, 0.2978, 0.2322, 0.2267],\n",
      "        [0.2433, 0.2978, 0.2322, 0.2267],\n",
      "        [0.2433, 0.2978, 0.2322, 0.2267],\n",
      "        [0.2433, 0.2978, 0.2322, 0.2267],\n",
      "        [0.2433, 0.2978, 0.2322, 0.2267],\n",
      "        [0.2433, 0.2978, 0.2322, 0.2267],\n",
      "        [0.2433, 0.2978, 0.2322, 0.2267],\n",
      "        [0.2433, 0.2978, 0.2322, 0.2267],\n",
      "        [0.2433, 0.2978, 0.2322, 0.2267],\n",
      "        [0.2433, 0.2978, 0.2322, 0.2267],\n",
      "        [0.2433, 0.2978, 0.2322, 0.2267],\n",
      "        [0.2433, 0.2978, 0.2322, 0.2267],\n",
      "        [0.2433, 0.2978, 0.2322, 0.2267],\n",
      "        [0.2433, 0.2978, 0.2322, 0.2267],\n",
      "        [0.2433, 0.2978, 0.2322, 0.2267],\n",
      "        [0.2433, 0.2978, 0.2322, 0.2267],\n",
      "        [0.2433, 0.2978, 0.2322, 0.2267],\n",
      "        [0.2433, 0.2978, 0.2322, 0.2267],\n",
      "        [0.2433, 0.2978, 0.2322, 0.2267],\n",
      "        [0.2433, 0.2978, 0.2322, 0.2267],\n",
      "        [0.2433, 0.2978, 0.2322, 0.2267],\n",
      "        [0.2433, 0.2978, 0.2322, 0.2267],\n",
      "        [0.2433, 0.2978, 0.2322, 0.2267],\n",
      "        [0.2433, 0.2978, 0.2322, 0.2267],\n",
      "        [0.2433, 0.2978, 0.2322, 0.2267],\n",
      "        [0.2433, 0.2978, 0.2322, 0.2267],\n",
      "        [0.2433, 0.2978, 0.2322, 0.2267],\n",
      "        [0.2433, 0.2978, 0.2322, 0.2267],\n",
      "        [0.2433, 0.2978, 0.2322, 0.2267],\n",
      "        [0.2433, 0.2978, 0.2322, 0.2267],\n",
      "        [0.2433, 0.2978, 0.2322, 0.2267],\n",
      "        [0.2433, 0.2977, 0.2322, 0.2267],\n",
      "        [0.2433, 0.2977, 0.2322, 0.2267],\n",
      "        [0.2433, 0.2977, 0.2323, 0.2267],\n",
      "        [0.2433, 0.2977, 0.2322, 0.2267],\n",
      "        [0.2433, 0.2978, 0.2322, 0.2267],\n",
      "        [0.2433, 0.2978, 0.2322, 0.2267],\n",
      "        [0.2433, 0.2978, 0.2322, 0.2267],\n",
      "        [0.2433, 0.2978, 0.2322, 0.2267],\n",
      "        [0.2433, 0.2977, 0.2323, 0.2267],\n",
      "        [0.2433, 0.2978, 0.2322, 0.2267],\n",
      "        [0.2433, 0.2978, 0.2322, 0.2267],\n",
      "        [0.2433, 0.2977, 0.2323, 0.2267],\n",
      "        [0.2433, 0.2977, 0.2322, 0.2267],\n",
      "        [0.2434, 0.2977, 0.2322, 0.2267],\n",
      "        [0.2433, 0.2978, 0.2322, 0.2267],\n",
      "        [0.2433, 0.2978, 0.2322, 0.2267],\n",
      "        [0.2433, 0.2977, 0.2322, 0.2267],\n",
      "        [0.2433, 0.2977, 0.2322, 0.2267],\n",
      "        [0.2433, 0.2977, 0.2322, 0.2267],\n",
      "        [0.2433, 0.2977, 0.2323, 0.2267],\n",
      "        [0.2433, 0.2978, 0.2322, 0.2267],\n",
      "        [0.2433, 0.2978, 0.2322, 0.2267],\n",
      "        [0.2433, 0.2978, 0.2322, 0.2267],\n",
      "        [0.2433, 0.2978, 0.2322, 0.2267],\n",
      "        [0.2433, 0.2978, 0.2322, 0.2267],\n",
      "        [0.2433, 0.2978, 0.2322, 0.2267],\n",
      "        [0.2433, 0.2978, 0.2322, 0.2267],\n",
      "        [0.2433, 0.2978, 0.2322, 0.2267],\n",
      "        [0.2433, 0.2978, 0.2322, 0.2267],\n",
      "        [0.2433, 0.2977, 0.2322, 0.2267],\n",
      "        [0.2433, 0.2978, 0.2322, 0.2267],\n",
      "        [0.2433, 0.2978, 0.2322, 0.2267]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3801, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(-0.0204, grad_fn=<AddBackward0>)\n",
      "batching here\n",
      "17: done 8 episodes, mean_reward=0.00, best_reward=1.00, speed=167.64\n",
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(0.0176, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.4066, -1.2292, -1.4556, -1.4730],\n",
      "        [-1.4066, -1.2289, -1.4557, -1.4733],\n",
      "        [-1.4067, -1.2289, -1.4557, -1.4731],\n",
      "        [-1.4067, -1.2288, -1.4558, -1.4732],\n",
      "        [-1.4067, -1.2288, -1.4558, -1.4732],\n",
      "        [-1.4067, -1.2289, -1.4558, -1.4732],\n",
      "        [-1.4066, -1.2288, -1.4558, -1.4732],\n",
      "        [-1.4067, -1.2289, -1.4558, -1.4732],\n",
      "        [-1.4067, -1.2290, -1.4557, -1.4731],\n",
      "        [-1.4067, -1.2290, -1.4558, -1.4730],\n",
      "        [-1.4067, -1.2289, -1.4558, -1.4731],\n",
      "        [-1.4066, -1.2290, -1.4558, -1.4731],\n",
      "        [-1.4067, -1.2288, -1.4557, -1.4732],\n",
      "        [-1.4066, -1.2289, -1.4557, -1.4732],\n",
      "        [-1.4066, -1.2290, -1.4557, -1.4731],\n",
      "        [-1.4067, -1.2289, -1.4558, -1.4731],\n",
      "        [-1.4067, -1.2289, -1.4558, -1.4731],\n",
      "        [-1.4067, -1.2290, -1.4557, -1.4730],\n",
      "        [-1.4067, -1.2289, -1.4557, -1.4731],\n",
      "        [-1.4066, -1.2289, -1.4558, -1.4731],\n",
      "        [-1.4067, -1.2289, -1.4557, -1.4732],\n",
      "        [-1.4066, -1.2290, -1.4558, -1.4730],\n",
      "        [-1.4067, -1.2289, -1.4558, -1.4731],\n",
      "        [-1.4066, -1.2289, -1.4558, -1.4730],\n",
      "        [-1.4067, -1.2287, -1.4558, -1.4733],\n",
      "        [-1.4068, -1.2287, -1.4558, -1.4732],\n",
      "        [-1.4067, -1.2290, -1.4556, -1.4732],\n",
      "        [-1.4067, -1.2291, -1.4556, -1.4731],\n",
      "        [-1.4067, -1.2288, -1.4557, -1.4732],\n",
      "        [-1.4067, -1.2289, -1.4558, -1.4731],\n",
      "        [-1.4067, -1.2288, -1.4558, -1.4732],\n",
      "        [-1.4066, -1.2289, -1.4558, -1.4731],\n",
      "        [-1.4066, -1.2290, -1.4557, -1.4731],\n",
      "        [-1.4066, -1.2292, -1.4557, -1.4730],\n",
      "        [-1.4066, -1.2291, -1.4557, -1.4730],\n",
      "        [-1.4066, -1.2291, -1.4556, -1.4730],\n",
      "        [-1.4067, -1.2288, -1.4557, -1.4732],\n",
      "        [-1.4067, -1.2289, -1.4558, -1.4731],\n",
      "        [-1.4067, -1.2288, -1.4558, -1.4732],\n",
      "        [-1.4066, -1.2289, -1.4558, -1.4731],\n",
      "        [-1.4068, -1.2290, -1.4556, -1.4730],\n",
      "        [-1.4065, -1.2292, -1.4556, -1.4730],\n",
      "        [-1.4067, -1.2293, -1.4555, -1.4729],\n",
      "        [-1.4069, -1.2292, -1.4555, -1.4729],\n",
      "        [-1.4067, -1.2290, -1.4557, -1.4730],\n",
      "        [-1.4066, -1.2291, -1.4557, -1.4730],\n",
      "        [-1.4068, -1.2288, -1.4557, -1.4732],\n",
      "        [-1.4066, -1.2292, -1.4556, -1.4729],\n",
      "        [-1.4065, -1.2291, -1.4556, -1.4732],\n",
      "        [-1.4066, -1.2290, -1.4556, -1.4732],\n",
      "        [-1.4066, -1.2291, -1.4556, -1.4732],\n",
      "        [-1.4064, -1.2292, -1.4556, -1.4731],\n",
      "        [-1.4067, -1.2288, -1.4557, -1.4732],\n",
      "        [-1.4066, -1.2289, -1.4557, -1.4732],\n",
      "        [-1.4066, -1.2290, -1.4557, -1.4731],\n",
      "        [-1.4067, -1.2289, -1.4558, -1.4731],\n",
      "        [-1.4067, -1.2289, -1.4557, -1.4732],\n",
      "        [-1.4067, -1.2289, -1.4558, -1.4731],\n",
      "        [-1.4067, -1.2288, -1.4558, -1.4732],\n",
      "        [-1.4066, -1.2289, -1.4558, -1.4731],\n",
      "        [-1.4066, -1.2289, -1.4557, -1.4732],\n",
      "        [-1.4067, -1.2290, -1.4557, -1.4732],\n",
      "        [-1.4066, -1.2291, -1.4556, -1.4731],\n",
      "        [-1.4066, -1.2290, -1.4557, -1.4731]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([ 0.0069,  0.0055,  0.0038,  0.0016,  0.0076,  0.0054,  0.0038,  0.0019,\n",
      "         0.1846,  0.0055,  0.0033,  0.0016,  0.1935,  0.0058,  0.0032,  0.0019,\n",
      "         0.1845,  0.0049,  0.0040,  0.0020,  0.1935,  0.0048,  0.0038,  0.0016,\n",
      "         0.0073,  0.0061,  0.0039,  0.0019,  0.1912,  0.0048,  0.0039,  0.0020,\n",
      "         0.0066,  0.0059,  0.0041,  0.0019,  0.1614,  0.0048,  0.0038,  0.0017,\n",
      "         0.0081,  0.0060,  0.0037,  0.0022,  0.0074,  0.0045,  0.0040,  0.0016,\n",
      "        -1.4482,  0.0057,  0.0035,  0.0017,  0.1614,  0.0058,  0.0039,  0.0016,\n",
      "         0.1935,  0.0048,  0.0040,  0.0017,  0.0077,  0.0050,  0.0037,  0.0020],\n",
      "       grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(-0.0038, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2450, 0.2925, 0.2333, 0.2292],\n",
      "        [0.2450, 0.2926, 0.2332, 0.2292],\n",
      "        [0.2449, 0.2926, 0.2332, 0.2292],\n",
      "        [0.2449, 0.2926, 0.2332, 0.2292],\n",
      "        [0.2450, 0.2926, 0.2332, 0.2292],\n",
      "        [0.2450, 0.2926, 0.2332, 0.2292],\n",
      "        [0.2450, 0.2926, 0.2332, 0.2292],\n",
      "        [0.2450, 0.2926, 0.2332, 0.2292],\n",
      "        [0.2450, 0.2926, 0.2332, 0.2292],\n",
      "        [0.2450, 0.2926, 0.2332, 0.2292],\n",
      "        [0.2450, 0.2926, 0.2332, 0.2292],\n",
      "        [0.2450, 0.2926, 0.2332, 0.2292],\n",
      "        [0.2450, 0.2926, 0.2332, 0.2292],\n",
      "        [0.2450, 0.2926, 0.2332, 0.2292],\n",
      "        [0.2450, 0.2926, 0.2332, 0.2292],\n",
      "        [0.2450, 0.2926, 0.2332, 0.2292],\n",
      "        [0.2450, 0.2926, 0.2332, 0.2292],\n",
      "        [0.2450, 0.2926, 0.2332, 0.2292],\n",
      "        [0.2450, 0.2926, 0.2332, 0.2292],\n",
      "        [0.2450, 0.2926, 0.2332, 0.2292],\n",
      "        [0.2450, 0.2926, 0.2332, 0.2292],\n",
      "        [0.2450, 0.2926, 0.2332, 0.2292],\n",
      "        [0.2450, 0.2926, 0.2332, 0.2292],\n",
      "        [0.2450, 0.2926, 0.2332, 0.2292],\n",
      "        [0.2449, 0.2927, 0.2332, 0.2292],\n",
      "        [0.2449, 0.2927, 0.2332, 0.2292],\n",
      "        [0.2450, 0.2926, 0.2333, 0.2292],\n",
      "        [0.2450, 0.2926, 0.2333, 0.2292],\n",
      "        [0.2449, 0.2926, 0.2332, 0.2292],\n",
      "        [0.2450, 0.2926, 0.2332, 0.2292],\n",
      "        [0.2450, 0.2926, 0.2332, 0.2292],\n",
      "        [0.2450, 0.2926, 0.2332, 0.2292],\n",
      "        [0.2450, 0.2926, 0.2332, 0.2292],\n",
      "        [0.2450, 0.2925, 0.2332, 0.2292],\n",
      "        [0.2450, 0.2926, 0.2332, 0.2292],\n",
      "        [0.2450, 0.2925, 0.2333, 0.2292],\n",
      "        [0.2450, 0.2926, 0.2332, 0.2292],\n",
      "        [0.2450, 0.2926, 0.2332, 0.2292],\n",
      "        [0.2450, 0.2926, 0.2332, 0.2292],\n",
      "        [0.2450, 0.2926, 0.2332, 0.2292],\n",
      "        [0.2449, 0.2926, 0.2333, 0.2292],\n",
      "        [0.2450, 0.2925, 0.2333, 0.2292],\n",
      "        [0.2450, 0.2925, 0.2333, 0.2293],\n",
      "        [0.2449, 0.2925, 0.2333, 0.2293],\n",
      "        [0.2450, 0.2926, 0.2332, 0.2292],\n",
      "        [0.2450, 0.2926, 0.2332, 0.2292],\n",
      "        [0.2449, 0.2926, 0.2332, 0.2292],\n",
      "        [0.2450, 0.2925, 0.2332, 0.2292],\n",
      "        [0.2450, 0.2926, 0.2333, 0.2292],\n",
      "        [0.2450, 0.2926, 0.2333, 0.2292],\n",
      "        [0.2450, 0.2925, 0.2333, 0.2292],\n",
      "        [0.2450, 0.2925, 0.2332, 0.2292],\n",
      "        [0.2450, 0.2926, 0.2332, 0.2292],\n",
      "        [0.2450, 0.2926, 0.2332, 0.2292],\n",
      "        [0.2450, 0.2926, 0.2332, 0.2292],\n",
      "        [0.2450, 0.2926, 0.2332, 0.2292],\n",
      "        [0.2450, 0.2926, 0.2332, 0.2292],\n",
      "        [0.2450, 0.2926, 0.2332, 0.2292],\n",
      "        [0.2450, 0.2926, 0.2332, 0.2292],\n",
      "        [0.2450, 0.2926, 0.2332, 0.2292],\n",
      "        [0.2450, 0.2926, 0.2332, 0.2292],\n",
      "        [0.2450, 0.2926, 0.2332, 0.2292],\n",
      "        [0.2450, 0.2926, 0.2333, 0.2292],\n",
      "        [0.2450, 0.2926, 0.2332, 0.2292]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3813, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(3.6976e-05, grad_fn=<AddBackward0>)\n",
      "batching here\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18: done 1 episodes, mean_reward=1.00, best_reward=1.00, speed=169.28\n",
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(0.1211, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.4231, -1.2724, -1.3784, -1.4833],\n",
      "        [-1.4230, -1.2725, -1.3785, -1.4832],\n",
      "        [-1.4232, -1.2723, -1.3784, -1.4833],\n",
      "        [-1.4232, -1.2724, -1.3784, -1.4832],\n",
      "        [-1.4231, -1.2723, -1.3784, -1.4834],\n",
      "        [-1.4231, -1.2723, -1.3784, -1.4834],\n",
      "        [-1.4231, -1.2723, -1.3784, -1.4833],\n",
      "        [-1.4231, -1.2723, -1.3784, -1.4834],\n",
      "        [-1.4231, -1.2725, -1.3784, -1.4832],\n",
      "        [-1.4232, -1.2726, -1.3784, -1.4829],\n",
      "        [-1.4232, -1.2723, -1.3784, -1.4832],\n",
      "        [-1.4232, -1.2724, -1.3783, -1.4833],\n",
      "        [-1.4231, -1.2725, -1.3784, -1.4832],\n",
      "        [-1.4231, -1.2724, -1.3785, -1.4832],\n",
      "        [-1.4231, -1.2724, -1.3784, -1.4832],\n",
      "        [-1.4231, -1.2723, -1.3783, -1.4834],\n",
      "        [-1.4230, -1.2726, -1.3784, -1.4831],\n",
      "        [-1.4231, -1.2726, -1.3784, -1.4830],\n",
      "        [-1.4231, -1.2723, -1.3784, -1.4833],\n",
      "        [-1.4232, -1.2724, -1.3783, -1.4832],\n",
      "        [-1.4231, -1.2725, -1.3784, -1.4832],\n",
      "        [-1.4232, -1.2726, -1.3784, -1.4830],\n",
      "        [-1.4232, -1.2723, -1.3785, -1.4833],\n",
      "        [-1.4232, -1.2724, -1.3784, -1.4832],\n",
      "        [-1.4231, -1.2725, -1.3783, -1.4834],\n",
      "        [-1.4230, -1.2726, -1.3784, -1.4831],\n",
      "        [-1.4230, -1.2724, -1.3785, -1.4833],\n",
      "        [-1.4230, -1.2724, -1.3784, -1.4833],\n",
      "        [-1.4231, -1.2725, -1.3784, -1.4832],\n",
      "        [-1.4231, -1.2726, -1.3784, -1.4830],\n",
      "        [-1.4232, -1.2722, -1.3785, -1.4833],\n",
      "        [-1.4232, -1.2723, -1.3783, -1.4833],\n",
      "        [-1.4230, -1.2726, -1.3784, -1.4831],\n",
      "        [-1.4231, -1.2724, -1.3784, -1.4833],\n",
      "        [-1.4230, -1.2725, -1.3783, -1.4833],\n",
      "        [-1.4230, -1.2725, -1.3784, -1.4832],\n",
      "        [-1.4231, -1.2725, -1.3784, -1.4832],\n",
      "        [-1.4231, -1.2726, -1.3784, -1.4830],\n",
      "        [-1.4232, -1.2722, -1.3785, -1.4833],\n",
      "        [-1.4232, -1.2723, -1.3783, -1.4833],\n",
      "        [-1.4229, -1.2728, -1.3784, -1.4830],\n",
      "        [-1.4231, -1.2724, -1.3784, -1.4833],\n",
      "        [-1.4232, -1.2726, -1.3784, -1.4830],\n",
      "        [-1.4231, -1.2725, -1.3783, -1.4833],\n",
      "        [-1.4231, -1.2725, -1.3784, -1.4831],\n",
      "        [-1.4230, -1.2725, -1.3783, -1.4833],\n",
      "        [-1.4232, -1.2724, -1.3785, -1.4831],\n",
      "        [-1.4231, -1.2725, -1.3784, -1.4832],\n",
      "        [-1.4231, -1.2726, -1.3783, -1.4833],\n",
      "        [-1.4230, -1.2723, -1.3784, -1.4835],\n",
      "        [-1.4231, -1.2723, -1.3784, -1.4834],\n",
      "        [-1.4231, -1.2724, -1.3784, -1.4834],\n",
      "        [-1.4231, -1.2725, -1.3784, -1.4832],\n",
      "        [-1.4231, -1.2724, -1.3785, -1.4832],\n",
      "        [-1.4231, -1.2724, -1.3784, -1.4832],\n",
      "        [-1.4231, -1.2723, -1.3783, -1.4834],\n",
      "        [-1.4231, -1.2725, -1.3784, -1.4832],\n",
      "        [-1.4231, -1.2726, -1.3784, -1.4830],\n",
      "        [-1.4232, -1.2722, -1.3785, -1.4833],\n",
      "        [-1.4232, -1.2723, -1.3784, -1.4833],\n",
      "        [-1.4231, -1.2725, -1.3784, -1.4832],\n",
      "        [-1.4231, -1.2725, -1.3784, -1.4831],\n",
      "        [-1.4232, -1.2721, -1.3784, -1.4835],\n",
      "        [-1.4230, -1.2726, -1.3784, -1.4831]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([ 5.1747e-03,  4.1566e-03,  2.4650e-03,  1.3379e-03,  1.3690e-01,\n",
      "         1.3690e-01,  2.6367e-03,  1.4041e-03,  5.8890e-03,  4.3918e-03,\n",
      "         2.5464e-03,  1.5283e-03,  5.6744e-03,  3.5231e-03,  2.8755e-03,\n",
      "         1.4188e-03,  5.1474e-03,  4.1586e-03,  2.9758e-03,  1.6718e-03,\n",
      "         6.0287e-03,  4.5487e-03,  3.2393e-03,  1.8717e-03, -1.3322e+00,\n",
      "        -1.3475e+00, -1.3620e+00, -1.4215e+00,  5.5972e-03,  4.3798e-03,\n",
      "         2.8049e-03,  1.4744e-03,  4.8124e-03,  4.2034e-03,  2.5196e-03,\n",
      "         1.3635e-03,  6.0228e-03,  4.7121e-03,  2.8958e-03,  1.5973e-03,\n",
      "         4.4762e-03,  3.8107e-03,  2.8428e-03,  1.7890e-03, -1.2299e+00,\n",
      "        -1.4497e+00, -1.3623e+00, -1.4819e+00,  5.7810e-03,  3.8839e-03,\n",
      "         2.9858e-03,  1.4175e-03,  5.6744e-03,  3.8169e-03,  2.6544e-03,\n",
      "         1.4188e-03,  5.1508e-03,  4.0236e-03,  2.8973e-03,  1.4755e-03,\n",
      "         6.1109e-03,  4.1886e-03,  3.1988e-03,  1.3690e-03],\n",
      "       grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(0.1645, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2410, 0.2802, 0.2520, 0.2269],\n",
      "        [0.2410, 0.2801, 0.2520, 0.2269],\n",
      "        [0.2409, 0.2802, 0.2520, 0.2269],\n",
      "        [0.2409, 0.2802, 0.2520, 0.2269],\n",
      "        [0.2410, 0.2802, 0.2520, 0.2269],\n",
      "        [0.2410, 0.2802, 0.2520, 0.2269],\n",
      "        [0.2410, 0.2802, 0.2520, 0.2269],\n",
      "        [0.2410, 0.2802, 0.2520, 0.2269],\n",
      "        [0.2410, 0.2801, 0.2520, 0.2269],\n",
      "        [0.2409, 0.2801, 0.2520, 0.2270],\n",
      "        [0.2409, 0.2802, 0.2520, 0.2269],\n",
      "        [0.2409, 0.2802, 0.2520, 0.2269],\n",
      "        [0.2410, 0.2801, 0.2520, 0.2269],\n",
      "        [0.2410, 0.2802, 0.2520, 0.2269],\n",
      "        [0.2410, 0.2801, 0.2520, 0.2269],\n",
      "        [0.2410, 0.2802, 0.2520, 0.2269],\n",
      "        [0.2410, 0.2801, 0.2520, 0.2269],\n",
      "        [0.2410, 0.2801, 0.2520, 0.2269],\n",
      "        [0.2410, 0.2802, 0.2520, 0.2269],\n",
      "        [0.2409, 0.2802, 0.2520, 0.2269],\n",
      "        [0.2410, 0.2801, 0.2520, 0.2269],\n",
      "        [0.2410, 0.2801, 0.2520, 0.2270],\n",
      "        [0.2410, 0.2802, 0.2520, 0.2269],\n",
      "        [0.2409, 0.2802, 0.2520, 0.2269],\n",
      "        [0.2410, 0.2801, 0.2520, 0.2269],\n",
      "        [0.2410, 0.2801, 0.2520, 0.2269],\n",
      "        [0.2410, 0.2802, 0.2520, 0.2269],\n",
      "        [0.2410, 0.2802, 0.2520, 0.2269],\n",
      "        [0.2410, 0.2801, 0.2520, 0.2269],\n",
      "        [0.2410, 0.2801, 0.2520, 0.2270],\n",
      "        [0.2410, 0.2802, 0.2520, 0.2269],\n",
      "        [0.2409, 0.2802, 0.2520, 0.2269],\n",
      "        [0.2410, 0.2801, 0.2520, 0.2269],\n",
      "        [0.2410, 0.2802, 0.2520, 0.2269],\n",
      "        [0.2410, 0.2801, 0.2520, 0.2269],\n",
      "        [0.2410, 0.2801, 0.2520, 0.2269],\n",
      "        [0.2410, 0.2801, 0.2520, 0.2269],\n",
      "        [0.2410, 0.2801, 0.2520, 0.2270],\n",
      "        [0.2410, 0.2802, 0.2520, 0.2269],\n",
      "        [0.2409, 0.2802, 0.2520, 0.2269],\n",
      "        [0.2410, 0.2801, 0.2520, 0.2270],\n",
      "        [0.2410, 0.2802, 0.2520, 0.2269],\n",
      "        [0.2409, 0.2801, 0.2520, 0.2270],\n",
      "        [0.2410, 0.2801, 0.2520, 0.2269],\n",
      "        [0.2410, 0.2801, 0.2520, 0.2269],\n",
      "        [0.2410, 0.2801, 0.2520, 0.2269],\n",
      "        [0.2410, 0.2802, 0.2520, 0.2269],\n",
      "        [0.2410, 0.2801, 0.2520, 0.2269],\n",
      "        [0.2410, 0.2801, 0.2520, 0.2269],\n",
      "        [0.2410, 0.2802, 0.2520, 0.2268],\n",
      "        [0.2410, 0.2802, 0.2520, 0.2269],\n",
      "        [0.2410, 0.2802, 0.2520, 0.2269],\n",
      "        [0.2410, 0.2801, 0.2520, 0.2269],\n",
      "        [0.2410, 0.2802, 0.2520, 0.2269],\n",
      "        [0.2410, 0.2801, 0.2520, 0.2269],\n",
      "        [0.2410, 0.2802, 0.2520, 0.2269],\n",
      "        [0.2410, 0.2801, 0.2520, 0.2269],\n",
      "        [0.2410, 0.2801, 0.2520, 0.2270],\n",
      "        [0.2410, 0.2802, 0.2520, 0.2269],\n",
      "        [0.2409, 0.2802, 0.2520, 0.2269],\n",
      "        [0.2410, 0.2801, 0.2520, 0.2269],\n",
      "        [0.2410, 0.2801, 0.2520, 0.2269],\n",
      "        [0.2410, 0.2802, 0.2520, 0.2268],\n",
      "        [0.2410, 0.2801, 0.2520, 0.2269]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3833, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(0.2717, grad_fn=<AddBackward0>)\n",
      "batching here\n",
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(7.2127e-05, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.4615, -1.2707, -1.3302, -1.5003],\n",
      "        [-1.4616, -1.2708, -1.3301, -1.5004],\n",
      "        [-1.4616, -1.2707, -1.3301, -1.5005],\n",
      "        [-1.4616, -1.2708, -1.3301, -1.5003],\n",
      "        [-1.4617, -1.2705, -1.3302, -1.5004],\n",
      "        [-1.4617, -1.2705, -1.3302, -1.5004],\n",
      "        [-1.4616, -1.2706, -1.3302, -1.5003],\n",
      "        [-1.4617, -1.2706, -1.3302, -1.5003],\n",
      "        [-1.4616, -1.2707, -1.3301, -1.5004],\n",
      "        [-1.4615, -1.2709, -1.3302, -1.5001],\n",
      "        [-1.4615, -1.2707, -1.3303, -1.5003],\n",
      "        [-1.4616, -1.2707, -1.3302, -1.5004],\n",
      "        [-1.4617, -1.2707, -1.3301, -1.5004],\n",
      "        [-1.4616, -1.2708, -1.3301, -1.5003],\n",
      "        [-1.4616, -1.2707, -1.3302, -1.5003],\n",
      "        [-1.4615, -1.2707, -1.3301, -1.5004],\n",
      "        [-1.4616, -1.2707, -1.3301, -1.5004],\n",
      "        [-1.4615, -1.2709, -1.3302, -1.5001],\n",
      "        [-1.4615, -1.2707, -1.3303, -1.5002],\n",
      "        [-1.4616, -1.2707, -1.3301, -1.5004],\n",
      "        [-1.4616, -1.2706, -1.3301, -1.5004],\n",
      "        [-1.4615, -1.2709, -1.3302, -1.5001],\n",
      "        [-1.4616, -1.2706, -1.3303, -1.5002],\n",
      "        [-1.4616, -1.2707, -1.3302, -1.5004],\n",
      "        [-1.4616, -1.2707, -1.3301, -1.5004],\n",
      "        [-1.4616, -1.2707, -1.3302, -1.5002],\n",
      "        [-1.4616, -1.2707, -1.3301, -1.5004],\n",
      "        [-1.4616, -1.2707, -1.3303, -1.5001],\n",
      "        [-1.4616, -1.2707, -1.3301, -1.5004],\n",
      "        [-1.4615, -1.2709, -1.3301, -1.5001],\n",
      "        [-1.4616, -1.2706, -1.3303, -1.5003],\n",
      "        [-1.4616, -1.2706, -1.3301, -1.5004],\n",
      "        [-1.4615, -1.2709, -1.3301, -1.5002],\n",
      "        [-1.4615, -1.2708, -1.3301, -1.5003],\n",
      "        [-1.4615, -1.2709, -1.3302, -1.5001],\n",
      "        [-1.4615, -1.2709, -1.3301, -1.5002],\n",
      "        [-1.4616, -1.2707, -1.3301, -1.5004],\n",
      "        [-1.4615, -1.2709, -1.3301, -1.5001],\n",
      "        [-1.4616, -1.2706, -1.3303, -1.5003],\n",
      "        [-1.4616, -1.2706, -1.3301, -1.5004],\n",
      "        [-1.4616, -1.2706, -1.3301, -1.5004],\n",
      "        [-1.4615, -1.2707, -1.3301, -1.5003],\n",
      "        [-1.4617, -1.2706, -1.3301, -1.5004],\n",
      "        [-1.4616, -1.2707, -1.3301, -1.5003],\n",
      "        [-1.4616, -1.2707, -1.3301, -1.5003],\n",
      "        [-1.4616, -1.2707, -1.3301, -1.5003],\n",
      "        [-1.4616, -1.2707, -1.3302, -1.5003],\n",
      "        [-1.4616, -1.2706, -1.3301, -1.5004],\n",
      "        [-1.4616, -1.2707, -1.3303, -1.5002],\n",
      "        [-1.4615, -1.2706, -1.3304, -1.5003],\n",
      "        [-1.4613, -1.2712, -1.3302, -1.4999],\n",
      "        [-1.4614, -1.2709, -1.3302, -1.5001],\n",
      "        [-1.4617, -1.2707, -1.3301, -1.5004],\n",
      "        [-1.4616, -1.2708, -1.3301, -1.5003],\n",
      "        [-1.4616, -1.2707, -1.3302, -1.5003],\n",
      "        [-1.4615, -1.2707, -1.3301, -1.5004],\n",
      "        [-1.4616, -1.2707, -1.3301, -1.5004],\n",
      "        [-1.4615, -1.2709, -1.3301, -1.5001],\n",
      "        [-1.4616, -1.2706, -1.3303, -1.5003],\n",
      "        [-1.4616, -1.2706, -1.3301, -1.5004],\n",
      "        [-1.4615, -1.2707, -1.3302, -1.5003],\n",
      "        [-1.4616, -1.2706, -1.3302, -1.5003],\n",
      "        [-1.4615, -1.2709, -1.3302, -1.5001],\n",
      "        [-1.4616, -1.2709, -1.3301, -1.5001]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([0.0168, 0.0121, 0.0097, 0.0044, 0.0179, 0.0123, 0.0082, 0.0044, 0.0164,\n",
      "        0.0142, 0.0081, 0.0043, 0.0165, 0.0136, 0.0078, 0.0048, 0.0167, 0.0138,\n",
      "        0.0075, 0.0043, 0.0158, 0.0122, 0.0082, 0.0043, 0.0159, 0.0121, 0.0083,\n",
      "        0.0038, 0.0168, 0.0135, 0.0090, 0.0042, 0.0161, 0.0139, 0.0080, 0.0044,\n",
      "        0.0189, 0.0117, 0.0082, 0.0050, 0.0164, 0.0118, 0.0090, 0.0049, 0.0156,\n",
      "        0.0140, 0.0081, 0.0044, 0.0158, 0.0121, 0.0074, 0.0042, 0.0165, 0.0123,\n",
      "        0.0082, 0.0041, 0.0161, 0.0123, 0.0090, 0.0045, 0.0181, 0.0139, 0.0077,\n",
      "        0.0044], grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(-0.0105, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2319, 0.2806, 0.2644, 0.2231],\n",
      "        [0.2319, 0.2806, 0.2645, 0.2230],\n",
      "        [0.2319, 0.2806, 0.2645, 0.2230],\n",
      "        [0.2319, 0.2806, 0.2644, 0.2231],\n",
      "        [0.2318, 0.2807, 0.2644, 0.2230],\n",
      "        [0.2318, 0.2807, 0.2644, 0.2230],\n",
      "        [0.2319, 0.2806, 0.2644, 0.2231],\n",
      "        [0.2318, 0.2807, 0.2644, 0.2231],\n",
      "        [0.2319, 0.2806, 0.2644, 0.2230],\n",
      "        [0.2319, 0.2806, 0.2644, 0.2231],\n",
      "        [0.2319, 0.2806, 0.2644, 0.2231],\n",
      "        [0.2319, 0.2806, 0.2644, 0.2230],\n",
      "        [0.2319, 0.2806, 0.2645, 0.2230],\n",
      "        [0.2319, 0.2806, 0.2645, 0.2231],\n",
      "        [0.2319, 0.2806, 0.2644, 0.2231],\n",
      "        [0.2319, 0.2806, 0.2644, 0.2230],\n",
      "        [0.2319, 0.2806, 0.2645, 0.2231],\n",
      "        [0.2319, 0.2806, 0.2644, 0.2231],\n",
      "        [0.2319, 0.2806, 0.2644, 0.2231],\n",
      "        [0.2319, 0.2806, 0.2645, 0.2230],\n",
      "        [0.2319, 0.2807, 0.2644, 0.2230],\n",
      "        [0.2319, 0.2806, 0.2644, 0.2231],\n",
      "        [0.2319, 0.2807, 0.2644, 0.2231],\n",
      "        [0.2319, 0.2806, 0.2644, 0.2230],\n",
      "        [0.2319, 0.2806, 0.2645, 0.2230],\n",
      "        [0.2319, 0.2806, 0.2644, 0.2231],\n",
      "        [0.2319, 0.2806, 0.2645, 0.2230],\n",
      "        [0.2319, 0.2806, 0.2644, 0.2231],\n",
      "        [0.2319, 0.2806, 0.2645, 0.2230],\n",
      "        [0.2319, 0.2806, 0.2644, 0.2231],\n",
      "        [0.2319, 0.2807, 0.2644, 0.2231],\n",
      "        [0.2319, 0.2807, 0.2644, 0.2230],\n",
      "        [0.2319, 0.2806, 0.2644, 0.2231],\n",
      "        [0.2319, 0.2806, 0.2644, 0.2231],\n",
      "        [0.2319, 0.2806, 0.2644, 0.2231],\n",
      "        [0.2319, 0.2806, 0.2644, 0.2231],\n",
      "        [0.2319, 0.2806, 0.2645, 0.2230],\n",
      "        [0.2319, 0.2806, 0.2644, 0.2231],\n",
      "        [0.2319, 0.2807, 0.2644, 0.2231],\n",
      "        [0.2319, 0.2807, 0.2644, 0.2230],\n",
      "        [0.2319, 0.2807, 0.2645, 0.2230],\n",
      "        [0.2319, 0.2806, 0.2644, 0.2231],\n",
      "        [0.2318, 0.2807, 0.2644, 0.2230],\n",
      "        [0.2319, 0.2806, 0.2644, 0.2231],\n",
      "        [0.2319, 0.2806, 0.2644, 0.2231],\n",
      "        [0.2319, 0.2806, 0.2644, 0.2231],\n",
      "        [0.2319, 0.2806, 0.2644, 0.2231],\n",
      "        [0.2319, 0.2807, 0.2644, 0.2230],\n",
      "        [0.2319, 0.2806, 0.2644, 0.2231],\n",
      "        [0.2319, 0.2807, 0.2644, 0.2231],\n",
      "        [0.2319, 0.2805, 0.2644, 0.2232],\n",
      "        [0.2319, 0.2806, 0.2644, 0.2231],\n",
      "        [0.2319, 0.2806, 0.2645, 0.2230],\n",
      "        [0.2319, 0.2806, 0.2645, 0.2231],\n",
      "        [0.2319, 0.2806, 0.2644, 0.2231],\n",
      "        [0.2319, 0.2806, 0.2644, 0.2230],\n",
      "        [0.2319, 0.2806, 0.2644, 0.2230],\n",
      "        [0.2319, 0.2806, 0.2644, 0.2231],\n",
      "        [0.2319, 0.2807, 0.2644, 0.2231],\n",
      "        [0.2319, 0.2807, 0.2644, 0.2230],\n",
      "        [0.2319, 0.2806, 0.2644, 0.2231],\n",
      "        [0.2319, 0.2807, 0.2644, 0.2231],\n",
      "        [0.2319, 0.2806, 0.2644, 0.2231],\n",
      "        [0.2319, 0.2806, 0.2645, 0.2231]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3819, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(-0.0243, grad_fn=<AddBackward0>)\n",
      "batching here\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(0.0600, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.4408, -1.2897, -1.3477, -1.4780],\n",
      "        [-1.4409, -1.2897, -1.3477, -1.4781],\n",
      "        [-1.4408, -1.2897, -1.3477, -1.4781],\n",
      "        [-1.4408, -1.2897, -1.3477, -1.4780],\n",
      "        [-1.4409, -1.2895, -1.3478, -1.4781],\n",
      "        [-1.4410, -1.2896, -1.3477, -1.4780],\n",
      "        [-1.4409, -1.2896, -1.3478, -1.4780],\n",
      "        [-1.4409, -1.2897, -1.3478, -1.4779],\n",
      "        [-1.4408, -1.2896, -1.3479, -1.4780],\n",
      "        [-1.4408, -1.2897, -1.3478, -1.4779],\n",
      "        [-1.4409, -1.2897, -1.3477, -1.4780],\n",
      "        [-1.4409, -1.2896, -1.3477, -1.4782],\n",
      "        [-1.4408, -1.2897, -1.3477, -1.4780],\n",
      "        [-1.4408, -1.2897, -1.3477, -1.4780],\n",
      "        [-1.4409, -1.2896, -1.3477, -1.4781],\n",
      "        [-1.4408, -1.2897, -1.3477, -1.4781],\n",
      "        [-1.4408, -1.2896, -1.3478, -1.4781],\n",
      "        [-1.4408, -1.2897, -1.3478, -1.4780],\n",
      "        [-1.4408, -1.2897, -1.3477, -1.4781],\n",
      "        [-1.4408, -1.2897, -1.3477, -1.4781],\n",
      "        [-1.4408, -1.2896, -1.3478, -1.4781],\n",
      "        [-1.4408, -1.2897, -1.3478, -1.4780],\n",
      "        [-1.4409, -1.2897, -1.3477, -1.4780],\n",
      "        [-1.4408, -1.2897, -1.3477, -1.4781],\n",
      "        [-1.4409, -1.2896, -1.3478, -1.4780],\n",
      "        [-1.4408, -1.2897, -1.3478, -1.4781],\n",
      "        [-1.4409, -1.2895, -1.3478, -1.4781],\n",
      "        [-1.4409, -1.2897, -1.3478, -1.4779],\n",
      "        [-1.4408, -1.2896, -1.3478, -1.4781],\n",
      "        [-1.4408, -1.2897, -1.3478, -1.4779],\n",
      "        [-1.4409, -1.2897, -1.3477, -1.4780],\n",
      "        [-1.4408, -1.2897, -1.3477, -1.4781],\n",
      "        [-1.4408, -1.2898, -1.3477, -1.4780],\n",
      "        [-1.4407, -1.2899, -1.3478, -1.4779],\n",
      "        [-1.4407, -1.2898, -1.3478, -1.4779],\n",
      "        [-1.4407, -1.2898, -1.3477, -1.4780],\n",
      "        [-1.4408, -1.2896, -1.3478, -1.4781],\n",
      "        [-1.4408, -1.2897, -1.3478, -1.4779],\n",
      "        [-1.4409, -1.2897, -1.3477, -1.4781],\n",
      "        [-1.4408, -1.2897, -1.3477, -1.4781],\n",
      "        [-1.4408, -1.2897, -1.3477, -1.4781],\n",
      "        [-1.4408, -1.2897, -1.3477, -1.4781],\n",
      "        [-1.4408, -1.2897, -1.3477, -1.4781],\n",
      "        [-1.4408, -1.2897, -1.3477, -1.4781],\n",
      "        [-1.4407, -1.2897, -1.3477, -1.4781],\n",
      "        [-1.4409, -1.2897, -1.3477, -1.4781],\n",
      "        [-1.4408, -1.2897, -1.3477, -1.4781],\n",
      "        [-1.4408, -1.2897, -1.3478, -1.4780],\n",
      "        [-1.4408, -1.2897, -1.3477, -1.4781],\n",
      "        [-1.4407, -1.2898, -1.3478, -1.4780],\n",
      "        [-1.4407, -1.2898, -1.3478, -1.4780],\n",
      "        [-1.4408, -1.2897, -1.3478, -1.4781],\n",
      "        [-1.4408, -1.2897, -1.3477, -1.4780],\n",
      "        [-1.4408, -1.2897, -1.3477, -1.4780],\n",
      "        [-1.4409, -1.2896, -1.3477, -1.4781],\n",
      "        [-1.4408, -1.2897, -1.3477, -1.4781],\n",
      "        [-1.4408, -1.2896, -1.3478, -1.4781],\n",
      "        [-1.4408, -1.2897, -1.3478, -1.4779],\n",
      "        [-1.4409, -1.2897, -1.3477, -1.4780],\n",
      "        [-1.4408, -1.2897, -1.3477, -1.4781],\n",
      "        [-1.4408, -1.2898, -1.3476, -1.4781],\n",
      "        [-1.4408, -1.2899, -1.3478, -1.4778],\n",
      "        [-1.4407, -1.2898, -1.3477, -1.4780],\n",
      "        [-1.4408, -1.2899, -1.3478, -1.4779]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([-1.3861, -1.4390, -1.4200, -1.2863,  0.0108,  0.0076,  0.0058,  0.0024,\n",
      "         0.0116,  0.0080,  0.0051,  0.0028,  0.0108,  0.0088,  0.0060,  0.0031,\n",
      "         0.0118,  0.0076,  0.0058,  0.0027,  0.0118,  0.0081,  0.0059,  0.0029,\n",
      "         0.0109,  0.0091,  0.0057,  0.0030,  0.0118,  0.0077,  0.0052,  0.0032,\n",
      "         0.0121,  0.0091,  0.0056,  0.0032,  0.0104,  0.0090,  0.0061,  0.0033,\n",
      "         0.0104,  0.0080,  0.0061,  0.0029,  0.0109,  0.0081,  0.0054,  0.0026,\n",
      "         0.0107,  0.0084,  0.0058,  0.0032,  0.0116,  0.0091,  0.0060,  0.0027,\n",
      "         0.0118,  0.0077,  0.0052,  0.0032,  0.0119,  0.0078,  0.0054,  0.0026],\n",
      "       grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(0.0798, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2367, 0.2753, 0.2598, 0.2281],\n",
      "        [0.2367, 0.2754, 0.2598, 0.2281],\n",
      "        [0.2367, 0.2754, 0.2598, 0.2281],\n",
      "        [0.2367, 0.2754, 0.2598, 0.2281],\n",
      "        [0.2367, 0.2754, 0.2598, 0.2281],\n",
      "        [0.2367, 0.2754, 0.2598, 0.2281],\n",
      "        [0.2367, 0.2754, 0.2598, 0.2281],\n",
      "        [0.2367, 0.2754, 0.2598, 0.2281],\n",
      "        [0.2367, 0.2754, 0.2598, 0.2281],\n",
      "        [0.2367, 0.2754, 0.2598, 0.2281],\n",
      "        [0.2367, 0.2754, 0.2598, 0.2281],\n",
      "        [0.2367, 0.2754, 0.2598, 0.2281],\n",
      "        [0.2367, 0.2753, 0.2598, 0.2281],\n",
      "        [0.2367, 0.2753, 0.2598, 0.2281],\n",
      "        [0.2367, 0.2754, 0.2598, 0.2281],\n",
      "        [0.2367, 0.2754, 0.2598, 0.2281],\n",
      "        [0.2367, 0.2754, 0.2598, 0.2281],\n",
      "        [0.2367, 0.2753, 0.2598, 0.2281],\n",
      "        [0.2367, 0.2754, 0.2598, 0.2281],\n",
      "        [0.2367, 0.2754, 0.2598, 0.2281],\n",
      "        [0.2367, 0.2754, 0.2598, 0.2281],\n",
      "        [0.2367, 0.2754, 0.2598, 0.2281],\n",
      "        [0.2367, 0.2754, 0.2598, 0.2281],\n",
      "        [0.2367, 0.2754, 0.2598, 0.2281],\n",
      "        [0.2367, 0.2754, 0.2598, 0.2281],\n",
      "        [0.2367, 0.2754, 0.2598, 0.2281],\n",
      "        [0.2367, 0.2754, 0.2598, 0.2281],\n",
      "        [0.2367, 0.2754, 0.2598, 0.2281],\n",
      "        [0.2367, 0.2754, 0.2598, 0.2281],\n",
      "        [0.2367, 0.2753, 0.2598, 0.2281],\n",
      "        [0.2367, 0.2754, 0.2598, 0.2281],\n",
      "        [0.2367, 0.2754, 0.2598, 0.2281],\n",
      "        [0.2367, 0.2753, 0.2598, 0.2281],\n",
      "        [0.2368, 0.2753, 0.2598, 0.2281],\n",
      "        [0.2368, 0.2753, 0.2598, 0.2281],\n",
      "        [0.2368, 0.2753, 0.2598, 0.2281],\n",
      "        [0.2367, 0.2754, 0.2598, 0.2281],\n",
      "        [0.2367, 0.2753, 0.2598, 0.2281],\n",
      "        [0.2367, 0.2754, 0.2598, 0.2281],\n",
      "        [0.2367, 0.2754, 0.2598, 0.2281],\n",
      "        [0.2367, 0.2754, 0.2598, 0.2281],\n",
      "        [0.2367, 0.2753, 0.2598, 0.2281],\n",
      "        [0.2367, 0.2754, 0.2598, 0.2281],\n",
      "        [0.2367, 0.2754, 0.2598, 0.2281],\n",
      "        [0.2368, 0.2753, 0.2598, 0.2281],\n",
      "        [0.2367, 0.2754, 0.2598, 0.2281],\n",
      "        [0.2367, 0.2754, 0.2598, 0.2281],\n",
      "        [0.2367, 0.2754, 0.2598, 0.2281],\n",
      "        [0.2367, 0.2753, 0.2598, 0.2281],\n",
      "        [0.2368, 0.2753, 0.2598, 0.2281],\n",
      "        [0.2368, 0.2753, 0.2598, 0.2281],\n",
      "        [0.2367, 0.2754, 0.2598, 0.2281],\n",
      "        [0.2367, 0.2753, 0.2598, 0.2281],\n",
      "        [0.2367, 0.2753, 0.2598, 0.2281],\n",
      "        [0.2367, 0.2754, 0.2598, 0.2281],\n",
      "        [0.2367, 0.2754, 0.2598, 0.2281],\n",
      "        [0.2367, 0.2754, 0.2598, 0.2281],\n",
      "        [0.2367, 0.2753, 0.2598, 0.2281],\n",
      "        [0.2367, 0.2754, 0.2598, 0.2281],\n",
      "        [0.2367, 0.2754, 0.2598, 0.2281],\n",
      "        [0.2367, 0.2753, 0.2599, 0.2281],\n",
      "        [0.2367, 0.2753, 0.2598, 0.2282],\n",
      "        [0.2368, 0.2753, 0.2598, 0.2281],\n",
      "        [0.2367, 0.2753, 0.2598, 0.2281]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3835, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(0.1260, grad_fn=<AddBackward0>)\n",
      "batching here\n",
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.3975, -1.2903, -1.3796, -1.4875],\n",
      "        [-1.3977, -1.2901, -1.3795, -1.4877],\n",
      "        [-1.3977, -1.2900, -1.3796, -1.4878],\n",
      "        [-1.3977, -1.2901, -1.3796, -1.4877],\n",
      "        [-1.3976, -1.2902, -1.3796, -1.4876],\n",
      "        [-1.3975, -1.2902, -1.3797, -1.4876],\n",
      "        [-1.3976, -1.2901, -1.3797, -1.4877],\n",
      "        [-1.3976, -1.2903, -1.3796, -1.4875],\n",
      "        [-1.3975, -1.2901, -1.3796, -1.4877],\n",
      "        [-1.3976, -1.2902, -1.3796, -1.4876],\n",
      "        [-1.3976, -1.2901, -1.3796, -1.4877],\n",
      "        [-1.3975, -1.2901, -1.3796, -1.4878],\n",
      "        [-1.3976, -1.2901, -1.3796, -1.4877],\n",
      "        [-1.3976, -1.2901, -1.3796, -1.4877],\n",
      "        [-1.3976, -1.2901, -1.3796, -1.4877],\n",
      "        [-1.3976, -1.2901, -1.3796, -1.4877],\n",
      "        [-1.3976, -1.2902, -1.3796, -1.4877],\n",
      "        [-1.3976, -1.2901, -1.3796, -1.4877],\n",
      "        [-1.3976, -1.2902, -1.3796, -1.4876],\n",
      "        [-1.3976, -1.2901, -1.3796, -1.4877],\n",
      "        [-1.3976, -1.2901, -1.3796, -1.4877],\n",
      "        [-1.3976, -1.2902, -1.3796, -1.4877],\n",
      "        [-1.3976, -1.2901, -1.3796, -1.4877],\n",
      "        [-1.3976, -1.2901, -1.3796, -1.4877],\n",
      "        [-1.3976, -1.2904, -1.3796, -1.4875],\n",
      "        [-1.3976, -1.2904, -1.3797, -1.4873],\n",
      "        [-1.3976, -1.2902, -1.3797, -1.4875],\n",
      "        [-1.3976, -1.2902, -1.3797, -1.4874],\n",
      "        [-1.3976, -1.2901, -1.3796, -1.4877],\n",
      "        [-1.3976, -1.2901, -1.3796, -1.4877],\n",
      "        [-1.3976, -1.2901, -1.3796, -1.4877],\n",
      "        [-1.3976, -1.2901, -1.3796, -1.4877],\n",
      "        [-1.3975, -1.2906, -1.3796, -1.4872],\n",
      "        [-1.3976, -1.2903, -1.3796, -1.4875],\n",
      "        [-1.3976, -1.2903, -1.3797, -1.4875],\n",
      "        [-1.3974, -1.2904, -1.3798, -1.4873],\n",
      "        [-1.3975, -1.2901, -1.3797, -1.4877],\n",
      "        [-1.3976, -1.2901, -1.3796, -1.4876],\n",
      "        [-1.3976, -1.2900, -1.3796, -1.4878],\n",
      "        [-1.3976, -1.2901, -1.3796, -1.4877],\n",
      "        [-1.3976, -1.2901, -1.3796, -1.4877],\n",
      "        [-1.3976, -1.2901, -1.3796, -1.4877],\n",
      "        [-1.3976, -1.2901, -1.3795, -1.4877],\n",
      "        [-1.3976, -1.2901, -1.3795, -1.4877],\n",
      "        [-1.3976, -1.2901, -1.3796, -1.4877],\n",
      "        [-1.3976, -1.2901, -1.3796, -1.4877],\n",
      "        [-1.3977, -1.2900, -1.3796, -1.4878],\n",
      "        [-1.3977, -1.2900, -1.3795, -1.4878],\n",
      "        [-1.3975, -1.2902, -1.3797, -1.4876],\n",
      "        [-1.3975, -1.2902, -1.3797, -1.4876],\n",
      "        [-1.3975, -1.2902, -1.3797, -1.4876],\n",
      "        [-1.3975, -1.2902, -1.3797, -1.4876],\n",
      "        [-1.3976, -1.2901, -1.3796, -1.4877],\n",
      "        [-1.3976, -1.2901, -1.3796, -1.4877],\n",
      "        [-1.3976, -1.2901, -1.3796, -1.4877],\n",
      "        [-1.3976, -1.2901, -1.3796, -1.4877],\n",
      "        [-1.3976, -1.2901, -1.3796, -1.4877],\n",
      "        [-1.3976, -1.2901, -1.3796, -1.4877],\n",
      "        [-1.3976, -1.2901, -1.3796, -1.4877],\n",
      "        [-1.3976, -1.2901, -1.3796, -1.4877],\n",
      "        [-1.3976, -1.2903, -1.3795, -1.4876],\n",
      "        [-1.3976, -1.2904, -1.3795, -1.4874],\n",
      "        [-1.3975, -1.2904, -1.3795, -1.4876],\n",
      "        [-1.3976, -1.2903, -1.3796, -1.4876]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([0.0230, 0.0175, 0.0120, 0.0058, 0.0241, 0.0164, 0.0109, 0.0058, 0.0232,\n",
      "        0.0173, 0.0117, 0.0070, 0.0244, 0.0184, 0.0124, 0.0067, 0.0246, 0.0186,\n",
      "        0.0113, 0.0058, 0.0262, 0.0170, 0.0113, 0.0066, 0.0237, 0.0176, 0.0130,\n",
      "        0.0057, 0.0226, 0.0195, 0.0122, 0.0061, 0.0237, 0.0187, 0.0131, 0.0053,\n",
      "        0.0243, 0.0184, 0.0141, 0.0065, 0.0265, 0.0184, 0.0126, 0.0058, 0.0230,\n",
      "        0.0175, 0.0131, 0.0068, 0.0226, 0.0171, 0.0133, 0.0060, 0.0247, 0.0186,\n",
      "        0.0124, 0.0058, 0.0260, 0.0182, 0.0121, 0.0059, 0.0257, 0.0168, 0.0117,\n",
      "        0.0062], grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(-0.0151, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2472, 0.2752, 0.2517, 0.2259],\n",
      "        [0.2472, 0.2753, 0.2517, 0.2259],\n",
      "        [0.2472, 0.2753, 0.2517, 0.2259],\n",
      "        [0.2472, 0.2752, 0.2517, 0.2259],\n",
      "        [0.2472, 0.2752, 0.2517, 0.2259],\n",
      "        [0.2472, 0.2752, 0.2517, 0.2259],\n",
      "        [0.2472, 0.2753, 0.2517, 0.2259],\n",
      "        [0.2472, 0.2752, 0.2517, 0.2259],\n",
      "        [0.2472, 0.2752, 0.2517, 0.2259],\n",
      "        [0.2472, 0.2752, 0.2517, 0.2259],\n",
      "        [0.2472, 0.2752, 0.2517, 0.2259],\n",
      "        [0.2472, 0.2753, 0.2517, 0.2259],\n",
      "        [0.2472, 0.2752, 0.2517, 0.2259],\n",
      "        [0.2472, 0.2752, 0.2517, 0.2259],\n",
      "        [0.2472, 0.2752, 0.2517, 0.2259],\n",
      "        [0.2472, 0.2752, 0.2517, 0.2259],\n",
      "        [0.2472, 0.2752, 0.2517, 0.2259],\n",
      "        [0.2472, 0.2752, 0.2517, 0.2259],\n",
      "        [0.2472, 0.2752, 0.2517, 0.2259],\n",
      "        [0.2472, 0.2752, 0.2517, 0.2259],\n",
      "        [0.2472, 0.2752, 0.2517, 0.2259],\n",
      "        [0.2472, 0.2752, 0.2517, 0.2259],\n",
      "        [0.2472, 0.2752, 0.2517, 0.2259],\n",
      "        [0.2472, 0.2752, 0.2517, 0.2259],\n",
      "        [0.2472, 0.2752, 0.2517, 0.2259],\n",
      "        [0.2472, 0.2752, 0.2517, 0.2260],\n",
      "        [0.2472, 0.2752, 0.2517, 0.2259],\n",
      "        [0.2472, 0.2752, 0.2517, 0.2260],\n",
      "        [0.2472, 0.2752, 0.2517, 0.2259],\n",
      "        [0.2472, 0.2752, 0.2517, 0.2259],\n",
      "        [0.2472, 0.2752, 0.2517, 0.2259],\n",
      "        [0.2472, 0.2752, 0.2517, 0.2259],\n",
      "        [0.2472, 0.2751, 0.2517, 0.2260],\n",
      "        [0.2472, 0.2752, 0.2517, 0.2259],\n",
      "        [0.2472, 0.2752, 0.2517, 0.2259],\n",
      "        [0.2472, 0.2752, 0.2516, 0.2260],\n",
      "        [0.2472, 0.2752, 0.2517, 0.2259],\n",
      "        [0.2472, 0.2752, 0.2517, 0.2259],\n",
      "        [0.2472, 0.2753, 0.2517, 0.2259],\n",
      "        [0.2472, 0.2752, 0.2517, 0.2259],\n",
      "        [0.2472, 0.2752, 0.2517, 0.2259],\n",
      "        [0.2472, 0.2752, 0.2517, 0.2259],\n",
      "        [0.2472, 0.2752, 0.2517, 0.2259],\n",
      "        [0.2472, 0.2752, 0.2517, 0.2259],\n",
      "        [0.2472, 0.2752, 0.2517, 0.2259],\n",
      "        [0.2472, 0.2752, 0.2517, 0.2259],\n",
      "        [0.2472, 0.2753, 0.2517, 0.2259],\n",
      "        [0.2472, 0.2753, 0.2517, 0.2259],\n",
      "        [0.2472, 0.2752, 0.2517, 0.2259],\n",
      "        [0.2472, 0.2752, 0.2517, 0.2259],\n",
      "        [0.2472, 0.2752, 0.2516, 0.2259],\n",
      "        [0.2472, 0.2752, 0.2517, 0.2259],\n",
      "        [0.2472, 0.2752, 0.2517, 0.2259],\n",
      "        [0.2472, 0.2752, 0.2517, 0.2259],\n",
      "        [0.2472, 0.2752, 0.2517, 0.2259],\n",
      "        [0.2472, 0.2752, 0.2517, 0.2259],\n",
      "        [0.2472, 0.2752, 0.2517, 0.2259],\n",
      "        [0.2472, 0.2752, 0.2517, 0.2259],\n",
      "        [0.2472, 0.2752, 0.2517, 0.2259],\n",
      "        [0.2472, 0.2752, 0.2517, 0.2259],\n",
      "        [0.2472, 0.2752, 0.2517, 0.2259],\n",
      "        [0.2472, 0.2752, 0.2517, 0.2259],\n",
      "        [0.2472, 0.2752, 0.2517, 0.2259],\n",
      "        [0.2472, 0.2752, 0.2517, 0.2259]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3838, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(-0.0288, grad_fn=<AddBackward0>)\n",
      "batching here\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22: done 7 episodes, mean_reward=0.00, best_reward=1.00, speed=168.45\n",
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(0.1067, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.3898, -1.3119, -1.3852, -1.4641],\n",
      "        [-1.3897, -1.3121, -1.3852, -1.4639],\n",
      "        [-1.3897, -1.3121, -1.3852, -1.4640],\n",
      "        [-1.3898, -1.3121, -1.3852, -1.4639],\n",
      "        [-1.3897, -1.3121, -1.3852, -1.4639],\n",
      "        [-1.3897, -1.3122, -1.3852, -1.4639],\n",
      "        [-1.3897, -1.3122, -1.3852, -1.4639],\n",
      "        [-1.3896, -1.3122, -1.3853, -1.4639],\n",
      "        [-1.3896, -1.3122, -1.3853, -1.4639],\n",
      "        [-1.3896, -1.3122, -1.3852, -1.4639],\n",
      "        [-1.3896, -1.3122, -1.3852, -1.4639],\n",
      "        [-1.3896, -1.3122, -1.3852, -1.4639],\n",
      "        [-1.3896, -1.3121, -1.3852, -1.4639],\n",
      "        [-1.3896, -1.3121, -1.3852, -1.4639],\n",
      "        [-1.3896, -1.3121, -1.3852, -1.4639],\n",
      "        [-1.3896, -1.3121, -1.3852, -1.4639],\n",
      "        [-1.3896, -1.3122, -1.3852, -1.4639],\n",
      "        [-1.3896, -1.3122, -1.3852, -1.4639],\n",
      "        [-1.3896, -1.3122, -1.3853, -1.4639],\n",
      "        [-1.3896, -1.3122, -1.3852, -1.4639],\n",
      "        [-1.3896, -1.3122, -1.3852, -1.4639],\n",
      "        [-1.3896, -1.3121, -1.3853, -1.4639],\n",
      "        [-1.3896, -1.3122, -1.3852, -1.4639],\n",
      "        [-1.3897, -1.3121, -1.3853, -1.4639],\n",
      "        [-1.3897, -1.3122, -1.3852, -1.4638],\n",
      "        [-1.3896, -1.3122, -1.3852, -1.4639],\n",
      "        [-1.3896, -1.3121, -1.3853, -1.4639],\n",
      "        [-1.3896, -1.3122, -1.3853, -1.4639],\n",
      "        [-1.3896, -1.3121, -1.3853, -1.4639],\n",
      "        [-1.3896, -1.3121, -1.3852, -1.4640],\n",
      "        [-1.3896, -1.3122, -1.3852, -1.4639],\n",
      "        [-1.3897, -1.3121, -1.3852, -1.4639],\n",
      "        [-1.3896, -1.3123, -1.3852, -1.4638],\n",
      "        [-1.3895, -1.3125, -1.3853, -1.4636],\n",
      "        [-1.3896, -1.3123, -1.3852, -1.4638],\n",
      "        [-1.3896, -1.3122, -1.3852, -1.4639],\n",
      "        [-1.3897, -1.3121, -1.3853, -1.4639],\n",
      "        [-1.3897, -1.3122, -1.3851, -1.4639],\n",
      "        [-1.3897, -1.3121, -1.3853, -1.4639],\n",
      "        [-1.3896, -1.3121, -1.3853, -1.4639],\n",
      "        [-1.3896, -1.3122, -1.3852, -1.4640],\n",
      "        [-1.3896, -1.3122, -1.3852, -1.4639],\n",
      "        [-1.3896, -1.3122, -1.3852, -1.4639],\n",
      "        [-1.3897, -1.3122, -1.3851, -1.4639],\n",
      "        [-1.3896, -1.3122, -1.3852, -1.4639],\n",
      "        [-1.3897, -1.3122, -1.3852, -1.4639],\n",
      "        [-1.3896, -1.3122, -1.3852, -1.4639],\n",
      "        [-1.3896, -1.3122, -1.3852, -1.4639],\n",
      "        [-1.3896, -1.3122, -1.3853, -1.4639],\n",
      "        [-1.3896, -1.3122, -1.3853, -1.4639],\n",
      "        [-1.3896, -1.3122, -1.3853, -1.4639],\n",
      "        [-1.3896, -1.3122, -1.3852, -1.4640],\n",
      "        [-1.3896, -1.3121, -1.3852, -1.4639],\n",
      "        [-1.3896, -1.3121, -1.3852, -1.4639],\n",
      "        [-1.3896, -1.3121, -1.3852, -1.4639],\n",
      "        [-1.3896, -1.3121, -1.3852, -1.4639],\n",
      "        [-1.3896, -1.3121, -1.3852, -1.4640],\n",
      "        [-1.3896, -1.3121, -1.3852, -1.4640],\n",
      "        [-1.3896, -1.3121, -1.3852, -1.4639],\n",
      "        [-1.3896, -1.3121, -1.3853, -1.4639],\n",
      "        [-1.3896, -1.3122, -1.3852, -1.4639],\n",
      "        [-1.3897, -1.3123, -1.3851, -1.4638],\n",
      "        [-1.3896, -1.3122, -1.3852, -1.4639],\n",
      "        [-1.3897, -1.3122, -1.3851, -1.4639]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([ 0.0152,  0.0109,  0.0080,  0.0039,  0.0148,  0.0106,  0.0079,  0.0034,\n",
      "         0.3521,  0.3521,  0.3718,  0.3733,  0.3719,  0.3930,  0.3730,  0.3522,\n",
      "         0.3718,  0.3930,  0.3928,  0.3717,  0.3930,  0.3730,  0.3718,  0.3718,\n",
      "         0.0147,  0.0103,  0.0069,  0.0040,  0.3523,  0.3932,  0.3731,  0.3929,\n",
      "        -1.2601, -1.2767, -1.3645, -1.4602,  0.0138,  0.0107,  0.0070,  0.0036,\n",
      "         0.0153,  0.0115,  0.0073,  0.0042,  0.0151,  0.0108,  0.0070,  0.0034,\n",
      "         0.0147,  0.0116,  0.0078,  0.0040,  0.3522,  0.3930,  0.3522,  0.3930,\n",
      "         0.3523,  0.3522,  0.3730,  0.3731, -1.2982,  0.0111,  0.0081,  0.0038],\n",
      "       grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(-0.0633, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2491, 0.2693, 0.2503, 0.2313],\n",
      "        [0.2492, 0.2693, 0.2503, 0.2313],\n",
      "        [0.2491, 0.2693, 0.2503, 0.2313],\n",
      "        [0.2491, 0.2693, 0.2503, 0.2313],\n",
      "        [0.2492, 0.2692, 0.2503, 0.2313],\n",
      "        [0.2492, 0.2692, 0.2503, 0.2313],\n",
      "        [0.2492, 0.2692, 0.2503, 0.2313],\n",
      "        [0.2492, 0.2692, 0.2503, 0.2313],\n",
      "        [0.2492, 0.2692, 0.2503, 0.2313],\n",
      "        [0.2492, 0.2692, 0.2503, 0.2313],\n",
      "        [0.2492, 0.2692, 0.2503, 0.2313],\n",
      "        [0.2492, 0.2692, 0.2503, 0.2313],\n",
      "        [0.2492, 0.2692, 0.2503, 0.2313],\n",
      "        [0.2492, 0.2692, 0.2503, 0.2313],\n",
      "        [0.2492, 0.2692, 0.2503, 0.2313],\n",
      "        [0.2492, 0.2692, 0.2503, 0.2313],\n",
      "        [0.2492, 0.2692, 0.2503, 0.2313],\n",
      "        [0.2492, 0.2692, 0.2503, 0.2313],\n",
      "        [0.2492, 0.2692, 0.2503, 0.2313],\n",
      "        [0.2492, 0.2692, 0.2503, 0.2313],\n",
      "        [0.2492, 0.2692, 0.2503, 0.2313],\n",
      "        [0.2492, 0.2692, 0.2503, 0.2313],\n",
      "        [0.2492, 0.2692, 0.2503, 0.2313],\n",
      "        [0.2492, 0.2693, 0.2503, 0.2313],\n",
      "        [0.2492, 0.2692, 0.2503, 0.2313],\n",
      "        [0.2492, 0.2692, 0.2503, 0.2313],\n",
      "        [0.2492, 0.2692, 0.2503, 0.2313],\n",
      "        [0.2492, 0.2692, 0.2503, 0.2313],\n",
      "        [0.2492, 0.2692, 0.2503, 0.2313],\n",
      "        [0.2492, 0.2692, 0.2503, 0.2313],\n",
      "        [0.2492, 0.2692, 0.2503, 0.2313],\n",
      "        [0.2492, 0.2692, 0.2503, 0.2313],\n",
      "        [0.2492, 0.2692, 0.2503, 0.2314],\n",
      "        [0.2492, 0.2692, 0.2503, 0.2314],\n",
      "        [0.2492, 0.2692, 0.2503, 0.2313],\n",
      "        [0.2492, 0.2692, 0.2503, 0.2313],\n",
      "        [0.2491, 0.2692, 0.2503, 0.2313],\n",
      "        [0.2492, 0.2692, 0.2503, 0.2313],\n",
      "        [0.2492, 0.2692, 0.2502, 0.2313],\n",
      "        [0.2492, 0.2692, 0.2503, 0.2313],\n",
      "        [0.2492, 0.2692, 0.2503, 0.2313],\n",
      "        [0.2492, 0.2692, 0.2503, 0.2313],\n",
      "        [0.2492, 0.2692, 0.2503, 0.2313],\n",
      "        [0.2491, 0.2692, 0.2503, 0.2313],\n",
      "        [0.2492, 0.2692, 0.2503, 0.2313],\n",
      "        [0.2492, 0.2692, 0.2503, 0.2313],\n",
      "        [0.2492, 0.2692, 0.2503, 0.2313],\n",
      "        [0.2492, 0.2692, 0.2503, 0.2313],\n",
      "        [0.2492, 0.2692, 0.2503, 0.2313],\n",
      "        [0.2492, 0.2692, 0.2503, 0.2313],\n",
      "        [0.2492, 0.2692, 0.2503, 0.2313],\n",
      "        [0.2492, 0.2692, 0.2503, 0.2313],\n",
      "        [0.2492, 0.2692, 0.2503, 0.2313],\n",
      "        [0.2492, 0.2692, 0.2503, 0.2313],\n",
      "        [0.2492, 0.2692, 0.2503, 0.2313],\n",
      "        [0.2492, 0.2692, 0.2503, 0.2313],\n",
      "        [0.2492, 0.2692, 0.2503, 0.2313],\n",
      "        [0.2492, 0.2693, 0.2503, 0.2313],\n",
      "        [0.2492, 0.2692, 0.2503, 0.2313],\n",
      "        [0.2492, 0.2692, 0.2503, 0.2313],\n",
      "        [0.2492, 0.2692, 0.2503, 0.2313],\n",
      "        [0.2492, 0.2692, 0.2503, 0.2314],\n",
      "        [0.2492, 0.2692, 0.2503, 0.2313],\n",
      "        [0.2492, 0.2692, 0.2503, 0.2313]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3849, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(0.0296, grad_fn=<AddBackward0>)\n",
      "batching here\n",
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(1.1573e-05, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.4235, -1.2615, -1.3895, -1.4842],\n",
      "        [-1.4234, -1.2616, -1.3895, -1.4841],\n",
      "        [-1.4233, -1.2616, -1.3895, -1.4841],\n",
      "        [-1.4234, -1.2616, -1.3895, -1.4841],\n",
      "        [-1.4233, -1.2616, -1.3896, -1.4841],\n",
      "        [-1.4233, -1.2616, -1.3896, -1.4841],\n",
      "        [-1.4233, -1.2616, -1.3896, -1.4841],\n",
      "        [-1.4233, -1.2616, -1.3896, -1.4841],\n",
      "        [-1.4232, -1.2619, -1.3896, -1.4839],\n",
      "        [-1.4232, -1.2617, -1.3896, -1.4841],\n",
      "        [-1.4232, -1.2618, -1.3896, -1.4840],\n",
      "        [-1.4233, -1.2619, -1.3895, -1.4839],\n",
      "        [-1.4233, -1.2618, -1.3895, -1.4839],\n",
      "        [-1.4233, -1.2617, -1.3896, -1.4840],\n",
      "        [-1.4232, -1.2618, -1.3895, -1.4839],\n",
      "        [-1.4233, -1.2618, -1.3895, -1.4840],\n",
      "        [-1.4232, -1.2619, -1.3895, -1.4839],\n",
      "        [-1.4232, -1.2620, -1.3894, -1.4839],\n",
      "        [-1.4232, -1.2620, -1.3895, -1.4839],\n",
      "        [-1.4232, -1.2620, -1.3895, -1.4839],\n",
      "        [-1.4233, -1.2618, -1.3895, -1.4839],\n",
      "        [-1.4233, -1.2617, -1.3896, -1.4840],\n",
      "        [-1.4233, -1.2618, -1.3895, -1.4840],\n",
      "        [-1.4233, -1.2618, -1.3895, -1.4840],\n",
      "        [-1.4233, -1.2616, -1.3896, -1.4841],\n",
      "        [-1.4233, -1.2616, -1.3896, -1.4841],\n",
      "        [-1.4233, -1.2616, -1.3896, -1.4841],\n",
      "        [-1.4233, -1.2616, -1.3896, -1.4841],\n",
      "        [-1.4233, -1.2618, -1.3896, -1.4840],\n",
      "        [-1.4232, -1.2618, -1.3896, -1.4840],\n",
      "        [-1.4233, -1.2617, -1.3895, -1.4841],\n",
      "        [-1.4234, -1.2617, -1.3894, -1.4840],\n",
      "        [-1.4233, -1.2616, -1.3894, -1.4842],\n",
      "        [-1.4235, -1.2617, -1.3894, -1.4840],\n",
      "        [-1.4232, -1.2620, -1.3893, -1.4839],\n",
      "        [-1.4232, -1.2620, -1.3894, -1.4839],\n",
      "        [-1.4233, -1.2619, -1.3895, -1.4839],\n",
      "        [-1.4233, -1.2618, -1.3894, -1.4840],\n",
      "        [-1.4232, -1.2619, -1.3895, -1.4839],\n",
      "        [-1.4233, -1.2618, -1.3895, -1.4839],\n",
      "        [-1.4233, -1.2618, -1.3894, -1.4841],\n",
      "        [-1.4234, -1.2616, -1.3895, -1.4840],\n",
      "        [-1.4233, -1.2618, -1.3895, -1.4840],\n",
      "        [-1.4233, -1.2618, -1.3895, -1.4840],\n",
      "        [-1.4233, -1.2617, -1.3895, -1.4841],\n",
      "        [-1.4233, -1.2617, -1.3895, -1.4840],\n",
      "        [-1.4234, -1.2617, -1.3895, -1.4841],\n",
      "        [-1.4233, -1.2617, -1.3895, -1.4841],\n",
      "        [-1.4233, -1.2617, -1.3895, -1.4840],\n",
      "        [-1.4233, -1.2618, -1.3895, -1.4840],\n",
      "        [-1.4233, -1.2617, -1.3895, -1.4841],\n",
      "        [-1.4233, -1.2617, -1.3895, -1.4840],\n",
      "        [-1.4233, -1.2617, -1.3896, -1.4840],\n",
      "        [-1.4233, -1.2617, -1.3896, -1.4841],\n",
      "        [-1.4233, -1.2616, -1.3895, -1.4841],\n",
      "        [-1.4233, -1.2617, -1.3895, -1.4841],\n",
      "        [-1.4233, -1.2618, -1.3895, -1.4839],\n",
      "        [-1.4232, -1.2619, -1.3895, -1.4839],\n",
      "        [-1.4232, -1.2619, -1.3895, -1.4839],\n",
      "        [-1.4232, -1.2619, -1.3895, -1.4840],\n",
      "        [-1.4234, -1.2618, -1.3894, -1.4840],\n",
      "        [-1.4234, -1.2618, -1.3894, -1.4840],\n",
      "        [-1.4233, -1.2618, -1.3894, -1.4840],\n",
      "        [-1.4233, -1.2618, -1.3895, -1.4840]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([0.0069, 0.0054, 0.0035, 0.0016, 0.0073, 0.0055, 0.0035, 0.0016, 0.0063,\n",
      "        0.0058, 0.0032, 0.0018, 0.0063, 0.0057, 0.0038, 0.0018, 0.0069, 0.0045,\n",
      "        0.0031, 0.0016, 0.0072, 0.0050, 0.0038, 0.0019, 0.0062, 0.0053, 0.0035,\n",
      "        0.0019, 0.0069, 0.0054, 0.0035, 0.0018, 0.0063, 0.0052, 0.0033, 0.0017,\n",
      "        0.0069, 0.0052, 0.0036, 0.0016, 0.0074, 0.0049, 0.0038, 0.0021, 0.0070,\n",
      "        0.0053, 0.0035, 0.0016, 0.0069, 0.0052, 0.0030, 0.0016, 0.0062, 0.0047,\n",
      "        0.0035, 0.0019, 0.0070, 0.0046, 0.0031, 0.0017, 0.0069, 0.0048, 0.0036,\n",
      "        0.0019], grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(-0.0043, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2409, 0.2832, 0.2492, 0.2267],\n",
      "        [0.2409, 0.2832, 0.2492, 0.2267],\n",
      "        [0.2409, 0.2832, 0.2492, 0.2267],\n",
      "        [0.2409, 0.2832, 0.2492, 0.2267],\n",
      "        [0.2409, 0.2832, 0.2492, 0.2267],\n",
      "        [0.2409, 0.2832, 0.2492, 0.2267],\n",
      "        [0.2409, 0.2832, 0.2492, 0.2267],\n",
      "        [0.2409, 0.2832, 0.2492, 0.2267],\n",
      "        [0.2409, 0.2831, 0.2492, 0.2267],\n",
      "        [0.2409, 0.2832, 0.2492, 0.2267],\n",
      "        [0.2410, 0.2831, 0.2492, 0.2267],\n",
      "        [0.2409, 0.2831, 0.2492, 0.2267],\n",
      "        [0.2409, 0.2831, 0.2492, 0.2267],\n",
      "        [0.2409, 0.2832, 0.2492, 0.2267],\n",
      "        [0.2409, 0.2831, 0.2492, 0.2267],\n",
      "        [0.2409, 0.2831, 0.2492, 0.2267],\n",
      "        [0.2409, 0.2831, 0.2492, 0.2267],\n",
      "        [0.2409, 0.2831, 0.2492, 0.2268],\n",
      "        [0.2409, 0.2831, 0.2492, 0.2268],\n",
      "        [0.2410, 0.2831, 0.2492, 0.2267],\n",
      "        [0.2409, 0.2831, 0.2492, 0.2267],\n",
      "        [0.2409, 0.2832, 0.2492, 0.2267],\n",
      "        [0.2409, 0.2831, 0.2492, 0.2267],\n",
      "        [0.2409, 0.2831, 0.2492, 0.2267],\n",
      "        [0.2409, 0.2832, 0.2492, 0.2267],\n",
      "        [0.2409, 0.2832, 0.2492, 0.2267],\n",
      "        [0.2409, 0.2832, 0.2492, 0.2267],\n",
      "        [0.2409, 0.2832, 0.2492, 0.2267],\n",
      "        [0.2409, 0.2832, 0.2492, 0.2267],\n",
      "        [0.2409, 0.2832, 0.2492, 0.2267],\n",
      "        [0.2409, 0.2832, 0.2492, 0.2267],\n",
      "        [0.2409, 0.2832, 0.2492, 0.2267],\n",
      "        [0.2409, 0.2832, 0.2492, 0.2267],\n",
      "        [0.2409, 0.2832, 0.2492, 0.2267],\n",
      "        [0.2409, 0.2831, 0.2492, 0.2268],\n",
      "        [0.2409, 0.2831, 0.2492, 0.2268],\n",
      "        [0.2409, 0.2831, 0.2492, 0.2268],\n",
      "        [0.2409, 0.2831, 0.2492, 0.2267],\n",
      "        [0.2409, 0.2831, 0.2492, 0.2267],\n",
      "        [0.2409, 0.2831, 0.2492, 0.2267],\n",
      "        [0.2409, 0.2831, 0.2492, 0.2267],\n",
      "        [0.2409, 0.2832, 0.2492, 0.2267],\n",
      "        [0.2409, 0.2831, 0.2492, 0.2267],\n",
      "        [0.2409, 0.2831, 0.2492, 0.2267],\n",
      "        [0.2409, 0.2832, 0.2492, 0.2267],\n",
      "        [0.2409, 0.2832, 0.2492, 0.2267],\n",
      "        [0.2409, 0.2832, 0.2492, 0.2267],\n",
      "        [0.2409, 0.2832, 0.2492, 0.2267],\n",
      "        [0.2409, 0.2832, 0.2492, 0.2267],\n",
      "        [0.2409, 0.2832, 0.2492, 0.2267],\n",
      "        [0.2409, 0.2832, 0.2492, 0.2267],\n",
      "        [0.2409, 0.2832, 0.2492, 0.2267],\n",
      "        [0.2409, 0.2832, 0.2492, 0.2267],\n",
      "        [0.2409, 0.2832, 0.2492, 0.2267],\n",
      "        [0.2409, 0.2832, 0.2492, 0.2267],\n",
      "        [0.2409, 0.2832, 0.2492, 0.2267],\n",
      "        [0.2409, 0.2831, 0.2492, 0.2267],\n",
      "        [0.2409, 0.2831, 0.2492, 0.2267],\n",
      "        [0.2409, 0.2831, 0.2492, 0.2268],\n",
      "        [0.2409, 0.2831, 0.2492, 0.2267],\n",
      "        [0.2409, 0.2832, 0.2492, 0.2267],\n",
      "        [0.2409, 0.2831, 0.2492, 0.2267],\n",
      "        [0.2409, 0.2832, 0.2492, 0.2267],\n",
      "        [0.2409, 0.2832, 0.2492, 0.2267]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3829, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(-0.0181, grad_fn=<AddBackward0>)\n",
      "batching here\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24: done 4 episodes, mean_reward=1.00, best_reward=2.00, speed=189.16\n",
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(0.0617, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.4216, -1.2672, -1.3891, -1.4795],\n",
      "        [-1.4216, -1.2673, -1.3891, -1.4794],\n",
      "        [-1.4216, -1.2673, -1.3891, -1.4794],\n",
      "        [-1.4216, -1.2674, -1.3891, -1.4794],\n",
      "        [-1.4215, -1.2674, -1.3892, -1.4793],\n",
      "        [-1.4216, -1.2675, -1.3891, -1.4792],\n",
      "        [-1.4216, -1.2674, -1.3892, -1.4792],\n",
      "        [-1.4214, -1.2676, -1.3892, -1.4791],\n",
      "        [-1.4215, -1.2677, -1.3892, -1.4790],\n",
      "        [-1.4215, -1.2674, -1.3892, -1.4792],\n",
      "        [-1.4214, -1.2676, -1.3891, -1.4793],\n",
      "        [-1.4214, -1.2676, -1.3891, -1.4792],\n",
      "        [-1.4215, -1.2677, -1.3891, -1.4791],\n",
      "        [-1.4215, -1.2674, -1.3892, -1.4792],\n",
      "        [-1.4214, -1.2676, -1.3891, -1.4792],\n",
      "        [-1.4215, -1.2676, -1.3890, -1.4791],\n",
      "        [-1.4214, -1.2677, -1.3890, -1.4792],\n",
      "        [-1.4214, -1.2676, -1.3891, -1.4791],\n",
      "        [-1.4214, -1.2676, -1.3891, -1.4793],\n",
      "        [-1.4214, -1.2676, -1.3890, -1.4792],\n",
      "        [-1.4215, -1.2677, -1.3891, -1.4791],\n",
      "        [-1.4215, -1.2674, -1.3892, -1.4792],\n",
      "        [-1.4214, -1.2676, -1.3891, -1.4792],\n",
      "        [-1.4215, -1.2676, -1.3890, -1.4791],\n",
      "        [-1.4215, -1.2674, -1.3891, -1.4793],\n",
      "        [-1.4215, -1.2675, -1.3892, -1.4792],\n",
      "        [-1.4215, -1.2675, -1.3891, -1.4792],\n",
      "        [-1.4215, -1.2676, -1.3891, -1.4792],\n",
      "        [-1.4214, -1.2676, -1.3891, -1.4792],\n",
      "        [-1.4215, -1.2676, -1.3891, -1.4792],\n",
      "        [-1.4214, -1.2676, -1.3891, -1.4792],\n",
      "        [-1.4215, -1.2676, -1.3890, -1.4792],\n",
      "        [-1.4216, -1.2674, -1.3889, -1.4793],\n",
      "        [-1.4216, -1.2673, -1.3890, -1.4795],\n",
      "        [-1.4215, -1.2676, -1.3890, -1.4793],\n",
      "        [-1.4215, -1.2675, -1.3891, -1.4792],\n",
      "        [-1.4215, -1.2676, -1.3891, -1.4792],\n",
      "        [-1.4215, -1.2675, -1.3891, -1.4792],\n",
      "        [-1.4214, -1.2676, -1.3891, -1.4791],\n",
      "        [-1.4215, -1.2676, -1.3891, -1.4792],\n",
      "        [-1.4214, -1.2677, -1.3891, -1.4790],\n",
      "        [-1.4215, -1.2678, -1.3890, -1.4789],\n",
      "        [-1.4214, -1.2677, -1.3891, -1.4791],\n",
      "        [-1.4215, -1.2676, -1.3890, -1.4792],\n",
      "        [-1.4215, -1.2675, -1.3891, -1.4793],\n",
      "        [-1.4215, -1.2676, -1.3891, -1.4792],\n",
      "        [-1.4215, -1.2675, -1.3891, -1.4792],\n",
      "        [-1.4214, -1.2676, -1.3891, -1.4792],\n",
      "        [-1.4216, -1.2674, -1.3891, -1.4793],\n",
      "        [-1.4216, -1.2674, -1.3890, -1.4793],\n",
      "        [-1.4215, -1.2674, -1.3891, -1.4793],\n",
      "        [-1.4216, -1.2674, -1.3891, -1.4793],\n",
      "        [-1.4214, -1.2676, -1.3891, -1.4792],\n",
      "        [-1.4215, -1.2676, -1.3891, -1.4792],\n",
      "        [-1.4214, -1.2676, -1.3891, -1.4792],\n",
      "        [-1.4215, -1.2676, -1.3890, -1.4792],\n",
      "        [-1.4214, -1.2676, -1.3891, -1.4792],\n",
      "        [-1.4214, -1.2676, -1.3891, -1.4791],\n",
      "        [-1.4215, -1.2675, -1.3891, -1.4793],\n",
      "        [-1.4215, -1.2676, -1.3890, -1.4792],\n",
      "        [-1.4215, -1.2676, -1.3890, -1.4792],\n",
      "        [-1.4214, -1.2677, -1.3890, -1.4792],\n",
      "        [-1.4215, -1.2676, -1.3890, -1.4792],\n",
      "        [-1.4215, -1.2678, -1.3890, -1.4790]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([ 5.5932e-03,  4.8581e-03,  3.1199e-03,  1.6142e-03,  1.5668e-01,\n",
      "         4.8336e-03,  2.7942e-03,  1.1949e-03,  6.1570e-03,  4.8913e-03,\n",
      "         2.9251e-03,  1.3544e-03,  5.6621e-03,  4.8259e-03,  3.1837e-03,\n",
      "         1.6073e-03,  6.1954e-03,  4.2746e-03,  2.8082e-03,  1.6435e-03,\n",
      "         6.5607e-03,  4.3634e-03,  2.8643e-03,  1.6188e-03,  1.6031e-01,\n",
      "         4.3906e-03,  3.3106e-03,  1.7801e-03,  5.7434e-03,  4.8322e-03,\n",
      "         3.1040e-03,  1.5278e-03,  6.1962e-03,  4.2290e-03,  2.7266e-03,\n",
      "         1.1802e-03,  6.2376e-03,  4.8789e-03,  2.7644e-03,  1.4652e-03,\n",
      "        -1.3420e+00, -1.3569e+00, -1.2522e+00, -1.4200e+00,  1.6044e-01,\n",
      "         4.7472e-03,  3.3485e-03,  1.4793e-03,  1.4317e-01,  1.6692e-01,\n",
      "         1.6043e-01,  1.6702e-01,  6.3286e-03,  4.2627e-03,  3.0984e-03,\n",
      "         1.5426e-03,  5.5429e-03,  4.6825e-03,  2.8432e-03,  1.5422e-03,\n",
      "         6.7263e-03,  5.2535e-03,  3.5904e-03,  1.4600e-03],\n",
      "       grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(0.0634, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2413, 0.2816, 0.2493, 0.2278],\n",
      "        [0.2413, 0.2816, 0.2493, 0.2278],\n",
      "        [0.2413, 0.2816, 0.2493, 0.2278],\n",
      "        [0.2413, 0.2816, 0.2493, 0.2278],\n",
      "        [0.2413, 0.2816, 0.2493, 0.2278],\n",
      "        [0.2413, 0.2815, 0.2493, 0.2278],\n",
      "        [0.2413, 0.2815, 0.2493, 0.2278],\n",
      "        [0.2414, 0.2815, 0.2493, 0.2278],\n",
      "        [0.2414, 0.2815, 0.2493, 0.2279],\n",
      "        [0.2414, 0.2816, 0.2493, 0.2278],\n",
      "        [0.2414, 0.2815, 0.2493, 0.2278],\n",
      "        [0.2414, 0.2815, 0.2493, 0.2278],\n",
      "        [0.2414, 0.2815, 0.2493, 0.2279],\n",
      "        [0.2413, 0.2816, 0.2493, 0.2278],\n",
      "        [0.2414, 0.2815, 0.2493, 0.2278],\n",
      "        [0.2414, 0.2815, 0.2493, 0.2278],\n",
      "        [0.2414, 0.2815, 0.2493, 0.2278],\n",
      "        [0.2414, 0.2815, 0.2493, 0.2278],\n",
      "        [0.2414, 0.2815, 0.2493, 0.2278],\n",
      "        [0.2414, 0.2815, 0.2493, 0.2278],\n",
      "        [0.2414, 0.2815, 0.2493, 0.2279],\n",
      "        [0.2413, 0.2816, 0.2493, 0.2278],\n",
      "        [0.2414, 0.2815, 0.2493, 0.2278],\n",
      "        [0.2413, 0.2815, 0.2493, 0.2278],\n",
      "        [0.2413, 0.2816, 0.2493, 0.2278],\n",
      "        [0.2413, 0.2815, 0.2493, 0.2278],\n",
      "        [0.2414, 0.2815, 0.2493, 0.2278],\n",
      "        [0.2414, 0.2815, 0.2493, 0.2278],\n",
      "        [0.2414, 0.2815, 0.2493, 0.2278],\n",
      "        [0.2414, 0.2815, 0.2493, 0.2278],\n",
      "        [0.2414, 0.2815, 0.2493, 0.2278],\n",
      "        [0.2414, 0.2815, 0.2493, 0.2278],\n",
      "        [0.2413, 0.2816, 0.2493, 0.2278],\n",
      "        [0.2413, 0.2816, 0.2493, 0.2278],\n",
      "        [0.2413, 0.2815, 0.2493, 0.2278],\n",
      "        [0.2414, 0.2815, 0.2493, 0.2278],\n",
      "        [0.2414, 0.2815, 0.2493, 0.2278],\n",
      "        [0.2414, 0.2815, 0.2493, 0.2278],\n",
      "        [0.2414, 0.2815, 0.2493, 0.2278],\n",
      "        [0.2413, 0.2815, 0.2493, 0.2278],\n",
      "        [0.2414, 0.2815, 0.2493, 0.2279],\n",
      "        [0.2414, 0.2814, 0.2493, 0.2279],\n",
      "        [0.2414, 0.2815, 0.2493, 0.2278],\n",
      "        [0.2413, 0.2815, 0.2493, 0.2278],\n",
      "        [0.2414, 0.2815, 0.2493, 0.2278],\n",
      "        [0.2414, 0.2815, 0.2493, 0.2278],\n",
      "        [0.2414, 0.2815, 0.2493, 0.2278],\n",
      "        [0.2414, 0.2815, 0.2493, 0.2278],\n",
      "        [0.2413, 0.2816, 0.2493, 0.2278],\n",
      "        [0.2413, 0.2816, 0.2493, 0.2278],\n",
      "        [0.2413, 0.2816, 0.2493, 0.2278],\n",
      "        [0.2413, 0.2816, 0.2493, 0.2278],\n",
      "        [0.2414, 0.2815, 0.2493, 0.2278],\n",
      "        [0.2414, 0.2815, 0.2493, 0.2278],\n",
      "        [0.2414, 0.2815, 0.2493, 0.2278],\n",
      "        [0.2414, 0.2815, 0.2493, 0.2278],\n",
      "        [0.2414, 0.2815, 0.2493, 0.2278],\n",
      "        [0.2414, 0.2815, 0.2493, 0.2278],\n",
      "        [0.2414, 0.2815, 0.2493, 0.2278],\n",
      "        [0.2414, 0.2815, 0.2493, 0.2278],\n",
      "        [0.2413, 0.2815, 0.2493, 0.2278],\n",
      "        [0.2414, 0.2815, 0.2493, 0.2278],\n",
      "        [0.2413, 0.2815, 0.2493, 0.2278],\n",
      "        [0.2413, 0.2815, 0.2493, 0.2279]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3832, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(0.1113, grad_fn=<AddBackward0>)\n",
      "batching here\n",
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(3.8289e-05, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.4391, -1.2379, -1.3494, -1.5442],\n",
      "        [-1.4390, -1.2380, -1.3495, -1.5442],\n",
      "        [-1.4390, -1.2379, -1.3495, -1.5442],\n",
      "        [-1.4390, -1.2379, -1.3495, -1.5443],\n",
      "        [-1.4390, -1.2380, -1.3495, -1.5441],\n",
      "        [-1.4390, -1.2381, -1.3495, -1.5441],\n",
      "        [-1.4390, -1.2381, -1.3495, -1.5441],\n",
      "        [-1.4390, -1.2380, -1.3494, -1.5442],\n",
      "        [-1.4388, -1.2382, -1.3495, -1.5440],\n",
      "        [-1.4389, -1.2381, -1.3495, -1.5440],\n",
      "        [-1.4388, -1.2381, -1.3495, -1.5441],\n",
      "        [-1.4388, -1.2382, -1.3496, -1.5441],\n",
      "        [-1.4389, -1.2382, -1.3495, -1.5440],\n",
      "        [-1.4390, -1.2381, -1.3495, -1.5440],\n",
      "        [-1.4389, -1.2381, -1.3494, -1.5441],\n",
      "        [-1.4389, -1.2381, -1.3495, -1.5441],\n",
      "        [-1.4389, -1.2383, -1.3494, -1.5440],\n",
      "        [-1.4389, -1.2382, -1.3494, -1.5440],\n",
      "        [-1.4388, -1.2383, -1.3495, -1.5441],\n",
      "        [-1.4388, -1.2383, -1.3495, -1.5440],\n",
      "        [-1.4389, -1.2382, -1.3495, -1.5441],\n",
      "        [-1.4389, -1.2381, -1.3495, -1.5441],\n",
      "        [-1.4389, -1.2382, -1.3494, -1.5441],\n",
      "        [-1.4389, -1.2381, -1.3495, -1.5441],\n",
      "        [-1.4389, -1.2381, -1.3494, -1.5441],\n",
      "        [-1.4390, -1.2381, -1.3494, -1.5441],\n",
      "        [-1.4389, -1.2381, -1.3495, -1.5440],\n",
      "        [-1.4390, -1.2380, -1.3494, -1.5443],\n",
      "        [-1.4389, -1.2382, -1.3495, -1.5441],\n",
      "        [-1.4389, -1.2382, -1.3494, -1.5441],\n",
      "        [-1.4388, -1.2382, -1.3494, -1.5441],\n",
      "        [-1.4388, -1.2382, -1.3494, -1.5441],\n",
      "        [-1.4388, -1.2382, -1.3494, -1.5441],\n",
      "        [-1.4388, -1.2383, -1.3494, -1.5440],\n",
      "        [-1.4389, -1.2383, -1.3494, -1.5440],\n",
      "        [-1.4388, -1.2383, -1.3494, -1.5440],\n",
      "        [-1.4390, -1.2381, -1.3494, -1.5441],\n",
      "        [-1.4389, -1.2381, -1.3495, -1.5441],\n",
      "        [-1.4389, -1.2381, -1.3494, -1.5441],\n",
      "        [-1.4389, -1.2382, -1.3494, -1.5442],\n",
      "        [-1.4390, -1.2381, -1.3493, -1.5441],\n",
      "        [-1.4390, -1.2380, -1.3494, -1.5442],\n",
      "        [-1.4390, -1.2381, -1.3493, -1.5442],\n",
      "        [-1.4390, -1.2381, -1.3494, -1.5442],\n",
      "        [-1.4390, -1.2381, -1.3494, -1.5441],\n",
      "        [-1.4390, -1.2382, -1.3494, -1.5440],\n",
      "        [-1.4390, -1.2380, -1.3495, -1.5442],\n",
      "        [-1.4390, -1.2381, -1.3494, -1.5441],\n",
      "        [-1.4388, -1.2381, -1.3495, -1.5441],\n",
      "        [-1.4388, -1.2381, -1.3496, -1.5441],\n",
      "        [-1.4387, -1.2381, -1.3496, -1.5441],\n",
      "        [-1.4388, -1.2381, -1.3495, -1.5441],\n",
      "        [-1.4389, -1.2382, -1.3495, -1.5440],\n",
      "        [-1.4389, -1.2382, -1.3494, -1.5440],\n",
      "        [-1.4388, -1.2382, -1.3494, -1.5441],\n",
      "        [-1.4388, -1.2382, -1.3494, -1.5441],\n",
      "        [-1.4389, -1.2382, -1.3494, -1.5440],\n",
      "        [-1.4389, -1.2382, -1.3494, -1.5441],\n",
      "        [-1.4388, -1.2382, -1.3494, -1.5441],\n",
      "        [-1.4388, -1.2382, -1.3494, -1.5441],\n",
      "        [-1.4388, -1.2383, -1.3495, -1.5440],\n",
      "        [-1.4388, -1.2382, -1.3494, -1.5441],\n",
      "        [-1.4389, -1.2382, -1.3494, -1.5440],\n",
      "        [-1.4389, -1.2382, -1.3494, -1.5441]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([0.0110, 0.0096, 0.0063, 0.0033, 0.0131, 0.0091, 0.0071, 0.0035, 0.0130,\n",
      "        0.0098, 0.0057, 0.0030, 0.0129, 0.0105, 0.0057, 0.0031, 0.0120, 0.0105,\n",
      "        0.0071, 0.0031, 0.0111, 0.0086, 0.0062, 0.0032, 0.0127, 0.0088, 0.0070,\n",
      "        0.0037, 0.0122, 0.0085, 0.0058, 0.0035, 0.0138, 0.0082, 0.0064, 0.0031,\n",
      "        0.0122, 0.0099, 0.0060, 0.0035, 0.0113, 0.0101, 0.0064, 0.0032, 0.0141,\n",
      "        0.0098, 0.0058, 0.0033, 0.0113, 0.0107, 0.0066, 0.0038, 0.0122, 0.0085,\n",
      "        0.0072, 0.0030, 0.0120, 0.0083, 0.0057, 0.0034, 0.0108, 0.0089, 0.0055,\n",
      "        0.0032], grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(-0.0078, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2372, 0.2900, 0.2594, 0.2135],\n",
      "        [0.2372, 0.2900, 0.2594, 0.2135],\n",
      "        [0.2372, 0.2900, 0.2594, 0.2135],\n",
      "        [0.2372, 0.2900, 0.2594, 0.2135],\n",
      "        [0.2372, 0.2900, 0.2594, 0.2135],\n",
      "        [0.2372, 0.2899, 0.2594, 0.2135],\n",
      "        [0.2372, 0.2899, 0.2594, 0.2135],\n",
      "        [0.2372, 0.2900, 0.2594, 0.2135],\n",
      "        [0.2372, 0.2899, 0.2594, 0.2135],\n",
      "        [0.2372, 0.2899, 0.2594, 0.2135],\n",
      "        [0.2372, 0.2899, 0.2594, 0.2135],\n",
      "        [0.2372, 0.2899, 0.2594, 0.2135],\n",
      "        [0.2372, 0.2899, 0.2594, 0.2135],\n",
      "        [0.2372, 0.2899, 0.2594, 0.2135],\n",
      "        [0.2372, 0.2899, 0.2594, 0.2135],\n",
      "        [0.2372, 0.2899, 0.2594, 0.2135],\n",
      "        [0.2372, 0.2899, 0.2594, 0.2135],\n",
      "        [0.2372, 0.2899, 0.2594, 0.2135],\n",
      "        [0.2372, 0.2899, 0.2594, 0.2135],\n",
      "        [0.2372, 0.2899, 0.2594, 0.2135],\n",
      "        [0.2372, 0.2899, 0.2594, 0.2135],\n",
      "        [0.2372, 0.2899, 0.2594, 0.2135],\n",
      "        [0.2372, 0.2899, 0.2594, 0.2135],\n",
      "        [0.2372, 0.2899, 0.2594, 0.2135],\n",
      "        [0.2372, 0.2899, 0.2594, 0.2135],\n",
      "        [0.2372, 0.2899, 0.2594, 0.2135],\n",
      "        [0.2372, 0.2899, 0.2594, 0.2135],\n",
      "        [0.2372, 0.2900, 0.2594, 0.2135],\n",
      "        [0.2372, 0.2899, 0.2594, 0.2135],\n",
      "        [0.2372, 0.2899, 0.2594, 0.2135],\n",
      "        [0.2372, 0.2899, 0.2594, 0.2135],\n",
      "        [0.2372, 0.2899, 0.2594, 0.2135],\n",
      "        [0.2372, 0.2899, 0.2594, 0.2135],\n",
      "        [0.2372, 0.2899, 0.2594, 0.2135],\n",
      "        [0.2372, 0.2899, 0.2594, 0.2135],\n",
      "        [0.2372, 0.2899, 0.2594, 0.2135],\n",
      "        [0.2372, 0.2899, 0.2594, 0.2135],\n",
      "        [0.2372, 0.2899, 0.2594, 0.2135],\n",
      "        [0.2372, 0.2899, 0.2594, 0.2135],\n",
      "        [0.2372, 0.2899, 0.2594, 0.2135],\n",
      "        [0.2372, 0.2899, 0.2594, 0.2135],\n",
      "        [0.2372, 0.2900, 0.2594, 0.2135],\n",
      "        [0.2372, 0.2899, 0.2594, 0.2135],\n",
      "        [0.2372, 0.2899, 0.2594, 0.2135],\n",
      "        [0.2372, 0.2899, 0.2594, 0.2135],\n",
      "        [0.2372, 0.2899, 0.2594, 0.2135],\n",
      "        [0.2372, 0.2900, 0.2594, 0.2135],\n",
      "        [0.2372, 0.2899, 0.2594, 0.2135],\n",
      "        [0.2372, 0.2899, 0.2594, 0.2135],\n",
      "        [0.2372, 0.2899, 0.2594, 0.2135],\n",
      "        [0.2372, 0.2899, 0.2593, 0.2135],\n",
      "        [0.2372, 0.2899, 0.2594, 0.2135],\n",
      "        [0.2372, 0.2899, 0.2594, 0.2135],\n",
      "        [0.2372, 0.2899, 0.2594, 0.2135],\n",
      "        [0.2372, 0.2899, 0.2594, 0.2135],\n",
      "        [0.2372, 0.2899, 0.2594, 0.2135],\n",
      "        [0.2372, 0.2899, 0.2594, 0.2135],\n",
      "        [0.2372, 0.2899, 0.2594, 0.2135],\n",
      "        [0.2372, 0.2899, 0.2594, 0.2135],\n",
      "        [0.2372, 0.2899, 0.2594, 0.2135],\n",
      "        [0.2372, 0.2899, 0.2594, 0.2135],\n",
      "        [0.2372, 0.2899, 0.2594, 0.2135],\n",
      "        [0.2372, 0.2899, 0.2594, 0.2135],\n",
      "        [0.2372, 0.2899, 0.2594, 0.2135]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3800, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(-0.0216, grad_fn=<AddBackward0>)\n",
      "batching here\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(2.8302e-05, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.4369, -1.2452, -1.3503, -1.5357],\n",
      "        [-1.4368, -1.2453, -1.3504, -1.5356],\n",
      "        [-1.4369, -1.2452, -1.3504, -1.5357],\n",
      "        [-1.4369, -1.2452, -1.3504, -1.5357],\n",
      "        [-1.4370, -1.2453, -1.3503, -1.5356],\n",
      "        [-1.4368, -1.2453, -1.3503, -1.5357],\n",
      "        [-1.4368, -1.2454, -1.3503, -1.5356],\n",
      "        [-1.4369, -1.2453, -1.3503, -1.5357],\n",
      "        [-1.4367, -1.2454, -1.3504, -1.5355],\n",
      "        [-1.4368, -1.2453, -1.3503, -1.5357],\n",
      "        [-1.4368, -1.2453, -1.3504, -1.5357],\n",
      "        [-1.4367, -1.2454, -1.3504, -1.5356],\n",
      "        [-1.4368, -1.2454, -1.3504, -1.5355],\n",
      "        [-1.4368, -1.2453, -1.3503, -1.5357],\n",
      "        [-1.4368, -1.2453, -1.3503, -1.5358],\n",
      "        [-1.4368, -1.2454, -1.3503, -1.5357],\n",
      "        [-1.4367, -1.2454, -1.3503, -1.5356],\n",
      "        [-1.4368, -1.2453, -1.3503, -1.5357],\n",
      "        [-1.4368, -1.2454, -1.3503, -1.5357],\n",
      "        [-1.4367, -1.2454, -1.3503, -1.5356],\n",
      "        [-1.4368, -1.2454, -1.3504, -1.5356],\n",
      "        [-1.4368, -1.2453, -1.3503, -1.5357],\n",
      "        [-1.4368, -1.2453, -1.3503, -1.5358],\n",
      "        [-1.4368, -1.2453, -1.3503, -1.5357],\n",
      "        [-1.4369, -1.2453, -1.3502, -1.5357],\n",
      "        [-1.4369, -1.2454, -1.3503, -1.5356],\n",
      "        [-1.4369, -1.2453, -1.3503, -1.5356],\n",
      "        [-1.4368, -1.2454, -1.3503, -1.5356],\n",
      "        [-1.4368, -1.2453, -1.3503, -1.5357],\n",
      "        [-1.4367, -1.2454, -1.3503, -1.5356],\n",
      "        [-1.4368, -1.2453, -1.3503, -1.5357],\n",
      "        [-1.4368, -1.2453, -1.3503, -1.5357],\n",
      "        [-1.4367, -1.2454, -1.3503, -1.5357],\n",
      "        [-1.4367, -1.2456, -1.3502, -1.5356],\n",
      "        [-1.4368, -1.2454, -1.3502, -1.5357],\n",
      "        [-1.4367, -1.2456, -1.3502, -1.5356],\n",
      "        [-1.4368, -1.2453, -1.3504, -1.5357],\n",
      "        [-1.4368, -1.2455, -1.3503, -1.5356],\n",
      "        [-1.4368, -1.2453, -1.3503, -1.5357],\n",
      "        [-1.4367, -1.2455, -1.3503, -1.5357],\n",
      "        [-1.4369, -1.2455, -1.3502, -1.5355],\n",
      "        [-1.4369, -1.2453, -1.3503, -1.5357],\n",
      "        [-1.4370, -1.2452, -1.3503, -1.5357],\n",
      "        [-1.4369, -1.2454, -1.3502, -1.5357],\n",
      "        [-1.4368, -1.2454, -1.3503, -1.5357],\n",
      "        [-1.4368, -1.2454, -1.3503, -1.5356],\n",
      "        [-1.4369, -1.2453, -1.3503, -1.5356],\n",
      "        [-1.4368, -1.2453, -1.3503, -1.5357],\n",
      "        [-1.4366, -1.2454, -1.3504, -1.5357],\n",
      "        [-1.4367, -1.2454, -1.3504, -1.5356],\n",
      "        [-1.4367, -1.2453, -1.3503, -1.5358],\n",
      "        [-1.4368, -1.2453, -1.3503, -1.5358],\n",
      "        [-1.4368, -1.2453, -1.3503, -1.5357],\n",
      "        [-1.4367, -1.2454, -1.3503, -1.5356],\n",
      "        [-1.4368, -1.2453, -1.3503, -1.5357],\n",
      "        [-1.4368, -1.2453, -1.3503, -1.5357],\n",
      "        [-1.4368, -1.2454, -1.3503, -1.5356],\n",
      "        [-1.4368, -1.2453, -1.3503, -1.5357],\n",
      "        [-1.4368, -1.2453, -1.3503, -1.5357],\n",
      "        [-1.4368, -1.2454, -1.3503, -1.5357],\n",
      "        [-1.4368, -1.2454, -1.3503, -1.5357],\n",
      "        [-1.4367, -1.2455, -1.3503, -1.5355],\n",
      "        [-1.4368, -1.2454, -1.3504, -1.5355],\n",
      "        [-1.4367, -1.2454, -1.3503, -1.5356]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([0.0111, 0.0070, 0.0059, 0.0028, 0.0097, 0.0086, 0.0059, 0.0026, 0.0102,\n",
      "        0.0089, 0.0053, 0.0026, 0.0095, 0.0072, 0.0053, 0.0024, 0.0103, 0.0091,\n",
      "        0.0056, 0.0023, 0.0096, 0.0078, 0.0061, 0.0030, 0.0118, 0.0072, 0.0052,\n",
      "        0.0025, 0.0095, 0.0078, 0.0049, 0.0026, 0.0115, 0.0073, 0.0064, 0.0024,\n",
      "        0.0112, 0.0090, 0.0060, 0.0028, 0.0111, 0.0078, 0.0058, 0.0030, 0.0121,\n",
      "        0.0086, 0.0056, 0.0030, 0.0095, 0.0077, 0.0048, 0.0028, 0.0109, 0.0083,\n",
      "        0.0054, 0.0028, 0.0096, 0.0074, 0.0049, 0.0028, 0.0097, 0.0077, 0.0047,\n",
      "        0.0030], grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(-0.0067, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2377, 0.2879, 0.2592, 0.2153],\n",
      "        [0.2377, 0.2879, 0.2591, 0.2153],\n",
      "        [0.2377, 0.2879, 0.2591, 0.2153],\n",
      "        [0.2377, 0.2879, 0.2591, 0.2153],\n",
      "        [0.2376, 0.2879, 0.2592, 0.2153],\n",
      "        [0.2377, 0.2878, 0.2592, 0.2153],\n",
      "        [0.2377, 0.2878, 0.2592, 0.2153],\n",
      "        [0.2377, 0.2879, 0.2592, 0.2153],\n",
      "        [0.2377, 0.2878, 0.2591, 0.2153],\n",
      "        [0.2377, 0.2878, 0.2592, 0.2153],\n",
      "        [0.2377, 0.2879, 0.2591, 0.2153],\n",
      "        [0.2377, 0.2878, 0.2591, 0.2153],\n",
      "        [0.2377, 0.2878, 0.2591, 0.2153],\n",
      "        [0.2377, 0.2878, 0.2592, 0.2153],\n",
      "        [0.2377, 0.2878, 0.2592, 0.2153],\n",
      "        [0.2377, 0.2878, 0.2592, 0.2153],\n",
      "        [0.2377, 0.2878, 0.2592, 0.2153],\n",
      "        [0.2377, 0.2878, 0.2592, 0.2153],\n",
      "        [0.2377, 0.2878, 0.2592, 0.2153],\n",
      "        [0.2377, 0.2878, 0.2592, 0.2153],\n",
      "        [0.2377, 0.2878, 0.2591, 0.2153],\n",
      "        [0.2377, 0.2878, 0.2592, 0.2153],\n",
      "        [0.2377, 0.2879, 0.2592, 0.2153],\n",
      "        [0.2377, 0.2878, 0.2592, 0.2153],\n",
      "        [0.2377, 0.2879, 0.2592, 0.2153],\n",
      "        [0.2377, 0.2878, 0.2592, 0.2153],\n",
      "        [0.2377, 0.2878, 0.2592, 0.2153],\n",
      "        [0.2377, 0.2878, 0.2592, 0.2153],\n",
      "        [0.2377, 0.2878, 0.2592, 0.2153],\n",
      "        [0.2377, 0.2878, 0.2592, 0.2153],\n",
      "        [0.2377, 0.2878, 0.2592, 0.2153],\n",
      "        [0.2377, 0.2878, 0.2592, 0.2153],\n",
      "        [0.2377, 0.2878, 0.2592, 0.2153],\n",
      "        [0.2377, 0.2878, 0.2592, 0.2153],\n",
      "        [0.2377, 0.2878, 0.2592, 0.2153],\n",
      "        [0.2377, 0.2878, 0.2592, 0.2153],\n",
      "        [0.2377, 0.2879, 0.2591, 0.2153],\n",
      "        [0.2377, 0.2878, 0.2592, 0.2153],\n",
      "        [0.2377, 0.2878, 0.2592, 0.2153],\n",
      "        [0.2377, 0.2878, 0.2592, 0.2153],\n",
      "        [0.2377, 0.2878, 0.2592, 0.2153],\n",
      "        [0.2377, 0.2878, 0.2592, 0.2153],\n",
      "        [0.2376, 0.2879, 0.2592, 0.2153],\n",
      "        [0.2377, 0.2878, 0.2592, 0.2153],\n",
      "        [0.2377, 0.2878, 0.2592, 0.2153],\n",
      "        [0.2377, 0.2878, 0.2592, 0.2153],\n",
      "        [0.2377, 0.2879, 0.2592, 0.2153],\n",
      "        [0.2377, 0.2878, 0.2592, 0.2153],\n",
      "        [0.2377, 0.2878, 0.2591, 0.2153],\n",
      "        [0.2377, 0.2878, 0.2591, 0.2153],\n",
      "        [0.2377, 0.2879, 0.2592, 0.2153],\n",
      "        [0.2377, 0.2879, 0.2592, 0.2153],\n",
      "        [0.2377, 0.2878, 0.2592, 0.2153],\n",
      "        [0.2377, 0.2878, 0.2592, 0.2153],\n",
      "        [0.2377, 0.2878, 0.2592, 0.2153],\n",
      "        [0.2377, 0.2878, 0.2592, 0.2153],\n",
      "        [0.2377, 0.2878, 0.2592, 0.2153],\n",
      "        [0.2377, 0.2879, 0.2592, 0.2153],\n",
      "        [0.2377, 0.2878, 0.2592, 0.2153],\n",
      "        [0.2377, 0.2878, 0.2592, 0.2153],\n",
      "        [0.2377, 0.2878, 0.2592, 0.2153],\n",
      "        [0.2377, 0.2878, 0.2592, 0.2153],\n",
      "        [0.2377, 0.2878, 0.2591, 0.2153],\n",
      "        [0.2377, 0.2878, 0.2592, 0.2153]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3806, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(-0.0204, grad_fn=<AddBackward0>)\n",
      "batching here\n",
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(0.0154, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.4347, -1.2530, -1.3504, -1.5278],\n",
      "        [-1.4346, -1.2530, -1.3504, -1.5278],\n",
      "        [-1.4347, -1.2530, -1.3503, -1.5279],\n",
      "        [-1.4347, -1.2530, -1.3503, -1.5279],\n",
      "        [-1.4346, -1.2532, -1.3503, -1.5277],\n",
      "        [-1.4347, -1.2531, -1.3503, -1.5279],\n",
      "        [-1.4346, -1.2531, -1.3503, -1.5278],\n",
      "        [-1.4346, -1.2531, -1.3503, -1.5277],\n",
      "        [-1.4346, -1.2531, -1.3504, -1.5278],\n",
      "        [-1.4346, -1.2531, -1.3504, -1.5278],\n",
      "        [-1.4346, -1.2532, -1.3503, -1.5277],\n",
      "        [-1.4345, -1.2532, -1.3503, -1.5277],\n",
      "        [-1.4346, -1.2532, -1.3503, -1.5277],\n",
      "        [-1.4346, -1.2532, -1.3503, -1.5278],\n",
      "        [-1.4346, -1.2531, -1.3503, -1.5278],\n",
      "        [-1.4346, -1.2531, -1.3503, -1.5278],\n",
      "        [-1.4345, -1.2531, -1.3503, -1.5278],\n",
      "        [-1.4345, -1.2532, -1.3503, -1.5278],\n",
      "        [-1.4345, -1.2531, -1.3504, -1.5278],\n",
      "        [-1.4346, -1.2531, -1.3503, -1.5278],\n",
      "        [-1.4345, -1.2532, -1.3503, -1.5277],\n",
      "        [-1.4345, -1.2531, -1.3503, -1.5278],\n",
      "        [-1.4345, -1.2532, -1.3503, -1.5278],\n",
      "        [-1.4346, -1.2531, -1.3503, -1.5278],\n",
      "        [-1.4346, -1.2532, -1.3503, -1.5277],\n",
      "        [-1.4346, -1.2532, -1.3503, -1.5277],\n",
      "        [-1.4347, -1.2531, -1.3502, -1.5279],\n",
      "        [-1.4347, -1.2531, -1.3502, -1.5278],\n",
      "        [-1.4346, -1.2531, -1.3503, -1.5278],\n",
      "        [-1.4346, -1.2532, -1.3504, -1.5277],\n",
      "        [-1.4346, -1.2531, -1.3503, -1.5278],\n",
      "        [-1.4346, -1.2531, -1.3503, -1.5278],\n",
      "        [-1.4345, -1.2533, -1.3503, -1.5277],\n",
      "        [-1.4345, -1.2532, -1.3503, -1.5278],\n",
      "        [-1.4346, -1.2533, -1.3503, -1.5276],\n",
      "        [-1.4344, -1.2534, -1.3503, -1.5276],\n",
      "        [-1.4346, -1.2531, -1.3503, -1.5278],\n",
      "        [-1.4345, -1.2533, -1.3503, -1.5276],\n",
      "        [-1.4347, -1.2530, -1.3503, -1.5278],\n",
      "        [-1.4348, -1.2530, -1.3502, -1.5278],\n",
      "        [-1.4346, -1.2532, -1.3503, -1.5277],\n",
      "        [-1.4346, -1.2533, -1.3502, -1.5276],\n",
      "        [-1.4347, -1.2532, -1.3502, -1.5277],\n",
      "        [-1.4346, -1.2532, -1.3503, -1.5277],\n",
      "        [-1.4346, -1.2532, -1.3503, -1.5277],\n",
      "        [-1.4346, -1.2532, -1.3503, -1.5276],\n",
      "        [-1.4346, -1.2532, -1.3502, -1.5277],\n",
      "        [-1.4346, -1.2531, -1.3502, -1.5278],\n",
      "        [-1.4344, -1.2531, -1.3504, -1.5279],\n",
      "        [-1.4344, -1.2532, -1.3504, -1.5278],\n",
      "        [-1.4344, -1.2531, -1.3504, -1.5279],\n",
      "        [-1.4344, -1.2532, -1.3504, -1.5278],\n",
      "        [-1.4346, -1.2531, -1.3503, -1.5278],\n",
      "        [-1.4346, -1.2532, -1.3504, -1.5277],\n",
      "        [-1.4346, -1.2531, -1.3503, -1.5278],\n",
      "        [-1.4346, -1.2531, -1.3503, -1.5278],\n",
      "        [-1.4346, -1.2531, -1.3503, -1.5278],\n",
      "        [-1.4346, -1.2531, -1.3503, -1.5278],\n",
      "        [-1.4346, -1.2531, -1.3503, -1.5278],\n",
      "        [-1.4346, -1.2531, -1.3503, -1.5278],\n",
      "        [-1.4346, -1.2532, -1.3503, -1.5277],\n",
      "        [-1.4345, -1.2532, -1.3503, -1.5277],\n",
      "        [-1.4345, -1.2532, -1.3503, -1.5278],\n",
      "        [-1.4345, -1.2532, -1.3503, -1.5277]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([ 0.0093,  0.0074,  0.0053,  0.0025,  0.0103,  0.0081,  0.0051,  0.0025,\n",
      "         0.0097,  0.0069,  0.0043,  0.0026,  0.0092,  0.0069,  0.0047,  0.0023,\n",
      "         0.0105,  0.0065,  0.0054,  0.0028,  0.0086,  0.0071,  0.0044,  0.0022,\n",
      "         0.0086,  0.0079,  0.0049,  0.0025,  0.0092,  0.0069,  0.0053,  0.0024,\n",
      "         0.0105,  0.0080,  0.0053,  0.0021, -1.4248,  0.0068,  0.0051,  0.0028,\n",
      "         0.0099,  0.0068,  0.0046,  0.0027,  0.0090,  0.0064,  0.0050,  0.0024,\n",
      "         0.0098,  0.0074,  0.0049,  0.0022,  0.0086,  0.0078,  0.0049,  0.0022,\n",
      "         0.0093,  0.0065,  0.0043,  0.0027,  0.0086,  0.0069,  0.0049,  0.0023],\n",
      "       grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(0.0164, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2382, 0.2856, 0.2591, 0.2170],\n",
      "        [0.2382, 0.2856, 0.2591, 0.2170],\n",
      "        [0.2382, 0.2857, 0.2592, 0.2170],\n",
      "        [0.2382, 0.2857, 0.2592, 0.2170],\n",
      "        [0.2382, 0.2856, 0.2592, 0.2170],\n",
      "        [0.2382, 0.2856, 0.2592, 0.2170],\n",
      "        [0.2382, 0.2856, 0.2592, 0.2170],\n",
      "        [0.2382, 0.2856, 0.2592, 0.2170],\n",
      "        [0.2382, 0.2856, 0.2591, 0.2170],\n",
      "        [0.2382, 0.2856, 0.2591, 0.2170],\n",
      "        [0.2382, 0.2856, 0.2592, 0.2170],\n",
      "        [0.2382, 0.2856, 0.2592, 0.2170],\n",
      "        [0.2382, 0.2856, 0.2592, 0.2170],\n",
      "        [0.2382, 0.2856, 0.2592, 0.2170],\n",
      "        [0.2382, 0.2856, 0.2592, 0.2170],\n",
      "        [0.2382, 0.2856, 0.2592, 0.2170],\n",
      "        [0.2382, 0.2856, 0.2592, 0.2170],\n",
      "        [0.2382, 0.2856, 0.2592, 0.2170],\n",
      "        [0.2382, 0.2856, 0.2591, 0.2170],\n",
      "        [0.2382, 0.2856, 0.2592, 0.2170],\n",
      "        [0.2382, 0.2856, 0.2592, 0.2170],\n",
      "        [0.2382, 0.2856, 0.2592, 0.2170],\n",
      "        [0.2382, 0.2856, 0.2592, 0.2170],\n",
      "        [0.2382, 0.2856, 0.2592, 0.2170],\n",
      "        [0.2382, 0.2856, 0.2592, 0.2170],\n",
      "        [0.2382, 0.2856, 0.2592, 0.2170],\n",
      "        [0.2382, 0.2856, 0.2592, 0.2170],\n",
      "        [0.2382, 0.2856, 0.2592, 0.2170],\n",
      "        [0.2382, 0.2856, 0.2592, 0.2170],\n",
      "        [0.2382, 0.2856, 0.2591, 0.2170],\n",
      "        [0.2382, 0.2856, 0.2592, 0.2170],\n",
      "        [0.2382, 0.2856, 0.2592, 0.2170],\n",
      "        [0.2382, 0.2856, 0.2592, 0.2170],\n",
      "        [0.2382, 0.2856, 0.2592, 0.2170],\n",
      "        [0.2382, 0.2856, 0.2592, 0.2171],\n",
      "        [0.2383, 0.2855, 0.2592, 0.2170],\n",
      "        [0.2382, 0.2856, 0.2592, 0.2170],\n",
      "        [0.2382, 0.2856, 0.2592, 0.2170],\n",
      "        [0.2382, 0.2856, 0.2592, 0.2170],\n",
      "        [0.2382, 0.2856, 0.2592, 0.2170],\n",
      "        [0.2382, 0.2856, 0.2592, 0.2170],\n",
      "        [0.2382, 0.2856, 0.2592, 0.2170],\n",
      "        [0.2382, 0.2856, 0.2592, 0.2170],\n",
      "        [0.2382, 0.2856, 0.2592, 0.2170],\n",
      "        [0.2382, 0.2856, 0.2592, 0.2170],\n",
      "        [0.2382, 0.2856, 0.2592, 0.2171],\n",
      "        [0.2382, 0.2856, 0.2592, 0.2170],\n",
      "        [0.2382, 0.2856, 0.2592, 0.2170],\n",
      "        [0.2383, 0.2856, 0.2591, 0.2170],\n",
      "        [0.2383, 0.2856, 0.2591, 0.2170],\n",
      "        [0.2383, 0.2856, 0.2591, 0.2170],\n",
      "        [0.2383, 0.2856, 0.2591, 0.2170],\n",
      "        [0.2382, 0.2856, 0.2592, 0.2170],\n",
      "        [0.2382, 0.2856, 0.2591, 0.2170],\n",
      "        [0.2382, 0.2856, 0.2592, 0.2170],\n",
      "        [0.2382, 0.2856, 0.2592, 0.2170],\n",
      "        [0.2382, 0.2856, 0.2592, 0.2170],\n",
      "        [0.2382, 0.2856, 0.2592, 0.2170],\n",
      "        [0.2382, 0.2856, 0.2592, 0.2170],\n",
      "        [0.2382, 0.2856, 0.2592, 0.2170],\n",
      "        [0.2382, 0.2856, 0.2592, 0.2170],\n",
      "        [0.2382, 0.2856, 0.2592, 0.2170],\n",
      "        [0.2382, 0.2856, 0.2592, 0.2170],\n",
      "        [0.2382, 0.2856, 0.2592, 0.2170]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3811, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(0.0181, grad_fn=<AddBackward0>)\n",
      "batching here\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28: done 6 episodes, mean_reward=0.17, best_reward=2.00, speed=187.43\n",
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(0.0138, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.3802, -1.2773, -1.3672, -1.5378],\n",
      "        [-1.3802, -1.2773, -1.3672, -1.5378],\n",
      "        [-1.3802, -1.2773, -1.3672, -1.5378],\n",
      "        [-1.3802, -1.2773, -1.3672, -1.5377],\n",
      "        [-1.3802, -1.2774, -1.3672, -1.5377],\n",
      "        [-1.3802, -1.2774, -1.3673, -1.5376],\n",
      "        [-1.3802, -1.2774, -1.3672, -1.5376],\n",
      "        [-1.3802, -1.2774, -1.3672, -1.5377],\n",
      "        [-1.3802, -1.2774, -1.3671, -1.5377],\n",
      "        [-1.3802, -1.2775, -1.3671, -1.5376],\n",
      "        [-1.3802, -1.2775, -1.3671, -1.5376],\n",
      "        [-1.3801, -1.2774, -1.3672, -1.5377],\n",
      "        [-1.3801, -1.2774, -1.3671, -1.5377],\n",
      "        [-1.3801, -1.2774, -1.3671, -1.5377],\n",
      "        [-1.3801, -1.2774, -1.3671, -1.5377],\n",
      "        [-1.3802, -1.2775, -1.3671, -1.5376],\n",
      "        [-1.3801, -1.2774, -1.3672, -1.5377],\n",
      "        [-1.3801, -1.2774, -1.3672, -1.5376],\n",
      "        [-1.3801, -1.2775, -1.3673, -1.5376],\n",
      "        [-1.3801, -1.2775, -1.3672, -1.5376],\n",
      "        [-1.3802, -1.2774, -1.3671, -1.5377],\n",
      "        [-1.3802, -1.2774, -1.3671, -1.5377],\n",
      "        [-1.3801, -1.2774, -1.3672, -1.5377],\n",
      "        [-1.3802, -1.2774, -1.3672, -1.5376],\n",
      "        [-1.3802, -1.2774, -1.3672, -1.5377],\n",
      "        [-1.3802, -1.2774, -1.3672, -1.5377],\n",
      "        [-1.3802, -1.2774, -1.3672, -1.5377],\n",
      "        [-1.3802, -1.2774, -1.3672, -1.5376],\n",
      "        [-1.3801, -1.2774, -1.3671, -1.5377],\n",
      "        [-1.3801, -1.2774, -1.3671, -1.5377],\n",
      "        [-1.3801, -1.2774, -1.3671, -1.5377],\n",
      "        [-1.3802, -1.2774, -1.3672, -1.5376],\n",
      "        [-1.3800, -1.2775, -1.3672, -1.5377],\n",
      "        [-1.3801, -1.2775, -1.3671, -1.5377],\n",
      "        [-1.3801, -1.2775, -1.3671, -1.5377],\n",
      "        [-1.3801, -1.2775, -1.3672, -1.5377],\n",
      "        [-1.3803, -1.2774, -1.3671, -1.5377],\n",
      "        [-1.3803, -1.2773, -1.3671, -1.5378],\n",
      "        [-1.3802, -1.2774, -1.3671, -1.5377],\n",
      "        [-1.3803, -1.2772, -1.3672, -1.5379],\n",
      "        [-1.3802, -1.2775, -1.3672, -1.5375],\n",
      "        [-1.3802, -1.2774, -1.3671, -1.5377],\n",
      "        [-1.3802, -1.2774, -1.3672, -1.5376],\n",
      "        [-1.3802, -1.2774, -1.3672, -1.5377],\n",
      "        [-1.3802, -1.2774, -1.3671, -1.5377],\n",
      "        [-1.3802, -1.2775, -1.3671, -1.5376],\n",
      "        [-1.3802, -1.2775, -1.3671, -1.5377],\n",
      "        [-1.3802, -1.2774, -1.3671, -1.5377],\n",
      "        [-1.3800, -1.2774, -1.3672, -1.5378],\n",
      "        [-1.3800, -1.2775, -1.3672, -1.5377],\n",
      "        [-1.3800, -1.2774, -1.3673, -1.5377],\n",
      "        [-1.3800, -1.2774, -1.3672, -1.5378],\n",
      "        [-1.3801, -1.2774, -1.3671, -1.5377],\n",
      "        [-1.3801, -1.2774, -1.3671, -1.5377],\n",
      "        [-1.3801, -1.2774, -1.3671, -1.5377],\n",
      "        [-1.3802, -1.2775, -1.3671, -1.5376],\n",
      "        [-1.3801, -1.2774, -1.3671, -1.5377],\n",
      "        [-1.3801, -1.2774, -1.3671, -1.5377],\n",
      "        [-1.3801, -1.2774, -1.3671, -1.5377],\n",
      "        [-1.3802, -1.2775, -1.3671, -1.5376],\n",
      "        [-1.3801, -1.2775, -1.3671, -1.5377],\n",
      "        [-1.3802, -1.2775, -1.3671, -1.5376],\n",
      "        [-1.3802, -1.2775, -1.3671, -1.5377],\n",
      "        [-1.3801, -1.2775, -1.3671, -1.5376]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([0.3032, 0.2832, 0.2832, 0.0029, 0.0132, 0.0089, 0.0068, 0.0029, 0.0123,\n",
      "        0.0083, 0.0064, 0.0033, 0.3060, 0.3060, 0.3060, 0.0028, 0.0111, 0.0090,\n",
      "        0.0060, 0.0025, 0.2832, 0.3410, 0.3032, 0.0030, 0.0133, 0.0101, 0.0055,\n",
      "        0.0029, 0.3409, 0.3409, 0.2832, 0.0034, 0.0118, 0.0086, 0.0062, 0.0031,\n",
      "        0.0119, 0.0093, 0.0071, 0.0031, 0.0120, 0.0102, 0.0060, 0.0030, 0.0120,\n",
      "        0.0090, 0.0060, 0.0034, 0.0122, 0.0091, 0.0056, 0.0031, 0.3031, 0.2832,\n",
      "        0.3060, 0.0031, 0.3031, 0.3031, 0.3031, 0.0030, 0.0133, 0.0088, 0.0059,\n",
      "        0.0028], grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(-0.0907, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2515, 0.2788, 0.2548, 0.2149],\n",
      "        [0.2515, 0.2788, 0.2548, 0.2149],\n",
      "        [0.2515, 0.2788, 0.2548, 0.2149],\n",
      "        [0.2515, 0.2788, 0.2548, 0.2149],\n",
      "        [0.2515, 0.2788, 0.2548, 0.2149],\n",
      "        [0.2515, 0.2788, 0.2548, 0.2149],\n",
      "        [0.2515, 0.2788, 0.2548, 0.2149],\n",
      "        [0.2515, 0.2788, 0.2548, 0.2149],\n",
      "        [0.2515, 0.2787, 0.2548, 0.2149],\n",
      "        [0.2515, 0.2787, 0.2548, 0.2149],\n",
      "        [0.2515, 0.2787, 0.2548, 0.2149],\n",
      "        [0.2515, 0.2788, 0.2548, 0.2149],\n",
      "        [0.2515, 0.2788, 0.2548, 0.2149],\n",
      "        [0.2515, 0.2788, 0.2548, 0.2149],\n",
      "        [0.2515, 0.2788, 0.2548, 0.2149],\n",
      "        [0.2515, 0.2787, 0.2548, 0.2149],\n",
      "        [0.2515, 0.2788, 0.2548, 0.2149],\n",
      "        [0.2515, 0.2787, 0.2548, 0.2149],\n",
      "        [0.2516, 0.2787, 0.2548, 0.2149],\n",
      "        [0.2516, 0.2787, 0.2548, 0.2149],\n",
      "        [0.2515, 0.2788, 0.2548, 0.2149],\n",
      "        [0.2515, 0.2788, 0.2548, 0.2149],\n",
      "        [0.2515, 0.2788, 0.2548, 0.2149],\n",
      "        [0.2515, 0.2787, 0.2548, 0.2149],\n",
      "        [0.2515, 0.2788, 0.2548, 0.2149],\n",
      "        [0.2515, 0.2788, 0.2548, 0.2149],\n",
      "        [0.2515, 0.2788, 0.2548, 0.2149],\n",
      "        [0.2515, 0.2788, 0.2548, 0.2149],\n",
      "        [0.2515, 0.2788, 0.2548, 0.2149],\n",
      "        [0.2515, 0.2788, 0.2548, 0.2149],\n",
      "        [0.2515, 0.2788, 0.2548, 0.2149],\n",
      "        [0.2515, 0.2787, 0.2548, 0.2149],\n",
      "        [0.2516, 0.2787, 0.2548, 0.2149],\n",
      "        [0.2516, 0.2787, 0.2548, 0.2149],\n",
      "        [0.2516, 0.2787, 0.2548, 0.2149],\n",
      "        [0.2516, 0.2787, 0.2548, 0.2149],\n",
      "        [0.2515, 0.2788, 0.2548, 0.2149],\n",
      "        [0.2515, 0.2788, 0.2549, 0.2149],\n",
      "        [0.2515, 0.2788, 0.2548, 0.2149],\n",
      "        [0.2515, 0.2788, 0.2548, 0.2148],\n",
      "        [0.2515, 0.2787, 0.2548, 0.2149],\n",
      "        [0.2515, 0.2788, 0.2548, 0.2149],\n",
      "        [0.2515, 0.2787, 0.2548, 0.2149],\n",
      "        [0.2515, 0.2788, 0.2548, 0.2149],\n",
      "        [0.2515, 0.2787, 0.2548, 0.2149],\n",
      "        [0.2515, 0.2787, 0.2548, 0.2149],\n",
      "        [0.2515, 0.2787, 0.2548, 0.2149],\n",
      "        [0.2515, 0.2788, 0.2548, 0.2149],\n",
      "        [0.2516, 0.2787, 0.2548, 0.2149],\n",
      "        [0.2516, 0.2787, 0.2548, 0.2149],\n",
      "        [0.2516, 0.2787, 0.2548, 0.2149],\n",
      "        [0.2516, 0.2788, 0.2548, 0.2149],\n",
      "        [0.2515, 0.2788, 0.2548, 0.2149],\n",
      "        [0.2515, 0.2788, 0.2548, 0.2149],\n",
      "        [0.2515, 0.2788, 0.2548, 0.2149],\n",
      "        [0.2515, 0.2787, 0.2548, 0.2149],\n",
      "        [0.2515, 0.2788, 0.2548, 0.2149],\n",
      "        [0.2515, 0.2788, 0.2548, 0.2149],\n",
      "        [0.2515, 0.2788, 0.2548, 0.2149],\n",
      "        [0.2515, 0.2787, 0.2548, 0.2149],\n",
      "        [0.2515, 0.2787, 0.2548, 0.2149],\n",
      "        [0.2515, 0.2787, 0.2548, 0.2149],\n",
      "        [0.2515, 0.2787, 0.2548, 0.2149],\n",
      "        [0.2515, 0.2787, 0.2548, 0.2149]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3821, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(-0.0907, grad_fn=<AddBackward0>)\n",
      "batching here\n",
      "29: done 3 episodes, mean_reward=0.00, best_reward=2.00, speed=190.29\n",
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(0.0026, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.3779, -1.2868, -1.3765, -1.5174],\n",
      "        [-1.3779, -1.2869, -1.3765, -1.5174],\n",
      "        [-1.3779, -1.2869, -1.3764, -1.5174],\n",
      "        [-1.3779, -1.2869, -1.3765, -1.5173],\n",
      "        [-1.3778, -1.2870, -1.3765, -1.5173],\n",
      "        [-1.3778, -1.2869, -1.3765, -1.5173],\n",
      "        [-1.3778, -1.2869, -1.3765, -1.5174],\n",
      "        [-1.3778, -1.2870, -1.3765, -1.5173],\n",
      "        [-1.3779, -1.2871, -1.3764, -1.5172],\n",
      "        [-1.3778, -1.2871, -1.3765, -1.5172],\n",
      "        [-1.3777, -1.2871, -1.3765, -1.5173],\n",
      "        [-1.3777, -1.2870, -1.3765, -1.5173],\n",
      "        [-1.3778, -1.2872, -1.3764, -1.5172],\n",
      "        [-1.3778, -1.2871, -1.3764, -1.5172],\n",
      "        [-1.3778, -1.2871, -1.3764, -1.5173],\n",
      "        [-1.3778, -1.2871, -1.3764, -1.5172],\n",
      "        [-1.3778, -1.2871, -1.3764, -1.5172],\n",
      "        [-1.3778, -1.2870, -1.3765, -1.5172],\n",
      "        [-1.3778, -1.2872, -1.3764, -1.5172],\n",
      "        [-1.3778, -1.2871, -1.3764, -1.5173],\n",
      "        [-1.3778, -1.2871, -1.3764, -1.5172],\n",
      "        [-1.3778, -1.2871, -1.3765, -1.5172],\n",
      "        [-1.3778, -1.2871, -1.3764, -1.5173],\n",
      "        [-1.3778, -1.2870, -1.3764, -1.5173],\n",
      "        [-1.3778, -1.2870, -1.3765, -1.5173],\n",
      "        [-1.3778, -1.2869, -1.3764, -1.5174],\n",
      "        [-1.3778, -1.2869, -1.3764, -1.5174],\n",
      "        [-1.3778, -1.2870, -1.3765, -1.5174],\n",
      "        [-1.3779, -1.2871, -1.3764, -1.5172],\n",
      "        [-1.3778, -1.2871, -1.3765, -1.5171],\n",
      "        [-1.3778, -1.2871, -1.3764, -1.5172],\n",
      "        [-1.3778, -1.2871, -1.3764, -1.5172],\n",
      "        [-1.3777, -1.2871, -1.3764, -1.5173],\n",
      "        [-1.3778, -1.2871, -1.3764, -1.5173],\n",
      "        [-1.3778, -1.2871, -1.3763, -1.5173],\n",
      "        [-1.3778, -1.2871, -1.3764, -1.5173],\n",
      "        [-1.3779, -1.2869, -1.3765, -1.5173],\n",
      "        [-1.3778, -1.2869, -1.3766, -1.5173],\n",
      "        [-1.3779, -1.2870, -1.3765, -1.5172],\n",
      "        [-1.3779, -1.2869, -1.3765, -1.5173],\n",
      "        [-1.3778, -1.2870, -1.3764, -1.5174],\n",
      "        [-1.3779, -1.2869, -1.3764, -1.5174],\n",
      "        [-1.3778, -1.2869, -1.3764, -1.5174],\n",
      "        [-1.3779, -1.2869, -1.3764, -1.5174],\n",
      "        [-1.3778, -1.2871, -1.3764, -1.5173],\n",
      "        [-1.3778, -1.2871, -1.3764, -1.5173],\n",
      "        [-1.3778, -1.2871, -1.3764, -1.5173],\n",
      "        [-1.3778, -1.2871, -1.3764, -1.5173],\n",
      "        [-1.3777, -1.2870, -1.3765, -1.5174],\n",
      "        [-1.3777, -1.2870, -1.3765, -1.5174],\n",
      "        [-1.3777, -1.2870, -1.3765, -1.5174],\n",
      "        [-1.3777, -1.2870, -1.3765, -1.5175],\n",
      "        [-1.3778, -1.2872, -1.3764, -1.5172],\n",
      "        [-1.3778, -1.2871, -1.3764, -1.5172],\n",
      "        [-1.3778, -1.2871, -1.3764, -1.5173],\n",
      "        [-1.3778, -1.2871, -1.3764, -1.5172],\n",
      "        [-1.3778, -1.2871, -1.3764, -1.5172],\n",
      "        [-1.3778, -1.2871, -1.3764, -1.5172],\n",
      "        [-1.3778, -1.2871, -1.3764, -1.5173],\n",
      "        [-1.3778, -1.2871, -1.3764, -1.5172],\n",
      "        [-1.3778, -1.2871, -1.3763, -1.5174],\n",
      "        [-1.3778, -1.2871, -1.3764, -1.5173],\n",
      "        [-1.3778, -1.2871, -1.3764, -1.5173],\n",
      "        [-1.3778, -1.2872, -1.3763, -1.5172]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([0.0064, 0.0048, 0.0032, 0.0014, 0.1795, 0.1523, 0.1630, 0.1629, 0.0071,\n",
      "        0.0047, 0.0028, 0.0016, 0.0064, 0.0045, 0.0029, 0.0017, 0.0059, 0.0047,\n",
      "        0.0029, 0.0016, 0.0061, 0.0047, 0.0034, 0.0015, 0.1796, 0.1523, 0.1628,\n",
      "        0.1628, 0.0073, 0.0045, 0.0035, 0.0018, 0.0064, 0.0046, 0.0037, 0.0017,\n",
      "        0.0068, 0.0046, 0.0027, 0.0015, 0.0063, 0.0052, 0.0036, 0.0016, 0.1631,\n",
      "        0.1524, 0.1524, 0.1631, 0.0059, 0.0048, 0.0032, 0.0015, 0.0064, 0.0045,\n",
      "        0.0031, 0.0014, 0.0060, 0.0048, 0.0031, 0.0016, 0.0067, 0.0050, 0.0034,\n",
      "        0.0015], grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(-0.0336, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2521, 0.2761, 0.2525, 0.2193],\n",
      "        [0.2521, 0.2761, 0.2525, 0.2193],\n",
      "        [0.2521, 0.2761, 0.2525, 0.2193],\n",
      "        [0.2521, 0.2761, 0.2525, 0.2193],\n",
      "        [0.2521, 0.2761, 0.2525, 0.2193],\n",
      "        [0.2521, 0.2761, 0.2525, 0.2193],\n",
      "        [0.2521, 0.2761, 0.2525, 0.2193],\n",
      "        [0.2521, 0.2761, 0.2525, 0.2193],\n",
      "        [0.2521, 0.2761, 0.2525, 0.2193],\n",
      "        [0.2521, 0.2761, 0.2525, 0.2193],\n",
      "        [0.2522, 0.2761, 0.2525, 0.2193],\n",
      "        [0.2522, 0.2761, 0.2525, 0.2193],\n",
      "        [0.2521, 0.2761, 0.2525, 0.2193],\n",
      "        [0.2521, 0.2761, 0.2525, 0.2193],\n",
      "        [0.2521, 0.2761, 0.2525, 0.2193],\n",
      "        [0.2521, 0.2761, 0.2525, 0.2193],\n",
      "        [0.2521, 0.2761, 0.2525, 0.2193],\n",
      "        [0.2521, 0.2761, 0.2525, 0.2193],\n",
      "        [0.2521, 0.2761, 0.2525, 0.2193],\n",
      "        [0.2521, 0.2761, 0.2525, 0.2193],\n",
      "        [0.2521, 0.2761, 0.2525, 0.2193],\n",
      "        [0.2521, 0.2761, 0.2525, 0.2193],\n",
      "        [0.2521, 0.2761, 0.2525, 0.2193],\n",
      "        [0.2521, 0.2761, 0.2525, 0.2193],\n",
      "        [0.2521, 0.2761, 0.2525, 0.2193],\n",
      "        [0.2521, 0.2761, 0.2525, 0.2193],\n",
      "        [0.2521, 0.2761, 0.2525, 0.2193],\n",
      "        [0.2521, 0.2761, 0.2525, 0.2193],\n",
      "        [0.2521, 0.2761, 0.2525, 0.2193],\n",
      "        [0.2521, 0.2761, 0.2525, 0.2193],\n",
      "        [0.2521, 0.2761, 0.2525, 0.2193],\n",
      "        [0.2521, 0.2761, 0.2525, 0.2193],\n",
      "        [0.2521, 0.2761, 0.2525, 0.2193],\n",
      "        [0.2521, 0.2761, 0.2525, 0.2193],\n",
      "        [0.2521, 0.2761, 0.2525, 0.2193],\n",
      "        [0.2521, 0.2761, 0.2525, 0.2193],\n",
      "        [0.2521, 0.2761, 0.2525, 0.2193],\n",
      "        [0.2521, 0.2761, 0.2524, 0.2193],\n",
      "        [0.2521, 0.2761, 0.2525, 0.2193],\n",
      "        [0.2521, 0.2761, 0.2525, 0.2193],\n",
      "        [0.2521, 0.2761, 0.2525, 0.2193],\n",
      "        [0.2521, 0.2761, 0.2525, 0.2193],\n",
      "        [0.2521, 0.2761, 0.2525, 0.2193],\n",
      "        [0.2521, 0.2761, 0.2525, 0.2193],\n",
      "        [0.2521, 0.2761, 0.2525, 0.2193],\n",
      "        [0.2521, 0.2761, 0.2525, 0.2193],\n",
      "        [0.2521, 0.2761, 0.2525, 0.2193],\n",
      "        [0.2521, 0.2761, 0.2525, 0.2193],\n",
      "        [0.2522, 0.2761, 0.2525, 0.2193],\n",
      "        [0.2522, 0.2761, 0.2525, 0.2193],\n",
      "        [0.2522, 0.2761, 0.2525, 0.2193],\n",
      "        [0.2522, 0.2761, 0.2525, 0.2193],\n",
      "        [0.2521, 0.2761, 0.2525, 0.2193],\n",
      "        [0.2521, 0.2761, 0.2525, 0.2193],\n",
      "        [0.2521, 0.2761, 0.2525, 0.2193],\n",
      "        [0.2521, 0.2761, 0.2525, 0.2193],\n",
      "        [0.2521, 0.2761, 0.2525, 0.2193],\n",
      "        [0.2521, 0.2761, 0.2525, 0.2193],\n",
      "        [0.2521, 0.2761, 0.2525, 0.2193],\n",
      "        [0.2521, 0.2761, 0.2525, 0.2193],\n",
      "        [0.2521, 0.2761, 0.2525, 0.2193],\n",
      "        [0.2521, 0.2761, 0.2525, 0.2193],\n",
      "        [0.2521, 0.2761, 0.2525, 0.2193],\n",
      "        [0.2521, 0.2760, 0.2525, 0.2193]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3830, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(-0.0448, grad_fn=<AddBackward0>)\n",
      "batching here\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30: done 2 episodes, mean_reward=1.00, best_reward=2.00, speed=199.20\n",
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(0.0310, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.3833, -1.3007, -1.3709, -1.5005],\n",
      "        [-1.3833, -1.3009, -1.3708, -1.5003],\n",
      "        [-1.3833, -1.3009, -1.3708, -1.5003],\n",
      "        [-1.3833, -1.3009, -1.3708, -1.5003],\n",
      "        [-1.3833, -1.3010, -1.3708, -1.5003],\n",
      "        [-1.3833, -1.3010, -1.3708, -1.5002],\n",
      "        [-1.3833, -1.3011, -1.3708, -1.5001],\n",
      "        [-1.3832, -1.3011, -1.3708, -1.5002],\n",
      "        [-1.3832, -1.3010, -1.3708, -1.5003],\n",
      "        [-1.3832, -1.3011, -1.3708, -1.5002],\n",
      "        [-1.3832, -1.3011, -1.3708, -1.5002],\n",
      "        [-1.3833, -1.3010, -1.3708, -1.5002],\n",
      "        [-1.3833, -1.3011, -1.3707, -1.5001],\n",
      "        [-1.3833, -1.3010, -1.3707, -1.5003],\n",
      "        [-1.3833, -1.3010, -1.3707, -1.5003],\n",
      "        [-1.3833, -1.3012, -1.3708, -1.5001],\n",
      "        [-1.3832, -1.3012, -1.3708, -1.5001],\n",
      "        [-1.3832, -1.3012, -1.3708, -1.5000],\n",
      "        [-1.3832, -1.3011, -1.3708, -1.5001],\n",
      "        [-1.3831, -1.3011, -1.3709, -1.5002],\n",
      "        [-1.3833, -1.3010, -1.3708, -1.5002],\n",
      "        [-1.3833, -1.3011, -1.3707, -1.5002],\n",
      "        [-1.3833, -1.3011, -1.3708, -1.5001],\n",
      "        [-1.3832, -1.3011, -1.3708, -1.5002],\n",
      "        [-1.3833, -1.3010, -1.3708, -1.5001],\n",
      "        [-1.3833, -1.3010, -1.3708, -1.5002],\n",
      "        [-1.3833, -1.3011, -1.3708, -1.5001],\n",
      "        [-1.3832, -1.3011, -1.3708, -1.5002],\n",
      "        [-1.3832, -1.3011, -1.3708, -1.5001],\n",
      "        [-1.3832, -1.3011, -1.3708, -1.5002],\n",
      "        [-1.3833, -1.3010, -1.3708, -1.5002],\n",
      "        [-1.3832, -1.3011, -1.3708, -1.5002],\n",
      "        [-1.3833, -1.3011, -1.3707, -1.5002],\n",
      "        [-1.3832, -1.3010, -1.3708, -1.5003],\n",
      "        [-1.3832, -1.3010, -1.3708, -1.5003],\n",
      "        [-1.3832, -1.3012, -1.3707, -1.5002],\n",
      "        [-1.3833, -1.3009, -1.3708, -1.5003],\n",
      "        [-1.3833, -1.3008, -1.3708, -1.5003],\n",
      "        [-1.3833, -1.3009, -1.3709, -1.5002],\n",
      "        [-1.3833, -1.3009, -1.3708, -1.5003],\n",
      "        [-1.3833, -1.3010, -1.3708, -1.5003],\n",
      "        [-1.3833, -1.3010, -1.3708, -1.5002],\n",
      "        [-1.3833, -1.3009, -1.3708, -1.5003],\n",
      "        [-1.3833, -1.3009, -1.3708, -1.5003],\n",
      "        [-1.3833, -1.3010, -1.3707, -1.5003],\n",
      "        [-1.3833, -1.3010, -1.3707, -1.5003],\n",
      "        [-1.3833, -1.3010, -1.3707, -1.5003],\n",
      "        [-1.3833, -1.3010, -1.3707, -1.5003],\n",
      "        [-1.3832, -1.3010, -1.3708, -1.5004],\n",
      "        [-1.3832, -1.3010, -1.3708, -1.5003],\n",
      "        [-1.3832, -1.3010, -1.3708, -1.5003],\n",
      "        [-1.3832, -1.3010, -1.3708, -1.5002],\n",
      "        [-1.3832, -1.3011, -1.3708, -1.5002],\n",
      "        [-1.3832, -1.3011, -1.3707, -1.5003],\n",
      "        [-1.3832, -1.3011, -1.3707, -1.5003],\n",
      "        [-1.3833, -1.3011, -1.3708, -1.5002],\n",
      "        [-1.3832, -1.3011, -1.3708, -1.5002],\n",
      "        [-1.3832, -1.3011, -1.3707, -1.5003],\n",
      "        [-1.3832, -1.3011, -1.3707, -1.5003],\n",
      "        [-1.3833, -1.3011, -1.3708, -1.5002],\n",
      "        [-1.3833, -1.3011, -1.3706, -1.5003],\n",
      "        [-1.3833, -1.3010, -1.3707, -1.5002],\n",
      "        [-1.3833, -1.3011, -1.3707, -1.5002],\n",
      "        [-1.3833, -1.3011, -1.3707, -1.5002]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([ 3.0169e-03,  2.2142e-03,  1.3576e-03,  6.1840e-04,  2.9950e-03,\n",
      "         2.2352e-03,  1.4457e-03,  6.4525e-04,  3.0700e-03,  2.3196e-03,\n",
      "         1.5458e-03,  7.4222e-04,  2.8112e-03,  2.0859e-03,  1.3673e-03,\n",
      "         6.1615e-04, -1.3539e+00, -1.2990e+00,  1.4157e-03,  4.6184e-04,\n",
      "         3.1330e-03,  2.1023e-03,  1.3047e-03,  5.3988e-04,  3.2222e-03,\n",
      "         2.4422e-03,  1.8255e-03,  9.2750e-04,  2.8913e-03,  1.9841e-03,\n",
      "         1.3665e-03,  6.7921e-04,  3.0431e-03,  2.4431e-03,  1.7906e-03,\n",
      "         7.8213e-04,  3.4869e-03,  2.2278e-03,  1.4480e-03,  7.4184e-04,\n",
      "         7.2178e-02,  2.2303e-03,  1.6047e-03,  8.1553e-04,  2.9564e-03,\n",
      "         2.4172e-03,  1.4857e-03,  7.7845e-04,  7.6818e-02,  7.2201e-02,\n",
      "         7.6173e-02,  6.1841e-04,  3.0620e-03,  2.2743e-03,  1.4940e-03,\n",
      "         6.9279e-04,  2.8744e-03,  2.2773e-03,  1.4808e-03,  6.8116e-04,\n",
      "         2.9887e-03,  2.5069e-03,  1.7729e-03,  9.2771e-04],\n",
      "       grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(0.0352, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2508, 0.2723, 0.2539, 0.2230],\n",
      "        [0.2508, 0.2723, 0.2539, 0.2231],\n",
      "        [0.2507, 0.2723, 0.2539, 0.2231],\n",
      "        [0.2508, 0.2723, 0.2539, 0.2231],\n",
      "        [0.2508, 0.2723, 0.2539, 0.2231],\n",
      "        [0.2507, 0.2723, 0.2539, 0.2231],\n",
      "        [0.2507, 0.2722, 0.2539, 0.2231],\n",
      "        [0.2508, 0.2722, 0.2539, 0.2231],\n",
      "        [0.2508, 0.2723, 0.2539, 0.2231],\n",
      "        [0.2508, 0.2722, 0.2539, 0.2231],\n",
      "        [0.2508, 0.2722, 0.2539, 0.2231],\n",
      "        [0.2508, 0.2722, 0.2539, 0.2231],\n",
      "        [0.2508, 0.2722, 0.2539, 0.2231],\n",
      "        [0.2508, 0.2722, 0.2539, 0.2231],\n",
      "        [0.2508, 0.2723, 0.2539, 0.2231],\n",
      "        [0.2508, 0.2722, 0.2539, 0.2231],\n",
      "        [0.2508, 0.2722, 0.2539, 0.2231],\n",
      "        [0.2508, 0.2722, 0.2539, 0.2231],\n",
      "        [0.2508, 0.2722, 0.2539, 0.2231],\n",
      "        [0.2508, 0.2722, 0.2539, 0.2231],\n",
      "        [0.2508, 0.2723, 0.2539, 0.2231],\n",
      "        [0.2508, 0.2722, 0.2539, 0.2231],\n",
      "        [0.2507, 0.2722, 0.2539, 0.2231],\n",
      "        [0.2508, 0.2722, 0.2539, 0.2231],\n",
      "        [0.2507, 0.2723, 0.2539, 0.2231],\n",
      "        [0.2507, 0.2723, 0.2539, 0.2231],\n",
      "        [0.2508, 0.2722, 0.2539, 0.2231],\n",
      "        [0.2508, 0.2722, 0.2539, 0.2231],\n",
      "        [0.2508, 0.2722, 0.2539, 0.2231],\n",
      "        [0.2508, 0.2722, 0.2539, 0.2231],\n",
      "        [0.2507, 0.2723, 0.2539, 0.2231],\n",
      "        [0.2508, 0.2722, 0.2539, 0.2231],\n",
      "        [0.2508, 0.2722, 0.2539, 0.2231],\n",
      "        [0.2508, 0.2723, 0.2539, 0.2231],\n",
      "        [0.2508, 0.2723, 0.2539, 0.2231],\n",
      "        [0.2508, 0.2722, 0.2539, 0.2231],\n",
      "        [0.2508, 0.2723, 0.2539, 0.2231],\n",
      "        [0.2507, 0.2723, 0.2539, 0.2231],\n",
      "        [0.2507, 0.2723, 0.2539, 0.2231],\n",
      "        [0.2508, 0.2723, 0.2539, 0.2231],\n",
      "        [0.2508, 0.2723, 0.2539, 0.2231],\n",
      "        [0.2507, 0.2723, 0.2539, 0.2231],\n",
      "        [0.2508, 0.2723, 0.2539, 0.2231],\n",
      "        [0.2508, 0.2723, 0.2539, 0.2231],\n",
      "        [0.2508, 0.2722, 0.2539, 0.2231],\n",
      "        [0.2508, 0.2723, 0.2539, 0.2231],\n",
      "        [0.2508, 0.2723, 0.2539, 0.2231],\n",
      "        [0.2507, 0.2723, 0.2539, 0.2231],\n",
      "        [0.2508, 0.2723, 0.2539, 0.2230],\n",
      "        [0.2508, 0.2723, 0.2539, 0.2231],\n",
      "        [0.2508, 0.2723, 0.2539, 0.2231],\n",
      "        [0.2508, 0.2722, 0.2539, 0.2231],\n",
      "        [0.2508, 0.2722, 0.2539, 0.2231],\n",
      "        [0.2508, 0.2722, 0.2539, 0.2231],\n",
      "        [0.2508, 0.2722, 0.2539, 0.2231],\n",
      "        [0.2508, 0.2722, 0.2539, 0.2231],\n",
      "        [0.2508, 0.2722, 0.2539, 0.2231],\n",
      "        [0.2508, 0.2722, 0.2539, 0.2231],\n",
      "        [0.2508, 0.2722, 0.2539, 0.2231],\n",
      "        [0.2508, 0.2722, 0.2539, 0.2231],\n",
      "        [0.2508, 0.2722, 0.2540, 0.2231],\n",
      "        [0.2507, 0.2722, 0.2539, 0.2231],\n",
      "        [0.2508, 0.2722, 0.2539, 0.2231],\n",
      "        [0.2508, 0.2722, 0.2539, 0.2231]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3838, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(0.0523, grad_fn=<AddBackward0>)\n",
      "batching here\n",
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(0.0603, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.4093, -1.2694, -1.3503, -1.5346],\n",
      "        [-1.4092, -1.2696, -1.3503, -1.5346],\n",
      "        [-1.4092, -1.2695, -1.3503, -1.5345],\n",
      "        [-1.4092, -1.2696, -1.3503, -1.5345],\n",
      "        [-1.4092, -1.2697, -1.3502, -1.5345],\n",
      "        [-1.4092, -1.2697, -1.3501, -1.5346],\n",
      "        [-1.4092, -1.2696, -1.3501, -1.5348],\n",
      "        [-1.4093, -1.2696, -1.3501, -1.5347],\n",
      "        [-1.4091, -1.2698, -1.3502, -1.5345],\n",
      "        [-1.4091, -1.2699, -1.3503, -1.5343],\n",
      "        [-1.4092, -1.2699, -1.3503, -1.5343],\n",
      "        [-1.4092, -1.2698, -1.3502, -1.5344],\n",
      "        [-1.4092, -1.2698, -1.3502, -1.5345],\n",
      "        [-1.4091, -1.2697, -1.3503, -1.5345],\n",
      "        [-1.4091, -1.2697, -1.3502, -1.5346],\n",
      "        [-1.4092, -1.2697, -1.3502, -1.5345],\n",
      "        [-1.4092, -1.2697, -1.3502, -1.5346],\n",
      "        [-1.4091, -1.2697, -1.3503, -1.5345],\n",
      "        [-1.4092, -1.2695, -1.3502, -1.5348],\n",
      "        [-1.4092, -1.2697, -1.3502, -1.5346],\n",
      "        [-1.4092, -1.2698, -1.3503, -1.5344],\n",
      "        [-1.4092, -1.2696, -1.3502, -1.5346],\n",
      "        [-1.4091, -1.2697, -1.3503, -1.5345],\n",
      "        [-1.4092, -1.2696, -1.3502, -1.5347],\n",
      "        [-1.4092, -1.2697, -1.3502, -1.5345],\n",
      "        [-1.4093, -1.2697, -1.3501, -1.5345],\n",
      "        [-1.4092, -1.2696, -1.3502, -1.5347],\n",
      "        [-1.4092, -1.2696, -1.3502, -1.5346],\n",
      "        [-1.4091, -1.2699, -1.3503, -1.5344],\n",
      "        [-1.4092, -1.2696, -1.3503, -1.5345],\n",
      "        [-1.4091, -1.2698, -1.3502, -1.5344],\n",
      "        [-1.4092, -1.2697, -1.3501, -1.5346],\n",
      "        [-1.4091, -1.2698, -1.3503, -1.5345],\n",
      "        [-1.4092, -1.2697, -1.3502, -1.5346],\n",
      "        [-1.4091, -1.2699, -1.3502, -1.5345],\n",
      "        [-1.4091, -1.2697, -1.3502, -1.5346],\n",
      "        [-1.4092, -1.2695, -1.3503, -1.5346],\n",
      "        [-1.4092, -1.2696, -1.3503, -1.5345],\n",
      "        [-1.4092, -1.2696, -1.3503, -1.5346],\n",
      "        [-1.4092, -1.2695, -1.3503, -1.5346],\n",
      "        [-1.4093, -1.2697, -1.3501, -1.5346],\n",
      "        [-1.4093, -1.2697, -1.3502, -1.5345],\n",
      "        [-1.4093, -1.2695, -1.3502, -1.5347],\n",
      "        [-1.4093, -1.2697, -1.3501, -1.5346],\n",
      "        [-1.4092, -1.2697, -1.3502, -1.5346],\n",
      "        [-1.4093, -1.2696, -1.3502, -1.5345],\n",
      "        [-1.4092, -1.2697, -1.3502, -1.5346],\n",
      "        [-1.4092, -1.2697, -1.3501, -1.5345],\n",
      "        [-1.4092, -1.2696, -1.3502, -1.5346],\n",
      "        [-1.4091, -1.2698, -1.3502, -1.5345],\n",
      "        [-1.4092, -1.2696, -1.3502, -1.5346],\n",
      "        [-1.4092, -1.2696, -1.3502, -1.5347],\n",
      "        [-1.4092, -1.2697, -1.3502, -1.5345],\n",
      "        [-1.4091, -1.2697, -1.3503, -1.5345],\n",
      "        [-1.4091, -1.2697, -1.3502, -1.5346],\n",
      "        [-1.4092, -1.2697, -1.3502, -1.5345],\n",
      "        [-1.4092, -1.2697, -1.3502, -1.5345],\n",
      "        [-1.4091, -1.2697, -1.3503, -1.5345],\n",
      "        [-1.4091, -1.2697, -1.3502, -1.5346],\n",
      "        [-1.4091, -1.2697, -1.3503, -1.5345],\n",
      "        [-1.4092, -1.2697, -1.3502, -1.5345],\n",
      "        [-1.4092, -1.2698, -1.3502, -1.5344],\n",
      "        [-1.4092, -1.2698, -1.3502, -1.5344],\n",
      "        [-1.4093, -1.2697, -1.3501, -1.5345]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([ 6.3342e-03,  4.6414e-03,  2.7538e-03,  1.4630e-03,  6.2593e-03,\n",
      "         4.7143e-03,  3.0517e-03,  1.6065e-03,  6.6945e-03,  4.3058e-03,\n",
      "         3.2528e-03,  1.7316e-03,  5.6108e-03,  4.6807e-03,  3.2659e-03,\n",
      "         1.3387e-03,  6.3341e-03,  4.2776e-03,  2.7799e-03,  1.6716e-03,\n",
      "         6.0776e-03,  5.2175e-03,  2.7710e-03,  1.7993e-03,  5.9989e-03,\n",
      "         5.2244e-03,  2.9693e-03,  1.4614e-03,  6.1865e-03,  5.0935e-03,\n",
      "         2.7287e-03,  1.4603e-03,  6.5838e-03,  4.2905e-03,  3.1479e-03,\n",
      "         1.3677e-03,  6.0959e-03,  4.3778e-03,  3.5647e-03,  1.5684e-03,\n",
      "         6.5338e-03,  4.5764e-03,  3.7444e-03,  1.7066e-03,  6.1074e-03,\n",
      "         4.7930e-03,  3.2157e-03,  1.3923e-03,  6.4023e-03,  4.2711e-03,\n",
      "         3.6404e-03,  1.5180e-03,  5.8874e-03,  4.3897e-03,  3.2097e-03,\n",
      "         1.3730e-03,  6.6454e-03,  4.9449e-03,  2.9173e-03,  1.5362e-03,\n",
      "        -1.3042e+00, -1.2404e+00, -1.3337e+00, -1.5328e+00],\n",
      "       grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(0.0809, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2443, 0.2810, 0.2592, 0.2155],\n",
      "        [0.2443, 0.2810, 0.2592, 0.2155],\n",
      "        [0.2443, 0.2810, 0.2592, 0.2156],\n",
      "        [0.2443, 0.2809, 0.2592, 0.2156],\n",
      "        [0.2443, 0.2809, 0.2592, 0.2156],\n",
      "        [0.2443, 0.2809, 0.2592, 0.2155],\n",
      "        [0.2443, 0.2810, 0.2592, 0.2155],\n",
      "        [0.2443, 0.2809, 0.2592, 0.2155],\n",
      "        [0.2444, 0.2809, 0.2592, 0.2156],\n",
      "        [0.2444, 0.2809, 0.2592, 0.2156],\n",
      "        [0.2443, 0.2809, 0.2592, 0.2156],\n",
      "        [0.2443, 0.2809, 0.2592, 0.2156],\n",
      "        [0.2443, 0.2809, 0.2592, 0.2156],\n",
      "        [0.2444, 0.2809, 0.2592, 0.2156],\n",
      "        [0.2444, 0.2809, 0.2592, 0.2155],\n",
      "        [0.2443, 0.2809, 0.2592, 0.2156],\n",
      "        [0.2444, 0.2809, 0.2592, 0.2156],\n",
      "        [0.2444, 0.2809, 0.2592, 0.2156],\n",
      "        [0.2443, 0.2810, 0.2592, 0.2155],\n",
      "        [0.2443, 0.2809, 0.2592, 0.2156],\n",
      "        [0.2443, 0.2809, 0.2592, 0.2156],\n",
      "        [0.2443, 0.2809, 0.2592, 0.2155],\n",
      "        [0.2444, 0.2809, 0.2592, 0.2156],\n",
      "        [0.2443, 0.2809, 0.2592, 0.2155],\n",
      "        [0.2443, 0.2809, 0.2592, 0.2156],\n",
      "        [0.2443, 0.2809, 0.2592, 0.2156],\n",
      "        [0.2443, 0.2810, 0.2592, 0.2155],\n",
      "        [0.2443, 0.2809, 0.2592, 0.2155],\n",
      "        [0.2444, 0.2809, 0.2592, 0.2156],\n",
      "        [0.2443, 0.2809, 0.2592, 0.2156],\n",
      "        [0.2444, 0.2809, 0.2592, 0.2156],\n",
      "        [0.2443, 0.2809, 0.2592, 0.2155],\n",
      "        [0.2444, 0.2809, 0.2592, 0.2156],\n",
      "        [0.2443, 0.2809, 0.2592, 0.2155],\n",
      "        [0.2444, 0.2809, 0.2592, 0.2156],\n",
      "        [0.2444, 0.2809, 0.2592, 0.2155],\n",
      "        [0.2443, 0.2810, 0.2592, 0.2156],\n",
      "        [0.2443, 0.2810, 0.2592, 0.2156],\n",
      "        [0.2443, 0.2809, 0.2592, 0.2156],\n",
      "        [0.2443, 0.2810, 0.2592, 0.2155],\n",
      "        [0.2443, 0.2809, 0.2592, 0.2155],\n",
      "        [0.2443, 0.2809, 0.2592, 0.2156],\n",
      "        [0.2443, 0.2810, 0.2592, 0.2155],\n",
      "        [0.2443, 0.2809, 0.2592, 0.2155],\n",
      "        [0.2443, 0.2809, 0.2592, 0.2155],\n",
      "        [0.2443, 0.2809, 0.2592, 0.2156],\n",
      "        [0.2443, 0.2809, 0.2592, 0.2155],\n",
      "        [0.2443, 0.2809, 0.2592, 0.2156],\n",
      "        [0.2443, 0.2809, 0.2592, 0.2155],\n",
      "        [0.2444, 0.2809, 0.2592, 0.2156],\n",
      "        [0.2443, 0.2809, 0.2592, 0.2155],\n",
      "        [0.2443, 0.2809, 0.2592, 0.2155],\n",
      "        [0.2443, 0.2809, 0.2592, 0.2156],\n",
      "        [0.2444, 0.2809, 0.2592, 0.2156],\n",
      "        [0.2444, 0.2809, 0.2592, 0.2155],\n",
      "        [0.2443, 0.2809, 0.2592, 0.2156],\n",
      "        [0.2443, 0.2809, 0.2592, 0.2156],\n",
      "        [0.2444, 0.2809, 0.2592, 0.2156],\n",
      "        [0.2444, 0.2809, 0.2592, 0.2155],\n",
      "        [0.2444, 0.2809, 0.2592, 0.2156],\n",
      "        [0.2443, 0.2809, 0.2592, 0.2156],\n",
      "        [0.2443, 0.2809, 0.2592, 0.2156],\n",
      "        [0.2443, 0.2809, 0.2592, 0.2156],\n",
      "        [0.2443, 0.2809, 0.2592, 0.2156]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3817, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(0.1274, grad_fn=<AddBackward0>)\n",
      "batching here\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(0.0305, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.4415, -1.2697, -1.3175, -1.5385],\n",
      "        [-1.4414, -1.2697, -1.3175, -1.5386],\n",
      "        [-1.4414, -1.2697, -1.3175, -1.5385],\n",
      "        [-1.4414, -1.2698, -1.3175, -1.5385],\n",
      "        [-1.4413, -1.2699, -1.3175, -1.5384],\n",
      "        [-1.4413, -1.2699, -1.3175, -1.5384],\n",
      "        [-1.4413, -1.2699, -1.3175, -1.5385],\n",
      "        [-1.4413, -1.2699, -1.3175, -1.5385],\n",
      "        [-1.4414, -1.2698, -1.3175, -1.5385],\n",
      "        [-1.4414, -1.2699, -1.3175, -1.5384],\n",
      "        [-1.4413, -1.2699, -1.3175, -1.5384],\n",
      "        [-1.4414, -1.2699, -1.3175, -1.5384],\n",
      "        [-1.4414, -1.2698, -1.3174, -1.5385],\n",
      "        [-1.4414, -1.2698, -1.3176, -1.5384],\n",
      "        [-1.4414, -1.2699, -1.3175, -1.5383],\n",
      "        [-1.4413, -1.2699, -1.3175, -1.5384],\n",
      "        [-1.4413, -1.2699, -1.3175, -1.5385],\n",
      "        [-1.4413, -1.2699, -1.3176, -1.5384],\n",
      "        [-1.4414, -1.2698, -1.3175, -1.5385],\n",
      "        [-1.4413, -1.2698, -1.3174, -1.5386],\n",
      "        [-1.4414, -1.2699, -1.3175, -1.5384],\n",
      "        [-1.4413, -1.2698, -1.3175, -1.5385],\n",
      "        [-1.4414, -1.2698, -1.3175, -1.5385],\n",
      "        [-1.4414, -1.2698, -1.3175, -1.5384],\n",
      "        [-1.4413, -1.2700, -1.3175, -1.5384],\n",
      "        [-1.4413, -1.2700, -1.3174, -1.5384],\n",
      "        [-1.4413, -1.2699, -1.3175, -1.5385],\n",
      "        [-1.4413, -1.2699, -1.3175, -1.5385],\n",
      "        [-1.4414, -1.2699, -1.3175, -1.5384],\n",
      "        [-1.4414, -1.2699, -1.3175, -1.5385],\n",
      "        [-1.4414, -1.2698, -1.3174, -1.5385],\n",
      "        [-1.4414, -1.2698, -1.3175, -1.5385],\n",
      "        [-1.4412, -1.2701, -1.3175, -1.5384],\n",
      "        [-1.4412, -1.2701, -1.3174, -1.5384],\n",
      "        [-1.4411, -1.2701, -1.3175, -1.5383],\n",
      "        [-1.4412, -1.2701, -1.3175, -1.5383],\n",
      "        [-1.4414, -1.2698, -1.3175, -1.5384],\n",
      "        [-1.4414, -1.2697, -1.3175, -1.5385],\n",
      "        [-1.4415, -1.2697, -1.3175, -1.5385],\n",
      "        [-1.4414, -1.2697, -1.3176, -1.5385],\n",
      "        [-1.4413, -1.2700, -1.3175, -1.5383],\n",
      "        [-1.4413, -1.2699, -1.3175, -1.5384],\n",
      "        [-1.4414, -1.2698, -1.3175, -1.5384],\n",
      "        [-1.4413, -1.2698, -1.3174, -1.5385],\n",
      "        [-1.4413, -1.2700, -1.3176, -1.5383],\n",
      "        [-1.4413, -1.2700, -1.3175, -1.5383],\n",
      "        [-1.4413, -1.2700, -1.3175, -1.5383],\n",
      "        [-1.4413, -1.2699, -1.3175, -1.5385],\n",
      "        [-1.4413, -1.2700, -1.3175, -1.5384],\n",
      "        [-1.4413, -1.2699, -1.3175, -1.5384],\n",
      "        [-1.4413, -1.2699, -1.3175, -1.5383],\n",
      "        [-1.4412, -1.2700, -1.3176, -1.5383],\n",
      "        [-1.4414, -1.2698, -1.3175, -1.5385],\n",
      "        [-1.4414, -1.2698, -1.3175, -1.5385],\n",
      "        [-1.4413, -1.2699, -1.3175, -1.5384],\n",
      "        [-1.4413, -1.2698, -1.3175, -1.5385],\n",
      "        [-1.4414, -1.2698, -1.3175, -1.5385],\n",
      "        [-1.4414, -1.2698, -1.3175, -1.5384],\n",
      "        [-1.4414, -1.2698, -1.3176, -1.5384],\n",
      "        [-1.4414, -1.2699, -1.3175, -1.5384],\n",
      "        [-1.4414, -1.2700, -1.3174, -1.5384],\n",
      "        [-1.4413, -1.2699, -1.3174, -1.5385],\n",
      "        [-1.4412, -1.2700, -1.3174, -1.5384],\n",
      "        [-1.4412, -1.2700, -1.3174, -1.5384]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([ 0.0121,  0.0087,  0.0052,  0.0030,  0.0114,  0.0075,  0.0062,  0.0028,\n",
      "        -1.2470, -1.4328,  0.0049,  0.0029,  0.0122,  0.0075,  0.0051,  0.0030,\n",
      "         0.0125,  0.0079,  0.0057,  0.0028,  0.0115,  0.0093,  0.0063,  0.0025,\n",
      "         0.0103,  0.0091,  0.0053,  0.0024,  0.0115,  0.0080,  0.0063,  0.0032,\n",
      "         0.0123,  0.0085,  0.0052,  0.0029,  0.0112,  0.0092,  0.0054,  0.0031,\n",
      "         0.0116,  0.0083,  0.0063,  0.0031,  0.0105,  0.0089,  0.0051,  0.0028,\n",
      "         0.0100,  0.0085,  0.0055,  0.0030,  0.0105,  0.0076,  0.0052,  0.0031,\n",
      "         0.0116,  0.0092,  0.0057,  0.0026,  0.0128,  0.0084,  0.0057,  0.0027],\n",
      "       grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(0.0351, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2366, 0.2809, 0.2678, 0.2147],\n",
      "        [0.2366, 0.2809, 0.2678, 0.2147],\n",
      "        [0.2366, 0.2809, 0.2678, 0.2147],\n",
      "        [0.2366, 0.2809, 0.2678, 0.2147],\n",
      "        [0.2366, 0.2809, 0.2678, 0.2147],\n",
      "        [0.2366, 0.2809, 0.2678, 0.2147],\n",
      "        [0.2366, 0.2809, 0.2678, 0.2147],\n",
      "        [0.2366, 0.2809, 0.2678, 0.2147],\n",
      "        [0.2366, 0.2809, 0.2678, 0.2147],\n",
      "        [0.2366, 0.2809, 0.2678, 0.2147],\n",
      "        [0.2366, 0.2809, 0.2678, 0.2147],\n",
      "        [0.2366, 0.2809, 0.2678, 0.2147],\n",
      "        [0.2366, 0.2809, 0.2678, 0.2147],\n",
      "        [0.2366, 0.2809, 0.2678, 0.2147],\n",
      "        [0.2366, 0.2809, 0.2678, 0.2147],\n",
      "        [0.2366, 0.2809, 0.2678, 0.2147],\n",
      "        [0.2366, 0.2809, 0.2678, 0.2147],\n",
      "        [0.2366, 0.2809, 0.2678, 0.2147],\n",
      "        [0.2366, 0.2809, 0.2678, 0.2147],\n",
      "        [0.2366, 0.2809, 0.2678, 0.2147],\n",
      "        [0.2366, 0.2809, 0.2678, 0.2147],\n",
      "        [0.2366, 0.2809, 0.2678, 0.2147],\n",
      "        [0.2366, 0.2809, 0.2678, 0.2147],\n",
      "        [0.2366, 0.2809, 0.2678, 0.2147],\n",
      "        [0.2366, 0.2808, 0.2678, 0.2147],\n",
      "        [0.2366, 0.2808, 0.2678, 0.2147],\n",
      "        [0.2366, 0.2809, 0.2678, 0.2147],\n",
      "        [0.2366, 0.2809, 0.2678, 0.2147],\n",
      "        [0.2366, 0.2809, 0.2678, 0.2147],\n",
      "        [0.2366, 0.2809, 0.2678, 0.2147],\n",
      "        [0.2366, 0.2809, 0.2678, 0.2147],\n",
      "        [0.2366, 0.2809, 0.2678, 0.2147],\n",
      "        [0.2366, 0.2808, 0.2678, 0.2147],\n",
      "        [0.2366, 0.2808, 0.2678, 0.2147],\n",
      "        [0.2367, 0.2808, 0.2678, 0.2147],\n",
      "        [0.2367, 0.2808, 0.2678, 0.2147],\n",
      "        [0.2366, 0.2809, 0.2678, 0.2147],\n",
      "        [0.2366, 0.2809, 0.2678, 0.2147],\n",
      "        [0.2366, 0.2809, 0.2678, 0.2147],\n",
      "        [0.2366, 0.2809, 0.2678, 0.2147],\n",
      "        [0.2366, 0.2808, 0.2678, 0.2147],\n",
      "        [0.2366, 0.2809, 0.2678, 0.2147],\n",
      "        [0.2366, 0.2809, 0.2678, 0.2147],\n",
      "        [0.2366, 0.2809, 0.2678, 0.2147],\n",
      "        [0.2366, 0.2808, 0.2678, 0.2148],\n",
      "        [0.2366, 0.2808, 0.2678, 0.2147],\n",
      "        [0.2366, 0.2808, 0.2678, 0.2147],\n",
      "        [0.2366, 0.2809, 0.2678, 0.2147],\n",
      "        [0.2366, 0.2808, 0.2678, 0.2147],\n",
      "        [0.2366, 0.2808, 0.2678, 0.2147],\n",
      "        [0.2366, 0.2808, 0.2678, 0.2147],\n",
      "        [0.2367, 0.2808, 0.2678, 0.2147],\n",
      "        [0.2366, 0.2809, 0.2678, 0.2147],\n",
      "        [0.2366, 0.2809, 0.2678, 0.2147],\n",
      "        [0.2366, 0.2809, 0.2678, 0.2147],\n",
      "        [0.2366, 0.2809, 0.2678, 0.2147],\n",
      "        [0.2366, 0.2809, 0.2678, 0.2147],\n",
      "        [0.2366, 0.2809, 0.2678, 0.2147],\n",
      "        [0.2366, 0.2809, 0.2678, 0.2147],\n",
      "        [0.2366, 0.2809, 0.2678, 0.2147],\n",
      "        [0.2366, 0.2808, 0.2678, 0.2147],\n",
      "        [0.2366, 0.2809, 0.2678, 0.2147],\n",
      "        [0.2366, 0.2808, 0.2678, 0.2147],\n",
      "        [0.2366, 0.2808, 0.2678, 0.2147]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3809, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(0.0518, grad_fn=<AddBackward0>)\n",
      "batching here\n",
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(7.0406e-05, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.4248, -1.2131, -1.3435, -1.6033],\n",
      "        [-1.4248, -1.2131, -1.3435, -1.6033],\n",
      "        [-1.4248, -1.2131, -1.3435, -1.6033],\n",
      "        [-1.4248, -1.2131, -1.3435, -1.6033],\n",
      "        [-1.4247, -1.2133, -1.3435, -1.6031],\n",
      "        [-1.4247, -1.2133, -1.3435, -1.6032],\n",
      "        [-1.4247, -1.2133, -1.3435, -1.6032],\n",
      "        [-1.4247, -1.2133, -1.3435, -1.6032],\n",
      "        [-1.4249, -1.2130, -1.3434, -1.6034],\n",
      "        [-1.4247, -1.2132, -1.3435, -1.6032],\n",
      "        [-1.4249, -1.2130, -1.3434, -1.6034],\n",
      "        [-1.4247, -1.2132, -1.3435, -1.6032],\n",
      "        [-1.4248, -1.2131, -1.3435, -1.6034],\n",
      "        [-1.4248, -1.2131, -1.3435, -1.6034],\n",
      "        [-1.4248, -1.2130, -1.3434, -1.6034],\n",
      "        [-1.4248, -1.2131, -1.3435, -1.6034],\n",
      "        [-1.4246, -1.2132, -1.3435, -1.6033],\n",
      "        [-1.4246, -1.2132, -1.3435, -1.6034],\n",
      "        [-1.4246, -1.2132, -1.3436, -1.6033],\n",
      "        [-1.4246, -1.2131, -1.3435, -1.6035],\n",
      "        [-1.4249, -1.2131, -1.3434, -1.6033],\n",
      "        [-1.4248, -1.2132, -1.3434, -1.6033],\n",
      "        [-1.4247, -1.2131, -1.3434, -1.6034],\n",
      "        [-1.4247, -1.2131, -1.3434, -1.6034],\n",
      "        [-1.4247, -1.2133, -1.3435, -1.6031],\n",
      "        [-1.4247, -1.2132, -1.3435, -1.6032],\n",
      "        [-1.4247, -1.2132, -1.3435, -1.6032],\n",
      "        [-1.4247, -1.2133, -1.3435, -1.6031],\n",
      "        [-1.4248, -1.2131, -1.3434, -1.6034],\n",
      "        [-1.4248, -1.2131, -1.3435, -1.6033],\n",
      "        [-1.4248, -1.2131, -1.3434, -1.6033],\n",
      "        [-1.4248, -1.2132, -1.3434, -1.6033],\n",
      "        [-1.4245, -1.2137, -1.3434, -1.6028],\n",
      "        [-1.4246, -1.2135, -1.3435, -1.6030],\n",
      "        [-1.4245, -1.2136, -1.3434, -1.6029],\n",
      "        [-1.4245, -1.2135, -1.3436, -1.6029],\n",
      "        [-1.4247, -1.2131, -1.3435, -1.6032],\n",
      "        [-1.4248, -1.2131, -1.3435, -1.6033],\n",
      "        [-1.4248, -1.2131, -1.3435, -1.6032],\n",
      "        [-1.4248, -1.2131, -1.3435, -1.6033],\n",
      "        [-1.4247, -1.2134, -1.3435, -1.6029],\n",
      "        [-1.4247, -1.2134, -1.3435, -1.6029],\n",
      "        [-1.4247, -1.2133, -1.3435, -1.6031],\n",
      "        [-1.4247, -1.2133, -1.3435, -1.6032],\n",
      "        [-1.4248, -1.2132, -1.3434, -1.6033],\n",
      "        [-1.4248, -1.2132, -1.3434, -1.6033],\n",
      "        [-1.4248, -1.2131, -1.3434, -1.6033],\n",
      "        [-1.4248, -1.2132, -1.3434, -1.6033],\n",
      "        [-1.4246, -1.2134, -1.3435, -1.6030],\n",
      "        [-1.4246, -1.2134, -1.3435, -1.6031],\n",
      "        [-1.4246, -1.2135, -1.3435, -1.6029],\n",
      "        [-1.4247, -1.2133, -1.3435, -1.6032],\n",
      "        [-1.4248, -1.2131, -1.3434, -1.6034],\n",
      "        [-1.4248, -1.2131, -1.3434, -1.6034],\n",
      "        [-1.4248, -1.2131, -1.3434, -1.6034],\n",
      "        [-1.4248, -1.2131, -1.3434, -1.6034],\n",
      "        [-1.4248, -1.2132, -1.3434, -1.6033],\n",
      "        [-1.4247, -1.2131, -1.3434, -1.6034],\n",
      "        [-1.4247, -1.2131, -1.3434, -1.6034],\n",
      "        [-1.4248, -1.2131, -1.3435, -1.6033],\n",
      "        [-1.4247, -1.2136, -1.3434, -1.6027],\n",
      "        [-1.4246, -1.2135, -1.3435, -1.6029],\n",
      "        [-1.4247, -1.2135, -1.3435, -1.6029],\n",
      "        [-1.4246, -1.2135, -1.3434, -1.6030]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([0.0196, 0.0124, 0.0099, 0.0044, 0.0173, 0.0132, 0.0099, 0.0049, 0.0197,\n",
      "        0.0108, 0.0082, 0.0034, 0.0166, 0.0112, 0.0090, 0.0038, 0.0164, 0.0112,\n",
      "        0.0083, 0.0057, 0.0149, 0.0112, 0.0089, 0.0038, 0.0165, 0.0114, 0.0089,\n",
      "        0.0044, 0.0199, 0.0148, 0.0088, 0.0043, 0.0162, 0.0148, 0.0098, 0.0042,\n",
      "        0.0163, 0.0134, 0.0075, 0.0042, 0.0188, 0.0108, 0.0073, 0.0046, 0.0161,\n",
      "        0.0112, 0.0089, 0.0043, 0.0150, 0.0112, 0.0080, 0.0054, 0.0174, 0.0112,\n",
      "        0.0075, 0.0042, 0.0194, 0.0130, 0.0086, 0.0036, 0.0150, 0.0150, 0.0103,\n",
      "        0.0041], grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(-0.0106, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2406, 0.2973, 0.2609, 0.2012],\n",
      "        [0.2406, 0.2973, 0.2609, 0.2012],\n",
      "        [0.2406, 0.2973, 0.2609, 0.2012],\n",
      "        [0.2406, 0.2973, 0.2609, 0.2012],\n",
      "        [0.2406, 0.2972, 0.2609, 0.2013],\n",
      "        [0.2406, 0.2972, 0.2609, 0.2013],\n",
      "        [0.2406, 0.2972, 0.2609, 0.2013],\n",
      "        [0.2406, 0.2972, 0.2609, 0.2013],\n",
      "        [0.2405, 0.2973, 0.2610, 0.2012],\n",
      "        [0.2406, 0.2972, 0.2609, 0.2013],\n",
      "        [0.2405, 0.2973, 0.2609, 0.2012],\n",
      "        [0.2406, 0.2973, 0.2609, 0.2012],\n",
      "        [0.2406, 0.2973, 0.2609, 0.2012],\n",
      "        [0.2406, 0.2973, 0.2609, 0.2012],\n",
      "        [0.2406, 0.2973, 0.2609, 0.2012],\n",
      "        [0.2406, 0.2973, 0.2609, 0.2012],\n",
      "        [0.2406, 0.2973, 0.2609, 0.2012],\n",
      "        [0.2406, 0.2972, 0.2609, 0.2012],\n",
      "        [0.2406, 0.2973, 0.2609, 0.2012],\n",
      "        [0.2406, 0.2973, 0.2609, 0.2012],\n",
      "        [0.2405, 0.2973, 0.2610, 0.2012],\n",
      "        [0.2406, 0.2972, 0.2610, 0.2012],\n",
      "        [0.2406, 0.2973, 0.2609, 0.2012],\n",
      "        [0.2406, 0.2973, 0.2609, 0.2012],\n",
      "        [0.2406, 0.2972, 0.2609, 0.2013],\n",
      "        [0.2406, 0.2972, 0.2609, 0.2013],\n",
      "        [0.2406, 0.2972, 0.2609, 0.2013],\n",
      "        [0.2406, 0.2972, 0.2609, 0.2013],\n",
      "        [0.2406, 0.2973, 0.2610, 0.2012],\n",
      "        [0.2406, 0.2973, 0.2609, 0.2012],\n",
      "        [0.2406, 0.2973, 0.2610, 0.2012],\n",
      "        [0.2406, 0.2972, 0.2609, 0.2012],\n",
      "        [0.2406, 0.2971, 0.2610, 0.2013],\n",
      "        [0.2406, 0.2972, 0.2609, 0.2013],\n",
      "        [0.2406, 0.2971, 0.2609, 0.2013],\n",
      "        [0.2406, 0.2971, 0.2609, 0.2013],\n",
      "        [0.2406, 0.2973, 0.2609, 0.2012],\n",
      "        [0.2406, 0.2973, 0.2609, 0.2012],\n",
      "        [0.2406, 0.2973, 0.2609, 0.2012],\n",
      "        [0.2406, 0.2973, 0.2609, 0.2012],\n",
      "        [0.2406, 0.2972, 0.2609, 0.2013],\n",
      "        [0.2406, 0.2972, 0.2609, 0.2013],\n",
      "        [0.2406, 0.2972, 0.2609, 0.2013],\n",
      "        [0.2406, 0.2972, 0.2609, 0.2012],\n",
      "        [0.2406, 0.2972, 0.2610, 0.2012],\n",
      "        [0.2406, 0.2972, 0.2610, 0.2012],\n",
      "        [0.2406, 0.2973, 0.2610, 0.2012],\n",
      "        [0.2406, 0.2973, 0.2610, 0.2012],\n",
      "        [0.2406, 0.2972, 0.2609, 0.2013],\n",
      "        [0.2406, 0.2972, 0.2609, 0.2013],\n",
      "        [0.2406, 0.2972, 0.2609, 0.2013],\n",
      "        [0.2406, 0.2972, 0.2609, 0.2013],\n",
      "        [0.2406, 0.2973, 0.2610, 0.2012],\n",
      "        [0.2406, 0.2973, 0.2610, 0.2012],\n",
      "        [0.2406, 0.2973, 0.2610, 0.2012],\n",
      "        [0.2406, 0.2973, 0.2610, 0.2012],\n",
      "        [0.2406, 0.2972, 0.2610, 0.2012],\n",
      "        [0.2406, 0.2973, 0.2609, 0.2012],\n",
      "        [0.2406, 0.2973, 0.2609, 0.2012],\n",
      "        [0.2406, 0.2973, 0.2609, 0.2012],\n",
      "        [0.2406, 0.2971, 0.2609, 0.2013],\n",
      "        [0.2406, 0.2971, 0.2609, 0.2013],\n",
      "        [0.2406, 0.2972, 0.2609, 0.2013],\n",
      "        [0.2406, 0.2972, 0.2609, 0.2013]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3766, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(-0.0243, grad_fn=<AddBackward0>)\n",
      "batching here\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34: done 6 episodes, mean_reward=0.17, best_reward=2.00, speed=197.96\n",
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(0.0449, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.4198, -1.2254, -1.3454, -1.5889],\n",
      "        [-1.4198, -1.2254, -1.3454, -1.5889],\n",
      "        [-1.4198, -1.2254, -1.3454, -1.5888],\n",
      "        [-1.4198, -1.2253, -1.3454, -1.5889],\n",
      "        [-1.4198, -1.2255, -1.3453, -1.5889],\n",
      "        [-1.4198, -1.2255, -1.3453, -1.5888],\n",
      "        [-1.4198, -1.2255, -1.3453, -1.5888],\n",
      "        [-1.4198, -1.2255, -1.3453, -1.5888],\n",
      "        [-1.4198, -1.2255, -1.3453, -1.5889],\n",
      "        [-1.4199, -1.2254, -1.3453, -1.5888],\n",
      "        [-1.4198, -1.2254, -1.3453, -1.5888],\n",
      "        [-1.4198, -1.2254, -1.3454, -1.5888],\n",
      "        [-1.4198, -1.2255, -1.3453, -1.5889],\n",
      "        [-1.4198, -1.2255, -1.3453, -1.5889],\n",
      "        [-1.4198, -1.2255, -1.3452, -1.5889],\n",
      "        [-1.4198, -1.2256, -1.3452, -1.5889],\n",
      "        [-1.4197, -1.2255, -1.3454, -1.5888],\n",
      "        [-1.4198, -1.2254, -1.3453, -1.5889],\n",
      "        [-1.4197, -1.2254, -1.3453, -1.5890],\n",
      "        [-1.4197, -1.2254, -1.3453, -1.5890],\n",
      "        [-1.4197, -1.2255, -1.3453, -1.5889],\n",
      "        [-1.4197, -1.2255, -1.3453, -1.5889],\n",
      "        [-1.4198, -1.2255, -1.3453, -1.5888],\n",
      "        [-1.4198, -1.2254, -1.3453, -1.5889],\n",
      "        [-1.4198, -1.2255, -1.3453, -1.5889],\n",
      "        [-1.4198, -1.2255, -1.3453, -1.5889],\n",
      "        [-1.4198, -1.2255, -1.3453, -1.5889],\n",
      "        [-1.4198, -1.2255, -1.3453, -1.5889],\n",
      "        [-1.4198, -1.2255, -1.3453, -1.5888],\n",
      "        [-1.4198, -1.2254, -1.3453, -1.5890],\n",
      "        [-1.4198, -1.2255, -1.3453, -1.5888],\n",
      "        [-1.4198, -1.2255, -1.3453, -1.5888],\n",
      "        [-1.4197, -1.2259, -1.3452, -1.5886],\n",
      "        [-1.4197, -1.2258, -1.3452, -1.5887],\n",
      "        [-1.4196, -1.2257, -1.3453, -1.5888],\n",
      "        [-1.4196, -1.2257, -1.3453, -1.5888],\n",
      "        [-1.4198, -1.2254, -1.3454, -1.5889],\n",
      "        [-1.4198, -1.2254, -1.3454, -1.5889],\n",
      "        [-1.4198, -1.2254, -1.3454, -1.5889],\n",
      "        [-1.4198, -1.2254, -1.3454, -1.5889],\n",
      "        [-1.4198, -1.2255, -1.3453, -1.5889],\n",
      "        [-1.4198, -1.2255, -1.3452, -1.5889],\n",
      "        [-1.4198, -1.2255, -1.3452, -1.5889],\n",
      "        [-1.4198, -1.2254, -1.3452, -1.5890],\n",
      "        [-1.4198, -1.2255, -1.3452, -1.5889],\n",
      "        [-1.4198, -1.2255, -1.3453, -1.5888],\n",
      "        [-1.4198, -1.2255, -1.3452, -1.5889],\n",
      "        [-1.4198, -1.2255, -1.3452, -1.5889],\n",
      "        [-1.4197, -1.2256, -1.3453, -1.5888],\n",
      "        [-1.4197, -1.2255, -1.3453, -1.5889],\n",
      "        [-1.4197, -1.2255, -1.3453, -1.5889],\n",
      "        [-1.4197, -1.2255, -1.3453, -1.5889],\n",
      "        [-1.4198, -1.2255, -1.3453, -1.5889],\n",
      "        [-1.4198, -1.2255, -1.3453, -1.5889],\n",
      "        [-1.4198, -1.2255, -1.3452, -1.5889],\n",
      "        [-1.4198, -1.2256, -1.3452, -1.5889],\n",
      "        [-1.4198, -1.2254, -1.3453, -1.5890],\n",
      "        [-1.4197, -1.2255, -1.3453, -1.5889],\n",
      "        [-1.4198, -1.2255, -1.3453, -1.5888],\n",
      "        [-1.4198, -1.2256, -1.3452, -1.5888],\n",
      "        [-1.4197, -1.2257, -1.3452, -1.5887],\n",
      "        [-1.4197, -1.2258, -1.3452, -1.5886],\n",
      "        [-1.4198, -1.2257, -1.3452, -1.5887],\n",
      "        [-1.4197, -1.2257, -1.3452, -1.5887]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([ 0.3149,  0.3457,  0.0062,  0.0037,  0.0123,  0.0118,  0.0060,  0.0030,\n",
      "         0.0147,  0.0106,  0.0074,  0.0032,  0.3651,  0.3460,  0.0064,  0.0032,\n",
      "         0.0134,  0.0123,  0.0074,  0.0041,  0.3650,  0.3650,  0.0081,  0.0032,\n",
      "         0.0124,  0.0102,  0.0063,  0.0041,  0.0123,  0.0092,  0.0069,  0.0035,\n",
      "        -1.3182, -1.2163,  0.0068,  0.0035,  0.3149,  0.3149,  0.3149,  0.4083,\n",
      "         0.0123,  0.0102,  0.0081,  0.0031,  0.0136,  0.0120,  0.0062,  0.0034,\n",
      "         0.0134,  0.0120,  0.0069,  0.0033,  0.3151,  0.3459,  0.0081,  0.0041,\n",
      "         0.3460,  0.3650,  0.0071,  0.0038,  0.0124,  0.0092,  0.0063,  0.0037],\n",
      "       grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(-0.0416, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2418, 0.2936, 0.2604, 0.2041],\n",
      "        [0.2418, 0.2936, 0.2604, 0.2041],\n",
      "        [0.2418, 0.2936, 0.2604, 0.2042],\n",
      "        [0.2418, 0.2937, 0.2604, 0.2041],\n",
      "        [0.2418, 0.2936, 0.2605, 0.2042],\n",
      "        [0.2418, 0.2936, 0.2605, 0.2042],\n",
      "        [0.2418, 0.2936, 0.2605, 0.2042],\n",
      "        [0.2418, 0.2936, 0.2605, 0.2042],\n",
      "        [0.2418, 0.2936, 0.2605, 0.2042],\n",
      "        [0.2417, 0.2936, 0.2605, 0.2042],\n",
      "        [0.2418, 0.2936, 0.2604, 0.2042],\n",
      "        [0.2418, 0.2936, 0.2604, 0.2042],\n",
      "        [0.2418, 0.2936, 0.2605, 0.2042],\n",
      "        [0.2418, 0.2936, 0.2605, 0.2041],\n",
      "        [0.2418, 0.2936, 0.2605, 0.2041],\n",
      "        [0.2418, 0.2936, 0.2605, 0.2042],\n",
      "        [0.2418, 0.2936, 0.2604, 0.2042],\n",
      "        [0.2418, 0.2936, 0.2605, 0.2041],\n",
      "        [0.2418, 0.2936, 0.2605, 0.2041],\n",
      "        [0.2418, 0.2936, 0.2605, 0.2041],\n",
      "        [0.2418, 0.2936, 0.2605, 0.2041],\n",
      "        [0.2418, 0.2936, 0.2605, 0.2041],\n",
      "        [0.2418, 0.2936, 0.2605, 0.2042],\n",
      "        [0.2418, 0.2936, 0.2605, 0.2041],\n",
      "        [0.2418, 0.2936, 0.2605, 0.2041],\n",
      "        [0.2418, 0.2936, 0.2605, 0.2042],\n",
      "        [0.2418, 0.2936, 0.2605, 0.2041],\n",
      "        [0.2418, 0.2936, 0.2605, 0.2042],\n",
      "        [0.2418, 0.2936, 0.2605, 0.2042],\n",
      "        [0.2418, 0.2936, 0.2605, 0.2041],\n",
      "        [0.2418, 0.2936, 0.2605, 0.2042],\n",
      "        [0.2418, 0.2936, 0.2605, 0.2042],\n",
      "        [0.2418, 0.2935, 0.2605, 0.2042],\n",
      "        [0.2418, 0.2935, 0.2605, 0.2042],\n",
      "        [0.2418, 0.2935, 0.2605, 0.2042],\n",
      "        [0.2418, 0.2936, 0.2605, 0.2042],\n",
      "        [0.2418, 0.2936, 0.2604, 0.2042],\n",
      "        [0.2418, 0.2936, 0.2604, 0.2041],\n",
      "        [0.2418, 0.2936, 0.2604, 0.2041],\n",
      "        [0.2418, 0.2936, 0.2604, 0.2041],\n",
      "        [0.2418, 0.2936, 0.2605, 0.2042],\n",
      "        [0.2418, 0.2936, 0.2605, 0.2042],\n",
      "        [0.2418, 0.2936, 0.2605, 0.2042],\n",
      "        [0.2418, 0.2936, 0.2605, 0.2041],\n",
      "        [0.2418, 0.2936, 0.2605, 0.2042],\n",
      "        [0.2418, 0.2936, 0.2605, 0.2042],\n",
      "        [0.2418, 0.2936, 0.2605, 0.2042],\n",
      "        [0.2418, 0.2936, 0.2605, 0.2042],\n",
      "        [0.2418, 0.2936, 0.2605, 0.2042],\n",
      "        [0.2418, 0.2936, 0.2605, 0.2041],\n",
      "        [0.2418, 0.2936, 0.2605, 0.2042],\n",
      "        [0.2418, 0.2936, 0.2605, 0.2041],\n",
      "        [0.2418, 0.2936, 0.2605, 0.2041],\n",
      "        [0.2418, 0.2936, 0.2605, 0.2041],\n",
      "        [0.2418, 0.2936, 0.2605, 0.2042],\n",
      "        [0.2418, 0.2936, 0.2605, 0.2042],\n",
      "        [0.2418, 0.2936, 0.2605, 0.2041],\n",
      "        [0.2418, 0.2936, 0.2605, 0.2041],\n",
      "        [0.2418, 0.2936, 0.2605, 0.2042],\n",
      "        [0.2418, 0.2936, 0.2605, 0.2042],\n",
      "        [0.2418, 0.2935, 0.2605, 0.2042],\n",
      "        [0.2418, 0.2935, 0.2605, 0.2042],\n",
      "        [0.2418, 0.2935, 0.2605, 0.2042],\n",
      "        [0.2418, 0.2935, 0.2605, 0.2042]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3779, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(-0.0105, grad_fn=<AddBackward0>)\n",
      "batching here\n",
      "35: done 5 episodes, mean_reward=0.20, best_reward=2.00, speed=206.82\n",
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(0.0066, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.4470, -1.2242, -1.3332, -1.5746],\n",
      "        [-1.4470, -1.2242, -1.3331, -1.5746],\n",
      "        [-1.4470, -1.2242, -1.3331, -1.5745],\n",
      "        [-1.4471, -1.2241, -1.3332, -1.5747],\n",
      "        [-1.4470, -1.2242, -1.3332, -1.5745],\n",
      "        [-1.4470, -1.2242, -1.3332, -1.5746],\n",
      "        [-1.4469, -1.2242, -1.3332, -1.5746],\n",
      "        [-1.4470, -1.2244, -1.3332, -1.5744],\n",
      "        [-1.4470, -1.2242, -1.3332, -1.5746],\n",
      "        [-1.4470, -1.2242, -1.3332, -1.5745],\n",
      "        [-1.4471, -1.2241, -1.3331, -1.5747],\n",
      "        [-1.4470, -1.2242, -1.3332, -1.5746],\n",
      "        [-1.4470, -1.2244, -1.3331, -1.5744],\n",
      "        [-1.4470, -1.2244, -1.3331, -1.5745],\n",
      "        [-1.4470, -1.2244, -1.3331, -1.5744],\n",
      "        [-1.4470, -1.2244, -1.3331, -1.5744],\n",
      "        [-1.4469, -1.2242, -1.3332, -1.5746],\n",
      "        [-1.4469, -1.2243, -1.3332, -1.5745],\n",
      "        [-1.4469, -1.2242, -1.3332, -1.5746],\n",
      "        [-1.4470, -1.2244, -1.3332, -1.5743],\n",
      "        [-1.4470, -1.2243, -1.3332, -1.5744],\n",
      "        [-1.4469, -1.2244, -1.3331, -1.5744],\n",
      "        [-1.4469, -1.2244, -1.3332, -1.5744],\n",
      "        [-1.4469, -1.2242, -1.3333, -1.5745],\n",
      "        [-1.4470, -1.2242, -1.3332, -1.5745],\n",
      "        [-1.4470, -1.2242, -1.3332, -1.5745],\n",
      "        [-1.4470, -1.2242, -1.3332, -1.5745],\n",
      "        [-1.4470, -1.2244, -1.3332, -1.5744],\n",
      "        [-1.4469, -1.2243, -1.3331, -1.5746],\n",
      "        [-1.4470, -1.2243, -1.3333, -1.5743],\n",
      "        [-1.4470, -1.2244, -1.3332, -1.5744],\n",
      "        [-1.4469, -1.2244, -1.3332, -1.5743],\n",
      "        [-1.4470, -1.2243, -1.3330, -1.5746],\n",
      "        [-1.4471, -1.2243, -1.3330, -1.5747],\n",
      "        [-1.4470, -1.2243, -1.3331, -1.5745],\n",
      "        [-1.4470, -1.2243, -1.3331, -1.5746],\n",
      "        [-1.4471, -1.2242, -1.3331, -1.5745],\n",
      "        [-1.4471, -1.2243, -1.3331, -1.5745],\n",
      "        [-1.4471, -1.2243, -1.3331, -1.5745],\n",
      "        [-1.4470, -1.2242, -1.3331, -1.5746],\n",
      "        [-1.4470, -1.2243, -1.3332, -1.5745],\n",
      "        [-1.4470, -1.2243, -1.3332, -1.5745],\n",
      "        [-1.4470, -1.2242, -1.3332, -1.5746],\n",
      "        [-1.4470, -1.2242, -1.3331, -1.5746],\n",
      "        [-1.4470, -1.2244, -1.3331, -1.5744],\n",
      "        [-1.4470, -1.2244, -1.3331, -1.5744],\n",
      "        [-1.4470, -1.2244, -1.3331, -1.5744],\n",
      "        [-1.4470, -1.2244, -1.3331, -1.5743],\n",
      "        [-1.4470, -1.2243, -1.3331, -1.5747],\n",
      "        [-1.4470, -1.2242, -1.3331, -1.5747],\n",
      "        [-1.4470, -1.2243, -1.3331, -1.5746],\n",
      "        [-1.4470, -1.2243, -1.3330, -1.5746],\n",
      "        [-1.4470, -1.2244, -1.3331, -1.5744],\n",
      "        [-1.4470, -1.2244, -1.3331, -1.5745],\n",
      "        [-1.4470, -1.2244, -1.3331, -1.5744],\n",
      "        [-1.4470, -1.2244, -1.3331, -1.5744],\n",
      "        [-1.4470, -1.2245, -1.3331, -1.5743],\n",
      "        [-1.4469, -1.2244, -1.3332, -1.5745],\n",
      "        [-1.4470, -1.2244, -1.3331, -1.5744],\n",
      "        [-1.4470, -1.2244, -1.3331, -1.5744],\n",
      "        [-1.4470, -1.2244, -1.3331, -1.5744],\n",
      "        [-1.4470, -1.2243, -1.3331, -1.5745],\n",
      "        [-1.4469, -1.2244, -1.3332, -1.5745],\n",
      "        [-1.4469, -1.2244, -1.3332, -1.5744]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([0.0101, 0.0070, 0.0039, 0.0022, 0.2555, 0.2163, 0.2163, 0.0021, 0.0077,\n",
      "        0.0074, 0.0040, 0.0019, 0.0092, 0.0063, 0.0042, 0.0020, 0.1985, 0.2160,\n",
      "        0.1984, 0.0020, 0.0079, 0.0059, 0.0039, 0.0022, 0.1985, 0.2553, 0.2346,\n",
      "        0.0022, 0.0087, 0.0065, 0.0048, 0.0026, 0.0086, 0.0075, 0.0051, 0.0019,\n",
      "        0.0102, 0.0071, 0.0041, 0.0023, 0.2349, 0.1987, 0.2164, 0.2349, 0.2163,\n",
      "        0.1986, 0.1986, 0.0019, 0.0086, 0.0071, 0.0039, 0.0023, 0.0085, 0.0074,\n",
      "        0.0044, 0.0019, 0.0077, 0.0057, 0.0037, 0.0019, 0.0094, 0.0060, 0.0040,\n",
      "        0.0020], grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(-0.0584, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2353, 0.2940, 0.2636, 0.2071],\n",
      "        [0.2353, 0.2940, 0.2637, 0.2071],\n",
      "        [0.2353, 0.2940, 0.2636, 0.2071],\n",
      "        [0.2353, 0.2940, 0.2636, 0.2071],\n",
      "        [0.2353, 0.2940, 0.2636, 0.2071],\n",
      "        [0.2353, 0.2940, 0.2636, 0.2071],\n",
      "        [0.2353, 0.2940, 0.2636, 0.2071],\n",
      "        [0.2353, 0.2939, 0.2636, 0.2071],\n",
      "        [0.2353, 0.2940, 0.2636, 0.2071],\n",
      "        [0.2353, 0.2940, 0.2636, 0.2071],\n",
      "        [0.2353, 0.2940, 0.2637, 0.2071],\n",
      "        [0.2353, 0.2940, 0.2636, 0.2071],\n",
      "        [0.2353, 0.2939, 0.2637, 0.2071],\n",
      "        [0.2353, 0.2939, 0.2637, 0.2071],\n",
      "        [0.2353, 0.2939, 0.2636, 0.2071],\n",
      "        [0.2353, 0.2939, 0.2637, 0.2071],\n",
      "        [0.2353, 0.2940, 0.2636, 0.2071],\n",
      "        [0.2353, 0.2940, 0.2636, 0.2071],\n",
      "        [0.2353, 0.2940, 0.2636, 0.2071],\n",
      "        [0.2353, 0.2939, 0.2636, 0.2071],\n",
      "        [0.2353, 0.2940, 0.2636, 0.2071],\n",
      "        [0.2353, 0.2939, 0.2637, 0.2071],\n",
      "        [0.2353, 0.2939, 0.2636, 0.2071],\n",
      "        [0.2353, 0.2940, 0.2636, 0.2071],\n",
      "        [0.2353, 0.2940, 0.2636, 0.2071],\n",
      "        [0.2353, 0.2940, 0.2636, 0.2071],\n",
      "        [0.2353, 0.2940, 0.2636, 0.2071],\n",
      "        [0.2353, 0.2939, 0.2636, 0.2071],\n",
      "        [0.2353, 0.2940, 0.2637, 0.2071],\n",
      "        [0.2353, 0.2940, 0.2636, 0.2071],\n",
      "        [0.2353, 0.2939, 0.2636, 0.2071],\n",
      "        [0.2353, 0.2939, 0.2636, 0.2072],\n",
      "        [0.2353, 0.2940, 0.2637, 0.2071],\n",
      "        [0.2353, 0.2940, 0.2637, 0.2071],\n",
      "        [0.2353, 0.2940, 0.2637, 0.2071],\n",
      "        [0.2353, 0.2940, 0.2637, 0.2071],\n",
      "        [0.2353, 0.2940, 0.2637, 0.2071],\n",
      "        [0.2353, 0.2940, 0.2637, 0.2071],\n",
      "        [0.2353, 0.2940, 0.2637, 0.2071],\n",
      "        [0.2353, 0.2940, 0.2637, 0.2071],\n",
      "        [0.2353, 0.2940, 0.2636, 0.2071],\n",
      "        [0.2353, 0.2940, 0.2636, 0.2071],\n",
      "        [0.2353, 0.2940, 0.2636, 0.2071],\n",
      "        [0.2353, 0.2940, 0.2636, 0.2071],\n",
      "        [0.2353, 0.2939, 0.2637, 0.2071],\n",
      "        [0.2353, 0.2939, 0.2637, 0.2071],\n",
      "        [0.2353, 0.2939, 0.2637, 0.2071],\n",
      "        [0.2353, 0.2939, 0.2636, 0.2071],\n",
      "        [0.2353, 0.2940, 0.2637, 0.2071],\n",
      "        [0.2353, 0.2940, 0.2637, 0.2071],\n",
      "        [0.2353, 0.2940, 0.2637, 0.2071],\n",
      "        [0.2353, 0.2940, 0.2637, 0.2071],\n",
      "        [0.2353, 0.2939, 0.2637, 0.2071],\n",
      "        [0.2353, 0.2939, 0.2637, 0.2071],\n",
      "        [0.2353, 0.2939, 0.2636, 0.2071],\n",
      "        [0.2353, 0.2939, 0.2637, 0.2071],\n",
      "        [0.2353, 0.2939, 0.2637, 0.2071],\n",
      "        [0.2353, 0.2939, 0.2636, 0.2071],\n",
      "        [0.2353, 0.2939, 0.2636, 0.2071],\n",
      "        [0.2353, 0.2939, 0.2637, 0.2071],\n",
      "        [0.2353, 0.2939, 0.2636, 0.2071],\n",
      "        [0.2353, 0.2939, 0.2637, 0.2071],\n",
      "        [0.2353, 0.2939, 0.2636, 0.2071],\n",
      "        [0.2353, 0.2939, 0.2636, 0.2071]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3779, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(-0.0656, grad_fn=<AddBackward0>)\n",
      "batching here\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(5.6596e-06, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.4340, -1.2500, -1.3415, -1.5431],\n",
      "        [-1.4340, -1.2501, -1.3415, -1.5431],\n",
      "        [-1.4341, -1.2500, -1.3415, -1.5431],\n",
      "        [-1.4341, -1.2499, -1.3415, -1.5432],\n",
      "        [-1.4340, -1.2502, -1.3415, -1.5430],\n",
      "        [-1.4339, -1.2503, -1.3416, -1.5430],\n",
      "        [-1.4339, -1.2502, -1.3415, -1.5430],\n",
      "        [-1.4339, -1.2502, -1.3415, -1.5430],\n",
      "        [-1.4340, -1.2500, -1.3415, -1.5432],\n",
      "        [-1.4341, -1.2500, -1.3415, -1.5431],\n",
      "        [-1.4341, -1.2500, -1.3414, -1.5432],\n",
      "        [-1.4341, -1.2500, -1.3415, -1.5432],\n",
      "        [-1.4339, -1.2502, -1.3415, -1.5431],\n",
      "        [-1.4339, -1.2502, -1.3415, -1.5430],\n",
      "        [-1.4339, -1.2503, -1.3415, -1.5430],\n",
      "        [-1.4339, -1.2503, -1.3415, -1.5430],\n",
      "        [-1.4339, -1.2502, -1.3416, -1.5429],\n",
      "        [-1.4339, -1.2503, -1.3416, -1.5429],\n",
      "        [-1.4339, -1.2503, -1.3415, -1.5429],\n",
      "        [-1.4339, -1.2503, -1.3416, -1.5429],\n",
      "        [-1.4339, -1.2502, -1.3416, -1.5429],\n",
      "        [-1.4339, -1.2502, -1.3416, -1.5429],\n",
      "        [-1.4339, -1.2503, -1.3416, -1.5428],\n",
      "        [-1.4340, -1.2501, -1.3416, -1.5429],\n",
      "        [-1.4339, -1.2502, -1.3415, -1.5430],\n",
      "        [-1.4339, -1.2502, -1.3415, -1.5430],\n",
      "        [-1.4339, -1.2502, -1.3415, -1.5430],\n",
      "        [-1.4339, -1.2501, -1.3415, -1.5431],\n",
      "        [-1.4339, -1.2504, -1.3416, -1.5428],\n",
      "        [-1.4339, -1.2503, -1.3415, -1.5429],\n",
      "        [-1.4339, -1.2503, -1.3415, -1.5429],\n",
      "        [-1.4339, -1.2503, -1.3415, -1.5429],\n",
      "        [-1.4340, -1.2501, -1.3414, -1.5432],\n",
      "        [-1.4340, -1.2501, -1.3415, -1.5432],\n",
      "        [-1.4340, -1.2500, -1.3415, -1.5432],\n",
      "        [-1.4341, -1.2500, -1.3414, -1.5433],\n",
      "        [-1.4341, -1.2500, -1.3415, -1.5431],\n",
      "        [-1.4341, -1.2500, -1.3415, -1.5431],\n",
      "        [-1.4340, -1.2500, -1.3415, -1.5433],\n",
      "        [-1.4341, -1.2500, -1.3415, -1.5432],\n",
      "        [-1.4340, -1.2501, -1.3416, -1.5430],\n",
      "        [-1.4340, -1.2501, -1.3416, -1.5430],\n",
      "        [-1.4339, -1.2501, -1.3416, -1.5430],\n",
      "        [-1.4339, -1.2502, -1.3415, -1.5430],\n",
      "        [-1.4339, -1.2503, -1.3415, -1.5429],\n",
      "        [-1.4340, -1.2503, -1.3415, -1.5429],\n",
      "        [-1.4339, -1.2502, -1.3415, -1.5430],\n",
      "        [-1.4339, -1.2502, -1.3416, -1.5429],\n",
      "        [-1.4340, -1.2501, -1.3415, -1.5432],\n",
      "        [-1.4340, -1.2501, -1.3415, -1.5432],\n",
      "        [-1.4340, -1.2502, -1.3414, -1.5432],\n",
      "        [-1.4339, -1.2502, -1.3416, -1.5430],\n",
      "        [-1.4339, -1.2502, -1.3415, -1.5431],\n",
      "        [-1.4339, -1.2502, -1.3415, -1.5430],\n",
      "        [-1.4339, -1.2502, -1.3415, -1.5430],\n",
      "        [-1.4339, -1.2503, -1.3415, -1.5430],\n",
      "        [-1.4339, -1.2502, -1.3415, -1.5431],\n",
      "        [-1.4340, -1.2502, -1.3415, -1.5430],\n",
      "        [-1.4340, -1.2502, -1.3415, -1.5430],\n",
      "        [-1.4340, -1.2503, -1.3415, -1.5430],\n",
      "        [-1.4339, -1.2501, -1.3415, -1.5430],\n",
      "        [-1.4340, -1.2501, -1.3415, -1.5431],\n",
      "        [-1.4340, -1.2501, -1.3415, -1.5432],\n",
      "        [-1.4340, -1.2501, -1.3415, -1.5431]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([0.0043, 0.0032, 0.0021, 0.0011, 0.0054, 0.0037, 0.0027, 0.0012, 0.0051,\n",
      "        0.0033, 0.0023, 0.0013, 0.0050, 0.0041, 0.0025, 0.0014, 0.0048, 0.0033,\n",
      "        0.0027, 0.0011, 0.0049, 0.0031, 0.0023, 0.0011, 0.0055, 0.0033, 0.0023,\n",
      "        0.0012, 0.0046, 0.0035, 0.0028, 0.0013, 0.0045, 0.0035, 0.0023, 0.0012,\n",
      "        0.0052, 0.0037, 0.0025, 0.0011, 0.0047, 0.0042, 0.0026, 0.0013, 0.0047,\n",
      "        0.0036, 0.0023, 0.0010, 0.0052, 0.0036, 0.0025, 0.0009, 0.0050, 0.0040,\n",
      "        0.0022, 0.0012, 0.0044, 0.0038, 0.0022, 0.0014, 0.0049, 0.0033, 0.0022,\n",
      "        0.0011], grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(-0.0030, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2383, 0.2865, 0.2614, 0.2137],\n",
      "        [0.2383, 0.2865, 0.2614, 0.2137],\n",
      "        [0.2383, 0.2865, 0.2614, 0.2137],\n",
      "        [0.2383, 0.2865, 0.2614, 0.2137],\n",
      "        [0.2384, 0.2864, 0.2614, 0.2137],\n",
      "        [0.2384, 0.2864, 0.2614, 0.2137],\n",
      "        [0.2384, 0.2865, 0.2614, 0.2137],\n",
      "        [0.2384, 0.2864, 0.2614, 0.2137],\n",
      "        [0.2384, 0.2865, 0.2614, 0.2137],\n",
      "        [0.2383, 0.2865, 0.2615, 0.2137],\n",
      "        [0.2383, 0.2865, 0.2615, 0.2137],\n",
      "        [0.2383, 0.2865, 0.2614, 0.2137],\n",
      "        [0.2384, 0.2864, 0.2615, 0.2137],\n",
      "        [0.2384, 0.2864, 0.2615, 0.2137],\n",
      "        [0.2384, 0.2864, 0.2615, 0.2137],\n",
      "        [0.2384, 0.2864, 0.2615, 0.2137],\n",
      "        [0.2384, 0.2864, 0.2614, 0.2138],\n",
      "        [0.2384, 0.2864, 0.2614, 0.2138],\n",
      "        [0.2384, 0.2864, 0.2615, 0.2138],\n",
      "        [0.2384, 0.2864, 0.2614, 0.2138],\n",
      "        [0.2384, 0.2864, 0.2614, 0.2138],\n",
      "        [0.2384, 0.2864, 0.2614, 0.2138],\n",
      "        [0.2384, 0.2864, 0.2614, 0.2138],\n",
      "        [0.2384, 0.2865, 0.2614, 0.2138],\n",
      "        [0.2384, 0.2864, 0.2614, 0.2137],\n",
      "        [0.2384, 0.2864, 0.2614, 0.2137],\n",
      "        [0.2384, 0.2865, 0.2614, 0.2137],\n",
      "        [0.2384, 0.2865, 0.2614, 0.2137],\n",
      "        [0.2384, 0.2864, 0.2614, 0.2138],\n",
      "        [0.2384, 0.2864, 0.2615, 0.2138],\n",
      "        [0.2384, 0.2864, 0.2615, 0.2138],\n",
      "        [0.2384, 0.2864, 0.2615, 0.2138],\n",
      "        [0.2384, 0.2865, 0.2615, 0.2137],\n",
      "        [0.2383, 0.2865, 0.2615, 0.2137],\n",
      "        [0.2384, 0.2865, 0.2615, 0.2137],\n",
      "        [0.2383, 0.2865, 0.2615, 0.2137],\n",
      "        [0.2383, 0.2865, 0.2614, 0.2137],\n",
      "        [0.2383, 0.2865, 0.2615, 0.2137],\n",
      "        [0.2383, 0.2865, 0.2615, 0.2137],\n",
      "        [0.2383, 0.2865, 0.2615, 0.2137],\n",
      "        [0.2384, 0.2865, 0.2614, 0.2137],\n",
      "        [0.2384, 0.2865, 0.2614, 0.2137],\n",
      "        [0.2384, 0.2865, 0.2614, 0.2137],\n",
      "        [0.2384, 0.2865, 0.2614, 0.2137],\n",
      "        [0.2384, 0.2864, 0.2615, 0.2138],\n",
      "        [0.2384, 0.2864, 0.2614, 0.2138],\n",
      "        [0.2384, 0.2864, 0.2614, 0.2137],\n",
      "        [0.2384, 0.2864, 0.2614, 0.2138],\n",
      "        [0.2384, 0.2865, 0.2615, 0.2137],\n",
      "        [0.2384, 0.2865, 0.2615, 0.2137],\n",
      "        [0.2384, 0.2865, 0.2615, 0.2137],\n",
      "        [0.2384, 0.2865, 0.2614, 0.2137],\n",
      "        [0.2384, 0.2864, 0.2615, 0.2137],\n",
      "        [0.2384, 0.2864, 0.2615, 0.2137],\n",
      "        [0.2384, 0.2864, 0.2615, 0.2137],\n",
      "        [0.2384, 0.2864, 0.2615, 0.2137],\n",
      "        [0.2384, 0.2864, 0.2615, 0.2137],\n",
      "        [0.2384, 0.2864, 0.2615, 0.2137],\n",
      "        [0.2384, 0.2864, 0.2615, 0.2137],\n",
      "        [0.2384, 0.2864, 0.2615, 0.2137],\n",
      "        [0.2384, 0.2865, 0.2614, 0.2137],\n",
      "        [0.2384, 0.2865, 0.2615, 0.2137],\n",
      "        [0.2384, 0.2865, 0.2615, 0.2137],\n",
      "        [0.2384, 0.2865, 0.2615, 0.2137]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3805, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(-0.0168, grad_fn=<AddBackward0>)\n",
      "batching here\n",
      "37: done 1 episodes, mean_reward=2.00, best_reward=2.00, speed=206.51\n",
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.4333, -1.2522, -1.3418, -1.5407],\n",
      "        [-1.4332, -1.2522, -1.3419, -1.5407],\n",
      "        [-1.4333, -1.2523, -1.3419, -1.5406],\n",
      "        [-1.4333, -1.2522, -1.3418, -1.5407],\n",
      "        [-1.4332, -1.2524, -1.3419, -1.5405],\n",
      "        [-1.4332, -1.2524, -1.3418, -1.5406],\n",
      "        [-1.4332, -1.2524, -1.3419, -1.5405],\n",
      "        [-1.4331, -1.2524, -1.3419, -1.5405],\n",
      "        [-1.4333, -1.2522, -1.3418, -1.5407],\n",
      "        [-1.4333, -1.2521, -1.3419, -1.5408],\n",
      "        [-1.4333, -1.2522, -1.3418, -1.5407],\n",
      "        [-1.4332, -1.2524, -1.3420, -1.5405],\n",
      "        [-1.4331, -1.2525, -1.3419, -1.5405],\n",
      "        [-1.4331, -1.2525, -1.3418, -1.5405],\n",
      "        [-1.4331, -1.2525, -1.3419, -1.5404],\n",
      "        [-1.4332, -1.2524, -1.3418, -1.5406],\n",
      "        [-1.4332, -1.2523, -1.3419, -1.5406],\n",
      "        [-1.4331, -1.2525, -1.3419, -1.5405],\n",
      "        [-1.4331, -1.2525, -1.3418, -1.5404],\n",
      "        [-1.4332, -1.2524, -1.3419, -1.5405],\n",
      "        [-1.4332, -1.2524, -1.3419, -1.5405],\n",
      "        [-1.4331, -1.2525, -1.3419, -1.5404],\n",
      "        [-1.4331, -1.2525, -1.3419, -1.5403],\n",
      "        [-1.4332, -1.2524, -1.3418, -1.5405],\n",
      "        [-1.4332, -1.2524, -1.3419, -1.5405],\n",
      "        [-1.4331, -1.2525, -1.3418, -1.5405],\n",
      "        [-1.4332, -1.2524, -1.3419, -1.5404],\n",
      "        [-1.4331, -1.2524, -1.3419, -1.5405],\n",
      "        [-1.4331, -1.2525, -1.3418, -1.5405],\n",
      "        [-1.4331, -1.2525, -1.3419, -1.5405],\n",
      "        [-1.4331, -1.2524, -1.3419, -1.5405],\n",
      "        [-1.4331, -1.2525, -1.3419, -1.5404],\n",
      "        [-1.4332, -1.2522, -1.3417, -1.5409],\n",
      "        [-1.4333, -1.2522, -1.3417, -1.5409],\n",
      "        [-1.4333, -1.2521, -1.3417, -1.5409],\n",
      "        [-1.4333, -1.2522, -1.3418, -1.5408],\n",
      "        [-1.4333, -1.2522, -1.3418, -1.5407],\n",
      "        [-1.4333, -1.2522, -1.3418, -1.5407],\n",
      "        [-1.4332, -1.2522, -1.3418, -1.5407],\n",
      "        [-1.4332, -1.2522, -1.3418, -1.5407],\n",
      "        [-1.4332, -1.2524, -1.3419, -1.5405],\n",
      "        [-1.4332, -1.2522, -1.3419, -1.5406],\n",
      "        [-1.4332, -1.2523, -1.3419, -1.5405],\n",
      "        [-1.4332, -1.2524, -1.3419, -1.5405],\n",
      "        [-1.4332, -1.2524, -1.3419, -1.5405],\n",
      "        [-1.4331, -1.2524, -1.3418, -1.5406],\n",
      "        [-1.4332, -1.2524, -1.3418, -1.5406],\n",
      "        [-1.4332, -1.2525, -1.3418, -1.5405],\n",
      "        [-1.4333, -1.2523, -1.3417, -1.5408],\n",
      "        [-1.4332, -1.2523, -1.3418, -1.5408],\n",
      "        [-1.4332, -1.2523, -1.3417, -1.5408],\n",
      "        [-1.4332, -1.2522, -1.3417, -1.5409],\n",
      "        [-1.4331, -1.2525, -1.3419, -1.5405],\n",
      "        [-1.4331, -1.2524, -1.3418, -1.5405],\n",
      "        [-1.4331, -1.2525, -1.3419, -1.5404],\n",
      "        [-1.4332, -1.2524, -1.3418, -1.5406],\n",
      "        [-1.4331, -1.2525, -1.3418, -1.5405],\n",
      "        [-1.4332, -1.2524, -1.3418, -1.5405],\n",
      "        [-1.4331, -1.2525, -1.3419, -1.5404],\n",
      "        [-1.4332, -1.2524, -1.3418, -1.5406],\n",
      "        [-1.4332, -1.2523, -1.3418, -1.5407],\n",
      "        [-1.4332, -1.2523, -1.3418, -1.5406],\n",
      "        [-1.4332, -1.2523, -1.3419, -1.5406],\n",
      "        [-1.4332, -1.2523, -1.3419, -1.5406]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([0.0051, 0.0035, 0.0024, 0.0012, 0.0041, 0.0030, 0.0024, 0.0011, 0.0055,\n",
      "        0.0040, 0.0023, 0.0011, 0.0041, 0.0030, 0.0025, 0.0012, 0.0047, 0.0030,\n",
      "        0.0024, 0.0011, 0.0041, 0.0030, 0.0022, 0.0012, 0.0050, 0.0030, 0.0020,\n",
      "        0.0010, 0.0044, 0.0035, 0.0024, 0.0010, 0.0045, 0.0033, 0.0021, 0.0009,\n",
      "        0.0042, 0.0034, 0.0023, 0.0012, 0.0041, 0.0032, 0.0024, 0.0012, 0.0051,\n",
      "        0.0031, 0.0021, 0.0011, 0.0043, 0.0036, 0.0022, 0.0011, 0.0048, 0.0038,\n",
      "        0.0026, 0.0012, 0.0041, 0.0035, 0.0024, 0.0014, 0.1192, 0.0038, 0.0021,\n",
      "        0.0011], grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(-0.0046, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2385, 0.2859, 0.2614, 0.2142],\n",
      "        [0.2385, 0.2859, 0.2614, 0.2142],\n",
      "        [0.2385, 0.2859, 0.2614, 0.2143],\n",
      "        [0.2385, 0.2859, 0.2614, 0.2142],\n",
      "        [0.2386, 0.2858, 0.2613, 0.2143],\n",
      "        [0.2385, 0.2858, 0.2614, 0.2143],\n",
      "        [0.2385, 0.2858, 0.2614, 0.2143],\n",
      "        [0.2386, 0.2858, 0.2614, 0.2143],\n",
      "        [0.2385, 0.2859, 0.2614, 0.2142],\n",
      "        [0.2385, 0.2859, 0.2614, 0.2142],\n",
      "        [0.2385, 0.2859, 0.2614, 0.2142],\n",
      "        [0.2386, 0.2858, 0.2613, 0.2143],\n",
      "        [0.2386, 0.2858, 0.2614, 0.2143],\n",
      "        [0.2386, 0.2858, 0.2614, 0.2143],\n",
      "        [0.2386, 0.2858, 0.2613, 0.2143],\n",
      "        [0.2386, 0.2858, 0.2614, 0.2143],\n",
      "        [0.2385, 0.2858, 0.2614, 0.2143],\n",
      "        [0.2386, 0.2858, 0.2614, 0.2143],\n",
      "        [0.2386, 0.2858, 0.2614, 0.2143],\n",
      "        [0.2386, 0.2858, 0.2614, 0.2143],\n",
      "        [0.2386, 0.2858, 0.2614, 0.2143],\n",
      "        [0.2386, 0.2858, 0.2613, 0.2143],\n",
      "        [0.2386, 0.2858, 0.2613, 0.2143],\n",
      "        [0.2386, 0.2858, 0.2614, 0.2143],\n",
      "        [0.2386, 0.2858, 0.2613, 0.2143],\n",
      "        [0.2386, 0.2858, 0.2614, 0.2143],\n",
      "        [0.2385, 0.2858, 0.2614, 0.2143],\n",
      "        [0.2386, 0.2858, 0.2614, 0.2143],\n",
      "        [0.2386, 0.2858, 0.2614, 0.2143],\n",
      "        [0.2386, 0.2858, 0.2614, 0.2143],\n",
      "        [0.2386, 0.2858, 0.2614, 0.2143],\n",
      "        [0.2386, 0.2858, 0.2614, 0.2143],\n",
      "        [0.2385, 0.2859, 0.2614, 0.2142],\n",
      "        [0.2385, 0.2859, 0.2614, 0.2142],\n",
      "        [0.2385, 0.2859, 0.2614, 0.2142],\n",
      "        [0.2385, 0.2859, 0.2614, 0.2142],\n",
      "        [0.2385, 0.2859, 0.2614, 0.2142],\n",
      "        [0.2385, 0.2859, 0.2614, 0.2142],\n",
      "        [0.2385, 0.2859, 0.2614, 0.2142],\n",
      "        [0.2385, 0.2859, 0.2614, 0.2142],\n",
      "        [0.2385, 0.2858, 0.2614, 0.2143],\n",
      "        [0.2385, 0.2859, 0.2613, 0.2142],\n",
      "        [0.2385, 0.2858, 0.2614, 0.2143],\n",
      "        [0.2385, 0.2858, 0.2614, 0.2143],\n",
      "        [0.2386, 0.2858, 0.2614, 0.2143],\n",
      "        [0.2386, 0.2858, 0.2614, 0.2143],\n",
      "        [0.2386, 0.2858, 0.2614, 0.2143],\n",
      "        [0.2386, 0.2858, 0.2614, 0.2143],\n",
      "        [0.2385, 0.2858, 0.2614, 0.2142],\n",
      "        [0.2386, 0.2858, 0.2614, 0.2142],\n",
      "        [0.2385, 0.2859, 0.2614, 0.2142],\n",
      "        [0.2386, 0.2859, 0.2614, 0.2142],\n",
      "        [0.2386, 0.2858, 0.2614, 0.2143],\n",
      "        [0.2386, 0.2858, 0.2614, 0.2143],\n",
      "        [0.2386, 0.2858, 0.2614, 0.2143],\n",
      "        [0.2386, 0.2858, 0.2614, 0.2143],\n",
      "        [0.2386, 0.2858, 0.2614, 0.2143],\n",
      "        [0.2386, 0.2858, 0.2614, 0.2143],\n",
      "        [0.2386, 0.2858, 0.2614, 0.2143],\n",
      "        [0.2385, 0.2858, 0.2614, 0.2143],\n",
      "        [0.2385, 0.2858, 0.2614, 0.2142],\n",
      "        [0.2385, 0.2858, 0.2614, 0.2143],\n",
      "        [0.2385, 0.2859, 0.2614, 0.2142],\n",
      "        [0.2386, 0.2858, 0.2614, 0.2143]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3806, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(-0.0183, grad_fn=<AddBackward0>)\n",
      "batching here\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(0.0457, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.4353, -1.2537, -1.3409, -1.5376],\n",
      "        [-1.4353, -1.2537, -1.3409, -1.5375],\n",
      "        [-1.4353, -1.2537, -1.3409, -1.5376],\n",
      "        [-1.4353, -1.2537, -1.3409, -1.5376],\n",
      "        [-1.4351, -1.2540, -1.3410, -1.5373],\n",
      "        [-1.4352, -1.2539, -1.3409, -1.5374],\n",
      "        [-1.4352, -1.2539, -1.3410, -1.5373],\n",
      "        [-1.4352, -1.2538, -1.3409, -1.5375],\n",
      "        [-1.4352, -1.2537, -1.3410, -1.5375],\n",
      "        [-1.4353, -1.2537, -1.3409, -1.5377],\n",
      "        [-1.4353, -1.2537, -1.3410, -1.5375],\n",
      "        [-1.4353, -1.2537, -1.3410, -1.5375],\n",
      "        [-1.4352, -1.2540, -1.3409, -1.5374],\n",
      "        [-1.4351, -1.2540, -1.3409, -1.5374],\n",
      "        [-1.4351, -1.2539, -1.3410, -1.5374],\n",
      "        [-1.4352, -1.2539, -1.3410, -1.5374],\n",
      "        [-1.4353, -1.2538, -1.3409, -1.5374],\n",
      "        [-1.4352, -1.2539, -1.3409, -1.5374],\n",
      "        [-1.4352, -1.2539, -1.3410, -1.5374],\n",
      "        [-1.4352, -1.2540, -1.3409, -1.5373],\n",
      "        [-1.4352, -1.2539, -1.3409, -1.5375],\n",
      "        [-1.4351, -1.2540, -1.3410, -1.5373],\n",
      "        [-1.4352, -1.2539, -1.3409, -1.5374],\n",
      "        [-1.4352, -1.2540, -1.3409, -1.5374],\n",
      "        [-1.4351, -1.2540, -1.3410, -1.5372],\n",
      "        [-1.4352, -1.2539, -1.3410, -1.5374],\n",
      "        [-1.4352, -1.2540, -1.3409, -1.5374],\n",
      "        [-1.4352, -1.2539, -1.3409, -1.5375],\n",
      "        [-1.4352, -1.2539, -1.3409, -1.5374],\n",
      "        [-1.4352, -1.2540, -1.3409, -1.5373],\n",
      "        [-1.4352, -1.2539, -1.3409, -1.5375],\n",
      "        [-1.4352, -1.2540, -1.3409, -1.5374],\n",
      "        [-1.4354, -1.2537, -1.3407, -1.5377],\n",
      "        [-1.4354, -1.2537, -1.3408, -1.5377],\n",
      "        [-1.4354, -1.2536, -1.3408, -1.5378],\n",
      "        [-1.4353, -1.2537, -1.3408, -1.5377],\n",
      "        [-1.4353, -1.2538, -1.3409, -1.5375],\n",
      "        [-1.4353, -1.2536, -1.3409, -1.5377],\n",
      "        [-1.4353, -1.2537, -1.3409, -1.5376],\n",
      "        [-1.4353, -1.2537, -1.3409, -1.5376],\n",
      "        [-1.4352, -1.2538, -1.3410, -1.5374],\n",
      "        [-1.4353, -1.2537, -1.3410, -1.5374],\n",
      "        [-1.4352, -1.2538, -1.3410, -1.5375],\n",
      "        [-1.4352, -1.2538, -1.3410, -1.5374],\n",
      "        [-1.4352, -1.2540, -1.3409, -1.5374],\n",
      "        [-1.4351, -1.2540, -1.3409, -1.5374],\n",
      "        [-1.4352, -1.2540, -1.3409, -1.5374],\n",
      "        [-1.4351, -1.2540, -1.3409, -1.5373],\n",
      "        [-1.4352, -1.2539, -1.3408, -1.5376],\n",
      "        [-1.4353, -1.2539, -1.3408, -1.5375],\n",
      "        [-1.4352, -1.2539, -1.3408, -1.5376],\n",
      "        [-1.4352, -1.2539, -1.3408, -1.5375],\n",
      "        [-1.4352, -1.2540, -1.3409, -1.5374],\n",
      "        [-1.4351, -1.2540, -1.3409, -1.5373],\n",
      "        [-1.4352, -1.2539, -1.3409, -1.5374],\n",
      "        [-1.4352, -1.2539, -1.3409, -1.5374],\n",
      "        [-1.4352, -1.2540, -1.3409, -1.5373],\n",
      "        [-1.4351, -1.2540, -1.3410, -1.5373],\n",
      "        [-1.4351, -1.2540, -1.3410, -1.5374],\n",
      "        [-1.4352, -1.2538, -1.3409, -1.5376],\n",
      "        [-1.4352, -1.2539, -1.3409, -1.5375],\n",
      "        [-1.4352, -1.2539, -1.3409, -1.5374],\n",
      "        [-1.4353, -1.2537, -1.3409, -1.5376],\n",
      "        [-1.4352, -1.2538, -1.3409, -1.5375]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([ 4.0172e-03,  3.1813e-03,  2.1469e-03,  1.1228e-03,  4.0968e-03,\n",
      "         3.0474e-03,  2.0944e-03,  9.6493e-04,  4.4559e-03,  3.3592e-03,\n",
      "         1.8569e-03,  9.5971e-04,  4.2681e-03,  3.3655e-03,  1.8112e-03,\n",
      "         9.4977e-04,  4.3595e-03,  2.8392e-03,  2.2321e-03,  1.0585e-03,\n",
      "         4.1163e-03,  2.9886e-03,  1.8860e-03,  9.2303e-04,  4.3779e-03,\n",
      "         3.4699e-03,  1.8628e-03,  1.1238e-03, -1.5021e+00, -1.2387e+00,\n",
      "        -1.2520e+00,  8.4506e-04,  4.5872e-03,  2.7678e-03,  1.8933e-03,\n",
      "         8.6972e-04,  3.6946e-03,  3.1030e-03,  2.1313e-03,  1.1246e-03,\n",
      "         3.7900e-03,  3.4032e-03,  1.9949e-03,  1.0351e-03,  4.2658e-03,\n",
      "         3.1821e-03,  2.0544e-03,  1.0087e-03,  3.7533e-03,  3.0314e-03,\n",
      "         1.8518e-03,  8.9268e-04,  3.9094e-03,  3.1661e-03,  2.0452e-03,\n",
      "         1.0933e-03,  3.8748e-03,  2.9503e-03,  1.8929e-03,  1.1018e-03,\n",
      "         4.3770e-03,  3.0424e-03,  2.4491e-03,  1.0991e-03],\n",
      "       grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(0.0600, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2380, 0.2854, 0.2616, 0.2149],\n",
      "        [0.2381, 0.2854, 0.2616, 0.2149],\n",
      "        [0.2380, 0.2855, 0.2616, 0.2149],\n",
      "        [0.2380, 0.2854, 0.2616, 0.2149],\n",
      "        [0.2381, 0.2854, 0.2616, 0.2150],\n",
      "        [0.2381, 0.2854, 0.2616, 0.2149],\n",
      "        [0.2381, 0.2854, 0.2616, 0.2150],\n",
      "        [0.2381, 0.2854, 0.2616, 0.2149],\n",
      "        [0.2381, 0.2854, 0.2616, 0.2149],\n",
      "        [0.2381, 0.2855, 0.2616, 0.2149],\n",
      "        [0.2381, 0.2854, 0.2616, 0.2149],\n",
      "        [0.2381, 0.2854, 0.2616, 0.2149],\n",
      "        [0.2381, 0.2854, 0.2616, 0.2149],\n",
      "        [0.2381, 0.2854, 0.2616, 0.2149],\n",
      "        [0.2381, 0.2854, 0.2616, 0.2149],\n",
      "        [0.2381, 0.2854, 0.2616, 0.2149],\n",
      "        [0.2380, 0.2854, 0.2616, 0.2149],\n",
      "        [0.2381, 0.2854, 0.2616, 0.2149],\n",
      "        [0.2381, 0.2854, 0.2616, 0.2149],\n",
      "        [0.2381, 0.2854, 0.2616, 0.2150],\n",
      "        [0.2381, 0.2854, 0.2616, 0.2149],\n",
      "        [0.2381, 0.2854, 0.2616, 0.2150],\n",
      "        [0.2381, 0.2854, 0.2616, 0.2149],\n",
      "        [0.2381, 0.2854, 0.2616, 0.2149],\n",
      "        [0.2381, 0.2853, 0.2616, 0.2150],\n",
      "        [0.2381, 0.2854, 0.2616, 0.2149],\n",
      "        [0.2381, 0.2854, 0.2616, 0.2149],\n",
      "        [0.2381, 0.2854, 0.2616, 0.2149],\n",
      "        [0.2381, 0.2854, 0.2616, 0.2149],\n",
      "        [0.2381, 0.2854, 0.2616, 0.2150],\n",
      "        [0.2381, 0.2854, 0.2616, 0.2149],\n",
      "        [0.2381, 0.2854, 0.2616, 0.2149],\n",
      "        [0.2380, 0.2855, 0.2617, 0.2149],\n",
      "        [0.2380, 0.2855, 0.2616, 0.2149],\n",
      "        [0.2380, 0.2855, 0.2616, 0.2149],\n",
      "        [0.2380, 0.2854, 0.2616, 0.2149],\n",
      "        [0.2381, 0.2854, 0.2616, 0.2149],\n",
      "        [0.2380, 0.2855, 0.2616, 0.2149],\n",
      "        [0.2380, 0.2854, 0.2616, 0.2149],\n",
      "        [0.2380, 0.2854, 0.2616, 0.2149],\n",
      "        [0.2381, 0.2854, 0.2616, 0.2149],\n",
      "        [0.2380, 0.2854, 0.2616, 0.2149],\n",
      "        [0.2381, 0.2854, 0.2616, 0.2149],\n",
      "        [0.2381, 0.2854, 0.2616, 0.2149],\n",
      "        [0.2381, 0.2854, 0.2616, 0.2149],\n",
      "        [0.2381, 0.2854, 0.2616, 0.2149],\n",
      "        [0.2381, 0.2854, 0.2616, 0.2149],\n",
      "        [0.2381, 0.2854, 0.2616, 0.2150],\n",
      "        [0.2381, 0.2854, 0.2616, 0.2149],\n",
      "        [0.2380, 0.2854, 0.2616, 0.2149],\n",
      "        [0.2381, 0.2854, 0.2616, 0.2149],\n",
      "        [0.2381, 0.2854, 0.2616, 0.2149],\n",
      "        [0.2381, 0.2854, 0.2616, 0.2149],\n",
      "        [0.2381, 0.2854, 0.2616, 0.2150],\n",
      "        [0.2381, 0.2854, 0.2616, 0.2149],\n",
      "        [0.2381, 0.2854, 0.2616, 0.2149],\n",
      "        [0.2381, 0.2854, 0.2616, 0.2150],\n",
      "        [0.2381, 0.2854, 0.2616, 0.2150],\n",
      "        [0.2381, 0.2854, 0.2616, 0.2149],\n",
      "        [0.2381, 0.2854, 0.2616, 0.2149],\n",
      "        [0.2381, 0.2854, 0.2616, 0.2149],\n",
      "        [0.2381, 0.2854, 0.2616, 0.2149],\n",
      "        [0.2380, 0.2855, 0.2616, 0.2149],\n",
      "        [0.2381, 0.2854, 0.2616, 0.2149]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3808, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(0.0919, grad_fn=<AddBackward0>)\n",
      "batching here\n",
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(1.6713e-05, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.4708, -1.1976, -1.3730, -1.5371],\n",
      "        [-1.4708, -1.1976, -1.3730, -1.5371],\n",
      "        [-1.4708, -1.1976, -1.3730, -1.5371],\n",
      "        [-1.4708, -1.1976, -1.3730, -1.5372],\n",
      "        [-1.4707, -1.1979, -1.3729, -1.5370],\n",
      "        [-1.4707, -1.1979, -1.3730, -1.5370],\n",
      "        [-1.4707, -1.1978, -1.3730, -1.5370],\n",
      "        [-1.4707, -1.1978, -1.3730, -1.5370],\n",
      "        [-1.4708, -1.1975, -1.3730, -1.5372],\n",
      "        [-1.4708, -1.1976, -1.3730, -1.5372],\n",
      "        [-1.4708, -1.1976, -1.3730, -1.5371],\n",
      "        [-1.4708, -1.1976, -1.3730, -1.5371],\n",
      "        [-1.4707, -1.1978, -1.3730, -1.5369],\n",
      "        [-1.4707, -1.1979, -1.3730, -1.5369],\n",
      "        [-1.4706, -1.1979, -1.3731, -1.5369],\n",
      "        [-1.4707, -1.1979, -1.3730, -1.5369],\n",
      "        [-1.4707, -1.1979, -1.3729, -1.5370],\n",
      "        [-1.4707, -1.1979, -1.3729, -1.5370],\n",
      "        [-1.4706, -1.1979, -1.3730, -1.5369],\n",
      "        [-1.4707, -1.1978, -1.3730, -1.5370],\n",
      "        [-1.4707, -1.1978, -1.3730, -1.5370],\n",
      "        [-1.4707, -1.1978, -1.3730, -1.5370],\n",
      "        [-1.4707, -1.1978, -1.3730, -1.5370],\n",
      "        [-1.4707, -1.1978, -1.3730, -1.5370],\n",
      "        [-1.4707, -1.1979, -1.3729, -1.5369],\n",
      "        [-1.4707, -1.1978, -1.3730, -1.5370],\n",
      "        [-1.4707, -1.1978, -1.3729, -1.5370],\n",
      "        [-1.4707, -1.1978, -1.3729, -1.5370],\n",
      "        [-1.4707, -1.1979, -1.3729, -1.5370],\n",
      "        [-1.4707, -1.1978, -1.3729, -1.5370],\n",
      "        [-1.4707, -1.1979, -1.3729, -1.5369],\n",
      "        [-1.4707, -1.1979, -1.3729, -1.5369],\n",
      "        [-1.4708, -1.1975, -1.3731, -1.5373],\n",
      "        [-1.4709, -1.1973, -1.3731, -1.5374],\n",
      "        [-1.4709, -1.1974, -1.3730, -1.5373],\n",
      "        [-1.4709, -1.1973, -1.3730, -1.5374],\n",
      "        [-1.4708, -1.1976, -1.3730, -1.5371],\n",
      "        [-1.4708, -1.1976, -1.3730, -1.5371],\n",
      "        [-1.4708, -1.1976, -1.3730, -1.5371],\n",
      "        [-1.4708, -1.1976, -1.3730, -1.5371],\n",
      "        [-1.4708, -1.1977, -1.3730, -1.5370],\n",
      "        [-1.4708, -1.1976, -1.3730, -1.5371],\n",
      "        [-1.4708, -1.1976, -1.3730, -1.5372],\n",
      "        [-1.4708, -1.1977, -1.3730, -1.5371],\n",
      "        [-1.4707, -1.1978, -1.3729, -1.5371],\n",
      "        [-1.4707, -1.1979, -1.3729, -1.5370],\n",
      "        [-1.4707, -1.1978, -1.3730, -1.5370],\n",
      "        [-1.4706, -1.1979, -1.3730, -1.5370],\n",
      "        [-1.4706, -1.1978, -1.3730, -1.5370],\n",
      "        [-1.4707, -1.1976, -1.3730, -1.5372],\n",
      "        [-1.4708, -1.1977, -1.3730, -1.5371],\n",
      "        [-1.4706, -1.1978, -1.3730, -1.5370],\n",
      "        [-1.4707, -1.1978, -1.3729, -1.5371],\n",
      "        [-1.4707, -1.1978, -1.3730, -1.5370],\n",
      "        [-1.4707, -1.1978, -1.3730, -1.5370],\n",
      "        [-1.4707, -1.1978, -1.3730, -1.5370],\n",
      "        [-1.4707, -1.1978, -1.3730, -1.5370],\n",
      "        [-1.4706, -1.1979, -1.3730, -1.5369],\n",
      "        [-1.4706, -1.1979, -1.3730, -1.5370],\n",
      "        [-1.4706, -1.1978, -1.3730, -1.5370],\n",
      "        [-1.4707, -1.1977, -1.3730, -1.5371],\n",
      "        [-1.4708, -1.1976, -1.3730, -1.5371],\n",
      "        [-1.4709, -1.1975, -1.3731, -1.5371],\n",
      "        [-1.4708, -1.1976, -1.3730, -1.5371]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([0.0071, 0.0053, 0.0041, 0.0018, 0.0087, 0.0061, 0.0036, 0.0021, 0.0090,\n",
      "        0.0067, 0.0039, 0.0017, 0.0071, 0.0068, 0.0039, 0.0019, 0.0082, 0.0062,\n",
      "        0.0042, 0.0024, 0.0087, 0.0061, 0.0036, 0.0023, 0.0070, 0.0069, 0.0041,\n",
      "        0.0017, 0.0082, 0.0054, 0.0045, 0.0019, 0.0073, 0.0073, 0.0046, 0.0028,\n",
      "        0.0091, 0.0061, 0.0036, 0.0021, 0.0091, 0.0062, 0.0047, 0.0020, 0.0093,\n",
      "        0.0069, 0.0041, 0.0023, 0.0092, 0.0070, 0.0037, 0.0020, 0.0087, 0.0065,\n",
      "        0.0044, 0.0022, 0.0070, 0.0053, 0.0040, 0.0017, 0.0092, 0.0070, 0.0047,\n",
      "        0.0019], grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(-0.0052, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2297, 0.3019, 0.2533, 0.2150],\n",
      "        [0.2297, 0.3019, 0.2533, 0.2150],\n",
      "        [0.2297, 0.3019, 0.2533, 0.2150],\n",
      "        [0.2297, 0.3019, 0.2533, 0.2150],\n",
      "        [0.2298, 0.3018, 0.2534, 0.2150],\n",
      "        [0.2298, 0.3018, 0.2534, 0.2150],\n",
      "        [0.2298, 0.3019, 0.2534, 0.2150],\n",
      "        [0.2298, 0.3019, 0.2534, 0.2150],\n",
      "        [0.2297, 0.3019, 0.2533, 0.2150],\n",
      "        [0.2297, 0.3019, 0.2533, 0.2150],\n",
      "        [0.2297, 0.3019, 0.2533, 0.2150],\n",
      "        [0.2297, 0.3019, 0.2533, 0.2150],\n",
      "        [0.2298, 0.3018, 0.2533, 0.2150],\n",
      "        [0.2298, 0.3018, 0.2534, 0.2150],\n",
      "        [0.2298, 0.3018, 0.2533, 0.2150],\n",
      "        [0.2298, 0.3018, 0.2533, 0.2150],\n",
      "        [0.2298, 0.3018, 0.2534, 0.2150],\n",
      "        [0.2298, 0.3018, 0.2534, 0.2150],\n",
      "        [0.2298, 0.3018, 0.2534, 0.2150],\n",
      "        [0.2298, 0.3018, 0.2534, 0.2150],\n",
      "        [0.2298, 0.3018, 0.2534, 0.2150],\n",
      "        [0.2298, 0.3018, 0.2534, 0.2150],\n",
      "        [0.2298, 0.3018, 0.2534, 0.2150],\n",
      "        [0.2298, 0.3018, 0.2534, 0.2150],\n",
      "        [0.2298, 0.3018, 0.2534, 0.2150],\n",
      "        [0.2298, 0.3018, 0.2533, 0.2150],\n",
      "        [0.2298, 0.3019, 0.2534, 0.2150],\n",
      "        [0.2298, 0.3018, 0.2534, 0.2150],\n",
      "        [0.2298, 0.3018, 0.2534, 0.2150],\n",
      "        [0.2298, 0.3018, 0.2534, 0.2150],\n",
      "        [0.2298, 0.3018, 0.2534, 0.2150],\n",
      "        [0.2298, 0.3018, 0.2534, 0.2150],\n",
      "        [0.2297, 0.3020, 0.2533, 0.2150],\n",
      "        [0.2297, 0.3020, 0.2533, 0.2149],\n",
      "        [0.2297, 0.3020, 0.2533, 0.2150],\n",
      "        [0.2297, 0.3020, 0.2533, 0.2149],\n",
      "        [0.2297, 0.3019, 0.2533, 0.2150],\n",
      "        [0.2297, 0.3019, 0.2533, 0.2150],\n",
      "        [0.2297, 0.3019, 0.2533, 0.2150],\n",
      "        [0.2297, 0.3019, 0.2533, 0.2150],\n",
      "        [0.2297, 0.3019, 0.2533, 0.2150],\n",
      "        [0.2297, 0.3019, 0.2534, 0.2150],\n",
      "        [0.2297, 0.3019, 0.2534, 0.2150],\n",
      "        [0.2297, 0.3019, 0.2533, 0.2150],\n",
      "        [0.2298, 0.3019, 0.2534, 0.2150],\n",
      "        [0.2298, 0.3018, 0.2534, 0.2150],\n",
      "        [0.2298, 0.3018, 0.2534, 0.2150],\n",
      "        [0.2298, 0.3018, 0.2534, 0.2150],\n",
      "        [0.2298, 0.3019, 0.2533, 0.2150],\n",
      "        [0.2298, 0.3019, 0.2533, 0.2150],\n",
      "        [0.2297, 0.3019, 0.2533, 0.2150],\n",
      "        [0.2298, 0.3018, 0.2533, 0.2150],\n",
      "        [0.2298, 0.3019, 0.2534, 0.2150],\n",
      "        [0.2298, 0.3019, 0.2534, 0.2150],\n",
      "        [0.2298, 0.3019, 0.2533, 0.2150],\n",
      "        [0.2298, 0.3019, 0.2534, 0.2150],\n",
      "        [0.2298, 0.3019, 0.2533, 0.2150],\n",
      "        [0.2298, 0.3018, 0.2534, 0.2150],\n",
      "        [0.2298, 0.3018, 0.2533, 0.2150],\n",
      "        [0.2298, 0.3018, 0.2533, 0.2150],\n",
      "        [0.2298, 0.3019, 0.2533, 0.2150],\n",
      "        [0.2297, 0.3019, 0.2533, 0.2150],\n",
      "        [0.2297, 0.3020, 0.2533, 0.2150],\n",
      "        [0.2297, 0.3019, 0.2533, 0.2150]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3778, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(-0.0190, grad_fn=<AddBackward0>)\n",
      "batching here\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40: done 7 episodes, mean_reward=0.14, best_reward=2.00, speed=210.33\n",
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(0.0335, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.4676, -1.2014, -1.3731, -1.5351],\n",
      "        [-1.4674, -1.2017, -1.3731, -1.5350],\n",
      "        [-1.4674, -1.2017, -1.3731, -1.5350],\n",
      "        [-1.4675, -1.2016, -1.3731, -1.5350],\n",
      "        [-1.4675, -1.2017, -1.3730, -1.5350],\n",
      "        [-1.4675, -1.2016, -1.3730, -1.5350],\n",
      "        [-1.4674, -1.2017, -1.3731, -1.5349],\n",
      "        [-1.4675, -1.2016, -1.3730, -1.5350],\n",
      "        [-1.4677, -1.2013, -1.3730, -1.5352],\n",
      "        [-1.4674, -1.2018, -1.3731, -1.5348],\n",
      "        [-1.4675, -1.2017, -1.3731, -1.5349],\n",
      "        [-1.4674, -1.2017, -1.3731, -1.5349],\n",
      "        [-1.4675, -1.2016, -1.3731, -1.5350],\n",
      "        [-1.4675, -1.2017, -1.3731, -1.5349],\n",
      "        [-1.4675, -1.2016, -1.3731, -1.5350],\n",
      "        [-1.4674, -1.2017, -1.3731, -1.5349],\n",
      "        [-1.4675, -1.2017, -1.3730, -1.5349],\n",
      "        [-1.4675, -1.2016, -1.3731, -1.5350],\n",
      "        [-1.4675, -1.2017, -1.3731, -1.5349],\n",
      "        [-1.4675, -1.2016, -1.3731, -1.5350],\n",
      "        [-1.4675, -1.2017, -1.3731, -1.5350],\n",
      "        [-1.4675, -1.2017, -1.3730, -1.5349],\n",
      "        [-1.4675, -1.2016, -1.3731, -1.5349],\n",
      "        [-1.4674, -1.2017, -1.3730, -1.5350],\n",
      "        [-1.4675, -1.2017, -1.3730, -1.5349],\n",
      "        [-1.4675, -1.2016, -1.3730, -1.5350],\n",
      "        [-1.4675, -1.2016, -1.3730, -1.5350],\n",
      "        [-1.4675, -1.2017, -1.3731, -1.5349],\n",
      "        [-1.4675, -1.2016, -1.3730, -1.5350],\n",
      "        [-1.4675, -1.2018, -1.3729, -1.5349],\n",
      "        [-1.4674, -1.2019, -1.3730, -1.5348],\n",
      "        [-1.4675, -1.2016, -1.3730, -1.5350],\n",
      "        [-1.4676, -1.2013, -1.3731, -1.5353],\n",
      "        [-1.4676, -1.2012, -1.3731, -1.5353],\n",
      "        [-1.4676, -1.2012, -1.3731, -1.5353],\n",
      "        [-1.4676, -1.2013, -1.3731, -1.5353],\n",
      "        [-1.4676, -1.2014, -1.3731, -1.5351],\n",
      "        [-1.4676, -1.2014, -1.3731, -1.5351],\n",
      "        [-1.4676, -1.2014, -1.3731, -1.5351],\n",
      "        [-1.4674, -1.2017, -1.3731, -1.5350],\n",
      "        [-1.4676, -1.2014, -1.3731, -1.5351],\n",
      "        [-1.4676, -1.2015, -1.3731, -1.5350],\n",
      "        [-1.4676, -1.2014, -1.3731, -1.5351],\n",
      "        [-1.4676, -1.2014, -1.3731, -1.5351],\n",
      "        [-1.4675, -1.2017, -1.3731, -1.5350],\n",
      "        [-1.4675, -1.2017, -1.3731, -1.5350],\n",
      "        [-1.4674, -1.2017, -1.3731, -1.5349],\n",
      "        [-1.4675, -1.2017, -1.3731, -1.5349],\n",
      "        [-1.4675, -1.2015, -1.3731, -1.5351],\n",
      "        [-1.4676, -1.2013, -1.3731, -1.5352],\n",
      "        [-1.4675, -1.2015, -1.3731, -1.5352],\n",
      "        [-1.4675, -1.2014, -1.3731, -1.5352],\n",
      "        [-1.4675, -1.2016, -1.3730, -1.5350],\n",
      "        [-1.4675, -1.2017, -1.3731, -1.5349],\n",
      "        [-1.4675, -1.2016, -1.3731, -1.5350],\n",
      "        [-1.4674, -1.2017, -1.3731, -1.5349],\n",
      "        [-1.4675, -1.2016, -1.3731, -1.5349],\n",
      "        [-1.4675, -1.2017, -1.3730, -1.5349],\n",
      "        [-1.4675, -1.2016, -1.3731, -1.5349],\n",
      "        [-1.4675, -1.2016, -1.3731, -1.5350],\n",
      "        [-1.4675, -1.2015, -1.3731, -1.5351],\n",
      "        [-1.4676, -1.2015, -1.3731, -1.5350],\n",
      "        [-1.4676, -1.2014, -1.3731, -1.5351],\n",
      "        [-1.4676, -1.2014, -1.3731, -1.5352]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([ 0.2176,  0.0056,  0.0042,  0.0021,  0.0078,  0.0052,  0.0034,  0.0024,\n",
      "         0.1947,  0.0064,  0.0035,  0.0022,  0.2177,  0.0065,  0.0035,  0.0022,\n",
      "         0.0081,  0.0062,  0.0043,  0.0021,  0.2080,  0.0049,  0.0038,  0.0018,\n",
      "         0.0078,  0.0063,  0.0042,  0.0016,  0.0078,  0.0060,  0.0034,  0.0021,\n",
      "         0.0084,  0.0057,  0.0041,  0.0020,  0.1703,  0.2080,  0.1946,  0.0018,\n",
      "         0.0081,  0.0050,  0.0041,  0.0019,  0.0067,  0.0066,  0.0045,  0.0020,\n",
      "        -1.1830, -1.1964,  0.0043,  0.0021,  0.1948,  0.0051,  0.0035,  0.0020,\n",
      "         0.1947,  0.0057,  0.0033,  0.0020,  0.0083,  0.0049,  0.0041,  0.0021],\n",
      "       grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(0.0054, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2305, 0.3008, 0.2533, 0.2154],\n",
      "        [0.2305, 0.3007, 0.2533, 0.2155],\n",
      "        [0.2305, 0.3007, 0.2533, 0.2155],\n",
      "        [0.2305, 0.3007, 0.2533, 0.2154],\n",
      "        [0.2305, 0.3007, 0.2533, 0.2155],\n",
      "        [0.2305, 0.3007, 0.2533, 0.2155],\n",
      "        [0.2305, 0.3007, 0.2533, 0.2155],\n",
      "        [0.2305, 0.3007, 0.2533, 0.2155],\n",
      "        [0.2305, 0.3008, 0.2533, 0.2154],\n",
      "        [0.2305, 0.3007, 0.2533, 0.2155],\n",
      "        [0.2305, 0.3007, 0.2533, 0.2155],\n",
      "        [0.2305, 0.3007, 0.2533, 0.2155],\n",
      "        [0.2305, 0.3007, 0.2533, 0.2155],\n",
      "        [0.2305, 0.3007, 0.2533, 0.2155],\n",
      "        [0.2305, 0.3007, 0.2533, 0.2155],\n",
      "        [0.2305, 0.3007, 0.2533, 0.2155],\n",
      "        [0.2305, 0.3007, 0.2533, 0.2155],\n",
      "        [0.2305, 0.3007, 0.2533, 0.2155],\n",
      "        [0.2305, 0.3007, 0.2533, 0.2155],\n",
      "        [0.2305, 0.3007, 0.2533, 0.2155],\n",
      "        [0.2305, 0.3007, 0.2533, 0.2155],\n",
      "        [0.2305, 0.3007, 0.2533, 0.2155],\n",
      "        [0.2305, 0.3007, 0.2533, 0.2155],\n",
      "        [0.2305, 0.3007, 0.2533, 0.2155],\n",
      "        [0.2305, 0.3007, 0.2533, 0.2155],\n",
      "        [0.2305, 0.3007, 0.2533, 0.2155],\n",
      "        [0.2305, 0.3007, 0.2533, 0.2155],\n",
      "        [0.2305, 0.3007, 0.2533, 0.2155],\n",
      "        [0.2305, 0.3007, 0.2533, 0.2155],\n",
      "        [0.2305, 0.3006, 0.2534, 0.2155],\n",
      "        [0.2305, 0.3006, 0.2533, 0.2155],\n",
      "        [0.2305, 0.3007, 0.2533, 0.2154],\n",
      "        [0.2305, 0.3008, 0.2533, 0.2154],\n",
      "        [0.2305, 0.3008, 0.2533, 0.2154],\n",
      "        [0.2305, 0.3008, 0.2533, 0.2154],\n",
      "        [0.2305, 0.3008, 0.2533, 0.2154],\n",
      "        [0.2305, 0.3008, 0.2533, 0.2154],\n",
      "        [0.2305, 0.3008, 0.2533, 0.2154],\n",
      "        [0.2305, 0.3008, 0.2533, 0.2154],\n",
      "        [0.2305, 0.3007, 0.2533, 0.2155],\n",
      "        [0.2305, 0.3008, 0.2533, 0.2154],\n",
      "        [0.2305, 0.3008, 0.2533, 0.2154],\n",
      "        [0.2305, 0.3008, 0.2533, 0.2154],\n",
      "        [0.2305, 0.3008, 0.2533, 0.2154],\n",
      "        [0.2305, 0.3007, 0.2533, 0.2155],\n",
      "        [0.2305, 0.3007, 0.2533, 0.2155],\n",
      "        [0.2305, 0.3007, 0.2533, 0.2155],\n",
      "        [0.2305, 0.3007, 0.2533, 0.2155],\n",
      "        [0.2305, 0.3007, 0.2533, 0.2154],\n",
      "        [0.2305, 0.3008, 0.2533, 0.2154],\n",
      "        [0.2305, 0.3008, 0.2533, 0.2154],\n",
      "        [0.2305, 0.3008, 0.2533, 0.2154],\n",
      "        [0.2305, 0.3007, 0.2533, 0.2155],\n",
      "        [0.2305, 0.3007, 0.2533, 0.2155],\n",
      "        [0.2305, 0.3007, 0.2533, 0.2155],\n",
      "        [0.2305, 0.3007, 0.2533, 0.2155],\n",
      "        [0.2305, 0.3007, 0.2533, 0.2155],\n",
      "        [0.2305, 0.3007, 0.2533, 0.2155],\n",
      "        [0.2305, 0.3007, 0.2533, 0.2155],\n",
      "        [0.2305, 0.3007, 0.2533, 0.2155],\n",
      "        [0.2305, 0.3008, 0.2533, 0.2154],\n",
      "        [0.2305, 0.3007, 0.2533, 0.2155],\n",
      "        [0.2305, 0.3008, 0.2533, 0.2154],\n",
      "        [0.2305, 0.3008, 0.2533, 0.2154]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3782, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(0.0251, grad_fn=<AddBackward0>)\n",
      "batching here\n",
      "41: done 4 episodes, mean_reward=0.00, best_reward=2.00, speed=213.91\n",
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(0.0040, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.5242, -1.0693, -1.4180, -1.6257],\n",
      "        [-1.5240, -1.0695, -1.4180, -1.6256],\n",
      "        [-1.5241, -1.0694, -1.4181, -1.6257],\n",
      "        [-1.5241, -1.0694, -1.4180, -1.6257],\n",
      "        [-1.5241, -1.0695, -1.4180, -1.6256],\n",
      "        [-1.5241, -1.0694, -1.4180, -1.6256],\n",
      "        [-1.5242, -1.0691, -1.4181, -1.6259],\n",
      "        [-1.5240, -1.0695, -1.4180, -1.6256],\n",
      "        [-1.5241, -1.0694, -1.4180, -1.6257],\n",
      "        [-1.5241, -1.0695, -1.4180, -1.6256],\n",
      "        [-1.5241, -1.0693, -1.4181, -1.6257],\n",
      "        [-1.5240, -1.0695, -1.4180, -1.6256],\n",
      "        [-1.5242, -1.0693, -1.4180, -1.6257],\n",
      "        [-1.5241, -1.0694, -1.4180, -1.6256],\n",
      "        [-1.5242, -1.0692, -1.4181, -1.6258],\n",
      "        [-1.5241, -1.0693, -1.4180, -1.6257],\n",
      "        [-1.5241, -1.0694, -1.4181, -1.6257],\n",
      "        [-1.5241, -1.0693, -1.4181, -1.6257],\n",
      "        [-1.5242, -1.0694, -1.4180, -1.6257],\n",
      "        [-1.5242, -1.0693, -1.4180, -1.6258],\n",
      "        [-1.5241, -1.0693, -1.4180, -1.6257],\n",
      "        [-1.5241, -1.0693, -1.4181, -1.6257],\n",
      "        [-1.5241, -1.0694, -1.4181, -1.6257],\n",
      "        [-1.5242, -1.0693, -1.4180, -1.6257],\n",
      "        [-1.5241, -1.0694, -1.4181, -1.6257],\n",
      "        [-1.5241, -1.0694, -1.4181, -1.6257],\n",
      "        [-1.5241, -1.0694, -1.4180, -1.6257],\n",
      "        [-1.5242, -1.0694, -1.4180, -1.6257],\n",
      "        [-1.5241, -1.0696, -1.4179, -1.6255],\n",
      "        [-1.5241, -1.0695, -1.4179, -1.6257],\n",
      "        [-1.5241, -1.0695, -1.4179, -1.6256],\n",
      "        [-1.5241, -1.0695, -1.4179, -1.6256],\n",
      "        [-1.5242, -1.0693, -1.4179, -1.6258],\n",
      "        [-1.5243, -1.0692, -1.4180, -1.6258],\n",
      "        [-1.5243, -1.0693, -1.4179, -1.6258],\n",
      "        [-1.5243, -1.0692, -1.4180, -1.6259],\n",
      "        [-1.5241, -1.0693, -1.4181, -1.6257],\n",
      "        [-1.5241, -1.0693, -1.4181, -1.6258],\n",
      "        [-1.5242, -1.0694, -1.4180, -1.6257],\n",
      "        [-1.5241, -1.0694, -1.4181, -1.6256],\n",
      "        [-1.5242, -1.0693, -1.4181, -1.6257],\n",
      "        [-1.5242, -1.0693, -1.4181, -1.6257],\n",
      "        [-1.5242, -1.0693, -1.4181, -1.6257],\n",
      "        [-1.5242, -1.0694, -1.4180, -1.6256],\n",
      "        [-1.5241, -1.0694, -1.4181, -1.6256],\n",
      "        [-1.5241, -1.0693, -1.4181, -1.6257],\n",
      "        [-1.5241, -1.0694, -1.4180, -1.6257],\n",
      "        [-1.5242, -1.0693, -1.4180, -1.6258],\n",
      "        [-1.5242, -1.0693, -1.4180, -1.6258],\n",
      "        [-1.5242, -1.0694, -1.4179, -1.6257],\n",
      "        [-1.5242, -1.0694, -1.4179, -1.6257],\n",
      "        [-1.5241, -1.0695, -1.4179, -1.6256],\n",
      "        [-1.5242, -1.0693, -1.4180, -1.6258],\n",
      "        [-1.5241, -1.0694, -1.4180, -1.6257],\n",
      "        [-1.5242, -1.0692, -1.4181, -1.6258],\n",
      "        [-1.5241, -1.0694, -1.4180, -1.6257],\n",
      "        [-1.5241, -1.0693, -1.4181, -1.6258],\n",
      "        [-1.5241, -1.0693, -1.4181, -1.6257],\n",
      "        [-1.5241, -1.0694, -1.4181, -1.6257],\n",
      "        [-1.5241, -1.0694, -1.4180, -1.6257],\n",
      "        [-1.5241, -1.0694, -1.4180, -1.6256],\n",
      "        [-1.5242, -1.0693, -1.4180, -1.6257],\n",
      "        [-1.5242, -1.0694, -1.4180, -1.6257],\n",
      "        [-1.5242, -1.0693, -1.4180, -1.6257]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([0.0103, 0.0082, 0.0057, 0.0030, 0.0069, 0.0052, 0.0052, 0.0025, 0.0073,\n",
      "        0.0055, 0.0057, 0.0019, 0.0073, 0.0076, 0.0036, 0.0029, 0.2559, 0.2731,\n",
      "        0.0049, 0.0019, 0.0103, 0.0082, 0.0036, 0.0018, 0.2381, 0.2559, 0.0054,\n",
      "        0.0025, 0.0070, 0.0072, 0.0048, 0.0028, 0.0094, 0.0071, 0.0051, 0.0019,\n",
      "        0.0093, 0.0075, 0.0046, 0.0025, 0.2559, 0.2729, 0.1795, 0.0026, 0.2559,\n",
      "        0.2560, 0.0058, 0.0031, 0.0094, 0.0053, 0.0046, 0.0024, 0.0103, 0.0077,\n",
      "        0.0056, 0.0026, 0.0111, 0.0073, 0.0056, 0.0028, 0.0070, 0.0071, 0.0054,\n",
      "        0.0027], grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(-0.0398, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2178, 0.3432, 0.2422, 0.1968],\n",
      "        [0.2178, 0.3432, 0.2422, 0.1968],\n",
      "        [0.2178, 0.3432, 0.2422, 0.1968],\n",
      "        [0.2178, 0.3432, 0.2422, 0.1968],\n",
      "        [0.2178, 0.3432, 0.2422, 0.1968],\n",
      "        [0.2178, 0.3432, 0.2422, 0.1968],\n",
      "        [0.2178, 0.3433, 0.2422, 0.1967],\n",
      "        [0.2178, 0.3432, 0.2422, 0.1968],\n",
      "        [0.2178, 0.3432, 0.2422, 0.1968],\n",
      "        [0.2178, 0.3432, 0.2422, 0.1968],\n",
      "        [0.2178, 0.3432, 0.2422, 0.1968],\n",
      "        [0.2178, 0.3432, 0.2422, 0.1968],\n",
      "        [0.2178, 0.3432, 0.2422, 0.1968],\n",
      "        [0.2178, 0.3432, 0.2422, 0.1968],\n",
      "        [0.2178, 0.3433, 0.2422, 0.1968],\n",
      "        [0.2178, 0.3432, 0.2422, 0.1968],\n",
      "        [0.2178, 0.3432, 0.2422, 0.1968],\n",
      "        [0.2178, 0.3432, 0.2422, 0.1968],\n",
      "        [0.2178, 0.3432, 0.2422, 0.1968],\n",
      "        [0.2178, 0.3433, 0.2422, 0.1968],\n",
      "        [0.2178, 0.3432, 0.2422, 0.1968],\n",
      "        [0.2178, 0.3432, 0.2422, 0.1968],\n",
      "        [0.2178, 0.3432, 0.2422, 0.1968],\n",
      "        [0.2178, 0.3432, 0.2422, 0.1968],\n",
      "        [0.2178, 0.3432, 0.2422, 0.1968],\n",
      "        [0.2178, 0.3432, 0.2422, 0.1968],\n",
      "        [0.2178, 0.3432, 0.2422, 0.1968],\n",
      "        [0.2178, 0.3432, 0.2422, 0.1968],\n",
      "        [0.2178, 0.3432, 0.2422, 0.1968],\n",
      "        [0.2178, 0.3432, 0.2422, 0.1968],\n",
      "        [0.2178, 0.3432, 0.2422, 0.1968],\n",
      "        [0.2178, 0.3432, 0.2422, 0.1968],\n",
      "        [0.2178, 0.3432, 0.2422, 0.1968],\n",
      "        [0.2178, 0.3433, 0.2422, 0.1968],\n",
      "        [0.2178, 0.3433, 0.2422, 0.1967],\n",
      "        [0.2178, 0.3433, 0.2422, 0.1967],\n",
      "        [0.2178, 0.3432, 0.2422, 0.1968],\n",
      "        [0.2178, 0.3433, 0.2422, 0.1968],\n",
      "        [0.2178, 0.3432, 0.2422, 0.1968],\n",
      "        [0.2178, 0.3432, 0.2422, 0.1968],\n",
      "        [0.2178, 0.3432, 0.2422, 0.1968],\n",
      "        [0.2178, 0.3432, 0.2422, 0.1968],\n",
      "        [0.2178, 0.3432, 0.2422, 0.1968],\n",
      "        [0.2178, 0.3432, 0.2422, 0.1968],\n",
      "        [0.2178, 0.3432, 0.2422, 0.1968],\n",
      "        [0.2178, 0.3432, 0.2422, 0.1968],\n",
      "        [0.2178, 0.3432, 0.2422, 0.1968],\n",
      "        [0.2178, 0.3433, 0.2422, 0.1968],\n",
      "        [0.2178, 0.3433, 0.2422, 0.1967],\n",
      "        [0.2178, 0.3432, 0.2422, 0.1968],\n",
      "        [0.2178, 0.3432, 0.2422, 0.1968],\n",
      "        [0.2178, 0.3432, 0.2422, 0.1968],\n",
      "        [0.2178, 0.3432, 0.2422, 0.1968],\n",
      "        [0.2178, 0.3432, 0.2422, 0.1968],\n",
      "        [0.2178, 0.3433, 0.2422, 0.1968],\n",
      "        [0.2178, 0.3432, 0.2422, 0.1968],\n",
      "        [0.2178, 0.3432, 0.2422, 0.1968],\n",
      "        [0.2178, 0.3432, 0.2422, 0.1968],\n",
      "        [0.2178, 0.3432, 0.2422, 0.1968],\n",
      "        [0.2178, 0.3432, 0.2422, 0.1968],\n",
      "        [0.2178, 0.3432, 0.2422, 0.1968],\n",
      "        [0.2178, 0.3432, 0.2422, 0.1968],\n",
      "        [0.2178, 0.3432, 0.2422, 0.1968],\n",
      "        [0.2178, 0.3432, 0.2422, 0.1968]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3623, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(-0.0495, grad_fn=<AddBackward0>)\n",
      "batching here\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42: done 2 episodes, mean_reward=1.00, best_reward=2.00, speed=221.73\n",
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.5654, -1.0454, -1.4061, -1.6381],\n",
      "        [-1.5654, -1.0454, -1.4061, -1.6380],\n",
      "        [-1.5654, -1.0454, -1.4061, -1.6382],\n",
      "        [-1.5654, -1.0453, -1.4061, -1.6382],\n",
      "        [-1.5656, -1.0452, -1.4062, -1.6383],\n",
      "        [-1.5655, -1.0452, -1.4061, -1.6383],\n",
      "        [-1.5655, -1.0453, -1.4061, -1.6382],\n",
      "        [-1.5655, -1.0452, -1.4061, -1.6383],\n",
      "        [-1.5654, -1.0454, -1.4061, -1.6381],\n",
      "        [-1.5655, -1.0452, -1.4062, -1.6382],\n",
      "        [-1.5657, -1.0450, -1.4062, -1.6384],\n",
      "        [-1.5654, -1.0453, -1.4062, -1.6382],\n",
      "        [-1.5655, -1.0453, -1.4061, -1.6382],\n",
      "        [-1.5655, -1.0452, -1.4062, -1.6382],\n",
      "        [-1.5657, -1.0450, -1.4062, -1.6383],\n",
      "        [-1.5655, -1.0452, -1.4062, -1.6382],\n",
      "        [-1.5655, -1.0452, -1.4061, -1.6383],\n",
      "        [-1.5655, -1.0452, -1.4061, -1.6383],\n",
      "        [-1.5655, -1.0452, -1.4061, -1.6383],\n",
      "        [-1.5655, -1.0452, -1.4062, -1.6382],\n",
      "        [-1.5656, -1.0452, -1.4061, -1.6383],\n",
      "        [-1.5655, -1.0452, -1.4062, -1.6383],\n",
      "        [-1.5655, -1.0453, -1.4061, -1.6382],\n",
      "        [-1.5655, -1.0452, -1.4061, -1.6383],\n",
      "        [-1.5655, -1.0453, -1.4061, -1.6382],\n",
      "        [-1.5656, -1.0452, -1.4061, -1.6383],\n",
      "        [-1.5655, -1.0452, -1.4061, -1.6382],\n",
      "        [-1.5656, -1.0452, -1.4061, -1.6383],\n",
      "        [-1.5654, -1.0454, -1.4060, -1.6381],\n",
      "        [-1.5655, -1.0453, -1.4060, -1.6382],\n",
      "        [-1.5655, -1.0453, -1.4060, -1.6382],\n",
      "        [-1.5656, -1.0452, -1.4061, -1.6383],\n",
      "        [-1.5657, -1.0450, -1.4061, -1.6385],\n",
      "        [-1.5657, -1.0450, -1.4061, -1.6385],\n",
      "        [-1.5658, -1.0449, -1.4061, -1.6385],\n",
      "        [-1.5657, -1.0450, -1.4061, -1.6385],\n",
      "        [-1.5655, -1.0452, -1.4061, -1.6383],\n",
      "        [-1.5655, -1.0452, -1.4061, -1.6383],\n",
      "        [-1.5655, -1.0452, -1.4061, -1.6383],\n",
      "        [-1.5655, -1.0453, -1.4061, -1.6382],\n",
      "        [-1.5656, -1.0452, -1.4061, -1.6383],\n",
      "        [-1.5656, -1.0453, -1.4061, -1.6382],\n",
      "        [-1.5656, -1.0451, -1.4061, -1.6383],\n",
      "        [-1.5656, -1.0452, -1.4061, -1.6382],\n",
      "        [-1.5654, -1.0453, -1.4061, -1.6382],\n",
      "        [-1.5655, -1.0453, -1.4061, -1.6382],\n",
      "        [-1.5655, -1.0452, -1.4062, -1.6382],\n",
      "        [-1.5655, -1.0453, -1.4062, -1.6382],\n",
      "        [-1.5656, -1.0452, -1.4061, -1.6384],\n",
      "        [-1.5655, -1.0454, -1.4059, -1.6382],\n",
      "        [-1.5654, -1.0454, -1.4061, -1.6381],\n",
      "        [-1.5657, -1.0451, -1.4061, -1.6384],\n",
      "        [-1.5655, -1.0453, -1.4061, -1.6382],\n",
      "        [-1.5656, -1.0452, -1.4062, -1.6383],\n",
      "        [-1.5657, -1.0450, -1.4062, -1.6383],\n",
      "        [-1.5656, -1.0451, -1.4062, -1.6383],\n",
      "        [-1.5655, -1.0452, -1.4062, -1.6382],\n",
      "        [-1.5655, -1.0452, -1.4062, -1.6382],\n",
      "        [-1.5655, -1.0453, -1.4062, -1.6382],\n",
      "        [-1.5655, -1.0452, -1.4062, -1.6382],\n",
      "        [-1.5656, -1.0452, -1.4061, -1.6382],\n",
      "        [-1.5656, -1.0452, -1.4061, -1.6382],\n",
      "        [-1.5656, -1.0453, -1.4061, -1.6382],\n",
      "        [-1.5656, -1.0452, -1.4062, -1.6383]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([0.0061, 0.0046, 0.0029, 0.0016, 0.0061, 0.0046, 0.0028, 0.0014, 0.0054,\n",
      "        0.0048, 0.0029, 0.0014, 0.0055, 0.0041, 0.0035, 0.0012, 0.0041, 0.0032,\n",
      "        0.0021, 0.0010, 0.0054, 0.0048, 0.0034, 0.0011, 0.0040, 0.0030, 0.0029,\n",
      "        0.0010, 0.0040, 0.0047, 0.0020, 0.0014, 0.1014, 0.0039, 0.0020, 0.0009,\n",
      "        0.0040, 0.0045, 0.0032, 0.0015, 0.0063, 0.0030, 0.0031, 0.0014, 0.0061,\n",
      "        0.0050, 0.0021, 0.0016, 0.0040, 0.0046, 0.0025, 0.0013, 0.0040, 0.0030,\n",
      "        0.0028, 0.0011, 0.0055, 0.0031, 0.0034, 0.0018, 0.1014, 0.1519, 0.1013,\n",
      "        0.1519], grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(-0.0125, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2090, 0.3516, 0.2451, 0.1943],\n",
      "        [0.2090, 0.3515, 0.2451, 0.1944],\n",
      "        [0.2090, 0.3515, 0.2451, 0.1943],\n",
      "        [0.2090, 0.3516, 0.2451, 0.1943],\n",
      "        [0.2090, 0.3516, 0.2451, 0.1943],\n",
      "        [0.2090, 0.3516, 0.2451, 0.1943],\n",
      "        [0.2090, 0.3516, 0.2451, 0.1943],\n",
      "        [0.2090, 0.3516, 0.2451, 0.1943],\n",
      "        [0.2090, 0.3516, 0.2451, 0.1943],\n",
      "        [0.2090, 0.3516, 0.2451, 0.1943],\n",
      "        [0.2089, 0.3517, 0.2451, 0.1943],\n",
      "        [0.2090, 0.3516, 0.2451, 0.1943],\n",
      "        [0.2090, 0.3516, 0.2451, 0.1943],\n",
      "        [0.2090, 0.3516, 0.2451, 0.1943],\n",
      "        [0.2089, 0.3517, 0.2451, 0.1943],\n",
      "        [0.2090, 0.3516, 0.2451, 0.1943],\n",
      "        [0.2090, 0.3516, 0.2451, 0.1943],\n",
      "        [0.2090, 0.3516, 0.2451, 0.1943],\n",
      "        [0.2090, 0.3516, 0.2451, 0.1943],\n",
      "        [0.2090, 0.3516, 0.2451, 0.1943],\n",
      "        [0.2090, 0.3516, 0.2451, 0.1943],\n",
      "        [0.2090, 0.3516, 0.2451, 0.1943],\n",
      "        [0.2090, 0.3516, 0.2451, 0.1943],\n",
      "        [0.2090, 0.3516, 0.2451, 0.1943],\n",
      "        [0.2090, 0.3516, 0.2451, 0.1943],\n",
      "        [0.2090, 0.3516, 0.2451, 0.1943],\n",
      "        [0.2090, 0.3516, 0.2451, 0.1943],\n",
      "        [0.2090, 0.3516, 0.2451, 0.1943],\n",
      "        [0.2090, 0.3515, 0.2451, 0.1943],\n",
      "        [0.2090, 0.3516, 0.2451, 0.1943],\n",
      "        [0.2090, 0.3516, 0.2451, 0.1943],\n",
      "        [0.2090, 0.3516, 0.2451, 0.1943],\n",
      "        [0.2089, 0.3517, 0.2451, 0.1943],\n",
      "        [0.2089, 0.3517, 0.2451, 0.1943],\n",
      "        [0.2089, 0.3517, 0.2451, 0.1943],\n",
      "        [0.2089, 0.3517, 0.2451, 0.1943],\n",
      "        [0.2090, 0.3516, 0.2451, 0.1943],\n",
      "        [0.2090, 0.3516, 0.2451, 0.1943],\n",
      "        [0.2090, 0.3516, 0.2451, 0.1943],\n",
      "        [0.2090, 0.3516, 0.2451, 0.1943],\n",
      "        [0.2090, 0.3516, 0.2451, 0.1943],\n",
      "        [0.2090, 0.3516, 0.2451, 0.1943],\n",
      "        [0.2090, 0.3516, 0.2451, 0.1943],\n",
      "        [0.2090, 0.3516, 0.2451, 0.1943],\n",
      "        [0.2090, 0.3516, 0.2451, 0.1943],\n",
      "        [0.2090, 0.3516, 0.2451, 0.1943],\n",
      "        [0.2090, 0.3516, 0.2451, 0.1943],\n",
      "        [0.2090, 0.3516, 0.2451, 0.1943],\n",
      "        [0.2090, 0.3516, 0.2451, 0.1943],\n",
      "        [0.2090, 0.3515, 0.2451, 0.1943],\n",
      "        [0.2090, 0.3515, 0.2451, 0.1943],\n",
      "        [0.2089, 0.3517, 0.2451, 0.1943],\n",
      "        [0.2090, 0.3516, 0.2451, 0.1943],\n",
      "        [0.2090, 0.3516, 0.2451, 0.1943],\n",
      "        [0.2089, 0.3517, 0.2451, 0.1943],\n",
      "        [0.2090, 0.3516, 0.2451, 0.1943],\n",
      "        [0.2090, 0.3516, 0.2451, 0.1943],\n",
      "        [0.2090, 0.3516, 0.2451, 0.1943],\n",
      "        [0.2090, 0.3516, 0.2451, 0.1943],\n",
      "        [0.2090, 0.3516, 0.2451, 0.1943],\n",
      "        [0.2090, 0.3516, 0.2451, 0.1943],\n",
      "        [0.2090, 0.3516, 0.2451, 0.1943],\n",
      "        [0.2090, 0.3516, 0.2451, 0.1943],\n",
      "        [0.2090, 0.3516, 0.2451, 0.1943]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3576, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(-0.0253, grad_fn=<AddBackward0>)\n",
      "batching here\n",
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(4.1695e-06, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.5611, -1.0696, -1.3953, -1.6135],\n",
      "        [-1.5611, -1.0696, -1.3953, -1.6135],\n",
      "        [-1.5610, -1.0698, -1.3953, -1.6134],\n",
      "        [-1.5611, -1.0695, -1.3953, -1.6135],\n",
      "        [-1.5610, -1.0698, -1.3952, -1.6134],\n",
      "        [-1.5610, -1.0698, -1.3952, -1.6133],\n",
      "        [-1.5611, -1.0698, -1.3952, -1.6134],\n",
      "        [-1.5611, -1.0697, -1.3952, -1.6134],\n",
      "        [-1.5610, -1.0697, -1.3953, -1.6134],\n",
      "        [-1.5610, -1.0698, -1.3953, -1.6133],\n",
      "        [-1.5610, -1.0697, -1.3953, -1.6134],\n",
      "        [-1.5611, -1.0696, -1.3953, -1.6135],\n",
      "        [-1.5610, -1.0697, -1.3953, -1.6134],\n",
      "        [-1.5610, -1.0698, -1.3953, -1.6133],\n",
      "        [-1.5611, -1.0697, -1.3952, -1.6135],\n",
      "        [-1.5610, -1.0698, -1.3953, -1.6133],\n",
      "        [-1.5612, -1.0696, -1.3953, -1.6135],\n",
      "        [-1.5612, -1.0696, -1.3953, -1.6135],\n",
      "        [-1.5611, -1.0696, -1.3953, -1.6135],\n",
      "        [-1.5611, -1.0697, -1.3953, -1.6134],\n",
      "        [-1.5611, -1.0696, -1.3953, -1.6134],\n",
      "        [-1.5612, -1.0695, -1.3953, -1.6136],\n",
      "        [-1.5611, -1.0697, -1.3952, -1.6135],\n",
      "        [-1.5611, -1.0696, -1.3953, -1.6135],\n",
      "        [-1.5611, -1.0696, -1.3953, -1.6136],\n",
      "        [-1.5611, -1.0696, -1.3952, -1.6135],\n",
      "        [-1.5612, -1.0696, -1.3953, -1.6135],\n",
      "        [-1.5612, -1.0696, -1.3952, -1.6135],\n",
      "        [-1.5611, -1.0697, -1.3952, -1.6134],\n",
      "        [-1.5611, -1.0696, -1.3952, -1.6135],\n",
      "        [-1.5611, -1.0697, -1.3952, -1.6135],\n",
      "        [-1.5611, -1.0697, -1.3952, -1.6134],\n",
      "        [-1.5614, -1.0694, -1.3952, -1.6137],\n",
      "        [-1.5613, -1.0694, -1.3952, -1.6137],\n",
      "        [-1.5613, -1.0695, -1.3952, -1.6136],\n",
      "        [-1.5613, -1.0694, -1.3952, -1.6136],\n",
      "        [-1.5610, -1.0697, -1.3952, -1.6134],\n",
      "        [-1.5611, -1.0696, -1.3952, -1.6135],\n",
      "        [-1.5611, -1.0696, -1.3953, -1.6135],\n",
      "        [-1.5611, -1.0696, -1.3953, -1.6135],\n",
      "        [-1.5612, -1.0696, -1.3953, -1.6135],\n",
      "        [-1.5612, -1.0696, -1.3953, -1.6135],\n",
      "        [-1.5612, -1.0696, -1.3952, -1.6135],\n",
      "        [-1.5612, -1.0696, -1.3953, -1.6135],\n",
      "        [-1.5611, -1.0696, -1.3953, -1.6134],\n",
      "        [-1.5611, -1.0697, -1.3953, -1.6134],\n",
      "        [-1.5611, -1.0696, -1.3953, -1.6135],\n",
      "        [-1.5610, -1.0698, -1.3953, -1.6133],\n",
      "        [-1.5612, -1.0696, -1.3952, -1.6135],\n",
      "        [-1.5611, -1.0697, -1.3951, -1.6136],\n",
      "        [-1.5612, -1.0696, -1.3952, -1.6135],\n",
      "        [-1.5612, -1.0697, -1.3951, -1.6135],\n",
      "        [-1.5611, -1.0697, -1.3953, -1.6134],\n",
      "        [-1.5611, -1.0697, -1.3953, -1.6134],\n",
      "        [-1.5612, -1.0696, -1.3952, -1.6135],\n",
      "        [-1.5611, -1.0696, -1.3953, -1.6135],\n",
      "        [-1.5610, -1.0698, -1.3953, -1.6133],\n",
      "        [-1.5612, -1.0696, -1.3953, -1.6135],\n",
      "        [-1.5611, -1.0697, -1.3953, -1.6134],\n",
      "        [-1.5611, -1.0696, -1.3953, -1.6134],\n",
      "        [-1.5612, -1.0696, -1.3952, -1.6134],\n",
      "        [-1.5612, -1.0695, -1.3953, -1.6135],\n",
      "        [-1.5612, -1.0696, -1.3953, -1.6135],\n",
      "        [-1.5612, -1.0696, -1.3952, -1.6135]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([0.0032, 0.0034, 0.0023, 0.0012, 0.0031, 0.0033, 0.0015, 0.0010, 0.0042,\n",
      "        0.0031, 0.0023, 0.0008, 0.0048, 0.0036, 0.0022, 0.0013, 0.0047, 0.0031,\n",
      "        0.0024, 0.0013, 0.0032, 0.0035, 0.0024, 0.0008, 0.0048, 0.0024, 0.0016,\n",
      "        0.0008, 0.0033, 0.0037, 0.0016, 0.0009, 0.0050, 0.0024, 0.0024, 0.0012,\n",
      "        0.0032, 0.0025, 0.0025, 0.0011, 0.0046, 0.0035, 0.0016, 0.0012, 0.0047,\n",
      "        0.0030, 0.0015, 0.0013, 0.0041, 0.0035, 0.0016, 0.0009, 0.0032, 0.0036,\n",
      "        0.0016, 0.0011, 0.0045, 0.0036, 0.0017, 0.0008, 0.0046, 0.0024, 0.0016,\n",
      "        0.0011], grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(-0.0026, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2099, 0.3431, 0.2478, 0.1992],\n",
      "        [0.2099, 0.3431, 0.2478, 0.1992],\n",
      "        [0.2099, 0.3431, 0.2478, 0.1992],\n",
      "        [0.2099, 0.3432, 0.2478, 0.1992],\n",
      "        [0.2099, 0.3431, 0.2478, 0.1992],\n",
      "        [0.2099, 0.3431, 0.2478, 0.1992],\n",
      "        [0.2099, 0.3431, 0.2478, 0.1992],\n",
      "        [0.2099, 0.3431, 0.2478, 0.1992],\n",
      "        [0.2099, 0.3431, 0.2478, 0.1992],\n",
      "        [0.2099, 0.3431, 0.2478, 0.1992],\n",
      "        [0.2099, 0.3431, 0.2478, 0.1992],\n",
      "        [0.2099, 0.3431, 0.2478, 0.1992],\n",
      "        [0.2099, 0.3431, 0.2478, 0.1992],\n",
      "        [0.2099, 0.3431, 0.2478, 0.1992],\n",
      "        [0.2099, 0.3431, 0.2478, 0.1992],\n",
      "        [0.2099, 0.3431, 0.2478, 0.1992],\n",
      "        [0.2099, 0.3432, 0.2478, 0.1992],\n",
      "        [0.2099, 0.3432, 0.2478, 0.1992],\n",
      "        [0.2099, 0.3432, 0.2478, 0.1992],\n",
      "        [0.2099, 0.3431, 0.2478, 0.1992],\n",
      "        [0.2099, 0.3431, 0.2478, 0.1992],\n",
      "        [0.2099, 0.3432, 0.2478, 0.1992],\n",
      "        [0.2099, 0.3431, 0.2478, 0.1992],\n",
      "        [0.2099, 0.3431, 0.2478, 0.1992],\n",
      "        [0.2099, 0.3432, 0.2478, 0.1992],\n",
      "        [0.2099, 0.3431, 0.2478, 0.1992],\n",
      "        [0.2099, 0.3431, 0.2478, 0.1992],\n",
      "        [0.2099, 0.3432, 0.2478, 0.1992],\n",
      "        [0.2099, 0.3431, 0.2478, 0.1992],\n",
      "        [0.2099, 0.3431, 0.2478, 0.1992],\n",
      "        [0.2099, 0.3431, 0.2478, 0.1992],\n",
      "        [0.2099, 0.3431, 0.2478, 0.1992],\n",
      "        [0.2098, 0.3432, 0.2478, 0.1992],\n",
      "        [0.2099, 0.3432, 0.2478, 0.1991],\n",
      "        [0.2099, 0.3432, 0.2478, 0.1992],\n",
      "        [0.2099, 0.3432, 0.2478, 0.1992],\n",
      "        [0.2099, 0.3431, 0.2478, 0.1992],\n",
      "        [0.2099, 0.3431, 0.2478, 0.1992],\n",
      "        [0.2099, 0.3431, 0.2478, 0.1992],\n",
      "        [0.2099, 0.3432, 0.2478, 0.1992],\n",
      "        [0.2099, 0.3432, 0.2478, 0.1992],\n",
      "        [0.2099, 0.3432, 0.2478, 0.1992],\n",
      "        [0.2099, 0.3431, 0.2478, 0.1992],\n",
      "        [0.2099, 0.3432, 0.2478, 0.1992],\n",
      "        [0.2099, 0.3431, 0.2477, 0.1992],\n",
      "        [0.2099, 0.3431, 0.2478, 0.1992],\n",
      "        [0.2099, 0.3432, 0.2478, 0.1992],\n",
      "        [0.2099, 0.3431, 0.2478, 0.1992],\n",
      "        [0.2099, 0.3431, 0.2478, 0.1992],\n",
      "        [0.2099, 0.3431, 0.2478, 0.1992],\n",
      "        [0.2099, 0.3431, 0.2478, 0.1992],\n",
      "        [0.2099, 0.3431, 0.2478, 0.1992],\n",
      "        [0.2099, 0.3431, 0.2478, 0.1992],\n",
      "        [0.2099, 0.3431, 0.2478, 0.1992],\n",
      "        [0.2099, 0.3432, 0.2478, 0.1992],\n",
      "        [0.2099, 0.3432, 0.2478, 0.1992],\n",
      "        [0.2099, 0.3431, 0.2477, 0.1992],\n",
      "        [0.2099, 0.3431, 0.2478, 0.1992],\n",
      "        [0.2099, 0.3431, 0.2478, 0.1992],\n",
      "        [0.2099, 0.3431, 0.2478, 0.1992],\n",
      "        [0.2099, 0.3431, 0.2478, 0.1992],\n",
      "        [0.2099, 0.3432, 0.2478, 0.1992],\n",
      "        [0.2099, 0.3432, 0.2478, 0.1992],\n",
      "        [0.2099, 0.3432, 0.2478, 0.1992]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3618, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(-0.0162, grad_fn=<AddBackward0>)\n",
      "batching here\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(3.7584e-06, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.5566, -1.0775, -1.3935, -1.6070],\n",
      "        [-1.5566, -1.0774, -1.3935, -1.6070],\n",
      "        [-1.5565, -1.0775, -1.3935, -1.6070],\n",
      "        [-1.5566, -1.0775, -1.3935, -1.6070],\n",
      "        [-1.5565, -1.0775, -1.3935, -1.6070],\n",
      "        [-1.5565, -1.0776, -1.3935, -1.6069],\n",
      "        [-1.5566, -1.0774, -1.3935, -1.6071],\n",
      "        [-1.5565, -1.0776, -1.3935, -1.6070],\n",
      "        [-1.5565, -1.0775, -1.3935, -1.6069],\n",
      "        [-1.5566, -1.0774, -1.3936, -1.6070],\n",
      "        [-1.5566, -1.0775, -1.3935, -1.6070],\n",
      "        [-1.5566, -1.0774, -1.3935, -1.6071],\n",
      "        [-1.5565, -1.0776, -1.3935, -1.6069],\n",
      "        [-1.5566, -1.0775, -1.3935, -1.6070],\n",
      "        [-1.5566, -1.0775, -1.3936, -1.6069],\n",
      "        [-1.5566, -1.0774, -1.3935, -1.6070],\n",
      "        [-1.5566, -1.0774, -1.3935, -1.6070],\n",
      "        [-1.5566, -1.0775, -1.3936, -1.6070],\n",
      "        [-1.5567, -1.0774, -1.3935, -1.6071],\n",
      "        [-1.5566, -1.0775, -1.3935, -1.6070],\n",
      "        [-1.5566, -1.0774, -1.3935, -1.6070],\n",
      "        [-1.5566, -1.0774, -1.3935, -1.6071],\n",
      "        [-1.5566, -1.0774, -1.3935, -1.6071],\n",
      "        [-1.5566, -1.0774, -1.3935, -1.6071],\n",
      "        [-1.5566, -1.0774, -1.3935, -1.6070],\n",
      "        [-1.5566, -1.0775, -1.3935, -1.6070],\n",
      "        [-1.5566, -1.0775, -1.3935, -1.6069],\n",
      "        [-1.5567, -1.0773, -1.3935, -1.6071],\n",
      "        [-1.5565, -1.0776, -1.3935, -1.6069],\n",
      "        [-1.5566, -1.0775, -1.3935, -1.6070],\n",
      "        [-1.5566, -1.0775, -1.3935, -1.6070],\n",
      "        [-1.5566, -1.0775, -1.3935, -1.6070],\n",
      "        [-1.5568, -1.0773, -1.3935, -1.6072],\n",
      "        [-1.5568, -1.0773, -1.3935, -1.6071],\n",
      "        [-1.5567, -1.0774, -1.3935, -1.6070],\n",
      "        [-1.5568, -1.0773, -1.3934, -1.6071],\n",
      "        [-1.5565, -1.0776, -1.3935, -1.6069],\n",
      "        [-1.5566, -1.0774, -1.3935, -1.6070],\n",
      "        [-1.5566, -1.0775, -1.3935, -1.6070],\n",
      "        [-1.5566, -1.0774, -1.3935, -1.6070],\n",
      "        [-1.5567, -1.0774, -1.3935, -1.6070],\n",
      "        [-1.5566, -1.0774, -1.3935, -1.6070],\n",
      "        [-1.5566, -1.0775, -1.3935, -1.6069],\n",
      "        [-1.5566, -1.0776, -1.3935, -1.6068],\n",
      "        [-1.5565, -1.0775, -1.3935, -1.6070],\n",
      "        [-1.5565, -1.0776, -1.3935, -1.6069],\n",
      "        [-1.5566, -1.0774, -1.3935, -1.6070],\n",
      "        [-1.5565, -1.0776, -1.3935, -1.6069],\n",
      "        [-1.5566, -1.0775, -1.3934, -1.6071],\n",
      "        [-1.5567, -1.0774, -1.3934, -1.6072],\n",
      "        [-1.5566, -1.0774, -1.3934, -1.6071],\n",
      "        [-1.5567, -1.0774, -1.3934, -1.6072],\n",
      "        [-1.5566, -1.0775, -1.3935, -1.6069],\n",
      "        [-1.5566, -1.0774, -1.3935, -1.6070],\n",
      "        [-1.5566, -1.0775, -1.3935, -1.6070],\n",
      "        [-1.5566, -1.0774, -1.3935, -1.6071],\n",
      "        [-1.5566, -1.0774, -1.3936, -1.6071],\n",
      "        [-1.5566, -1.0774, -1.3936, -1.6070],\n",
      "        [-1.5565, -1.0775, -1.3935, -1.6069],\n",
      "        [-1.5565, -1.0776, -1.3936, -1.6069],\n",
      "        [-1.5567, -1.0774, -1.3935, -1.6071],\n",
      "        [-1.5568, -1.0773, -1.3935, -1.6071],\n",
      "        [-1.5567, -1.0774, -1.3935, -1.6070],\n",
      "        [-1.5566, -1.0775, -1.3935, -1.6070]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([0.0030, 0.0034, 0.0015, 0.0010, 0.0031, 0.0023, 0.0023, 0.0013, 0.0030,\n",
      "        0.0033, 0.0020, 0.0008, 0.0043, 0.0022, 0.0015, 0.0011, 0.0030, 0.0022,\n",
      "        0.0015, 0.0011, 0.0044, 0.0024, 0.0021, 0.0011, 0.0044, 0.0033, 0.0023,\n",
      "        0.0008, 0.0030, 0.0033, 0.0022, 0.0010, 0.0039, 0.0030, 0.0016, 0.0012,\n",
      "        0.0031, 0.0034, 0.0015, 0.0011, 0.0044, 0.0022, 0.0014, 0.0008, 0.0045,\n",
      "        0.0029, 0.0023, 0.0011, 0.0039, 0.0030, 0.0015, 0.0010, 0.0030, 0.0034,\n",
      "        0.0019, 0.0011, 0.0039, 0.0029, 0.0019, 0.0007, 0.0031, 0.0031, 0.0016,\n",
      "        0.0008], grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(-0.0023, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2109, 0.3405, 0.2482, 0.2005],\n",
      "        [0.2109, 0.3405, 0.2482, 0.2005],\n",
      "        [0.2109, 0.3404, 0.2482, 0.2005],\n",
      "        [0.2109, 0.3405, 0.2482, 0.2005],\n",
      "        [0.2109, 0.3404, 0.2482, 0.2005],\n",
      "        [0.2109, 0.3404, 0.2482, 0.2005],\n",
      "        [0.2108, 0.3405, 0.2482, 0.2005],\n",
      "        [0.2109, 0.3404, 0.2482, 0.2005],\n",
      "        [0.2109, 0.3404, 0.2482, 0.2005],\n",
      "        [0.2109, 0.3405, 0.2482, 0.2005],\n",
      "        [0.2109, 0.3405, 0.2482, 0.2005],\n",
      "        [0.2108, 0.3405, 0.2482, 0.2005],\n",
      "        [0.2109, 0.3404, 0.2482, 0.2005],\n",
      "        [0.2109, 0.3405, 0.2482, 0.2005],\n",
      "        [0.2109, 0.3404, 0.2482, 0.2005],\n",
      "        [0.2109, 0.3405, 0.2482, 0.2005],\n",
      "        [0.2109, 0.3405, 0.2482, 0.2005],\n",
      "        [0.2109, 0.3405, 0.2482, 0.2005],\n",
      "        [0.2108, 0.3405, 0.2482, 0.2005],\n",
      "        [0.2109, 0.3404, 0.2482, 0.2005],\n",
      "        [0.2109, 0.3405, 0.2482, 0.2005],\n",
      "        [0.2108, 0.3405, 0.2482, 0.2005],\n",
      "        [0.2108, 0.3405, 0.2482, 0.2005],\n",
      "        [0.2108, 0.3405, 0.2482, 0.2005],\n",
      "        [0.2109, 0.3405, 0.2482, 0.2005],\n",
      "        [0.2109, 0.3405, 0.2482, 0.2005],\n",
      "        [0.2109, 0.3404, 0.2482, 0.2005],\n",
      "        [0.2108, 0.3405, 0.2482, 0.2005],\n",
      "        [0.2109, 0.3404, 0.2482, 0.2005],\n",
      "        [0.2109, 0.3405, 0.2482, 0.2005],\n",
      "        [0.2108, 0.3405, 0.2482, 0.2005],\n",
      "        [0.2108, 0.3405, 0.2482, 0.2005],\n",
      "        [0.2108, 0.3405, 0.2482, 0.2005],\n",
      "        [0.2108, 0.3405, 0.2482, 0.2005],\n",
      "        [0.2108, 0.3405, 0.2482, 0.2005],\n",
      "        [0.2108, 0.3405, 0.2482, 0.2005],\n",
      "        [0.2109, 0.3404, 0.2482, 0.2005],\n",
      "        [0.2108, 0.3405, 0.2482, 0.2005],\n",
      "        [0.2109, 0.3404, 0.2482, 0.2005],\n",
      "        [0.2109, 0.3405, 0.2482, 0.2005],\n",
      "        [0.2108, 0.3405, 0.2482, 0.2005],\n",
      "        [0.2108, 0.3405, 0.2482, 0.2005],\n",
      "        [0.2108, 0.3404, 0.2482, 0.2005],\n",
      "        [0.2109, 0.3404, 0.2482, 0.2005],\n",
      "        [0.2109, 0.3404, 0.2482, 0.2005],\n",
      "        [0.2109, 0.3404, 0.2482, 0.2005],\n",
      "        [0.2109, 0.3405, 0.2482, 0.2005],\n",
      "        [0.2109, 0.3404, 0.2482, 0.2005],\n",
      "        [0.2109, 0.3404, 0.2482, 0.2005],\n",
      "        [0.2108, 0.3405, 0.2482, 0.2005],\n",
      "        [0.2109, 0.3405, 0.2482, 0.2005],\n",
      "        [0.2108, 0.3405, 0.2482, 0.2005],\n",
      "        [0.2109, 0.3404, 0.2482, 0.2005],\n",
      "        [0.2109, 0.3405, 0.2482, 0.2005],\n",
      "        [0.2109, 0.3405, 0.2482, 0.2005],\n",
      "        [0.2108, 0.3405, 0.2482, 0.2005],\n",
      "        [0.2108, 0.3405, 0.2482, 0.2005],\n",
      "        [0.2108, 0.3405, 0.2482, 0.2005],\n",
      "        [0.2109, 0.3404, 0.2482, 0.2005],\n",
      "        [0.2109, 0.3404, 0.2482, 0.2005],\n",
      "        [0.2108, 0.3405, 0.2482, 0.2005],\n",
      "        [0.2108, 0.3405, 0.2482, 0.2005],\n",
      "        [0.2108, 0.3405, 0.2482, 0.2005],\n",
      "        [0.2108, 0.3405, 0.2482, 0.2005]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3631, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(-0.0160, grad_fn=<AddBackward0>)\n",
      "batching here\n",
      "45: done 4 episodes, mean_reward=0.00, best_reward=2.00, speed=215.72\n",
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(0.0167, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.5517, -1.0860, -1.3928, -1.5987],\n",
      "        [-1.5516, -1.0860, -1.3928, -1.5987],\n",
      "        [-1.5516, -1.0861, -1.3928, -1.5987],\n",
      "        [-1.5516, -1.0860, -1.3928, -1.5987],\n",
      "        [-1.5517, -1.0859, -1.3928, -1.5988],\n",
      "        [-1.5516, -1.0861, -1.3927, -1.5987],\n",
      "        [-1.5516, -1.0860, -1.3927, -1.5987],\n",
      "        [-1.5517, -1.0860, -1.3928, -1.5988],\n",
      "        [-1.5517, -1.0860, -1.3928, -1.5988],\n",
      "        [-1.5517, -1.0860, -1.3928, -1.5988],\n",
      "        [-1.5517, -1.0860, -1.3928, -1.5988],\n",
      "        [-1.5517, -1.0860, -1.3928, -1.5988],\n",
      "        [-1.5516, -1.0860, -1.3928, -1.5987],\n",
      "        [-1.5516, -1.0861, -1.3927, -1.5987],\n",
      "        [-1.5516, -1.0861, -1.3927, -1.5987],\n",
      "        [-1.5515, -1.0863, -1.3927, -1.5985],\n",
      "        [-1.5517, -1.0860, -1.3928, -1.5987],\n",
      "        [-1.5517, -1.0859, -1.3928, -1.5988],\n",
      "        [-1.5517, -1.0860, -1.3928, -1.5988],\n",
      "        [-1.5517, -1.0860, -1.3928, -1.5988],\n",
      "        [-1.5517, -1.0860, -1.3927, -1.5987],\n",
      "        [-1.5517, -1.0859, -1.3928, -1.5988],\n",
      "        [-1.5517, -1.0860, -1.3928, -1.5987],\n",
      "        [-1.5516, -1.0860, -1.3928, -1.5987],\n",
      "        [-1.5516, -1.0860, -1.3928, -1.5987],\n",
      "        [-1.5517, -1.0860, -1.3928, -1.5987],\n",
      "        [-1.5517, -1.0860, -1.3928, -1.5987],\n",
      "        [-1.5517, -1.0860, -1.3927, -1.5988],\n",
      "        [-1.5517, -1.0860, -1.3927, -1.5988],\n",
      "        [-1.5517, -1.0860, -1.3927, -1.5988],\n",
      "        [-1.5516, -1.0861, -1.3927, -1.5987],\n",
      "        [-1.5517, -1.0860, -1.3927, -1.5987],\n",
      "        [-1.5518, -1.0858, -1.3928, -1.5989],\n",
      "        [-1.5519, -1.0857, -1.3927, -1.5990],\n",
      "        [-1.5518, -1.0859, -1.3927, -1.5989],\n",
      "        [-1.5519, -1.0858, -1.3927, -1.5989],\n",
      "        [-1.5516, -1.0861, -1.3928, -1.5987],\n",
      "        [-1.5516, -1.0860, -1.3928, -1.5987],\n",
      "        [-1.5516, -1.0861, -1.3927, -1.5986],\n",
      "        [-1.5517, -1.0860, -1.3928, -1.5987],\n",
      "        [-1.5518, -1.0859, -1.3928, -1.5988],\n",
      "        [-1.5517, -1.0860, -1.3928, -1.5986],\n",
      "        [-1.5517, -1.0860, -1.3928, -1.5987],\n",
      "        [-1.5517, -1.0860, -1.3928, -1.5987],\n",
      "        [-1.5516, -1.0861, -1.3927, -1.5987],\n",
      "        [-1.5516, -1.0860, -1.3927, -1.5987],\n",
      "        [-1.5517, -1.0860, -1.3927, -1.5987],\n",
      "        [-1.5517, -1.0860, -1.3927, -1.5988],\n",
      "        [-1.5517, -1.0859, -1.3927, -1.5989],\n",
      "        [-1.5517, -1.0860, -1.3927, -1.5988],\n",
      "        [-1.5517, -1.0860, -1.3927, -1.5988],\n",
      "        [-1.5517, -1.0859, -1.3927, -1.5988],\n",
      "        [-1.5517, -1.0860, -1.3928, -1.5988],\n",
      "        [-1.5517, -1.0860, -1.3928, -1.5988],\n",
      "        [-1.5517, -1.0860, -1.3928, -1.5988],\n",
      "        [-1.5517, -1.0860, -1.3928, -1.5988],\n",
      "        [-1.5517, -1.0860, -1.3928, -1.5988],\n",
      "        [-1.5517, -1.0860, -1.3928, -1.5988],\n",
      "        [-1.5517, -1.0860, -1.3928, -1.5988],\n",
      "        [-1.5517, -1.0860, -1.3928, -1.5988],\n",
      "        [-1.5518, -1.0860, -1.3927, -1.5987],\n",
      "        [-1.5519, -1.0858, -1.3928, -1.5988],\n",
      "        [-1.5518, -1.0859, -1.3927, -1.5988],\n",
      "        [-1.5517, -1.0860, -1.3927, -1.5987]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([ 1.0779e-01,  7.3217e-02,  7.3179e-02,  9.3872e-02, -1.5477e+00,\n",
      "         2.1713e-03,  2.2878e-03,  1.1457e-03,  7.3197e-02,  7.3204e-02,\n",
      "         9.3885e-02,  7.3204e-02,  4.1990e-03,  2.2038e-03,  2.1522e-03,\n",
      "         1.0931e-03,  2.9080e-03,  2.2146e-03,  2.1631e-03,  9.9843e-04,\n",
      "         2.8055e-03,  3.2885e-03,  1.8493e-03,  1.0751e-03,  4.1522e-03,\n",
      "         2.1304e-03,  1.7458e-03,  9.4191e-04,  4.0271e-03,  2.7239e-03,\n",
      "         1.7501e-03,  7.1151e-04,  3.7095e-03,  2.2240e-03,  1.4332e-03,\n",
      "         9.0217e-04,  3.6929e-03,  2.8288e-03,  1.9201e-03,  1.0524e-03,\n",
      "         3.0174e-03,  3.2873e-03,  2.2051e-03,  7.4459e-04,  4.1526e-03,\n",
      "         3.0726e-03,  2.0443e-03,  1.0998e-03,  3.7766e-03,  2.1689e-03,\n",
      "         1.9234e-03,  9.7555e-04,  1.0458e-01,  9.3885e-02,  1.0460e-01,\n",
      "         7.3204e-02,  9.3900e-02,  9.3885e-02,  1.0460e-01,  9.3885e-02,\n",
      "         3.7360e-03,  3.3439e-03,  1.5333e-03,  1.1133e-03],\n",
      "       grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(0.0003, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2119, 0.3376, 0.2484, 0.2022],\n",
      "        [0.2119, 0.3376, 0.2484, 0.2022],\n",
      "        [0.2119, 0.3375, 0.2484, 0.2022],\n",
      "        [0.2119, 0.3376, 0.2484, 0.2022],\n",
      "        [0.2119, 0.3376, 0.2484, 0.2021],\n",
      "        [0.2119, 0.3375, 0.2484, 0.2022],\n",
      "        [0.2119, 0.3375, 0.2484, 0.2022],\n",
      "        [0.2119, 0.3376, 0.2484, 0.2021],\n",
      "        [0.2119, 0.3376, 0.2484, 0.2021],\n",
      "        [0.2119, 0.3376, 0.2484, 0.2021],\n",
      "        [0.2119, 0.3376, 0.2484, 0.2021],\n",
      "        [0.2119, 0.3376, 0.2484, 0.2021],\n",
      "        [0.2119, 0.3376, 0.2484, 0.2022],\n",
      "        [0.2119, 0.3375, 0.2484, 0.2022],\n",
      "        [0.2119, 0.3375, 0.2484, 0.2022],\n",
      "        [0.2119, 0.3375, 0.2484, 0.2022],\n",
      "        [0.2119, 0.3376, 0.2484, 0.2022],\n",
      "        [0.2119, 0.3376, 0.2484, 0.2021],\n",
      "        [0.2119, 0.3376, 0.2484, 0.2021],\n",
      "        [0.2119, 0.3376, 0.2484, 0.2021],\n",
      "        [0.2119, 0.3376, 0.2484, 0.2022],\n",
      "        [0.2119, 0.3376, 0.2484, 0.2021],\n",
      "        [0.2119, 0.3376, 0.2484, 0.2022],\n",
      "        [0.2119, 0.3375, 0.2484, 0.2022],\n",
      "        [0.2119, 0.3376, 0.2484, 0.2022],\n",
      "        [0.2119, 0.3376, 0.2484, 0.2022],\n",
      "        [0.2119, 0.3376, 0.2484, 0.2022],\n",
      "        [0.2119, 0.3376, 0.2484, 0.2021],\n",
      "        [0.2119, 0.3376, 0.2484, 0.2021],\n",
      "        [0.2119, 0.3376, 0.2484, 0.2021],\n",
      "        [0.2119, 0.3375, 0.2484, 0.2022],\n",
      "        [0.2119, 0.3375, 0.2484, 0.2022],\n",
      "        [0.2119, 0.3376, 0.2484, 0.2021],\n",
      "        [0.2118, 0.3376, 0.2484, 0.2021],\n",
      "        [0.2119, 0.3376, 0.2484, 0.2021],\n",
      "        [0.2119, 0.3376, 0.2484, 0.2021],\n",
      "        [0.2119, 0.3375, 0.2484, 0.2022],\n",
      "        [0.2119, 0.3376, 0.2484, 0.2022],\n",
      "        [0.2119, 0.3375, 0.2484, 0.2022],\n",
      "        [0.2119, 0.3376, 0.2484, 0.2022],\n",
      "        [0.2119, 0.3376, 0.2484, 0.2021],\n",
      "        [0.2119, 0.3375, 0.2484, 0.2022],\n",
      "        [0.2119, 0.3376, 0.2484, 0.2022],\n",
      "        [0.2119, 0.3376, 0.2484, 0.2022],\n",
      "        [0.2119, 0.3375, 0.2484, 0.2022],\n",
      "        [0.2119, 0.3375, 0.2484, 0.2022],\n",
      "        [0.2119, 0.3376, 0.2484, 0.2022],\n",
      "        [0.2119, 0.3376, 0.2484, 0.2021],\n",
      "        [0.2119, 0.3376, 0.2484, 0.2021],\n",
      "        [0.2119, 0.3376, 0.2484, 0.2021],\n",
      "        [0.2119, 0.3376, 0.2484, 0.2021],\n",
      "        [0.2119, 0.3376, 0.2484, 0.2021],\n",
      "        [0.2119, 0.3376, 0.2484, 0.2021],\n",
      "        [0.2119, 0.3376, 0.2484, 0.2021],\n",
      "        [0.2119, 0.3376, 0.2484, 0.2021],\n",
      "        [0.2119, 0.3376, 0.2484, 0.2021],\n",
      "        [0.2119, 0.3376, 0.2484, 0.2021],\n",
      "        [0.2119, 0.3376, 0.2484, 0.2021],\n",
      "        [0.2119, 0.3376, 0.2484, 0.2021],\n",
      "        [0.2119, 0.3376, 0.2484, 0.2021],\n",
      "        [0.2119, 0.3376, 0.2484, 0.2022],\n",
      "        [0.2118, 0.3376, 0.2484, 0.2021],\n",
      "        [0.2119, 0.3376, 0.2484, 0.2021],\n",
      "        [0.2119, 0.3376, 0.2484, 0.2022]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3645, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(0.0033, grad_fn=<AddBackward0>)\n",
      "batching here\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46: done 1 episodes, mean_reward=0.00, best_reward=2.00, speed=213.41\n",
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.4853, -1.1372, -1.4095, -1.5675],\n",
      "        [-1.4853, -1.1370, -1.4095, -1.5676],\n",
      "        [-1.4853, -1.1371, -1.4095, -1.5675],\n",
      "        [-1.4853, -1.1371, -1.4095, -1.5676],\n",
      "        [-1.4854, -1.1372, -1.4094, -1.5675],\n",
      "        [-1.4853, -1.1371, -1.4095, -1.5675],\n",
      "        [-1.4854, -1.1371, -1.4095, -1.5676],\n",
      "        [-1.4854, -1.1371, -1.4095, -1.5675],\n",
      "        [-1.4854, -1.1371, -1.4095, -1.5675],\n",
      "        [-1.4854, -1.1370, -1.4095, -1.5676],\n",
      "        [-1.4853, -1.1371, -1.4095, -1.5676],\n",
      "        [-1.4854, -1.1370, -1.4095, -1.5676],\n",
      "        [-1.4852, -1.1371, -1.4095, -1.5675],\n",
      "        [-1.4852, -1.1371, -1.4095, -1.5676],\n",
      "        [-1.4853, -1.1371, -1.4095, -1.5676],\n",
      "        [-1.4853, -1.1371, -1.4095, -1.5675],\n",
      "        [-1.4853, -1.1371, -1.4095, -1.5676],\n",
      "        [-1.4854, -1.1370, -1.4095, -1.5676],\n",
      "        [-1.4854, -1.1371, -1.4095, -1.5676],\n",
      "        [-1.4853, -1.1371, -1.4095, -1.5676],\n",
      "        [-1.4854, -1.1370, -1.4095, -1.5676],\n",
      "        [-1.4853, -1.1371, -1.4096, -1.5675],\n",
      "        [-1.4854, -1.1370, -1.4095, -1.5676],\n",
      "        [-1.4853, -1.1371, -1.4095, -1.5676],\n",
      "        [-1.4853, -1.1371, -1.4095, -1.5676],\n",
      "        [-1.4853, -1.1371, -1.4095, -1.5675],\n",
      "        [-1.4854, -1.1370, -1.4095, -1.5676],\n",
      "        [-1.4853, -1.1371, -1.4095, -1.5676],\n",
      "        [-1.4853, -1.1372, -1.4094, -1.5675],\n",
      "        [-1.4853, -1.1371, -1.4095, -1.5676],\n",
      "        [-1.4855, -1.1369, -1.4094, -1.5678],\n",
      "        [-1.4853, -1.1373, -1.4094, -1.5674],\n",
      "        [-1.4856, -1.1367, -1.4095, -1.5679],\n",
      "        [-1.4856, -1.1367, -1.4095, -1.5679],\n",
      "        [-1.4856, -1.1367, -1.4095, -1.5679],\n",
      "        [-1.4855, -1.1367, -1.4095, -1.5679],\n",
      "        [-1.4853, -1.1370, -1.4095, -1.5676],\n",
      "        [-1.4854, -1.1370, -1.4095, -1.5676],\n",
      "        [-1.4854, -1.1371, -1.4095, -1.5675],\n",
      "        [-1.4854, -1.1370, -1.4095, -1.5676],\n",
      "        [-1.4854, -1.1370, -1.4095, -1.5675],\n",
      "        [-1.4854, -1.1370, -1.4096, -1.5676],\n",
      "        [-1.4855, -1.1369, -1.4095, -1.5676],\n",
      "        [-1.4854, -1.1370, -1.4095, -1.5676],\n",
      "        [-1.4853, -1.1371, -1.4095, -1.5676],\n",
      "        [-1.4853, -1.1371, -1.4095, -1.5675],\n",
      "        [-1.4853, -1.1371, -1.4095, -1.5676],\n",
      "        [-1.4853, -1.1371, -1.4095, -1.5676],\n",
      "        [-1.4854, -1.1370, -1.4094, -1.5677],\n",
      "        [-1.4854, -1.1369, -1.4094, -1.5678],\n",
      "        [-1.4855, -1.1369, -1.4095, -1.5678],\n",
      "        [-1.4854, -1.1369, -1.4094, -1.5678],\n",
      "        [-1.4854, -1.1370, -1.4095, -1.5677],\n",
      "        [-1.4854, -1.1370, -1.4095, -1.5677],\n",
      "        [-1.4854, -1.1369, -1.4095, -1.5677],\n",
      "        [-1.4854, -1.1370, -1.4095, -1.5677],\n",
      "        [-1.4854, -1.1371, -1.4095, -1.5675],\n",
      "        [-1.4854, -1.1370, -1.4095, -1.5676],\n",
      "        [-1.4854, -1.1370, -1.4095, -1.5676],\n",
      "        [-1.4854, -1.1371, -1.4094, -1.5676],\n",
      "        [-1.4854, -1.1371, -1.4095, -1.5675],\n",
      "        [-1.4854, -1.1369, -1.4095, -1.5677],\n",
      "        [-1.4854, -1.1369, -1.4095, -1.5677],\n",
      "        [-1.4854, -1.1370, -1.4095, -1.5676]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([0.0039, 0.0028, 0.0019, 0.0009, 0.0028, 0.0029, 0.0013, 0.0011, 0.0040,\n",
      "        0.0031, 0.0018, 0.0012, 0.0036, 0.0027, 0.0021, 0.0011, 0.0037, 0.0030,\n",
      "        0.0015, 0.0010, 0.0028, 0.0022, 0.0020, 0.0009, 0.0036, 0.0021, 0.0019,\n",
      "        0.0007, 0.0039, 0.0021, 0.0015, 0.0009, 0.0028, 0.0026, 0.0017, 0.0007,\n",
      "        0.0716, 0.0888, 0.0019, 0.0009, 0.0035, 0.0029, 0.0020, 0.0007, 0.0028,\n",
      "        0.0020, 0.0014, 0.0009, 0.0037, 0.0025, 0.0017, 0.0009, 0.0028, 0.0029,\n",
      "        0.0019, 0.0008, 0.0037, 0.0028, 0.0020, 0.0010, 0.0028, 0.0027, 0.0020,\n",
      "        0.0009], grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(-0.0046, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2264, 0.3207, 0.2443, 0.2086],\n",
      "        [0.2264, 0.3208, 0.2443, 0.2085],\n",
      "        [0.2264, 0.3207, 0.2443, 0.2086],\n",
      "        [0.2264, 0.3207, 0.2443, 0.2086],\n",
      "        [0.2264, 0.3207, 0.2443, 0.2086],\n",
      "        [0.2264, 0.3207, 0.2443, 0.2086],\n",
      "        [0.2264, 0.3208, 0.2443, 0.2086],\n",
      "        [0.2264, 0.3207, 0.2443, 0.2086],\n",
      "        [0.2264, 0.3207, 0.2443, 0.2086],\n",
      "        [0.2264, 0.3208, 0.2443, 0.2085],\n",
      "        [0.2264, 0.3208, 0.2443, 0.2085],\n",
      "        [0.2264, 0.3208, 0.2443, 0.2085],\n",
      "        [0.2264, 0.3207, 0.2443, 0.2086],\n",
      "        [0.2264, 0.3207, 0.2443, 0.2085],\n",
      "        [0.2264, 0.3208, 0.2443, 0.2085],\n",
      "        [0.2264, 0.3207, 0.2443, 0.2086],\n",
      "        [0.2264, 0.3208, 0.2443, 0.2085],\n",
      "        [0.2264, 0.3208, 0.2443, 0.2085],\n",
      "        [0.2264, 0.3208, 0.2443, 0.2086],\n",
      "        [0.2264, 0.3208, 0.2443, 0.2086],\n",
      "        [0.2264, 0.3208, 0.2443, 0.2085],\n",
      "        [0.2264, 0.3207, 0.2442, 0.2086],\n",
      "        [0.2264, 0.3208, 0.2443, 0.2085],\n",
      "        [0.2264, 0.3207, 0.2443, 0.2085],\n",
      "        [0.2264, 0.3208, 0.2443, 0.2086],\n",
      "        [0.2264, 0.3208, 0.2443, 0.2086],\n",
      "        [0.2264, 0.3208, 0.2443, 0.2085],\n",
      "        [0.2264, 0.3208, 0.2443, 0.2086],\n",
      "        [0.2264, 0.3207, 0.2443, 0.2086],\n",
      "        [0.2264, 0.3207, 0.2443, 0.2085],\n",
      "        [0.2264, 0.3208, 0.2443, 0.2085],\n",
      "        [0.2264, 0.3207, 0.2443, 0.2086],\n",
      "        [0.2264, 0.3209, 0.2443, 0.2085],\n",
      "        [0.2264, 0.3209, 0.2443, 0.2085],\n",
      "        [0.2264, 0.3209, 0.2443, 0.2085],\n",
      "        [0.2264, 0.3209, 0.2443, 0.2085],\n",
      "        [0.2264, 0.3208, 0.2443, 0.2085],\n",
      "        [0.2264, 0.3208, 0.2443, 0.2085],\n",
      "        [0.2264, 0.3208, 0.2443, 0.2086],\n",
      "        [0.2264, 0.3208, 0.2443, 0.2085],\n",
      "        [0.2264, 0.3208, 0.2443, 0.2086],\n",
      "        [0.2264, 0.3208, 0.2442, 0.2086],\n",
      "        [0.2264, 0.3208, 0.2443, 0.2085],\n",
      "        [0.2264, 0.3208, 0.2443, 0.2085],\n",
      "        [0.2264, 0.3208, 0.2443, 0.2086],\n",
      "        [0.2264, 0.3207, 0.2443, 0.2086],\n",
      "        [0.2264, 0.3208, 0.2443, 0.2085],\n",
      "        [0.2264, 0.3207, 0.2443, 0.2086],\n",
      "        [0.2264, 0.3208, 0.2443, 0.2085],\n",
      "        [0.2264, 0.3208, 0.2443, 0.2085],\n",
      "        [0.2264, 0.3208, 0.2443, 0.2085],\n",
      "        [0.2264, 0.3208, 0.2443, 0.2085],\n",
      "        [0.2264, 0.3208, 0.2443, 0.2085],\n",
      "        [0.2264, 0.3208, 0.2443, 0.2085],\n",
      "        [0.2264, 0.3208, 0.2443, 0.2085],\n",
      "        [0.2264, 0.3208, 0.2443, 0.2085],\n",
      "        [0.2264, 0.3208, 0.2443, 0.2086],\n",
      "        [0.2264, 0.3208, 0.2443, 0.2085],\n",
      "        [0.2264, 0.3208, 0.2443, 0.2085],\n",
      "        [0.2264, 0.3208, 0.2443, 0.2085],\n",
      "        [0.2264, 0.3208, 0.2443, 0.2086],\n",
      "        [0.2264, 0.3208, 0.2443, 0.2085],\n",
      "        [0.2264, 0.3208, 0.2443, 0.2085],\n",
      "        [0.2264, 0.3208, 0.2443, 0.2085]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3722, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(-0.0182, grad_fn=<AddBackward0>)\n",
      "batching here\n",
      "47: done 4 episodes, mean_reward=0.00, best_reward=2.00, speed=216.32\n",
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(0.0460, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.4812, -1.1438, -1.4105, -1.5607],\n",
      "        [-1.4813, -1.1437, -1.4105, -1.5607],\n",
      "        [-1.4812, -1.1438, -1.4105, -1.5606],\n",
      "        [-1.4812, -1.1438, -1.4104, -1.5607],\n",
      "        [-1.4812, -1.1439, -1.4104, -1.5606],\n",
      "        [-1.4812, -1.1440, -1.4104, -1.5605],\n",
      "        [-1.4813, -1.1439, -1.4104, -1.5605],\n",
      "        [-1.4813, -1.1439, -1.4103, -1.5606],\n",
      "        [-1.4813, -1.1437, -1.4105, -1.5607],\n",
      "        [-1.4813, -1.1437, -1.4105, -1.5606],\n",
      "        [-1.4813, -1.1437, -1.4105, -1.5607],\n",
      "        [-1.4813, -1.1437, -1.4105, -1.5607],\n",
      "        [-1.4811, -1.1439, -1.4105, -1.5605],\n",
      "        [-1.4811, -1.1439, -1.4105, -1.5605],\n",
      "        [-1.4812, -1.1438, -1.4105, -1.5606],\n",
      "        [-1.4812, -1.1438, -1.4105, -1.5606],\n",
      "        [-1.4812, -1.1438, -1.4105, -1.5606],\n",
      "        [-1.4812, -1.1437, -1.4106, -1.5606],\n",
      "        [-1.4813, -1.1437, -1.4105, -1.5607],\n",
      "        [-1.4812, -1.1437, -1.4105, -1.5607],\n",
      "        [-1.4813, -1.1436, -1.4105, -1.5608],\n",
      "        [-1.4813, -1.1436, -1.4105, -1.5608],\n",
      "        [-1.4812, -1.1438, -1.4104, -1.5606],\n",
      "        [-1.4813, -1.1438, -1.4104, -1.5606],\n",
      "        [-1.4813, -1.1437, -1.4105, -1.5607],\n",
      "        [-1.4813, -1.1438, -1.4104, -1.5606],\n",
      "        [-1.4813, -1.1437, -1.4105, -1.5607],\n",
      "        [-1.4813, -1.1437, -1.4105, -1.5607],\n",
      "        [-1.4813, -1.1437, -1.4105, -1.5607],\n",
      "        [-1.4813, -1.1438, -1.4104, -1.5607],\n",
      "        [-1.4812, -1.1438, -1.4104, -1.5606],\n",
      "        [-1.4812, -1.1439, -1.4104, -1.5606],\n",
      "        [-1.4814, -1.1434, -1.4105, -1.5609],\n",
      "        [-1.4814, -1.1435, -1.4105, -1.5609],\n",
      "        [-1.4815, -1.1434, -1.4105, -1.5610],\n",
      "        [-1.4815, -1.1434, -1.4105, -1.5610],\n",
      "        [-1.4813, -1.1437, -1.4105, -1.5607],\n",
      "        [-1.4813, -1.1437, -1.4104, -1.5607],\n",
      "        [-1.4813, -1.1437, -1.4105, -1.5607],\n",
      "        [-1.4813, -1.1436, -1.4105, -1.5607],\n",
      "        [-1.4813, -1.1436, -1.4106, -1.5607],\n",
      "        [-1.4813, -1.1437, -1.4105, -1.5606],\n",
      "        [-1.4812, -1.1437, -1.4105, -1.5607],\n",
      "        [-1.4813, -1.1437, -1.4105, -1.5607],\n",
      "        [-1.4812, -1.1437, -1.4105, -1.5607],\n",
      "        [-1.4812, -1.1439, -1.4104, -1.5606],\n",
      "        [-1.4813, -1.1438, -1.4105, -1.5607],\n",
      "        [-1.4812, -1.1438, -1.4105, -1.5607],\n",
      "        [-1.4814, -1.1435, -1.4104, -1.5609],\n",
      "        [-1.4814, -1.1436, -1.4104, -1.5609],\n",
      "        [-1.4813, -1.1437, -1.4104, -1.5608],\n",
      "        [-1.4814, -1.1436, -1.4104, -1.5608],\n",
      "        [-1.4812, -1.1438, -1.4105, -1.5606],\n",
      "        [-1.4813, -1.1437, -1.4105, -1.5607],\n",
      "        [-1.4813, -1.1437, -1.4105, -1.5608],\n",
      "        [-1.4812, -1.1437, -1.4104, -1.5608],\n",
      "        [-1.4813, -1.1437, -1.4105, -1.5607],\n",
      "        [-1.4813, -1.1436, -1.4105, -1.5607],\n",
      "        [-1.4812, -1.1437, -1.4105, -1.5607],\n",
      "        [-1.4813, -1.1438, -1.4104, -1.5607],\n",
      "        [-1.4813, -1.1436, -1.4105, -1.5607],\n",
      "        [-1.4813, -1.1436, -1.4105, -1.5607],\n",
      "        [-1.4814, -1.1436, -1.4105, -1.5608],\n",
      "        [-1.4814, -1.1436, -1.4105, -1.5607]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([ 3.1681e-03,  2.5735e-03,  1.7012e-03,  8.4852e-04,  2.6146e-03,\n",
      "         2.4046e-03,  1.8768e-03,  7.7475e-04,  3.1117e-03,  2.3783e-03,\n",
      "         1.2703e-03,  6.2491e-04, -1.4483e+00, -1.3940e+00, -1.4793e+00,\n",
      "         8.3754e-04,  7.9059e-02,  2.3536e-03,  1.7969e-03,  9.2673e-04,\n",
      "         3.0538e-03,  2.3490e-03,  1.6123e-03,  6.5781e-04,  6.4118e-02,\n",
      "         2.5222e-03,  1.6066e-03,  6.9166e-04,  3.3219e-03,  2.3889e-03,\n",
      "         1.6087e-03,  7.7879e-04,  3.3207e-03,  2.6349e-03,  1.8020e-03,\n",
      "         9.1879e-04,  3.2401e-03,  1.8956e-03,  1.6556e-03,  8.5286e-04,\n",
      "         7.8859e-02,  8.2817e-02,  1.7298e-03,  8.1445e-04,  7.9112e-02,\n",
      "         2.5142e-03,  1.6746e-03,  7.7653e-04,  3.2061e-03,  1.8425e-03,\n",
      "         1.5062e-03,  7.7399e-04,  2.5276e-03,  2.4253e-03,  1.7145e-03,\n",
      "         7.0890e-04,  3.1556e-03,  2.5697e-03,  1.7649e-03,  8.8724e-04,\n",
      "         3.2148e-03,  1.8626e-03,  1.7212e-03,  8.1532e-04],\n",
      "       grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(0.0599, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2274, 0.3186, 0.2440, 0.2100],\n",
      "        [0.2273, 0.3187, 0.2440, 0.2100],\n",
      "        [0.2274, 0.3186, 0.2440, 0.2100],\n",
      "        [0.2274, 0.3186, 0.2440, 0.2100],\n",
      "        [0.2274, 0.3186, 0.2440, 0.2100],\n",
      "        [0.2274, 0.3185, 0.2441, 0.2100],\n",
      "        [0.2274, 0.3186, 0.2440, 0.2100],\n",
      "        [0.2273, 0.3186, 0.2441, 0.2100],\n",
      "        [0.2274, 0.3186, 0.2440, 0.2100],\n",
      "        [0.2273, 0.3186, 0.2440, 0.2100],\n",
      "        [0.2273, 0.3186, 0.2440, 0.2100],\n",
      "        [0.2273, 0.3186, 0.2440, 0.2100],\n",
      "        [0.2274, 0.3186, 0.2440, 0.2100],\n",
      "        [0.2274, 0.3186, 0.2440, 0.2100],\n",
      "        [0.2274, 0.3186, 0.2440, 0.2100],\n",
      "        [0.2274, 0.3186, 0.2440, 0.2100],\n",
      "        [0.2274, 0.3186, 0.2440, 0.2100],\n",
      "        [0.2274, 0.3186, 0.2440, 0.2100],\n",
      "        [0.2273, 0.3187, 0.2440, 0.2100],\n",
      "        [0.2274, 0.3186, 0.2440, 0.2100],\n",
      "        [0.2273, 0.3187, 0.2440, 0.2100],\n",
      "        [0.2274, 0.3187, 0.2440, 0.2100],\n",
      "        [0.2274, 0.3186, 0.2440, 0.2100],\n",
      "        [0.2273, 0.3186, 0.2440, 0.2100],\n",
      "        [0.2273, 0.3186, 0.2440, 0.2100],\n",
      "        [0.2273, 0.3186, 0.2440, 0.2100],\n",
      "        [0.2273, 0.3187, 0.2440, 0.2100],\n",
      "        [0.2274, 0.3186, 0.2440, 0.2100],\n",
      "        [0.2273, 0.3186, 0.2440, 0.2100],\n",
      "        [0.2274, 0.3186, 0.2440, 0.2100],\n",
      "        [0.2274, 0.3186, 0.2440, 0.2100],\n",
      "        [0.2274, 0.3186, 0.2441, 0.2100],\n",
      "        [0.2273, 0.3187, 0.2440, 0.2099],\n",
      "        [0.2273, 0.3187, 0.2440, 0.2099],\n",
      "        [0.2273, 0.3187, 0.2440, 0.2099],\n",
      "        [0.2273, 0.3187, 0.2440, 0.2099],\n",
      "        [0.2274, 0.3186, 0.2440, 0.2100],\n",
      "        [0.2273, 0.3186, 0.2440, 0.2100],\n",
      "        [0.2273, 0.3186, 0.2440, 0.2100],\n",
      "        [0.2273, 0.3187, 0.2440, 0.2100],\n",
      "        [0.2273, 0.3187, 0.2440, 0.2100],\n",
      "        [0.2274, 0.3186, 0.2440, 0.2100],\n",
      "        [0.2274, 0.3186, 0.2440, 0.2100],\n",
      "        [0.2273, 0.3186, 0.2440, 0.2100],\n",
      "        [0.2274, 0.3186, 0.2440, 0.2100],\n",
      "        [0.2274, 0.3186, 0.2440, 0.2100],\n",
      "        [0.2274, 0.3186, 0.2440, 0.2100],\n",
      "        [0.2274, 0.3186, 0.2440, 0.2100],\n",
      "        [0.2273, 0.3187, 0.2440, 0.2099],\n",
      "        [0.2273, 0.3187, 0.2440, 0.2100],\n",
      "        [0.2273, 0.3186, 0.2440, 0.2100],\n",
      "        [0.2273, 0.3187, 0.2440, 0.2100],\n",
      "        [0.2274, 0.3186, 0.2440, 0.2100],\n",
      "        [0.2273, 0.3186, 0.2440, 0.2100],\n",
      "        [0.2274, 0.3186, 0.2440, 0.2100],\n",
      "        [0.2274, 0.3186, 0.2440, 0.2100],\n",
      "        [0.2273, 0.3186, 0.2440, 0.2100],\n",
      "        [0.2273, 0.3187, 0.2440, 0.2100],\n",
      "        [0.2274, 0.3186, 0.2440, 0.2100],\n",
      "        [0.2274, 0.3186, 0.2440, 0.2100],\n",
      "        [0.2273, 0.3187, 0.2440, 0.2100],\n",
      "        [0.2273, 0.3187, 0.2440, 0.2100],\n",
      "        [0.2273, 0.3187, 0.2440, 0.2100],\n",
      "        [0.2273, 0.3187, 0.2440, 0.2100]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3731, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(0.0922, grad_fn=<AddBackward0>)\n",
      "batching here\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48: done 1 episodes, mean_reward=0.00, best_reward=2.00, speed=214.01\n",
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.4211, -1.2059, -1.3953, -1.5542],\n",
      "        [-1.4212, -1.2058, -1.3953, -1.5542],\n",
      "        [-1.4211, -1.2060, -1.3952, -1.5542],\n",
      "        [-1.4211, -1.2060, -1.3952, -1.5541],\n",
      "        [-1.4210, -1.2064, -1.3951, -1.5538],\n",
      "        [-1.4211, -1.2059, -1.3952, -1.5543],\n",
      "        [-1.4212, -1.2059, -1.3952, -1.5542],\n",
      "        [-1.4212, -1.2060, -1.3952, -1.5542],\n",
      "        [-1.4211, -1.2059, -1.3953, -1.5542],\n",
      "        [-1.4211, -1.2059, -1.3953, -1.5541],\n",
      "        [-1.4211, -1.2058, -1.3953, -1.5543],\n",
      "        [-1.4211, -1.2059, -1.3953, -1.5541],\n",
      "        [-1.4211, -1.2058, -1.3953, -1.5543],\n",
      "        [-1.4211, -1.2060, -1.3952, -1.5541],\n",
      "        [-1.4211, -1.2057, -1.3953, -1.5544],\n",
      "        [-1.4212, -1.2057, -1.3953, -1.5544],\n",
      "        [-1.4211, -1.2059, -1.3953, -1.5541],\n",
      "        [-1.4211, -1.2060, -1.3952, -1.5541],\n",
      "        [-1.4211, -1.2058, -1.3953, -1.5542],\n",
      "        [-1.4210, -1.2060, -1.3953, -1.5540],\n",
      "        [-1.4210, -1.2062, -1.3952, -1.5540],\n",
      "        [-1.4211, -1.2061, -1.3953, -1.5539],\n",
      "        [-1.4210, -1.2061, -1.3953, -1.5540],\n",
      "        [-1.4210, -1.2061, -1.3953, -1.5539],\n",
      "        [-1.4211, -1.2058, -1.3953, -1.5542],\n",
      "        [-1.4211, -1.2058, -1.3953, -1.5543],\n",
      "        [-1.4211, -1.2059, -1.3953, -1.5542],\n",
      "        [-1.4211, -1.2059, -1.3953, -1.5541],\n",
      "        [-1.4211, -1.2061, -1.3951, -1.5541],\n",
      "        [-1.4210, -1.2061, -1.3952, -1.5540],\n",
      "        [-1.4211, -1.2060, -1.3952, -1.5541],\n",
      "        [-1.4211, -1.2062, -1.3951, -1.5540],\n",
      "        [-1.4212, -1.2056, -1.3953, -1.5546],\n",
      "        [-1.4212, -1.2057, -1.3953, -1.5544],\n",
      "        [-1.4211, -1.2059, -1.3953, -1.5541],\n",
      "        [-1.4211, -1.2058, -1.3952, -1.5544],\n",
      "        [-1.4211, -1.2059, -1.3953, -1.5542],\n",
      "        [-1.4211, -1.2060, -1.3952, -1.5541],\n",
      "        [-1.4211, -1.2059, -1.3953, -1.5542],\n",
      "        [-1.4212, -1.2058, -1.3953, -1.5543],\n",
      "        [-1.4211, -1.2058, -1.3953, -1.5543],\n",
      "        [-1.4212, -1.2058, -1.3952, -1.5543],\n",
      "        [-1.4211, -1.2058, -1.3953, -1.5543],\n",
      "        [-1.4212, -1.2059, -1.3952, -1.5542],\n",
      "        [-1.4211, -1.2059, -1.3953, -1.5542],\n",
      "        [-1.4211, -1.2058, -1.3953, -1.5543],\n",
      "        [-1.4212, -1.2059, -1.3953, -1.5542],\n",
      "        [-1.4211, -1.2059, -1.3952, -1.5541],\n",
      "        [-1.4211, -1.2057, -1.3953, -1.5545],\n",
      "        [-1.4211, -1.2056, -1.3953, -1.5545],\n",
      "        [-1.4212, -1.2056, -1.3953, -1.5545],\n",
      "        [-1.4210, -1.2059, -1.3953, -1.5543],\n",
      "        [-1.4211, -1.2060, -1.3953, -1.5541],\n",
      "        [-1.4211, -1.2060, -1.3952, -1.5541],\n",
      "        [-1.4211, -1.2060, -1.3952, -1.5542],\n",
      "        [-1.4211, -1.2059, -1.3953, -1.5543],\n",
      "        [-1.4211, -1.2059, -1.3953, -1.5542],\n",
      "        [-1.4212, -1.2058, -1.3953, -1.5542],\n",
      "        [-1.4211, -1.2059, -1.3952, -1.5542],\n",
      "        [-1.4211, -1.2059, -1.3953, -1.5542],\n",
      "        [-1.4212, -1.2056, -1.3953, -1.5545],\n",
      "        [-1.4212, -1.2056, -1.3953, -1.5545],\n",
      "        [-1.4212, -1.2056, -1.3953, -1.5545],\n",
      "        [-1.4212, -1.2058, -1.3952, -1.5543]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([0.0072, 0.0061, 0.0037, 0.0020, 0.0065, 0.0061, 0.0038, 0.0016, 0.0076,\n",
      "        0.0045, 0.0029, 0.0016, 0.0059, 0.0046, 0.0031, 0.0015, 0.0079, 0.0047,\n",
      "        0.0036, 0.0020, 0.0066, 0.0049, 0.0033, 0.0015, 0.0063, 0.0047, 0.0037,\n",
      "        0.0018, 0.0075, 0.0044, 0.0033, 0.0016, 0.0074, 0.0062, 0.0036, 0.0018,\n",
      "        0.0071, 0.0051, 0.0036, 0.0021, 0.0081, 0.0062, 0.0043, 0.0020, 0.0073,\n",
      "        0.0048, 0.0032, 0.0017, 0.0060, 0.0055, 0.0040, 0.0013, 0.0070, 0.0046,\n",
      "        0.0037, 0.0019, 0.0071, 0.0046, 0.0035, 0.0016, 0.1541, 0.1541, 0.1541,\n",
      "        0.0015], grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(-0.0113, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2414, 0.2994, 0.2478, 0.2114],\n",
      "        [0.2414, 0.2994, 0.2478, 0.2114],\n",
      "        [0.2414, 0.2994, 0.2478, 0.2114],\n",
      "        [0.2414, 0.2994, 0.2478, 0.2114],\n",
      "        [0.2415, 0.2993, 0.2478, 0.2114],\n",
      "        [0.2414, 0.2994, 0.2478, 0.2113],\n",
      "        [0.2414, 0.2994, 0.2478, 0.2114],\n",
      "        [0.2414, 0.2994, 0.2478, 0.2114],\n",
      "        [0.2414, 0.2994, 0.2478, 0.2114],\n",
      "        [0.2414, 0.2994, 0.2478, 0.2114],\n",
      "        [0.2414, 0.2995, 0.2478, 0.2113],\n",
      "        [0.2415, 0.2994, 0.2478, 0.2114],\n",
      "        [0.2414, 0.2994, 0.2478, 0.2113],\n",
      "        [0.2414, 0.2994, 0.2478, 0.2114],\n",
      "        [0.2414, 0.2995, 0.2478, 0.2113],\n",
      "        [0.2414, 0.2995, 0.2478, 0.2113],\n",
      "        [0.2414, 0.2994, 0.2478, 0.2114],\n",
      "        [0.2414, 0.2994, 0.2478, 0.2114],\n",
      "        [0.2414, 0.2994, 0.2478, 0.2114],\n",
      "        [0.2415, 0.2994, 0.2478, 0.2114],\n",
      "        [0.2415, 0.2993, 0.2478, 0.2114],\n",
      "        [0.2415, 0.2994, 0.2478, 0.2114],\n",
      "        [0.2415, 0.2994, 0.2478, 0.2114],\n",
      "        [0.2415, 0.2994, 0.2478, 0.2114],\n",
      "        [0.2414, 0.2995, 0.2478, 0.2113],\n",
      "        [0.2414, 0.2995, 0.2478, 0.2113],\n",
      "        [0.2414, 0.2994, 0.2478, 0.2114],\n",
      "        [0.2414, 0.2994, 0.2478, 0.2114],\n",
      "        [0.2415, 0.2994, 0.2478, 0.2114],\n",
      "        [0.2415, 0.2994, 0.2478, 0.2114],\n",
      "        [0.2415, 0.2994, 0.2478, 0.2114],\n",
      "        [0.2415, 0.2993, 0.2478, 0.2114],\n",
      "        [0.2414, 0.2995, 0.2478, 0.2113],\n",
      "        [0.2414, 0.2995, 0.2478, 0.2113],\n",
      "        [0.2414, 0.2994, 0.2478, 0.2114],\n",
      "        [0.2414, 0.2995, 0.2478, 0.2113],\n",
      "        [0.2415, 0.2994, 0.2478, 0.2114],\n",
      "        [0.2414, 0.2994, 0.2478, 0.2114],\n",
      "        [0.2414, 0.2994, 0.2478, 0.2114],\n",
      "        [0.2414, 0.2995, 0.2478, 0.2113],\n",
      "        [0.2414, 0.2995, 0.2478, 0.2113],\n",
      "        [0.2414, 0.2994, 0.2478, 0.2113],\n",
      "        [0.2414, 0.2994, 0.2478, 0.2113],\n",
      "        [0.2414, 0.2994, 0.2478, 0.2114],\n",
      "        [0.2415, 0.2994, 0.2478, 0.2114],\n",
      "        [0.2414, 0.2995, 0.2478, 0.2113],\n",
      "        [0.2414, 0.2994, 0.2478, 0.2114],\n",
      "        [0.2414, 0.2994, 0.2478, 0.2114],\n",
      "        [0.2415, 0.2995, 0.2478, 0.2113],\n",
      "        [0.2414, 0.2995, 0.2478, 0.2113],\n",
      "        [0.2414, 0.2995, 0.2478, 0.2113],\n",
      "        [0.2415, 0.2994, 0.2478, 0.2113],\n",
      "        [0.2414, 0.2994, 0.2478, 0.2114],\n",
      "        [0.2414, 0.2994, 0.2478, 0.2114],\n",
      "        [0.2415, 0.2994, 0.2478, 0.2114],\n",
      "        [0.2415, 0.2994, 0.2478, 0.2113],\n",
      "        [0.2414, 0.2994, 0.2478, 0.2114],\n",
      "        [0.2414, 0.2994, 0.2478, 0.2114],\n",
      "        [0.2414, 0.2994, 0.2478, 0.2114],\n",
      "        [0.2414, 0.2994, 0.2478, 0.2114],\n",
      "        [0.2414, 0.2995, 0.2478, 0.2113],\n",
      "        [0.2414, 0.2995, 0.2478, 0.2113],\n",
      "        [0.2414, 0.2995, 0.2478, 0.2113],\n",
      "        [0.2414, 0.2995, 0.2478, 0.2113]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3784, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(-0.0243, grad_fn=<AddBackward0>)\n",
      "batching here\n",
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(0.0913, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.4145, -1.2279, -1.3911, -1.5359],\n",
      "        [-1.4146, -1.2277, -1.3911, -1.5361],\n",
      "        [-1.4146, -1.2277, -1.3911, -1.5362],\n",
      "        [-1.4146, -1.2277, -1.3911, -1.5361],\n",
      "        [-1.4147, -1.2278, -1.3911, -1.5359],\n",
      "        [-1.4145, -1.2280, -1.3911, -1.5358],\n",
      "        [-1.4147, -1.2279, -1.3912, -1.5358],\n",
      "        [-1.4146, -1.2278, -1.3911, -1.5360],\n",
      "        [-1.4147, -1.2275, -1.3911, -1.5362],\n",
      "        [-1.4146, -1.2277, -1.3911, -1.5362],\n",
      "        [-1.4146, -1.2276, -1.3911, -1.5362],\n",
      "        [-1.4147, -1.2275, -1.3911, -1.5363],\n",
      "        [-1.4147, -1.2275, -1.3911, -1.5363],\n",
      "        [-1.4147, -1.2275, -1.3911, -1.5363],\n",
      "        [-1.4146, -1.2275, -1.3911, -1.5363],\n",
      "        [-1.4147, -1.2276, -1.3911, -1.5362],\n",
      "        [-1.4146, -1.2278, -1.3911, -1.5360],\n",
      "        [-1.4146, -1.2278, -1.3912, -1.5359],\n",
      "        [-1.4147, -1.2278, -1.3911, -1.5359],\n",
      "        [-1.4146, -1.2278, -1.3911, -1.5360],\n",
      "        [-1.4147, -1.2277, -1.3911, -1.5360],\n",
      "        [-1.4146, -1.2279, -1.3911, -1.5359],\n",
      "        [-1.4147, -1.2277, -1.3911, -1.5361],\n",
      "        [-1.4147, -1.2276, -1.3911, -1.5362],\n",
      "        [-1.4147, -1.2277, -1.3911, -1.5360],\n",
      "        [-1.4146, -1.2277, -1.3911, -1.5361],\n",
      "        [-1.4146, -1.2278, -1.3911, -1.5360],\n",
      "        [-1.4147, -1.2276, -1.3911, -1.5361],\n",
      "        [-1.4146, -1.2278, -1.3911, -1.5360],\n",
      "        [-1.4146, -1.2278, -1.3911, -1.5361],\n",
      "        [-1.4146, -1.2277, -1.3911, -1.5361],\n",
      "        [-1.4146, -1.2277, -1.3911, -1.5361],\n",
      "        [-1.4146, -1.2277, -1.3912, -1.5361],\n",
      "        [-1.4146, -1.2275, -1.3912, -1.5363],\n",
      "        [-1.4146, -1.2274, -1.3912, -1.5365],\n",
      "        [-1.4147, -1.2275, -1.3911, -1.5364],\n",
      "        [-1.4146, -1.2277, -1.3911, -1.5361],\n",
      "        [-1.4147, -1.2277, -1.3911, -1.5360],\n",
      "        [-1.4146, -1.2277, -1.3911, -1.5360],\n",
      "        [-1.4147, -1.2277, -1.3911, -1.5362],\n",
      "        [-1.4146, -1.2278, -1.3911, -1.5360],\n",
      "        [-1.4146, -1.2279, -1.3911, -1.5358],\n",
      "        [-1.4146, -1.2279, -1.3911, -1.5358],\n",
      "        [-1.4146, -1.2279, -1.3911, -1.5359],\n",
      "        [-1.4147, -1.2278, -1.3911, -1.5359],\n",
      "        [-1.4146, -1.2278, -1.3911, -1.5360],\n",
      "        [-1.4146, -1.2279, -1.3911, -1.5358],\n",
      "        [-1.4147, -1.2276, -1.3911, -1.5361],\n",
      "        [-1.4146, -1.2275, -1.3911, -1.5363],\n",
      "        [-1.4146, -1.2276, -1.3911, -1.5363],\n",
      "        [-1.4146, -1.2277, -1.3911, -1.5361],\n",
      "        [-1.4146, -1.2276, -1.3911, -1.5363],\n",
      "        [-1.4146, -1.2277, -1.3912, -1.5361],\n",
      "        [-1.4146, -1.2278, -1.3912, -1.5360],\n",
      "        [-1.4147, -1.2276, -1.3911, -1.5362],\n",
      "        [-1.4147, -1.2276, -1.3911, -1.5362],\n",
      "        [-1.4146, -1.2277, -1.3911, -1.5360],\n",
      "        [-1.4147, -1.2277, -1.3911, -1.5362],\n",
      "        [-1.4146, -1.2276, -1.3911, -1.5362],\n",
      "        [-1.4146, -1.2277, -1.3911, -1.5361],\n",
      "        [-1.4147, -1.2276, -1.3911, -1.5362],\n",
      "        [-1.4146, -1.2277, -1.3911, -1.5362],\n",
      "        [-1.4147, -1.2275, -1.3911, -1.5363],\n",
      "        [-1.4147, -1.2275, -1.3911, -1.5363]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([ 5.0918e-03,  4.4420e-03,  2.7739e-03,  1.5451e-03,  6.2548e-03,\n",
      "         4.2254e-03,  2.4609e-03,  1.5437e-03,  5.9425e-03,  4.4814e-03,\n",
      "         2.5550e-03,  1.5108e-03,  6.0551e-03,  4.5448e-03,  2.9391e-03,\n",
      "         1.3355e-03,  6.5458e-03,  4.5001e-03,  2.7092e-03,  1.3175e-03,\n",
      "        -1.3575e+00, -1.3728e+00, -1.3883e+00,  1.5000e-03,  6.6348e-03,\n",
      "         5.1388e-03,  3.2488e-03,  1.6691e-03,  5.9051e-03,  3.7251e-03,\n",
      "         3.1395e-03,  1.3658e-03, -1.4993e+00, -1.2113e+00, -1.4114e+00,\n",
      "         1.7357e-03,  5.8123e-03,  4.2085e-03,  2.5652e-03,  1.4368e-03,\n",
      "         6.5689e-03,  4.4300e-03,  3.1280e-03,  1.6397e-03,  6.0475e-03,\n",
      "         5.0844e-03,  3.1118e-03,  1.7940e-03,  6.1036e-03,  3.9578e-03,\n",
      "         2.9094e-03,  1.4093e-03,  5.0930e-03,  4.4578e-03,  3.0860e-03,\n",
      "         1.4399e-03,  5.2008e-03,  4.8912e-03,  3.0469e-03,  1.6326e-03,\n",
      "         5.2366e-03,  3.6776e-03,  3.2424e-03,  1.6488e-03],\n",
      "       grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(0.1255, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2430, 0.2929, 0.2488, 0.2153],\n",
      "        [0.2430, 0.2930, 0.2488, 0.2152],\n",
      "        [0.2430, 0.2930, 0.2488, 0.2152],\n",
      "        [0.2430, 0.2930, 0.2488, 0.2152],\n",
      "        [0.2430, 0.2929, 0.2488, 0.2153],\n",
      "        [0.2430, 0.2929, 0.2488, 0.2153],\n",
      "        [0.2430, 0.2929, 0.2488, 0.2153],\n",
      "        [0.2430, 0.2929, 0.2488, 0.2152],\n",
      "        [0.2430, 0.2930, 0.2488, 0.2152],\n",
      "        [0.2430, 0.2930, 0.2488, 0.2152],\n",
      "        [0.2430, 0.2930, 0.2488, 0.2152],\n",
      "        [0.2430, 0.2930, 0.2488, 0.2152],\n",
      "        [0.2430, 0.2930, 0.2488, 0.2152],\n",
      "        [0.2430, 0.2930, 0.2488, 0.2152],\n",
      "        [0.2430, 0.2930, 0.2488, 0.2152],\n",
      "        [0.2430, 0.2930, 0.2488, 0.2152],\n",
      "        [0.2430, 0.2929, 0.2488, 0.2152],\n",
      "        [0.2430, 0.2929, 0.2488, 0.2153],\n",
      "        [0.2430, 0.2929, 0.2488, 0.2153],\n",
      "        [0.2430, 0.2929, 0.2488, 0.2152],\n",
      "        [0.2430, 0.2930, 0.2488, 0.2152],\n",
      "        [0.2430, 0.2929, 0.2488, 0.2153],\n",
      "        [0.2430, 0.2930, 0.2488, 0.2152],\n",
      "        [0.2430, 0.2930, 0.2488, 0.2152],\n",
      "        [0.2430, 0.2930, 0.2488, 0.2152],\n",
      "        [0.2430, 0.2930, 0.2488, 0.2152],\n",
      "        [0.2430, 0.2929, 0.2488, 0.2152],\n",
      "        [0.2430, 0.2930, 0.2488, 0.2152],\n",
      "        [0.2430, 0.2929, 0.2488, 0.2152],\n",
      "        [0.2430, 0.2929, 0.2488, 0.2152],\n",
      "        [0.2430, 0.2930, 0.2488, 0.2152],\n",
      "        [0.2430, 0.2930, 0.2488, 0.2152],\n",
      "        [0.2430, 0.2930, 0.2488, 0.2152],\n",
      "        [0.2430, 0.2930, 0.2488, 0.2152],\n",
      "        [0.2430, 0.2931, 0.2488, 0.2151],\n",
      "        [0.2430, 0.2930, 0.2488, 0.2152],\n",
      "        [0.2430, 0.2930, 0.2488, 0.2152],\n",
      "        [0.2430, 0.2930, 0.2488, 0.2152],\n",
      "        [0.2430, 0.2930, 0.2488, 0.2152],\n",
      "        [0.2430, 0.2930, 0.2488, 0.2152],\n",
      "        [0.2430, 0.2929, 0.2488, 0.2152],\n",
      "        [0.2430, 0.2929, 0.2488, 0.2153],\n",
      "        [0.2430, 0.2929, 0.2488, 0.2153],\n",
      "        [0.2430, 0.2929, 0.2488, 0.2153],\n",
      "        [0.2430, 0.2929, 0.2488, 0.2153],\n",
      "        [0.2430, 0.2929, 0.2488, 0.2152],\n",
      "        [0.2430, 0.2929, 0.2488, 0.2153],\n",
      "        [0.2430, 0.2930, 0.2488, 0.2152],\n",
      "        [0.2430, 0.2930, 0.2488, 0.2152],\n",
      "        [0.2430, 0.2930, 0.2488, 0.2152],\n",
      "        [0.2430, 0.2930, 0.2488, 0.2152],\n",
      "        [0.2430, 0.2930, 0.2488, 0.2152],\n",
      "        [0.2430, 0.2930, 0.2488, 0.2152],\n",
      "        [0.2430, 0.2929, 0.2488, 0.2152],\n",
      "        [0.2430, 0.2930, 0.2488, 0.2152],\n",
      "        [0.2430, 0.2930, 0.2488, 0.2152],\n",
      "        [0.2430, 0.2930, 0.2488, 0.2152],\n",
      "        [0.2430, 0.2930, 0.2488, 0.2152],\n",
      "        [0.2430, 0.2930, 0.2488, 0.2152],\n",
      "        [0.2430, 0.2930, 0.2488, 0.2152],\n",
      "        [0.2430, 0.2930, 0.2488, 0.2152],\n",
      "        [0.2430, 0.2930, 0.2488, 0.2152],\n",
      "        [0.2430, 0.2930, 0.2488, 0.2152],\n",
      "        [0.2430, 0.2930, 0.2488, 0.2152]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3802, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(0.2030, grad_fn=<AddBackward0>)\n",
      "batching here\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(0.0306, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.4216, -1.2301, -1.3712, -1.5483],\n",
      "        [-1.4215, -1.2302, -1.3712, -1.5482],\n",
      "        [-1.4216, -1.2300, -1.3712, -1.5484],\n",
      "        [-1.4216, -1.2300, -1.3712, -1.5484],\n",
      "        [-1.4216, -1.2300, -1.3712, -1.5483],\n",
      "        [-1.4216, -1.2301, -1.3712, -1.5483],\n",
      "        [-1.4216, -1.2301, -1.3713, -1.5483],\n",
      "        [-1.4216, -1.2301, -1.3712, -1.5483],\n",
      "        [-1.4216, -1.2300, -1.3712, -1.5484],\n",
      "        [-1.4216, -1.2300, -1.3712, -1.5484],\n",
      "        [-1.4216, -1.2300, -1.3712, -1.5484],\n",
      "        [-1.4216, -1.2301, -1.3712, -1.5483],\n",
      "        [-1.4217, -1.2298, -1.3713, -1.5485],\n",
      "        [-1.4216, -1.2299, -1.3713, -1.5484],\n",
      "        [-1.4216, -1.2300, -1.3713, -1.5483],\n",
      "        [-1.4216, -1.2299, -1.3713, -1.5484],\n",
      "        [-1.4215, -1.2302, -1.3712, -1.5482],\n",
      "        [-1.4215, -1.2302, -1.3712, -1.5482],\n",
      "        [-1.4216, -1.2302, -1.3712, -1.5483],\n",
      "        [-1.4216, -1.2301, -1.3712, -1.5484],\n",
      "        [-1.4216, -1.2301, -1.3712, -1.5483],\n",
      "        [-1.4216, -1.2301, -1.3712, -1.5484],\n",
      "        [-1.4215, -1.2301, -1.3712, -1.5483],\n",
      "        [-1.4216, -1.2301, -1.3712, -1.5484],\n",
      "        [-1.4215, -1.2302, -1.3713, -1.5481],\n",
      "        [-1.4216, -1.2299, -1.3712, -1.5485],\n",
      "        [-1.4216, -1.2301, -1.3712, -1.5484],\n",
      "        [-1.4216, -1.2300, -1.3712, -1.5484],\n",
      "        [-1.4216, -1.2300, -1.3712, -1.5485],\n",
      "        [-1.4216, -1.2301, -1.3712, -1.5483],\n",
      "        [-1.4216, -1.2301, -1.3712, -1.5484],\n",
      "        [-1.4216, -1.2300, -1.3712, -1.5484],\n",
      "        [-1.4216, -1.2298, -1.3713, -1.5486],\n",
      "        [-1.4215, -1.2300, -1.3712, -1.5486],\n",
      "        [-1.4216, -1.2299, -1.3712, -1.5486],\n",
      "        [-1.4215, -1.2300, -1.3712, -1.5485],\n",
      "        [-1.4216, -1.2300, -1.3712, -1.5484],\n",
      "        [-1.4216, -1.2301, -1.3712, -1.5483],\n",
      "        [-1.4216, -1.2301, -1.3712, -1.5483],\n",
      "        [-1.4216, -1.2301, -1.3712, -1.5483],\n",
      "        [-1.4215, -1.2303, -1.3712, -1.5482],\n",
      "        [-1.4215, -1.2302, -1.3712, -1.5483],\n",
      "        [-1.4216, -1.2300, -1.3712, -1.5484],\n",
      "        [-1.4216, -1.2301, -1.3712, -1.5482],\n",
      "        [-1.4215, -1.2302, -1.3712, -1.5482],\n",
      "        [-1.4216, -1.2301, -1.3712, -1.5484],\n",
      "        [-1.4216, -1.2301, -1.3712, -1.5483],\n",
      "        [-1.4216, -1.2300, -1.3712, -1.5484],\n",
      "        [-1.4216, -1.2300, -1.3711, -1.5485],\n",
      "        [-1.4215, -1.2302, -1.3712, -1.5483],\n",
      "        [-1.4215, -1.2301, -1.3712, -1.5484],\n",
      "        [-1.4214, -1.2303, -1.3712, -1.5483],\n",
      "        [-1.4216, -1.2300, -1.3712, -1.5484],\n",
      "        [-1.4216, -1.2301, -1.3712, -1.5483],\n",
      "        [-1.4216, -1.2300, -1.3712, -1.5484],\n",
      "        [-1.4216, -1.2300, -1.3712, -1.5484],\n",
      "        [-1.4216, -1.2301, -1.3712, -1.5483],\n",
      "        [-1.4216, -1.2301, -1.3712, -1.5483],\n",
      "        [-1.4216, -1.2300, -1.3712, -1.5484],\n",
      "        [-1.4216, -1.2300, -1.3712, -1.5484],\n",
      "        [-1.4216, -1.2300, -1.3712, -1.5485],\n",
      "        [-1.4216, -1.2300, -1.3712, -1.5485],\n",
      "        [-1.4216, -1.2300, -1.3712, -1.5484],\n",
      "        [-1.4215, -1.2301, -1.3712, -1.5484]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([ 0.0104,  0.0060,  0.0049,  0.0021,  0.0093,  0.0070,  0.0044,  0.0022,\n",
      "         0.0084,  0.0070,  0.0049,  0.0024,  0.0103,  0.0069,  0.0044,  0.0025,\n",
      "         0.0095,  0.0063,  0.0049,  0.0023,  0.0096,  0.0073,  0.0048,  0.0024,\n",
      "         0.0093,  0.0064,  0.0048,  0.0020, -1.3977, -1.5404,  0.0045,  0.0027,\n",
      "         0.0106,  0.0075,  0.0048,  0.0020,  0.0094,  0.0072,  0.0053,  0.0020,\n",
      "         0.0088,  0.0067,  0.0043,  0.0021,  0.0079,  0.0072,  0.0051,  0.0023,\n",
      "         0.0083,  0.0061,  0.0048,  0.0023,  0.0105,  0.0078,  0.0053,  0.0024,\n",
      "         0.0083,  0.0078,  0.0049,  0.0021,  0.0082,  0.0070,  0.0040,  0.0025],\n",
      "       grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(0.0404, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2413, 0.2923, 0.2538, 0.2126],\n",
      "        [0.2413, 0.2922, 0.2538, 0.2126],\n",
      "        [0.2413, 0.2923, 0.2538, 0.2126],\n",
      "        [0.2413, 0.2923, 0.2538, 0.2126],\n",
      "        [0.2413, 0.2923, 0.2538, 0.2126],\n",
      "        [0.2413, 0.2923, 0.2538, 0.2126],\n",
      "        [0.2413, 0.2923, 0.2538, 0.2126],\n",
      "        [0.2413, 0.2923, 0.2538, 0.2126],\n",
      "        [0.2413, 0.2923, 0.2538, 0.2126],\n",
      "        [0.2413, 0.2923, 0.2538, 0.2126],\n",
      "        [0.2413, 0.2923, 0.2538, 0.2126],\n",
      "        [0.2413, 0.2923, 0.2538, 0.2126],\n",
      "        [0.2413, 0.2923, 0.2538, 0.2126],\n",
      "        [0.2413, 0.2923, 0.2538, 0.2126],\n",
      "        [0.2413, 0.2923, 0.2538, 0.2126],\n",
      "        [0.2413, 0.2923, 0.2538, 0.2126],\n",
      "        [0.2413, 0.2922, 0.2538, 0.2126],\n",
      "        [0.2413, 0.2922, 0.2538, 0.2126],\n",
      "        [0.2413, 0.2922, 0.2538, 0.2126],\n",
      "        [0.2413, 0.2923, 0.2538, 0.2126],\n",
      "        [0.2413, 0.2923, 0.2538, 0.2126],\n",
      "        [0.2413, 0.2923, 0.2538, 0.2126],\n",
      "        [0.2413, 0.2923, 0.2538, 0.2126],\n",
      "        [0.2413, 0.2923, 0.2538, 0.2126],\n",
      "        [0.2413, 0.2922, 0.2538, 0.2126],\n",
      "        [0.2413, 0.2923, 0.2538, 0.2126],\n",
      "        [0.2413, 0.2923, 0.2538, 0.2126],\n",
      "        [0.2413, 0.2923, 0.2538, 0.2126],\n",
      "        [0.2413, 0.2923, 0.2538, 0.2126],\n",
      "        [0.2413, 0.2923, 0.2538, 0.2126],\n",
      "        [0.2413, 0.2923, 0.2538, 0.2126],\n",
      "        [0.2413, 0.2923, 0.2538, 0.2126],\n",
      "        [0.2413, 0.2923, 0.2538, 0.2125],\n",
      "        [0.2413, 0.2923, 0.2538, 0.2125],\n",
      "        [0.2413, 0.2923, 0.2538, 0.2126],\n",
      "        [0.2413, 0.2923, 0.2538, 0.2126],\n",
      "        [0.2413, 0.2923, 0.2538, 0.2126],\n",
      "        [0.2413, 0.2923, 0.2538, 0.2126],\n",
      "        [0.2413, 0.2923, 0.2538, 0.2126],\n",
      "        [0.2413, 0.2923, 0.2538, 0.2126],\n",
      "        [0.2413, 0.2922, 0.2538, 0.2126],\n",
      "        [0.2414, 0.2922, 0.2538, 0.2126],\n",
      "        [0.2413, 0.2923, 0.2538, 0.2126],\n",
      "        [0.2413, 0.2923, 0.2538, 0.2126],\n",
      "        [0.2413, 0.2922, 0.2538, 0.2126],\n",
      "        [0.2413, 0.2923, 0.2538, 0.2126],\n",
      "        [0.2413, 0.2923, 0.2538, 0.2126],\n",
      "        [0.2413, 0.2923, 0.2538, 0.2126],\n",
      "        [0.2413, 0.2923, 0.2538, 0.2126],\n",
      "        [0.2414, 0.2922, 0.2538, 0.2126],\n",
      "        [0.2413, 0.2923, 0.2538, 0.2126],\n",
      "        [0.2414, 0.2922, 0.2538, 0.2126],\n",
      "        [0.2413, 0.2923, 0.2538, 0.2126],\n",
      "        [0.2413, 0.2923, 0.2538, 0.2126],\n",
      "        [0.2413, 0.2923, 0.2538, 0.2126],\n",
      "        [0.2413, 0.2923, 0.2538, 0.2126],\n",
      "        [0.2413, 0.2923, 0.2538, 0.2126],\n",
      "        [0.2413, 0.2923, 0.2538, 0.2126],\n",
      "        [0.2413, 0.2923, 0.2538, 0.2126],\n",
      "        [0.2413, 0.2923, 0.2538, 0.2126],\n",
      "        [0.2413, 0.2923, 0.2538, 0.2126],\n",
      "        [0.2413, 0.2923, 0.2538, 0.2126],\n",
      "        [0.2413, 0.2923, 0.2538, 0.2126],\n",
      "        [0.2413, 0.2923, 0.2538, 0.2126]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3798, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(0.0572, grad_fn=<AddBackward0>)\n",
      "batching here\n",
      "51: done 3 episodes, mean_reward=0.00, best_reward=2.00, speed=207.52\n",
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(0.0084, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.4004, -1.2675, -1.3875, -1.5038],\n",
      "        [-1.4004, -1.2675, -1.3875, -1.5038],\n",
      "        [-1.4004, -1.2675, -1.3875, -1.5038],\n",
      "        [-1.4004, -1.2676, -1.3875, -1.5037],\n",
      "        [-1.4005, -1.2675, -1.3875, -1.5039],\n",
      "        [-1.4005, -1.2675, -1.3875, -1.5038],\n",
      "        [-1.4005, -1.2675, -1.3875, -1.5038],\n",
      "        [-1.4005, -1.2676, -1.3875, -1.5037],\n",
      "        [-1.4005, -1.2675, -1.3875, -1.5038],\n",
      "        [-1.4005, -1.2676, -1.3875, -1.5037],\n",
      "        [-1.4005, -1.2676, -1.3875, -1.5037],\n",
      "        [-1.4005, -1.2674, -1.3875, -1.5039],\n",
      "        [-1.4005, -1.2674, -1.3875, -1.5039],\n",
      "        [-1.4005, -1.2675, -1.3876, -1.5037],\n",
      "        [-1.4005, -1.2674, -1.3876, -1.5039],\n",
      "        [-1.4005, -1.2674, -1.3875, -1.5038],\n",
      "        [-1.4004, -1.2676, -1.3875, -1.5037],\n",
      "        [-1.4004, -1.2676, -1.3875, -1.5037],\n",
      "        [-1.4004, -1.2676, -1.3875, -1.5037],\n",
      "        [-1.4004, -1.2676, -1.3875, -1.5038],\n",
      "        [-1.4005, -1.2676, -1.3875, -1.5037],\n",
      "        [-1.4004, -1.2676, -1.3875, -1.5037],\n",
      "        [-1.4005, -1.2676, -1.3874, -1.5037],\n",
      "        [-1.4004, -1.2677, -1.3875, -1.5037],\n",
      "        [-1.4005, -1.2675, -1.3875, -1.5038],\n",
      "        [-1.4005, -1.2675, -1.3875, -1.5038],\n",
      "        [-1.4005, -1.2675, -1.3875, -1.5038],\n",
      "        [-1.4005, -1.2675, -1.3875, -1.5038],\n",
      "        [-1.4005, -1.2675, -1.3875, -1.5038],\n",
      "        [-1.4005, -1.2676, -1.3874, -1.5037],\n",
      "        [-1.4005, -1.2675, -1.3875, -1.5037],\n",
      "        [-1.4005, -1.2676, -1.3875, -1.5037],\n",
      "        [-1.4003, -1.2676, -1.3875, -1.5038],\n",
      "        [-1.4003, -1.2676, -1.3875, -1.5037],\n",
      "        [-1.4003, -1.2676, -1.3876, -1.5038],\n",
      "        [-1.4002, -1.2677, -1.3875, -1.5038],\n",
      "        [-1.4005, -1.2675, -1.3875, -1.5038],\n",
      "        [-1.4005, -1.2675, -1.3875, -1.5038],\n",
      "        [-1.4005, -1.2675, -1.3875, -1.5038],\n",
      "        [-1.4005, -1.2675, -1.3875, -1.5038],\n",
      "        [-1.4005, -1.2675, -1.3875, -1.5038],\n",
      "        [-1.4005, -1.2675, -1.3875, -1.5038],\n",
      "        [-1.4005, -1.2675, -1.3875, -1.5038],\n",
      "        [-1.4004, -1.2675, -1.3875, -1.5038],\n",
      "        [-1.4005, -1.2675, -1.3874, -1.5038],\n",
      "        [-1.4005, -1.2675, -1.3874, -1.5038],\n",
      "        [-1.4004, -1.2676, -1.3875, -1.5037],\n",
      "        [-1.4004, -1.2676, -1.3875, -1.5038],\n",
      "        [-1.4004, -1.2675, -1.3875, -1.5038],\n",
      "        [-1.4004, -1.2676, -1.3874, -1.5038],\n",
      "        [-1.4004, -1.2675, -1.3875, -1.5038],\n",
      "        [-1.4004, -1.2675, -1.3875, -1.5039],\n",
      "        [-1.4005, -1.2675, -1.3875, -1.5038],\n",
      "        [-1.4005, -1.2675, -1.3875, -1.5038],\n",
      "        [-1.4005, -1.2675, -1.3875, -1.5038],\n",
      "        [-1.4005, -1.2675, -1.3875, -1.5038],\n",
      "        [-1.4005, -1.2675, -1.3875, -1.5038],\n",
      "        [-1.4005, -1.2675, -1.3875, -1.5038],\n",
      "        [-1.4005, -1.2675, -1.3875, -1.5038],\n",
      "        [-1.4005, -1.2675, -1.3875, -1.5038],\n",
      "        [-1.4004, -1.2676, -1.3875, -1.5037],\n",
      "        [-1.4004, -1.2676, -1.3875, -1.5038],\n",
      "        [-1.4004, -1.2677, -1.3876, -1.5036],\n",
      "        [-1.4004, -1.2676, -1.3875, -1.5038]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([0.3676, 0.3098, 0.3392, 0.0033, 0.0122, 0.0099, 0.0061, 0.0028, 0.0152,\n",
      "        0.0113, 0.0070, 0.0045, 0.0135, 0.0095, 0.0062, 0.0031, 0.0133, 0.0092,\n",
      "        0.0068, 0.0037, 0.0126, 0.0105, 0.0077, 0.0037, 0.0139, 0.0104, 0.0064,\n",
      "        0.0038, 0.0147, 0.0093, 0.0069, 0.0033, 0.0148, 0.0098, 0.0065, 0.0033,\n",
      "        0.0133, 0.0101, 0.0068, 0.0034, 0.0123, 0.0106, 0.0068, 0.0033, 0.0146,\n",
      "        0.0110, 0.0072, 0.0033, 0.0131, 0.0096, 0.0071, 0.0033, 0.3098, 0.3423,\n",
      "        0.3099, 0.0035, 0.3676, 0.3391, 0.3676, 0.0037, 0.0135, 0.0100, 0.0058,\n",
      "        0.0035], grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(-0.0548, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2465, 0.2815, 0.2497, 0.2223],\n",
      "        [0.2465, 0.2815, 0.2497, 0.2223],\n",
      "        [0.2465, 0.2815, 0.2497, 0.2223],\n",
      "        [0.2465, 0.2815, 0.2497, 0.2223],\n",
      "        [0.2465, 0.2815, 0.2497, 0.2223],\n",
      "        [0.2465, 0.2815, 0.2497, 0.2223],\n",
      "        [0.2465, 0.2815, 0.2497, 0.2223],\n",
      "        [0.2465, 0.2815, 0.2497, 0.2223],\n",
      "        [0.2465, 0.2815, 0.2497, 0.2223],\n",
      "        [0.2465, 0.2815, 0.2497, 0.2223],\n",
      "        [0.2465, 0.2815, 0.2497, 0.2223],\n",
      "        [0.2465, 0.2816, 0.2497, 0.2223],\n",
      "        [0.2465, 0.2816, 0.2497, 0.2223],\n",
      "        [0.2465, 0.2815, 0.2497, 0.2223],\n",
      "        [0.2465, 0.2816, 0.2497, 0.2223],\n",
      "        [0.2465, 0.2816, 0.2497, 0.2223],\n",
      "        [0.2465, 0.2815, 0.2497, 0.2223],\n",
      "        [0.2465, 0.2815, 0.2497, 0.2223],\n",
      "        [0.2465, 0.2815, 0.2497, 0.2223],\n",
      "        [0.2465, 0.2815, 0.2497, 0.2223],\n",
      "        [0.2465, 0.2815, 0.2497, 0.2223],\n",
      "        [0.2465, 0.2815, 0.2497, 0.2223],\n",
      "        [0.2465, 0.2815, 0.2497, 0.2223],\n",
      "        [0.2465, 0.2815, 0.2497, 0.2223],\n",
      "        [0.2465, 0.2815, 0.2497, 0.2223],\n",
      "        [0.2465, 0.2815, 0.2497, 0.2223],\n",
      "        [0.2465, 0.2815, 0.2497, 0.2223],\n",
      "        [0.2465, 0.2815, 0.2497, 0.2223],\n",
      "        [0.2465, 0.2815, 0.2497, 0.2223],\n",
      "        [0.2465, 0.2815, 0.2497, 0.2223],\n",
      "        [0.2465, 0.2815, 0.2497, 0.2223],\n",
      "        [0.2465, 0.2815, 0.2497, 0.2223],\n",
      "        [0.2465, 0.2815, 0.2497, 0.2223],\n",
      "        [0.2465, 0.2815, 0.2497, 0.2223],\n",
      "        [0.2465, 0.2815, 0.2497, 0.2223],\n",
      "        [0.2465, 0.2815, 0.2497, 0.2223],\n",
      "        [0.2465, 0.2815, 0.2497, 0.2223],\n",
      "        [0.2465, 0.2815, 0.2497, 0.2223],\n",
      "        [0.2465, 0.2815, 0.2497, 0.2223],\n",
      "        [0.2465, 0.2815, 0.2497, 0.2223],\n",
      "        [0.2465, 0.2815, 0.2497, 0.2223],\n",
      "        [0.2465, 0.2815, 0.2497, 0.2223],\n",
      "        [0.2465, 0.2815, 0.2497, 0.2223],\n",
      "        [0.2465, 0.2815, 0.2497, 0.2223],\n",
      "        [0.2465, 0.2815, 0.2497, 0.2223],\n",
      "        [0.2465, 0.2815, 0.2497, 0.2223],\n",
      "        [0.2465, 0.2815, 0.2497, 0.2223],\n",
      "        [0.2465, 0.2815, 0.2497, 0.2223],\n",
      "        [0.2465, 0.2815, 0.2497, 0.2223],\n",
      "        [0.2465, 0.2815, 0.2497, 0.2223],\n",
      "        [0.2465, 0.2815, 0.2497, 0.2223],\n",
      "        [0.2465, 0.2815, 0.2497, 0.2223],\n",
      "        [0.2465, 0.2815, 0.2497, 0.2223],\n",
      "        [0.2465, 0.2815, 0.2497, 0.2223],\n",
      "        [0.2465, 0.2815, 0.2497, 0.2223],\n",
      "        [0.2465, 0.2815, 0.2497, 0.2223],\n",
      "        [0.2465, 0.2815, 0.2497, 0.2223],\n",
      "        [0.2465, 0.2815, 0.2497, 0.2223],\n",
      "        [0.2465, 0.2815, 0.2497, 0.2223],\n",
      "        [0.2465, 0.2815, 0.2497, 0.2223],\n",
      "        [0.2465, 0.2815, 0.2497, 0.2223],\n",
      "        [0.2465, 0.2815, 0.2497, 0.2223],\n",
      "        [0.2465, 0.2815, 0.2497, 0.2223],\n",
      "        [0.2465, 0.2815, 0.2497, 0.2223]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3828, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(-0.0601, grad_fn=<AddBackward0>)\n",
      "batching here\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52: done 3 episodes, mean_reward=0.00, best_reward=2.00, speed=208.73\n",
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(0.0494, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.3935, -1.2754, -1.3880, -1.5010],\n",
      "        [-1.3935, -1.2755, -1.3879, -1.5010],\n",
      "        [-1.3935, -1.2756, -1.3879, -1.5009],\n",
      "        [-1.3935, -1.2755, -1.3879, -1.5009],\n",
      "        [-1.3935, -1.2754, -1.3879, -1.5011],\n",
      "        [-1.3936, -1.2754, -1.3880, -1.5010],\n",
      "        [-1.3936, -1.2754, -1.3880, -1.5010],\n",
      "        [-1.3936, -1.2755, -1.3879, -1.5009],\n",
      "        [-1.3935, -1.2756, -1.3880, -1.5008],\n",
      "        [-1.3936, -1.2755, -1.3879, -1.5009],\n",
      "        [-1.3936, -1.2755, -1.3879, -1.5009],\n",
      "        [-1.3936, -1.2755, -1.3879, -1.5009],\n",
      "        [-1.3935, -1.2754, -1.3880, -1.5010],\n",
      "        [-1.3936, -1.2754, -1.3880, -1.5010],\n",
      "        [-1.3936, -1.2753, -1.3879, -1.5010],\n",
      "        [-1.3936, -1.2754, -1.3880, -1.5010],\n",
      "        [-1.3935, -1.2755, -1.3880, -1.5010],\n",
      "        [-1.3935, -1.2755, -1.3879, -1.5010],\n",
      "        [-1.3935, -1.2754, -1.3879, -1.5010],\n",
      "        [-1.3935, -1.2755, -1.3879, -1.5010],\n",
      "        [-1.3936, -1.2756, -1.3879, -1.5008],\n",
      "        [-1.3936, -1.2756, -1.3879, -1.5009],\n",
      "        [-1.3935, -1.2757, -1.3879, -1.5007],\n",
      "        [-1.3935, -1.2756, -1.3879, -1.5009],\n",
      "        [-1.3936, -1.2754, -1.3880, -1.5010],\n",
      "        [-1.3935, -1.2754, -1.3879, -1.5010],\n",
      "        [-1.3936, -1.2755, -1.3879, -1.5009],\n",
      "        [-1.3936, -1.2755, -1.3880, -1.5009],\n",
      "        [-1.3936, -1.2755, -1.3879, -1.5009],\n",
      "        [-1.3935, -1.2756, -1.3879, -1.5008],\n",
      "        [-1.3936, -1.2756, -1.3879, -1.5009],\n",
      "        [-1.3935, -1.2755, -1.3879, -1.5010],\n",
      "        [-1.3935, -1.2755, -1.3880, -1.5009],\n",
      "        [-1.3935, -1.2754, -1.3880, -1.5009],\n",
      "        [-1.3935, -1.2754, -1.3880, -1.5010],\n",
      "        [-1.3935, -1.2754, -1.3880, -1.5010],\n",
      "        [-1.3935, -1.2754, -1.3879, -1.5010],\n",
      "        [-1.3936, -1.2755, -1.3879, -1.5009],\n",
      "        [-1.3936, -1.2756, -1.3879, -1.5009],\n",
      "        [-1.3936, -1.2756, -1.3879, -1.5008],\n",
      "        [-1.3935, -1.2754, -1.3879, -1.5010],\n",
      "        [-1.3936, -1.2754, -1.3879, -1.5010],\n",
      "        [-1.3936, -1.2754, -1.3879, -1.5010],\n",
      "        [-1.3936, -1.2754, -1.3879, -1.5010],\n",
      "        [-1.3935, -1.2755, -1.3879, -1.5010],\n",
      "        [-1.3936, -1.2754, -1.3879, -1.5010],\n",
      "        [-1.3935, -1.2754, -1.3879, -1.5010],\n",
      "        [-1.3935, -1.2754, -1.3879, -1.5010],\n",
      "        [-1.3935, -1.2753, -1.3880, -1.5012],\n",
      "        [-1.3935, -1.2754, -1.3880, -1.5011],\n",
      "        [-1.3934, -1.2754, -1.3880, -1.5011],\n",
      "        [-1.3935, -1.2753, -1.3880, -1.5012],\n",
      "        [-1.3936, -1.2754, -1.3879, -1.5010],\n",
      "        [-1.3936, -1.2755, -1.3879, -1.5009],\n",
      "        [-1.3936, -1.2754, -1.3879, -1.5010],\n",
      "        [-1.3936, -1.2755, -1.3879, -1.5010],\n",
      "        [-1.3936, -1.2756, -1.3879, -1.5009],\n",
      "        [-1.3936, -1.2756, -1.3879, -1.5008],\n",
      "        [-1.3935, -1.2755, -1.3879, -1.5009],\n",
      "        [-1.3936, -1.2755, -1.3880, -1.5009],\n",
      "        [-1.3936, -1.2754, -1.3879, -1.5011],\n",
      "        [-1.3936, -1.2753, -1.3879, -1.5011],\n",
      "        [-1.3936, -1.2753, -1.3879, -1.5012],\n",
      "        [-1.3936, -1.2753, -1.3879, -1.5011]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([ 0.0088,  0.0071,  0.0051,  0.0022,  0.0089,  0.0078,  0.0052,  0.0021,\n",
      "         0.0096,  0.0068,  0.0048,  0.0021,  0.0091,  0.0066,  0.0043,  0.0022,\n",
      "         0.2306,  0.2120,  0.2122,  0.2308,  0.0089,  0.0073,  0.0044,  0.0023,\n",
      "         0.0083,  0.0076,  0.0044,  0.0021,  0.0099,  0.0067,  0.0046,  0.0024,\n",
      "         0.0089,  0.0070,  0.0041,  0.0018,  0.2121,  0.0072,  0.0043,  0.0023,\n",
      "         0.0083,  0.0068,  0.0042,  0.0021,  0.2308,  0.2317,  0.2120,  0.2120,\n",
      "        -1.3568, -1.2564, -1.3891,  0.0023,  0.0091,  0.0066,  0.0044,  0.0023,\n",
      "         0.0089,  0.0060,  0.0046,  0.0020,  0.0098,  0.0073,  0.0046,  0.0023],\n",
      "       grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(0.0270, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2482, 0.2793, 0.2496, 0.2229],\n",
      "        [0.2482, 0.2793, 0.2496, 0.2229],\n",
      "        [0.2482, 0.2793, 0.2496, 0.2229],\n",
      "        [0.2482, 0.2793, 0.2496, 0.2229],\n",
      "        [0.2482, 0.2793, 0.2496, 0.2229],\n",
      "        [0.2482, 0.2793, 0.2496, 0.2229],\n",
      "        [0.2482, 0.2793, 0.2496, 0.2229],\n",
      "        [0.2482, 0.2793, 0.2496, 0.2229],\n",
      "        [0.2482, 0.2793, 0.2496, 0.2229],\n",
      "        [0.2482, 0.2793, 0.2496, 0.2229],\n",
      "        [0.2482, 0.2793, 0.2496, 0.2229],\n",
      "        [0.2482, 0.2793, 0.2496, 0.2229],\n",
      "        [0.2482, 0.2793, 0.2496, 0.2229],\n",
      "        [0.2482, 0.2793, 0.2496, 0.2229],\n",
      "        [0.2482, 0.2793, 0.2496, 0.2229],\n",
      "        [0.2482, 0.2793, 0.2496, 0.2229],\n",
      "        [0.2482, 0.2793, 0.2496, 0.2229],\n",
      "        [0.2482, 0.2793, 0.2496, 0.2229],\n",
      "        [0.2482, 0.2793, 0.2496, 0.2229],\n",
      "        [0.2482, 0.2793, 0.2496, 0.2229],\n",
      "        [0.2482, 0.2793, 0.2496, 0.2229],\n",
      "        [0.2482, 0.2793, 0.2496, 0.2229],\n",
      "        [0.2482, 0.2792, 0.2496, 0.2230],\n",
      "        [0.2482, 0.2793, 0.2496, 0.2229],\n",
      "        [0.2482, 0.2793, 0.2496, 0.2229],\n",
      "        [0.2482, 0.2793, 0.2496, 0.2229],\n",
      "        [0.2482, 0.2793, 0.2496, 0.2229],\n",
      "        [0.2482, 0.2793, 0.2496, 0.2229],\n",
      "        [0.2482, 0.2793, 0.2496, 0.2229],\n",
      "        [0.2482, 0.2793, 0.2496, 0.2230],\n",
      "        [0.2482, 0.2793, 0.2496, 0.2229],\n",
      "        [0.2482, 0.2793, 0.2496, 0.2229],\n",
      "        [0.2482, 0.2793, 0.2496, 0.2229],\n",
      "        [0.2482, 0.2793, 0.2496, 0.2229],\n",
      "        [0.2482, 0.2793, 0.2496, 0.2229],\n",
      "        [0.2482, 0.2793, 0.2496, 0.2229],\n",
      "        [0.2482, 0.2793, 0.2496, 0.2229],\n",
      "        [0.2482, 0.2793, 0.2496, 0.2229],\n",
      "        [0.2482, 0.2793, 0.2496, 0.2229],\n",
      "        [0.2482, 0.2793, 0.2496, 0.2229],\n",
      "        [0.2482, 0.2793, 0.2496, 0.2229],\n",
      "        [0.2482, 0.2793, 0.2496, 0.2229],\n",
      "        [0.2482, 0.2793, 0.2496, 0.2229],\n",
      "        [0.2482, 0.2793, 0.2496, 0.2229],\n",
      "        [0.2482, 0.2793, 0.2496, 0.2229],\n",
      "        [0.2482, 0.2793, 0.2496, 0.2229],\n",
      "        [0.2482, 0.2793, 0.2496, 0.2229],\n",
      "        [0.2482, 0.2793, 0.2496, 0.2229],\n",
      "        [0.2482, 0.2793, 0.2496, 0.2229],\n",
      "        [0.2482, 0.2793, 0.2496, 0.2229],\n",
      "        [0.2482, 0.2793, 0.2496, 0.2229],\n",
      "        [0.2482, 0.2793, 0.2496, 0.2229],\n",
      "        [0.2482, 0.2793, 0.2496, 0.2229],\n",
      "        [0.2482, 0.2793, 0.2496, 0.2229],\n",
      "        [0.2482, 0.2793, 0.2496, 0.2229],\n",
      "        [0.2482, 0.2793, 0.2496, 0.2229],\n",
      "        [0.2482, 0.2793, 0.2496, 0.2229],\n",
      "        [0.2482, 0.2793, 0.2496, 0.2229],\n",
      "        [0.2482, 0.2793, 0.2496, 0.2229],\n",
      "        [0.2482, 0.2793, 0.2496, 0.2229],\n",
      "        [0.2482, 0.2793, 0.2496, 0.2229],\n",
      "        [0.2482, 0.2793, 0.2496, 0.2229],\n",
      "        [0.2482, 0.2793, 0.2496, 0.2229],\n",
      "        [0.2482, 0.2793, 0.2496, 0.2229]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3831, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(0.0626, grad_fn=<AddBackward0>)\n",
      "batching here\n",
      "53: done 1 episodes, mean_reward=0.00, best_reward=2.00, speed=206.86\n",
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.3549, -1.2795, -1.4106, -1.5149],\n",
      "        [-1.3548, -1.2795, -1.4106, -1.5149],\n",
      "        [-1.3549, -1.2796, -1.4105, -1.5148],\n",
      "        [-1.3548, -1.2796, -1.4105, -1.5148],\n",
      "        [-1.3548, -1.2795, -1.4105, -1.5150],\n",
      "        [-1.3549, -1.2794, -1.4106, -1.5150],\n",
      "        [-1.3549, -1.2795, -1.4105, -1.5150],\n",
      "        [-1.3549, -1.2795, -1.4105, -1.5149],\n",
      "        [-1.3549, -1.2796, -1.4105, -1.5148],\n",
      "        [-1.3549, -1.2796, -1.4105, -1.5148],\n",
      "        [-1.3550, -1.2797, -1.4105, -1.5147],\n",
      "        [-1.3549, -1.2796, -1.4105, -1.5148],\n",
      "        [-1.3549, -1.2794, -1.4106, -1.5150],\n",
      "        [-1.3549, -1.2794, -1.4105, -1.5151],\n",
      "        [-1.3549, -1.2794, -1.4105, -1.5150],\n",
      "        [-1.3549, -1.2794, -1.4106, -1.5150],\n",
      "        [-1.3548, -1.2795, -1.4105, -1.5150],\n",
      "        [-1.3548, -1.2795, -1.4105, -1.5150],\n",
      "        [-1.3549, -1.2795, -1.4105, -1.5149],\n",
      "        [-1.3549, -1.2795, -1.4105, -1.5149],\n",
      "        [-1.3549, -1.2796, -1.4104, -1.5149],\n",
      "        [-1.3549, -1.2796, -1.4104, -1.5149],\n",
      "        [-1.3548, -1.2794, -1.4105, -1.5151],\n",
      "        [-1.3549, -1.2796, -1.4104, -1.5148],\n",
      "        [-1.3548, -1.2794, -1.4106, -1.5150],\n",
      "        [-1.3549, -1.2796, -1.4105, -1.5148],\n",
      "        [-1.3549, -1.2794, -1.4106, -1.5149],\n",
      "        [-1.3549, -1.2795, -1.4105, -1.5149],\n",
      "        [-1.3548, -1.2796, -1.4105, -1.5149],\n",
      "        [-1.3550, -1.2796, -1.4103, -1.5148],\n",
      "        [-1.3550, -1.2796, -1.4104, -1.5148],\n",
      "        [-1.3550, -1.2796, -1.4104, -1.5148],\n",
      "        [-1.3548, -1.2793, -1.4106, -1.5151],\n",
      "        [-1.3548, -1.2793, -1.4107, -1.5150],\n",
      "        [-1.3548, -1.2793, -1.4106, -1.5151],\n",
      "        [-1.3548, -1.2793, -1.4107, -1.5150],\n",
      "        [-1.3549, -1.2796, -1.4105, -1.5149],\n",
      "        [-1.3549, -1.2795, -1.4105, -1.5148],\n",
      "        [-1.3548, -1.2795, -1.4105, -1.5149],\n",
      "        [-1.3549, -1.2795, -1.4105, -1.5150],\n",
      "        [-1.3549, -1.2795, -1.4105, -1.5150],\n",
      "        [-1.3549, -1.2795, -1.4105, -1.5150],\n",
      "        [-1.3549, -1.2795, -1.4105, -1.5149],\n",
      "        [-1.3549, -1.2796, -1.4105, -1.5148],\n",
      "        [-1.3548, -1.2795, -1.4105, -1.5149],\n",
      "        [-1.3548, -1.2794, -1.4106, -1.5150],\n",
      "        [-1.3548, -1.2795, -1.4106, -1.5150],\n",
      "        [-1.3549, -1.2795, -1.4105, -1.5148],\n",
      "        [-1.3547, -1.2794, -1.4106, -1.5152],\n",
      "        [-1.3547, -1.2794, -1.4106, -1.5152],\n",
      "        [-1.3548, -1.2793, -1.4106, -1.5153],\n",
      "        [-1.3547, -1.2795, -1.4106, -1.5151],\n",
      "        [-1.3549, -1.2795, -1.4106, -1.5149],\n",
      "        [-1.3549, -1.2794, -1.4105, -1.5150],\n",
      "        [-1.3549, -1.2795, -1.4105, -1.5149],\n",
      "        [-1.3548, -1.2795, -1.4106, -1.5149],\n",
      "        [-1.3548, -1.2795, -1.4105, -1.5149],\n",
      "        [-1.3549, -1.2795, -1.4105, -1.5150],\n",
      "        [-1.3549, -1.2795, -1.4105, -1.5149],\n",
      "        [-1.3549, -1.2795, -1.4105, -1.5149],\n",
      "        [-1.3548, -1.2794, -1.4105, -1.5150],\n",
      "        [-1.3548, -1.2794, -1.4105, -1.5151],\n",
      "        [-1.3549, -1.2794, -1.4106, -1.5150],\n",
      "        [-1.3548, -1.2794, -1.4106, -1.5150]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([0.0111, 0.0095, 0.0054, 0.0027, 0.0106, 0.0098, 0.0055, 0.0029, 0.0108,\n",
      "        0.0083, 0.0057, 0.0033, 0.0119, 0.0095, 0.0058, 0.0028, 0.0105, 0.0092,\n",
      "        0.0051, 0.0032, 0.0109, 0.0082, 0.0058, 0.0034, 0.0109, 0.0078, 0.0055,\n",
      "        0.0028, 0.0116, 0.0096, 0.0055, 0.0028, 0.0112, 0.0077, 0.0063, 0.0026,\n",
      "        0.0110, 0.0086, 0.0055, 0.0027, 0.3184, 0.0093, 0.0056, 0.0033, 0.0111,\n",
      "        0.0088, 0.0063, 0.0025, 0.0108, 0.0090, 0.0064, 0.0030, 0.0123, 0.0090,\n",
      "        0.0053, 0.0026, 0.0106, 0.0079, 0.0052, 0.0026, 0.0116, 0.0084, 0.0060,\n",
      "        0.0032], grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(-0.0119, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2580, 0.2782, 0.2440, 0.2198],\n",
      "        [0.2580, 0.2782, 0.2440, 0.2198],\n",
      "        [0.2580, 0.2782, 0.2440, 0.2199],\n",
      "        [0.2580, 0.2782, 0.2440, 0.2198],\n",
      "        [0.2580, 0.2782, 0.2440, 0.2198],\n",
      "        [0.2580, 0.2782, 0.2440, 0.2198],\n",
      "        [0.2580, 0.2782, 0.2440, 0.2198],\n",
      "        [0.2580, 0.2782, 0.2440, 0.2198],\n",
      "        [0.2580, 0.2781, 0.2440, 0.2199],\n",
      "        [0.2580, 0.2782, 0.2440, 0.2198],\n",
      "        [0.2580, 0.2781, 0.2440, 0.2199],\n",
      "        [0.2580, 0.2782, 0.2440, 0.2198],\n",
      "        [0.2580, 0.2782, 0.2440, 0.2198],\n",
      "        [0.2580, 0.2782, 0.2440, 0.2198],\n",
      "        [0.2580, 0.2782, 0.2440, 0.2198],\n",
      "        [0.2580, 0.2782, 0.2440, 0.2198],\n",
      "        [0.2580, 0.2782, 0.2440, 0.2198],\n",
      "        [0.2580, 0.2782, 0.2440, 0.2198],\n",
      "        [0.2580, 0.2782, 0.2440, 0.2198],\n",
      "        [0.2580, 0.2782, 0.2440, 0.2198],\n",
      "        [0.2580, 0.2782, 0.2440, 0.2198],\n",
      "        [0.2580, 0.2782, 0.2440, 0.2198],\n",
      "        [0.2580, 0.2782, 0.2440, 0.2198],\n",
      "        [0.2580, 0.2781, 0.2440, 0.2199],\n",
      "        [0.2580, 0.2782, 0.2440, 0.2198],\n",
      "        [0.2580, 0.2781, 0.2440, 0.2199],\n",
      "        [0.2580, 0.2782, 0.2440, 0.2198],\n",
      "        [0.2580, 0.2782, 0.2440, 0.2198],\n",
      "        [0.2580, 0.2781, 0.2440, 0.2198],\n",
      "        [0.2580, 0.2781, 0.2441, 0.2198],\n",
      "        [0.2580, 0.2781, 0.2440, 0.2199],\n",
      "        [0.2580, 0.2781, 0.2440, 0.2199],\n",
      "        [0.2580, 0.2782, 0.2440, 0.2198],\n",
      "        [0.2580, 0.2782, 0.2440, 0.2198],\n",
      "        [0.2580, 0.2782, 0.2440, 0.2198],\n",
      "        [0.2580, 0.2782, 0.2440, 0.2198],\n",
      "        [0.2580, 0.2782, 0.2440, 0.2198],\n",
      "        [0.2580, 0.2782, 0.2440, 0.2198],\n",
      "        [0.2580, 0.2782, 0.2440, 0.2198],\n",
      "        [0.2580, 0.2782, 0.2440, 0.2198],\n",
      "        [0.2580, 0.2782, 0.2440, 0.2198],\n",
      "        [0.2580, 0.2782, 0.2440, 0.2198],\n",
      "        [0.2580, 0.2782, 0.2440, 0.2198],\n",
      "        [0.2580, 0.2782, 0.2440, 0.2198],\n",
      "        [0.2580, 0.2782, 0.2440, 0.2198],\n",
      "        [0.2580, 0.2782, 0.2440, 0.2198],\n",
      "        [0.2580, 0.2782, 0.2440, 0.2198],\n",
      "        [0.2580, 0.2782, 0.2440, 0.2198],\n",
      "        [0.2580, 0.2782, 0.2440, 0.2198],\n",
      "        [0.2580, 0.2782, 0.2440, 0.2198],\n",
      "        [0.2580, 0.2782, 0.2440, 0.2198],\n",
      "        [0.2580, 0.2782, 0.2440, 0.2198],\n",
      "        [0.2580, 0.2782, 0.2440, 0.2198],\n",
      "        [0.2580, 0.2782, 0.2440, 0.2198],\n",
      "        [0.2580, 0.2782, 0.2440, 0.2198],\n",
      "        [0.2580, 0.2782, 0.2440, 0.2198],\n",
      "        [0.2580, 0.2782, 0.2440, 0.2198],\n",
      "        [0.2580, 0.2782, 0.2440, 0.2198],\n",
      "        [0.2580, 0.2782, 0.2440, 0.2198],\n",
      "        [0.2580, 0.2782, 0.2440, 0.2198],\n",
      "        [0.2580, 0.2782, 0.2440, 0.2198],\n",
      "        [0.2580, 0.2782, 0.2440, 0.2198],\n",
      "        [0.2580, 0.2782, 0.2440, 0.2198],\n",
      "        [0.2580, 0.2782, 0.2440, 0.2198]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3827, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(-0.0250, grad_fn=<AddBackward0>)\n",
      "batching here\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54: done 1 episodes, mean_reward=1.00, best_reward=2.00, speed=206.97\n",
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(0.0624, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.3536, -1.2802, -1.4073, -1.5190],\n",
      "        [-1.3536, -1.2802, -1.4072, -1.5191],\n",
      "        [-1.3537, -1.2802, -1.4073, -1.5190],\n",
      "        [-1.3536, -1.2802, -1.4073, -1.5190],\n",
      "        [-1.3537, -1.2801, -1.4073, -1.5191],\n",
      "        [-1.3537, -1.2802, -1.4073, -1.5191],\n",
      "        [-1.3536, -1.2801, -1.4073, -1.5193],\n",
      "        [-1.3537, -1.2801, -1.4073, -1.5192],\n",
      "        [-1.3538, -1.2803, -1.4072, -1.5188],\n",
      "        [-1.3537, -1.2801, -1.4073, -1.5191],\n",
      "        [-1.3536, -1.2802, -1.4073, -1.5190],\n",
      "        [-1.3537, -1.2801, -1.4073, -1.5192],\n",
      "        [-1.3537, -1.2802, -1.4072, -1.5191],\n",
      "        [-1.3538, -1.2802, -1.4073, -1.5189],\n",
      "        [-1.3537, -1.2802, -1.4073, -1.5189],\n",
      "        [-1.3537, -1.2801, -1.4073, -1.5191],\n",
      "        [-1.3536, -1.2802, -1.4072, -1.5192],\n",
      "        [-1.3537, -1.2802, -1.4073, -1.5190],\n",
      "        [-1.3536, -1.2802, -1.4073, -1.5190],\n",
      "        [-1.3537, -1.2801, -1.4073, -1.5191],\n",
      "        [-1.3537, -1.2804, -1.4072, -1.5188],\n",
      "        [-1.3537, -1.2802, -1.4073, -1.5190],\n",
      "        [-1.3536, -1.2803, -1.4073, -1.5190],\n",
      "        [-1.3536, -1.2802, -1.4073, -1.5191],\n",
      "        [-1.3537, -1.2802, -1.4073, -1.5190],\n",
      "        [-1.3537, -1.2803, -1.4072, -1.5190],\n",
      "        [-1.3537, -1.2803, -1.4072, -1.5190],\n",
      "        [-1.3538, -1.2804, -1.4072, -1.5187],\n",
      "        [-1.3537, -1.2803, -1.4073, -1.5190],\n",
      "        [-1.3537, -1.2803, -1.4072, -1.5189],\n",
      "        [-1.3537, -1.2801, -1.4073, -1.5191],\n",
      "        [-1.3537, -1.2802, -1.4073, -1.5191],\n",
      "        [-1.3536, -1.2799, -1.4075, -1.5194],\n",
      "        [-1.3535, -1.2798, -1.4075, -1.5194],\n",
      "        [-1.3536, -1.2799, -1.4075, -1.5194],\n",
      "        [-1.3536, -1.2799, -1.4074, -1.5194],\n",
      "        [-1.3537, -1.2802, -1.4073, -1.5191],\n",
      "        [-1.3537, -1.2802, -1.4073, -1.5190],\n",
      "        [-1.3536, -1.2802, -1.4073, -1.5191],\n",
      "        [-1.3536, -1.2801, -1.4073, -1.5191],\n",
      "        [-1.3537, -1.2802, -1.4073, -1.5190],\n",
      "        [-1.3537, -1.2802, -1.4073, -1.5190],\n",
      "        [-1.3536, -1.2802, -1.4073, -1.5191],\n",
      "        [-1.3537, -1.2801, -1.4073, -1.5192],\n",
      "        [-1.3537, -1.2802, -1.4073, -1.5190],\n",
      "        [-1.3537, -1.2801, -1.4074, -1.5191],\n",
      "        [-1.3536, -1.2801, -1.4074, -1.5191],\n",
      "        [-1.3537, -1.2803, -1.4073, -1.5189],\n",
      "        [-1.3536, -1.2799, -1.4074, -1.5195],\n",
      "        [-1.3535, -1.2799, -1.4074, -1.5194],\n",
      "        [-1.3536, -1.2801, -1.4074, -1.5192],\n",
      "        [-1.3536, -1.2800, -1.4073, -1.5193],\n",
      "        [-1.3536, -1.2802, -1.4073, -1.5190],\n",
      "        [-1.3537, -1.2801, -1.4074, -1.5191],\n",
      "        [-1.3536, -1.2802, -1.4074, -1.5190],\n",
      "        [-1.3536, -1.2801, -1.4073, -1.5192],\n",
      "        [-1.3536, -1.2802, -1.4073, -1.5191],\n",
      "        [-1.3536, -1.2801, -1.4073, -1.5191],\n",
      "        [-1.3536, -1.2802, -1.4073, -1.5191],\n",
      "        [-1.3536, -1.2802, -1.4073, -1.5191],\n",
      "        [-1.3537, -1.2801, -1.4073, -1.5191],\n",
      "        [-1.3537, -1.2801, -1.4073, -1.5192],\n",
      "        [-1.3536, -1.2800, -1.4074, -1.5193],\n",
      "        [-1.3536, -1.2802, -1.4073, -1.5190]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([ 0.0102,  0.0070,  0.0046,  0.0027, -1.2331, -1.2479, -1.3878, -1.4045,\n",
      "         0.0088,  0.0076,  0.0049,  0.0025,  0.0098,  0.0081,  0.0046,  0.0026,\n",
      "         0.0106,  0.0079,  0.0046,  0.0027,  0.0094,  0.0075,  0.0047,  0.0026,\n",
      "         0.0105,  0.0073,  0.0050,  0.0024,  0.0103,  0.0080,  0.0054,  0.0030,\n",
      "         0.2552,  0.2867,  0.2415,  0.2553,  0.0094,  0.0084,  0.0057,  0.0026,\n",
      "         0.0098,  0.0073,  0.0049,  0.0027,  0.0102,  0.0077,  0.0057,  0.0020,\n",
      "         0.0100,  0.0091,  0.0056,  0.0031,  0.0100,  0.0074,  0.0047,  0.0024,\n",
      "         0.0097,  0.0069,  0.0049,  0.0024,  0.0112,  0.0077,  0.0052,  0.0020],\n",
      "       grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(0.0606, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2583, 0.2780, 0.2448, 0.2189],\n",
      "        [0.2583, 0.2780, 0.2448, 0.2189],\n",
      "        [0.2583, 0.2780, 0.2448, 0.2189],\n",
      "        [0.2583, 0.2780, 0.2448, 0.2189],\n",
      "        [0.2583, 0.2780, 0.2448, 0.2189],\n",
      "        [0.2583, 0.2780, 0.2448, 0.2189],\n",
      "        [0.2583, 0.2780, 0.2448, 0.2189],\n",
      "        [0.2583, 0.2780, 0.2448, 0.2189],\n",
      "        [0.2583, 0.2779, 0.2448, 0.2190],\n",
      "        [0.2583, 0.2780, 0.2448, 0.2189],\n",
      "        [0.2583, 0.2780, 0.2448, 0.2189],\n",
      "        [0.2583, 0.2780, 0.2448, 0.2189],\n",
      "        [0.2583, 0.2780, 0.2448, 0.2189],\n",
      "        [0.2583, 0.2780, 0.2448, 0.2190],\n",
      "        [0.2583, 0.2780, 0.2448, 0.2189],\n",
      "        [0.2583, 0.2780, 0.2448, 0.2189],\n",
      "        [0.2583, 0.2780, 0.2448, 0.2189],\n",
      "        [0.2583, 0.2780, 0.2448, 0.2189],\n",
      "        [0.2583, 0.2780, 0.2448, 0.2189],\n",
      "        [0.2583, 0.2780, 0.2448, 0.2189],\n",
      "        [0.2583, 0.2779, 0.2448, 0.2190],\n",
      "        [0.2583, 0.2780, 0.2448, 0.2189],\n",
      "        [0.2583, 0.2780, 0.2448, 0.2189],\n",
      "        [0.2583, 0.2780, 0.2448, 0.2189],\n",
      "        [0.2583, 0.2780, 0.2448, 0.2189],\n",
      "        [0.2583, 0.2780, 0.2448, 0.2189],\n",
      "        [0.2583, 0.2780, 0.2448, 0.2189],\n",
      "        [0.2583, 0.2779, 0.2448, 0.2190],\n",
      "        [0.2583, 0.2780, 0.2448, 0.2189],\n",
      "        [0.2583, 0.2780, 0.2448, 0.2189],\n",
      "        [0.2583, 0.2780, 0.2448, 0.2189],\n",
      "        [0.2583, 0.2780, 0.2448, 0.2189],\n",
      "        [0.2583, 0.2781, 0.2448, 0.2188],\n",
      "        [0.2583, 0.2781, 0.2448, 0.2188],\n",
      "        [0.2583, 0.2781, 0.2448, 0.2188],\n",
      "        [0.2583, 0.2781, 0.2448, 0.2189],\n",
      "        [0.2583, 0.2780, 0.2448, 0.2189],\n",
      "        [0.2583, 0.2780, 0.2448, 0.2189],\n",
      "        [0.2583, 0.2780, 0.2448, 0.2189],\n",
      "        [0.2583, 0.2780, 0.2448, 0.2189],\n",
      "        [0.2583, 0.2780, 0.2448, 0.2189],\n",
      "        [0.2583, 0.2780, 0.2448, 0.2189],\n",
      "        [0.2583, 0.2780, 0.2448, 0.2189],\n",
      "        [0.2583, 0.2780, 0.2448, 0.2189],\n",
      "        [0.2583, 0.2780, 0.2448, 0.2189],\n",
      "        [0.2583, 0.2780, 0.2448, 0.2189],\n",
      "        [0.2583, 0.2780, 0.2448, 0.2189],\n",
      "        [0.2583, 0.2780, 0.2448, 0.2190],\n",
      "        [0.2583, 0.2781, 0.2448, 0.2188],\n",
      "        [0.2583, 0.2781, 0.2448, 0.2188],\n",
      "        [0.2583, 0.2780, 0.2448, 0.2189],\n",
      "        [0.2583, 0.2780, 0.2448, 0.2189],\n",
      "        [0.2583, 0.2780, 0.2448, 0.2189],\n",
      "        [0.2583, 0.2780, 0.2448, 0.2189],\n",
      "        [0.2583, 0.2780, 0.2448, 0.2189],\n",
      "        [0.2583, 0.2780, 0.2448, 0.2189],\n",
      "        [0.2583, 0.2780, 0.2448, 0.2189],\n",
      "        [0.2583, 0.2780, 0.2448, 0.2189],\n",
      "        [0.2583, 0.2780, 0.2448, 0.2189],\n",
      "        [0.2583, 0.2780, 0.2448, 0.2189],\n",
      "        [0.2583, 0.2780, 0.2448, 0.2189],\n",
      "        [0.2583, 0.2780, 0.2448, 0.2189],\n",
      "        [0.2583, 0.2780, 0.2448, 0.2189],\n",
      "        [0.2583, 0.2780, 0.2448, 0.2189]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3826, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(0.1092, grad_fn=<AddBackward0>)\n",
      "batching here\n",
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(0.0608, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.3720, -1.2439, -1.3923, -1.5625],\n",
      "        [-1.3719, -1.2438, -1.3923, -1.5626],\n",
      "        [-1.3720, -1.2438, -1.3924, -1.5625],\n",
      "        [-1.3720, -1.2438, -1.3923, -1.5626],\n",
      "        [-1.3721, -1.2438, -1.3922, -1.5625],\n",
      "        [-1.3720, -1.2437, -1.3923, -1.5626],\n",
      "        [-1.3720, -1.2438, -1.3923, -1.5626],\n",
      "        [-1.3721, -1.2438, -1.3923, -1.5625],\n",
      "        [-1.3720, -1.2438, -1.3923, -1.5625],\n",
      "        [-1.3720, -1.2437, -1.3923, -1.5626],\n",
      "        [-1.3720, -1.2439, -1.3922, -1.5625],\n",
      "        [-1.3720, -1.2438, -1.3923, -1.5626],\n",
      "        [-1.3721, -1.2436, -1.3923, -1.5626],\n",
      "        [-1.3720, -1.2438, -1.3924, -1.5624],\n",
      "        [-1.3721, -1.2437, -1.3923, -1.5625],\n",
      "        [-1.3720, -1.2437, -1.3924, -1.5626],\n",
      "        [-1.3719, -1.2438, -1.3923, -1.5626],\n",
      "        [-1.3720, -1.2438, -1.3923, -1.5625],\n",
      "        [-1.3720, -1.2438, -1.3923, -1.5625],\n",
      "        [-1.3719, -1.2439, -1.3924, -1.5624],\n",
      "        [-1.3719, -1.2439, -1.3923, -1.5624],\n",
      "        [-1.3719, -1.2439, -1.3923, -1.5625],\n",
      "        [-1.3719, -1.2439, -1.3923, -1.5624],\n",
      "        [-1.3720, -1.2439, -1.3923, -1.5625],\n",
      "        [-1.3720, -1.2438, -1.3923, -1.5625],\n",
      "        [-1.3720, -1.2439, -1.3923, -1.5623],\n",
      "        [-1.3720, -1.2438, -1.3923, -1.5625],\n",
      "        [-1.3720, -1.2438, -1.3923, -1.5625],\n",
      "        [-1.3720, -1.2440, -1.3923, -1.5623],\n",
      "        [-1.3720, -1.2439, -1.3922, -1.5625],\n",
      "        [-1.3719, -1.2439, -1.3923, -1.5624],\n",
      "        [-1.3719, -1.2440, -1.3922, -1.5624],\n",
      "        [-1.3720, -1.2437, -1.3924, -1.5626],\n",
      "        [-1.3720, -1.2437, -1.3924, -1.5626],\n",
      "        [-1.3720, -1.2437, -1.3924, -1.5626],\n",
      "        [-1.3720, -1.2437, -1.3924, -1.5626],\n",
      "        [-1.3719, -1.2439, -1.3923, -1.5625],\n",
      "        [-1.3720, -1.2439, -1.3923, -1.5625],\n",
      "        [-1.3720, -1.2438, -1.3923, -1.5626],\n",
      "        [-1.3719, -1.2438, -1.3923, -1.5625],\n",
      "        [-1.3720, -1.2438, -1.3923, -1.5625],\n",
      "        [-1.3720, -1.2439, -1.3923, -1.5625],\n",
      "        [-1.3719, -1.2439, -1.3923, -1.5625],\n",
      "        [-1.3720, -1.2438, -1.3923, -1.5626],\n",
      "        [-1.3720, -1.2438, -1.3923, -1.5625],\n",
      "        [-1.3720, -1.2438, -1.3924, -1.5625],\n",
      "        [-1.3720, -1.2438, -1.3923, -1.5626],\n",
      "        [-1.3720, -1.2439, -1.3924, -1.5623],\n",
      "        [-1.3718, -1.2439, -1.3923, -1.5625],\n",
      "        [-1.3719, -1.2437, -1.3923, -1.5627],\n",
      "        [-1.3719, -1.2439, -1.3923, -1.5626],\n",
      "        [-1.3719, -1.2438, -1.3923, -1.5626],\n",
      "        [-1.3720, -1.2438, -1.3923, -1.5625],\n",
      "        [-1.3720, -1.2438, -1.3923, -1.5626],\n",
      "        [-1.3720, -1.2438, -1.3923, -1.5626],\n",
      "        [-1.3720, -1.2438, -1.3923, -1.5626],\n",
      "        [-1.3720, -1.2438, -1.3923, -1.5626],\n",
      "        [-1.3719, -1.2438, -1.3923, -1.5625],\n",
      "        [-1.3720, -1.2438, -1.3923, -1.5625],\n",
      "        [-1.3719, -1.2438, -1.3923, -1.5626],\n",
      "        [-1.3721, -1.2438, -1.3923, -1.5625],\n",
      "        [-1.3720, -1.2439, -1.3923, -1.5624],\n",
      "        [-1.3720, -1.2439, -1.3923, -1.5624],\n",
      "        [-1.3720, -1.2439, -1.3923, -1.5624]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([ 0.0123,  0.0101,  0.0068,  0.0039,  0.0143,  0.0098,  0.0066,  0.0034,\n",
      "        -1.3646, -1.3817,  0.0079,  0.0036, -1.5314, -1.3622,  0.0062,  0.0033,\n",
      "         0.0153,  0.0103,  0.0062,  0.0036,  0.0135,  0.0103,  0.0063,  0.0031,\n",
      "         0.0122,  0.0111,  0.0062,  0.0038,  0.0117,  0.0116,  0.0075,  0.0031,\n",
      "         0.0142,  0.0096,  0.0073,  0.0042,  0.0134,  0.0093,  0.0063,  0.0039,\n",
      "         0.0136,  0.0103,  0.0061,  0.0035,  0.0137,  0.0103,  0.0063,  0.0026,\n",
      "         0.0133,  0.0105,  0.0079,  0.0040,  0.0122,  0.0101,  0.0062,  0.0034,\n",
      "         0.0135,  0.0103,  0.0061,  0.0034,  0.0136,  0.0103,  0.0077,  0.0039],\n",
      "       grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(0.0804, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2536, 0.2883, 0.2485, 0.2096],\n",
      "        [0.2536, 0.2883, 0.2485, 0.2096],\n",
      "        [0.2536, 0.2883, 0.2485, 0.2096],\n",
      "        [0.2536, 0.2883, 0.2485, 0.2096],\n",
      "        [0.2536, 0.2883, 0.2485, 0.2096],\n",
      "        [0.2536, 0.2883, 0.2485, 0.2096],\n",
      "        [0.2536, 0.2883, 0.2485, 0.2096],\n",
      "        [0.2536, 0.2883, 0.2485, 0.2096],\n",
      "        [0.2536, 0.2883, 0.2485, 0.2096],\n",
      "        [0.2536, 0.2883, 0.2485, 0.2096],\n",
      "        [0.2536, 0.2883, 0.2485, 0.2096],\n",
      "        [0.2536, 0.2883, 0.2485, 0.2096],\n",
      "        [0.2536, 0.2883, 0.2485, 0.2096],\n",
      "        [0.2536, 0.2883, 0.2485, 0.2096],\n",
      "        [0.2536, 0.2883, 0.2485, 0.2096],\n",
      "        [0.2536, 0.2883, 0.2485, 0.2096],\n",
      "        [0.2536, 0.2883, 0.2485, 0.2096],\n",
      "        [0.2536, 0.2883, 0.2485, 0.2096],\n",
      "        [0.2536, 0.2883, 0.2485, 0.2096],\n",
      "        [0.2536, 0.2883, 0.2485, 0.2096],\n",
      "        [0.2536, 0.2883, 0.2485, 0.2096],\n",
      "        [0.2536, 0.2883, 0.2485, 0.2096],\n",
      "        [0.2536, 0.2882, 0.2485, 0.2096],\n",
      "        [0.2536, 0.2883, 0.2485, 0.2096],\n",
      "        [0.2536, 0.2883, 0.2485, 0.2096],\n",
      "        [0.2536, 0.2882, 0.2485, 0.2097],\n",
      "        [0.2536, 0.2883, 0.2485, 0.2096],\n",
      "        [0.2536, 0.2883, 0.2485, 0.2096],\n",
      "        [0.2536, 0.2882, 0.2485, 0.2097],\n",
      "        [0.2536, 0.2883, 0.2485, 0.2096],\n",
      "        [0.2536, 0.2882, 0.2485, 0.2096],\n",
      "        [0.2536, 0.2882, 0.2485, 0.2096],\n",
      "        [0.2536, 0.2883, 0.2485, 0.2096],\n",
      "        [0.2536, 0.2883, 0.2485, 0.2096],\n",
      "        [0.2536, 0.2883, 0.2485, 0.2096],\n",
      "        [0.2536, 0.2883, 0.2485, 0.2096],\n",
      "        [0.2536, 0.2883, 0.2485, 0.2096],\n",
      "        [0.2536, 0.2883, 0.2485, 0.2096],\n",
      "        [0.2536, 0.2883, 0.2485, 0.2096],\n",
      "        [0.2536, 0.2883, 0.2485, 0.2096],\n",
      "        [0.2536, 0.2883, 0.2485, 0.2096],\n",
      "        [0.2536, 0.2883, 0.2485, 0.2096],\n",
      "        [0.2536, 0.2883, 0.2485, 0.2096],\n",
      "        [0.2536, 0.2883, 0.2485, 0.2096],\n",
      "        [0.2536, 0.2883, 0.2485, 0.2096],\n",
      "        [0.2536, 0.2883, 0.2485, 0.2096],\n",
      "        [0.2536, 0.2883, 0.2485, 0.2096],\n",
      "        [0.2536, 0.2883, 0.2485, 0.2096],\n",
      "        [0.2536, 0.2882, 0.2485, 0.2096],\n",
      "        [0.2536, 0.2883, 0.2485, 0.2096],\n",
      "        [0.2536, 0.2883, 0.2485, 0.2096],\n",
      "        [0.2536, 0.2883, 0.2485, 0.2096],\n",
      "        [0.2536, 0.2883, 0.2485, 0.2096],\n",
      "        [0.2536, 0.2883, 0.2485, 0.2096],\n",
      "        [0.2536, 0.2883, 0.2485, 0.2096],\n",
      "        [0.2536, 0.2883, 0.2485, 0.2096],\n",
      "        [0.2536, 0.2883, 0.2485, 0.2096],\n",
      "        [0.2536, 0.2883, 0.2485, 0.2096],\n",
      "        [0.2536, 0.2883, 0.2485, 0.2096],\n",
      "        [0.2536, 0.2883, 0.2485, 0.2096],\n",
      "        [0.2536, 0.2883, 0.2485, 0.2096],\n",
      "        [0.2536, 0.2883, 0.2485, 0.2096],\n",
      "        [0.2536, 0.2883, 0.2485, 0.2096],\n",
      "        [0.2536, 0.2882, 0.2485, 0.2096]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3800, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(0.1274, grad_fn=<AddBackward0>)\n",
      "batching here\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(0.0451, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.3690, -1.2672, -1.3762, -1.5537],\n",
      "        [-1.3690, -1.2671, -1.3761, -1.5538],\n",
      "        [-1.3691, -1.2671, -1.3761, -1.5538],\n",
      "        [-1.3690, -1.2671, -1.3761, -1.5538],\n",
      "        [-1.3691, -1.2673, -1.3761, -1.5535],\n",
      "        [-1.3691, -1.2671, -1.3762, -1.5537],\n",
      "        [-1.3692, -1.2671, -1.3761, -1.5537],\n",
      "        [-1.3691, -1.2672, -1.3761, -1.5537],\n",
      "        [-1.3691, -1.2671, -1.3761, -1.5538],\n",
      "        [-1.3691, -1.2671, -1.3761, -1.5537],\n",
      "        [-1.3690, -1.2671, -1.3762, -1.5537],\n",
      "        [-1.3690, -1.2671, -1.3761, -1.5537],\n",
      "        [-1.3691, -1.2670, -1.3762, -1.5538],\n",
      "        [-1.3691, -1.2671, -1.3762, -1.5536],\n",
      "        [-1.3691, -1.2671, -1.3762, -1.5537],\n",
      "        [-1.3691, -1.2670, -1.3762, -1.5538],\n",
      "        [-1.3690, -1.2671, -1.3762, -1.5537],\n",
      "        [-1.3690, -1.2671, -1.3761, -1.5537],\n",
      "        [-1.3690, -1.2671, -1.3761, -1.5538],\n",
      "        [-1.3690, -1.2671, -1.3761, -1.5537],\n",
      "        [-1.3690, -1.2672, -1.3762, -1.5536],\n",
      "        [-1.3691, -1.2672, -1.3762, -1.5535],\n",
      "        [-1.3691, -1.2672, -1.3762, -1.5535],\n",
      "        [-1.3691, -1.2672, -1.3762, -1.5536],\n",
      "        [-1.3691, -1.2671, -1.3762, -1.5537],\n",
      "        [-1.3691, -1.2671, -1.3761, -1.5537],\n",
      "        [-1.3691, -1.2671, -1.3761, -1.5537],\n",
      "        [-1.3690, -1.2672, -1.3761, -1.5537],\n",
      "        [-1.3690, -1.2673, -1.3761, -1.5536],\n",
      "        [-1.3691, -1.2672, -1.3761, -1.5536],\n",
      "        [-1.3690, -1.2672, -1.3761, -1.5536],\n",
      "        [-1.3691, -1.2672, -1.3761, -1.5536],\n",
      "        [-1.3691, -1.2673, -1.3762, -1.5533],\n",
      "        [-1.3691, -1.2673, -1.3762, -1.5534],\n",
      "        [-1.3691, -1.2672, -1.3762, -1.5536],\n",
      "        [-1.3690, -1.2673, -1.3762, -1.5534],\n",
      "        [-1.3690, -1.2671, -1.3762, -1.5537],\n",
      "        [-1.3690, -1.2671, -1.3762, -1.5538],\n",
      "        [-1.3690, -1.2671, -1.3762, -1.5538],\n",
      "        [-1.3690, -1.2671, -1.3762, -1.5538],\n",
      "        [-1.3690, -1.2672, -1.3762, -1.5536],\n",
      "        [-1.3690, -1.2672, -1.3762, -1.5536],\n",
      "        [-1.3690, -1.2671, -1.3762, -1.5537],\n",
      "        [-1.3690, -1.2671, -1.3762, -1.5537],\n",
      "        [-1.3690, -1.2672, -1.3762, -1.5536],\n",
      "        [-1.3691, -1.2671, -1.3762, -1.5537],\n",
      "        [-1.3690, -1.2671, -1.3761, -1.5538],\n",
      "        [-1.3691, -1.2671, -1.3762, -1.5536],\n",
      "        [-1.3690, -1.2671, -1.3761, -1.5537],\n",
      "        [-1.3691, -1.2672, -1.3761, -1.5537],\n",
      "        [-1.3691, -1.2672, -1.3761, -1.5536],\n",
      "        [-1.3691, -1.2671, -1.3762, -1.5536],\n",
      "        [-1.3690, -1.2671, -1.3762, -1.5537],\n",
      "        [-1.3690, -1.2672, -1.3762, -1.5537],\n",
      "        [-1.3691, -1.2671, -1.3762, -1.5537],\n",
      "        [-1.3690, -1.2672, -1.3762, -1.5537],\n",
      "        [-1.3690, -1.2671, -1.3762, -1.5537],\n",
      "        [-1.3690, -1.2671, -1.3762, -1.5537],\n",
      "        [-1.3690, -1.2671, -1.3762, -1.5537],\n",
      "        [-1.3690, -1.2671, -1.3762, -1.5537],\n",
      "        [-1.3691, -1.2673, -1.3761, -1.5535],\n",
      "        [-1.3691, -1.2674, -1.3761, -1.5532],\n",
      "        [-1.3691, -1.2673, -1.3761, -1.5534],\n",
      "        [-1.3691, -1.2674, -1.3761, -1.5533]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([ 0.0176,  0.0133,  0.0083,  0.0049,  0.0202,  0.0155,  0.0086,  0.0050,\n",
      "         0.0200,  0.0123,  0.0091,  0.0049,  0.0206,  0.0134,  0.0092,  0.0046,\n",
      "         0.0197,  0.0121,  0.0082,  0.0040,  0.0200,  0.0132,  0.0101,  0.0042,\n",
      "        -1.3313, -1.2423, -1.3602,  0.0040,  0.0178,  0.0134,  0.0090,  0.0042,\n",
      "         0.0196,  0.0135,  0.0084,  0.0053,  0.0175,  0.0133,  0.0091,  0.0047,\n",
      "         0.0176,  0.0135,  0.0091,  0.0042,  0.0176,  0.0133,  0.0091,  0.0042,\n",
      "         0.0180,  0.0125,  0.0107,  0.0049,  0.0183,  0.0128,  0.0084,  0.0048,\n",
      "         0.0177,  0.0134,  0.0083,  0.0042,  0.0162,  0.0142,  0.0087,  0.0035],\n",
      "       grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(0.0508, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2544, 0.2816, 0.2525, 0.2115],\n",
      "        [0.2544, 0.2816, 0.2526, 0.2114],\n",
      "        [0.2543, 0.2816, 0.2526, 0.2115],\n",
      "        [0.2544, 0.2816, 0.2526, 0.2115],\n",
      "        [0.2543, 0.2816, 0.2526, 0.2115],\n",
      "        [0.2543, 0.2816, 0.2525, 0.2115],\n",
      "        [0.2543, 0.2817, 0.2526, 0.2115],\n",
      "        [0.2543, 0.2816, 0.2526, 0.2115],\n",
      "        [0.2543, 0.2817, 0.2526, 0.2115],\n",
      "        [0.2543, 0.2816, 0.2525, 0.2115],\n",
      "        [0.2544, 0.2816, 0.2525, 0.2115],\n",
      "        [0.2544, 0.2816, 0.2525, 0.2115],\n",
      "        [0.2543, 0.2817, 0.2525, 0.2114],\n",
      "        [0.2543, 0.2816, 0.2525, 0.2115],\n",
      "        [0.2543, 0.2816, 0.2525, 0.2115],\n",
      "        [0.2543, 0.2817, 0.2525, 0.2114],\n",
      "        [0.2544, 0.2817, 0.2525, 0.2115],\n",
      "        [0.2544, 0.2816, 0.2526, 0.2115],\n",
      "        [0.2544, 0.2816, 0.2526, 0.2114],\n",
      "        [0.2544, 0.2816, 0.2526, 0.2115],\n",
      "        [0.2544, 0.2816, 0.2525, 0.2115],\n",
      "        [0.2543, 0.2816, 0.2525, 0.2115],\n",
      "        [0.2543, 0.2816, 0.2525, 0.2115],\n",
      "        [0.2543, 0.2816, 0.2525, 0.2115],\n",
      "        [0.2543, 0.2816, 0.2525, 0.2115],\n",
      "        [0.2543, 0.2816, 0.2526, 0.2115],\n",
      "        [0.2543, 0.2816, 0.2526, 0.2115],\n",
      "        [0.2543, 0.2816, 0.2526, 0.2115],\n",
      "        [0.2544, 0.2816, 0.2526, 0.2115],\n",
      "        [0.2543, 0.2816, 0.2526, 0.2115],\n",
      "        [0.2543, 0.2816, 0.2526, 0.2115],\n",
      "        [0.2543, 0.2816, 0.2526, 0.2115],\n",
      "        [0.2543, 0.2816, 0.2525, 0.2116],\n",
      "        [0.2543, 0.2816, 0.2525, 0.2115],\n",
      "        [0.2543, 0.2816, 0.2525, 0.2115],\n",
      "        [0.2544, 0.2816, 0.2525, 0.2115],\n",
      "        [0.2544, 0.2816, 0.2525, 0.2115],\n",
      "        [0.2544, 0.2816, 0.2525, 0.2114],\n",
      "        [0.2544, 0.2816, 0.2525, 0.2115],\n",
      "        [0.2544, 0.2816, 0.2525, 0.2114],\n",
      "        [0.2544, 0.2816, 0.2525, 0.2115],\n",
      "        [0.2544, 0.2816, 0.2525, 0.2115],\n",
      "        [0.2544, 0.2816, 0.2525, 0.2115],\n",
      "        [0.2544, 0.2816, 0.2525, 0.2115],\n",
      "        [0.2544, 0.2816, 0.2525, 0.2115],\n",
      "        [0.2543, 0.2816, 0.2525, 0.2115],\n",
      "        [0.2544, 0.2816, 0.2526, 0.2114],\n",
      "        [0.2543, 0.2816, 0.2525, 0.2115],\n",
      "        [0.2543, 0.2816, 0.2526, 0.2115],\n",
      "        [0.2543, 0.2816, 0.2526, 0.2115],\n",
      "        [0.2543, 0.2816, 0.2526, 0.2115],\n",
      "        [0.2543, 0.2816, 0.2525, 0.2115],\n",
      "        [0.2544, 0.2816, 0.2525, 0.2115],\n",
      "        [0.2544, 0.2816, 0.2525, 0.2115],\n",
      "        [0.2543, 0.2816, 0.2525, 0.2115],\n",
      "        [0.2544, 0.2816, 0.2525, 0.2115],\n",
      "        [0.2544, 0.2816, 0.2525, 0.2115],\n",
      "        [0.2544, 0.2816, 0.2525, 0.2115],\n",
      "        [0.2544, 0.2816, 0.2525, 0.2115],\n",
      "        [0.2544, 0.2816, 0.2525, 0.2115],\n",
      "        [0.2543, 0.2816, 0.2526, 0.2115],\n",
      "        [0.2543, 0.2816, 0.2526, 0.2116],\n",
      "        [0.2543, 0.2816, 0.2526, 0.2115],\n",
      "        [0.2543, 0.2816, 0.2526, 0.2116]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3812, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(0.0821, grad_fn=<AddBackward0>)\n",
      "batching here\n",
      "57: done 4 episodes, mean_reward=0.25, best_reward=2.00, speed=205.69\n",
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(0.0281, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.3647, -1.2473, -1.3708, -1.5927],\n",
      "        [-1.3648, -1.2473, -1.3708, -1.5927],\n",
      "        [-1.3648, -1.2472, -1.3708, -1.5928],\n",
      "        [-1.3648, -1.2472, -1.3708, -1.5929],\n",
      "        [-1.3649, -1.2475, -1.3707, -1.5924],\n",
      "        [-1.3648, -1.2476, -1.3707, -1.5923],\n",
      "        [-1.3648, -1.2476, -1.3708, -1.5923],\n",
      "        [-1.3648, -1.2476, -1.3708, -1.5924],\n",
      "        [-1.3648, -1.2472, -1.3708, -1.5927],\n",
      "        [-1.3649, -1.2473, -1.3708, -1.5926],\n",
      "        [-1.3649, -1.2474, -1.3707, -1.5925],\n",
      "        [-1.3649, -1.2473, -1.3707, -1.5926],\n",
      "        [-1.3648, -1.2471, -1.3709, -1.5927],\n",
      "        [-1.3649, -1.2473, -1.3708, -1.5925],\n",
      "        [-1.3649, -1.2473, -1.3709, -1.5925],\n",
      "        [-1.3649, -1.2473, -1.3709, -1.5925],\n",
      "        [-1.3648, -1.2472, -1.3708, -1.5928],\n",
      "        [-1.3648, -1.2472, -1.3708, -1.5928],\n",
      "        [-1.3648, -1.2473, -1.3708, -1.5927],\n",
      "        [-1.3648, -1.2472, -1.3708, -1.5928],\n",
      "        [-1.3648, -1.2473, -1.3709, -1.5926],\n",
      "        [-1.3648, -1.2473, -1.3708, -1.5926],\n",
      "        [-1.3648, -1.2473, -1.3708, -1.5926],\n",
      "        [-1.3648, -1.2474, -1.3708, -1.5925],\n",
      "        [-1.3648, -1.2472, -1.3708, -1.5928],\n",
      "        [-1.3648, -1.2472, -1.3708, -1.5928],\n",
      "        [-1.3648, -1.2473, -1.3708, -1.5927],\n",
      "        [-1.3648, -1.2473, -1.3708, -1.5926],\n",
      "        [-1.3648, -1.2474, -1.3708, -1.5926],\n",
      "        [-1.3648, -1.2473, -1.3708, -1.5926],\n",
      "        [-1.3648, -1.2474, -1.3708, -1.5926],\n",
      "        [-1.3648, -1.2473, -1.3708, -1.5926],\n",
      "        [-1.3648, -1.2478, -1.3709, -1.5919],\n",
      "        [-1.3648, -1.2478, -1.3708, -1.5919],\n",
      "        [-1.3648, -1.2477, -1.3709, -1.5920],\n",
      "        [-1.3648, -1.2477, -1.3709, -1.5919],\n",
      "        [-1.3648, -1.2472, -1.3708, -1.5928],\n",
      "        [-1.3648, -1.2472, -1.3708, -1.5927],\n",
      "        [-1.3648, -1.2472, -1.3708, -1.5928],\n",
      "        [-1.3648, -1.2472, -1.3708, -1.5928],\n",
      "        [-1.3648, -1.2472, -1.3708, -1.5927],\n",
      "        [-1.3648, -1.2472, -1.3708, -1.5928],\n",
      "        [-1.3648, -1.2472, -1.3708, -1.5927],\n",
      "        [-1.3648, -1.2472, -1.3708, -1.5928],\n",
      "        [-1.3648, -1.2472, -1.3708, -1.5927],\n",
      "        [-1.3648, -1.2472, -1.3708, -1.5927],\n",
      "        [-1.3648, -1.2472, -1.3708, -1.5928],\n",
      "        [-1.3648, -1.2472, -1.3708, -1.5928],\n",
      "        [-1.3648, -1.2475, -1.3708, -1.5923],\n",
      "        [-1.3647, -1.2475, -1.3708, -1.5923],\n",
      "        [-1.3647, -1.2474, -1.3709, -1.5925],\n",
      "        [-1.3647, -1.2474, -1.3708, -1.5926],\n",
      "        [-1.3648, -1.2473, -1.3708, -1.5926],\n",
      "        [-1.3648, -1.2474, -1.3708, -1.5925],\n",
      "        [-1.3648, -1.2472, -1.3708, -1.5928],\n",
      "        [-1.3647, -1.2474, -1.3709, -1.5925],\n",
      "        [-1.3648, -1.2472, -1.3708, -1.5928],\n",
      "        [-1.3648, -1.2472, -1.3708, -1.5928],\n",
      "        [-1.3648, -1.2472, -1.3708, -1.5928],\n",
      "        [-1.3648, -1.2472, -1.3708, -1.5928],\n",
      "        [-1.3648, -1.2475, -1.3707, -1.5925],\n",
      "        [-1.3648, -1.2476, -1.3707, -1.5924],\n",
      "        [-1.3648, -1.2475, -1.3707, -1.5924],\n",
      "        [-1.3648, -1.2475, -1.3707, -1.5924]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([0.5778, 0.5777, 0.0104, 0.0059, 0.0270, 0.0173, 0.0116, 0.0069, 0.0271,\n",
      "        0.0199, 0.0102, 0.0055, 0.0210, 0.0170, 0.0112, 0.0051, 0.0231, 0.0172,\n",
      "        0.0106, 0.0059, 0.5780, 0.5283, 0.0107, 0.0056, 0.0234, 0.0161, 0.0119,\n",
      "        0.0064, 0.0228, 0.0172, 0.0114, 0.0067, 0.0225, 0.0198, 0.0115, 0.0050,\n",
      "        0.5805, 0.5805, 0.6745, 0.5805, 0.0206, 0.0172, 0.0104, 0.0058, 0.0207,\n",
      "        0.0200, 0.0115, 0.0058, 0.0219, 0.0191, 0.0117, 0.0058, 0.0263, 0.0150,\n",
      "        0.0108, 0.0046, 0.5282, 0.6745, 0.0111, 0.0050, 0.0209, 0.0203, 0.0116,\n",
      "        0.0059], grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(-0.1035, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2555, 0.2873, 0.2539, 0.2034],\n",
      "        [0.2554, 0.2873, 0.2539, 0.2034],\n",
      "        [0.2554, 0.2873, 0.2539, 0.2034],\n",
      "        [0.2554, 0.2873, 0.2539, 0.2033],\n",
      "        [0.2554, 0.2872, 0.2539, 0.2034],\n",
      "        [0.2554, 0.2872, 0.2539, 0.2035],\n",
      "        [0.2554, 0.2872, 0.2539, 0.2035],\n",
      "        [0.2554, 0.2872, 0.2539, 0.2034],\n",
      "        [0.2554, 0.2873, 0.2539, 0.2034],\n",
      "        [0.2554, 0.2873, 0.2539, 0.2034],\n",
      "        [0.2554, 0.2873, 0.2539, 0.2034],\n",
      "        [0.2554, 0.2873, 0.2539, 0.2034],\n",
      "        [0.2554, 0.2873, 0.2539, 0.2034],\n",
      "        [0.2554, 0.2873, 0.2539, 0.2034],\n",
      "        [0.2554, 0.2873, 0.2539, 0.2034],\n",
      "        [0.2554, 0.2873, 0.2539, 0.2034],\n",
      "        [0.2554, 0.2873, 0.2539, 0.2034],\n",
      "        [0.2554, 0.2873, 0.2539, 0.2034],\n",
      "        [0.2554, 0.2873, 0.2539, 0.2034],\n",
      "        [0.2554, 0.2873, 0.2539, 0.2034],\n",
      "        [0.2554, 0.2873, 0.2539, 0.2034],\n",
      "        [0.2554, 0.2873, 0.2539, 0.2034],\n",
      "        [0.2554, 0.2873, 0.2539, 0.2034],\n",
      "        [0.2554, 0.2873, 0.2539, 0.2034],\n",
      "        [0.2554, 0.2873, 0.2539, 0.2033],\n",
      "        [0.2554, 0.2873, 0.2539, 0.2034],\n",
      "        [0.2554, 0.2873, 0.2539, 0.2034],\n",
      "        [0.2554, 0.2873, 0.2539, 0.2034],\n",
      "        [0.2554, 0.2873, 0.2539, 0.2034],\n",
      "        [0.2554, 0.2873, 0.2539, 0.2034],\n",
      "        [0.2554, 0.2873, 0.2539, 0.2034],\n",
      "        [0.2554, 0.2873, 0.2539, 0.2034],\n",
      "        [0.2554, 0.2871, 0.2539, 0.2035],\n",
      "        [0.2554, 0.2871, 0.2539, 0.2035],\n",
      "        [0.2554, 0.2872, 0.2539, 0.2035],\n",
      "        [0.2554, 0.2872, 0.2539, 0.2035],\n",
      "        [0.2554, 0.2873, 0.2539, 0.2034],\n",
      "        [0.2554, 0.2873, 0.2539, 0.2034],\n",
      "        [0.2554, 0.2873, 0.2539, 0.2034],\n",
      "        [0.2554, 0.2873, 0.2539, 0.2034],\n",
      "        [0.2554, 0.2873, 0.2539, 0.2034],\n",
      "        [0.2554, 0.2873, 0.2539, 0.2034],\n",
      "        [0.2554, 0.2873, 0.2539, 0.2034],\n",
      "        [0.2554, 0.2873, 0.2539, 0.2034],\n",
      "        [0.2554, 0.2873, 0.2539, 0.2034],\n",
      "        [0.2554, 0.2873, 0.2539, 0.2034],\n",
      "        [0.2554, 0.2873, 0.2539, 0.2034],\n",
      "        [0.2554, 0.2873, 0.2539, 0.2034],\n",
      "        [0.2554, 0.2872, 0.2539, 0.2035],\n",
      "        [0.2554, 0.2872, 0.2539, 0.2035],\n",
      "        [0.2555, 0.2872, 0.2539, 0.2034],\n",
      "        [0.2554, 0.2873, 0.2539, 0.2034],\n",
      "        [0.2554, 0.2873, 0.2539, 0.2034],\n",
      "        [0.2554, 0.2873, 0.2539, 0.2034],\n",
      "        [0.2554, 0.2873, 0.2539, 0.2033],\n",
      "        [0.2554, 0.2872, 0.2539, 0.2034],\n",
      "        [0.2554, 0.2873, 0.2539, 0.2034],\n",
      "        [0.2554, 0.2873, 0.2539, 0.2034],\n",
      "        [0.2554, 0.2873, 0.2539, 0.2034],\n",
      "        [0.2554, 0.2873, 0.2539, 0.2034],\n",
      "        [0.2554, 0.2872, 0.2539, 0.2034],\n",
      "        [0.2554, 0.2872, 0.2539, 0.2034],\n",
      "        [0.2554, 0.2872, 0.2539, 0.2034],\n",
      "        [0.2554, 0.2872, 0.2539, 0.2034]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3789, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(-0.0892, grad_fn=<AddBackward0>)\n",
      "batching here\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58: done 4 episodes, mean_reward=0.50, best_reward=2.00, speed=213.44\n",
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(0.0638, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.3684, -1.2533, -1.3747, -1.5749],\n",
      "        [-1.3685, -1.2535, -1.3746, -1.5748],\n",
      "        [-1.3684, -1.2534, -1.3747, -1.5747],\n",
      "        [-1.3684, -1.2533, -1.3748, -1.5748],\n",
      "        [-1.3685, -1.2536, -1.3747, -1.5744],\n",
      "        [-1.3685, -1.2535, -1.3747, -1.5747],\n",
      "        [-1.3685, -1.2535, -1.3747, -1.5746],\n",
      "        [-1.3685, -1.2534, -1.3747, -1.5747],\n",
      "        [-1.3685, -1.2535, -1.3746, -1.5747],\n",
      "        [-1.3685, -1.2536, -1.3746, -1.5746],\n",
      "        [-1.3685, -1.2535, -1.3746, -1.5747],\n",
      "        [-1.3685, -1.2535, -1.3747, -1.5746],\n",
      "        [-1.3686, -1.2532, -1.3747, -1.5748],\n",
      "        [-1.3686, -1.2533, -1.3747, -1.5748],\n",
      "        [-1.3685, -1.2533, -1.3747, -1.5748],\n",
      "        [-1.3686, -1.2533, -1.3747, -1.5748],\n",
      "        [-1.3685, -1.2534, -1.3747, -1.5748],\n",
      "        [-1.3685, -1.2535, -1.3747, -1.5747],\n",
      "        [-1.3684, -1.2535, -1.3747, -1.5746],\n",
      "        [-1.3685, -1.2534, -1.3747, -1.5748],\n",
      "        [-1.3685, -1.2535, -1.3747, -1.5747],\n",
      "        [-1.3685, -1.2535, -1.3747, -1.5747],\n",
      "        [-1.3686, -1.2535, -1.3747, -1.5746],\n",
      "        [-1.3684, -1.2535, -1.3747, -1.5747],\n",
      "        [-1.3685, -1.2535, -1.3747, -1.5746],\n",
      "        [-1.3685, -1.2535, -1.3747, -1.5747],\n",
      "        [-1.3685, -1.2535, -1.3747, -1.5747],\n",
      "        [-1.3685, -1.2536, -1.3746, -1.5745],\n",
      "        [-1.3684, -1.2536, -1.3746, -1.5746],\n",
      "        [-1.3684, -1.2536, -1.3746, -1.5746],\n",
      "        [-1.3685, -1.2535, -1.3746, -1.5747],\n",
      "        [-1.3685, -1.2535, -1.3747, -1.5746],\n",
      "        [-1.3685, -1.2534, -1.3748, -1.5747],\n",
      "        [-1.3685, -1.2534, -1.3748, -1.5747],\n",
      "        [-1.3685, -1.2535, -1.3748, -1.5746],\n",
      "        [-1.3684, -1.2534, -1.3748, -1.5748],\n",
      "        [-1.3685, -1.2534, -1.3747, -1.5747],\n",
      "        [-1.3685, -1.2534, -1.3747, -1.5747],\n",
      "        [-1.3685, -1.2533, -1.3747, -1.5748],\n",
      "        [-1.3685, -1.2534, -1.3747, -1.5746],\n",
      "        [-1.3685, -1.2534, -1.3747, -1.5747],\n",
      "        [-1.3685, -1.2534, -1.3747, -1.5747],\n",
      "        [-1.3685, -1.2534, -1.3747, -1.5747],\n",
      "        [-1.3685, -1.2534, -1.3747, -1.5747],\n",
      "        [-1.3685, -1.2534, -1.3747, -1.5747],\n",
      "        [-1.3685, -1.2534, -1.3747, -1.5747],\n",
      "        [-1.3685, -1.2534, -1.3747, -1.5747],\n",
      "        [-1.3685, -1.2534, -1.3747, -1.5747],\n",
      "        [-1.3684, -1.2535, -1.3747, -1.5747],\n",
      "        [-1.3684, -1.2536, -1.3747, -1.5746],\n",
      "        [-1.3684, -1.2535, -1.3747, -1.5747],\n",
      "        [-1.3684, -1.2535, -1.3747, -1.5747],\n",
      "        [-1.3686, -1.2534, -1.3747, -1.5747],\n",
      "        [-1.3685, -1.2535, -1.3747, -1.5746],\n",
      "        [-1.3685, -1.2534, -1.3747, -1.5748],\n",
      "        [-1.3685, -1.2535, -1.3747, -1.5747],\n",
      "        [-1.3685, -1.2533, -1.3747, -1.5749],\n",
      "        [-1.3686, -1.2534, -1.3747, -1.5747],\n",
      "        [-1.3685, -1.2536, -1.3747, -1.5745],\n",
      "        [-1.3685, -1.2534, -1.3747, -1.5747],\n",
      "        [-1.3685, -1.2535, -1.3747, -1.5746],\n",
      "        [-1.3685, -1.2536, -1.3746, -1.5746],\n",
      "        [-1.3684, -1.2535, -1.3747, -1.5747],\n",
      "        [-1.3685, -1.2535, -1.3746, -1.5747]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([ 0.0198,  0.0142,  0.0082,  0.0039,  0.0189,  0.0130,  0.0077,  0.0046,\n",
      "         0.0168,  0.0120,  0.0080,  0.0039,  0.0174,  0.0154,  0.0082,  0.0047,\n",
      "         0.4967,  0.4967,  0.4334,  0.0043,  0.0169,  0.0127,  0.0076,  0.0047,\n",
      "         0.0168,  0.0130,  0.0085,  0.0047,  0.3954,  0.4316,  0.0084,  0.0037,\n",
      "         0.0192,  0.0124,  0.0074,  0.0043,  0.0200,  0.0132,  0.0106,  0.0045,\n",
      "         0.4335,  0.3953,  0.4315,  0.4335,  0.4315,  0.4966,  0.4966,  0.0043,\n",
      "         0.0157,  0.0117,  0.0102,  0.0045,  0.0172,  0.0117,  0.0103,  0.0051,\n",
      "         0.0174,  0.0115,  0.0074,  0.0038, -1.2134, -1.3423, -1.3601,  0.0042],\n",
      "       grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(-0.0306, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2545, 0.2856, 0.2529, 0.2070],\n",
      "        [0.2545, 0.2855, 0.2529, 0.2071],\n",
      "        [0.2545, 0.2855, 0.2529, 0.2071],\n",
      "        [0.2545, 0.2855, 0.2529, 0.2071],\n",
      "        [0.2545, 0.2855, 0.2529, 0.2071],\n",
      "        [0.2545, 0.2855, 0.2529, 0.2071],\n",
      "        [0.2545, 0.2855, 0.2529, 0.2071],\n",
      "        [0.2545, 0.2855, 0.2529, 0.2071],\n",
      "        [0.2545, 0.2855, 0.2529, 0.2071],\n",
      "        [0.2545, 0.2855, 0.2529, 0.2071],\n",
      "        [0.2545, 0.2855, 0.2529, 0.2071],\n",
      "        [0.2545, 0.2855, 0.2529, 0.2071],\n",
      "        [0.2545, 0.2856, 0.2529, 0.2070],\n",
      "        [0.2545, 0.2856, 0.2529, 0.2070],\n",
      "        [0.2545, 0.2856, 0.2529, 0.2070],\n",
      "        [0.2545, 0.2856, 0.2529, 0.2071],\n",
      "        [0.2545, 0.2855, 0.2529, 0.2071],\n",
      "        [0.2545, 0.2855, 0.2529, 0.2071],\n",
      "        [0.2545, 0.2855, 0.2529, 0.2071],\n",
      "        [0.2545, 0.2855, 0.2529, 0.2071],\n",
      "        [0.2545, 0.2855, 0.2529, 0.2071],\n",
      "        [0.2545, 0.2855, 0.2529, 0.2071],\n",
      "        [0.2545, 0.2855, 0.2529, 0.2071],\n",
      "        [0.2545, 0.2855, 0.2529, 0.2071],\n",
      "        [0.2545, 0.2855, 0.2529, 0.2071],\n",
      "        [0.2545, 0.2855, 0.2529, 0.2071],\n",
      "        [0.2545, 0.2855, 0.2529, 0.2071],\n",
      "        [0.2545, 0.2855, 0.2529, 0.2071],\n",
      "        [0.2545, 0.2855, 0.2529, 0.2071],\n",
      "        [0.2545, 0.2855, 0.2529, 0.2071],\n",
      "        [0.2545, 0.2855, 0.2529, 0.2071],\n",
      "        [0.2545, 0.2855, 0.2529, 0.2071],\n",
      "        [0.2545, 0.2855, 0.2529, 0.2071],\n",
      "        [0.2545, 0.2855, 0.2529, 0.2071],\n",
      "        [0.2545, 0.2855, 0.2529, 0.2071],\n",
      "        [0.2545, 0.2855, 0.2529, 0.2071],\n",
      "        [0.2545, 0.2855, 0.2529, 0.2071],\n",
      "        [0.2545, 0.2855, 0.2529, 0.2071],\n",
      "        [0.2545, 0.2855, 0.2529, 0.2071],\n",
      "        [0.2545, 0.2855, 0.2529, 0.2071],\n",
      "        [0.2545, 0.2855, 0.2529, 0.2071],\n",
      "        [0.2545, 0.2855, 0.2529, 0.2071],\n",
      "        [0.2545, 0.2855, 0.2529, 0.2071],\n",
      "        [0.2545, 0.2855, 0.2529, 0.2071],\n",
      "        [0.2545, 0.2855, 0.2529, 0.2071],\n",
      "        [0.2545, 0.2855, 0.2529, 0.2071],\n",
      "        [0.2545, 0.2855, 0.2529, 0.2071],\n",
      "        [0.2545, 0.2855, 0.2529, 0.2071],\n",
      "        [0.2545, 0.2855, 0.2529, 0.2071],\n",
      "        [0.2545, 0.2855, 0.2529, 0.2071],\n",
      "        [0.2545, 0.2855, 0.2529, 0.2071],\n",
      "        [0.2545, 0.2855, 0.2529, 0.2071],\n",
      "        [0.2545, 0.2855, 0.2529, 0.2071],\n",
      "        [0.2545, 0.2855, 0.2529, 0.2071],\n",
      "        [0.2545, 0.2855, 0.2529, 0.2071],\n",
      "        [0.2545, 0.2855, 0.2529, 0.2071],\n",
      "        [0.2545, 0.2856, 0.2529, 0.2070],\n",
      "        [0.2545, 0.2855, 0.2529, 0.2071],\n",
      "        [0.2545, 0.2855, 0.2529, 0.2071],\n",
      "        [0.2545, 0.2855, 0.2529, 0.2071],\n",
      "        [0.2545, 0.2855, 0.2529, 0.2071],\n",
      "        [0.2545, 0.2855, 0.2529, 0.2071],\n",
      "        [0.2545, 0.2855, 0.2529, 0.2071],\n",
      "        [0.2545, 0.2855, 0.2529, 0.2071]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3799, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(0.0194, grad_fn=<AddBackward0>)\n",
      "batching here\n",
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(4.1280e-05, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.3342, -1.2150, -1.4005, -1.6428],\n",
      "        [-1.3343, -1.2154, -1.4004, -1.6422],\n",
      "        [-1.3342, -1.2151, -1.4004, -1.6427],\n",
      "        [-1.3343, -1.2152, -1.4005, -1.6424],\n",
      "        [-1.3343, -1.2151, -1.4004, -1.6425],\n",
      "        [-1.3343, -1.2150, -1.4005, -1.6427],\n",
      "        [-1.3343, -1.2151, -1.4005, -1.6426],\n",
      "        [-1.3343, -1.2151, -1.4005, -1.6426],\n",
      "        [-1.3343, -1.2151, -1.4003, -1.6428],\n",
      "        [-1.3344, -1.2152, -1.4004, -1.6424],\n",
      "        [-1.3344, -1.2154, -1.4003, -1.6421],\n",
      "        [-1.3344, -1.2154, -1.4004, -1.6421],\n",
      "        [-1.3343, -1.2150, -1.4005, -1.6427],\n",
      "        [-1.3342, -1.2149, -1.4004, -1.6430],\n",
      "        [-1.3343, -1.2149, -1.4004, -1.6429],\n",
      "        [-1.3343, -1.2149, -1.4004, -1.6430],\n",
      "        [-1.3343, -1.2153, -1.4004, -1.6424],\n",
      "        [-1.3343, -1.2153, -1.4004, -1.6423],\n",
      "        [-1.3342, -1.2151, -1.4005, -1.6427],\n",
      "        [-1.3343, -1.2152, -1.4004, -1.6425],\n",
      "        [-1.3343, -1.2151, -1.4004, -1.6427],\n",
      "        [-1.3342, -1.2152, -1.4004, -1.6425],\n",
      "        [-1.3342, -1.2152, -1.4004, -1.6426],\n",
      "        [-1.3343, -1.2152, -1.4003, -1.6425],\n",
      "        [-1.3343, -1.2154, -1.4003, -1.6422],\n",
      "        [-1.3343, -1.2153, -1.4004, -1.6422],\n",
      "        [-1.3343, -1.2155, -1.4003, -1.6420],\n",
      "        [-1.3343, -1.2153, -1.4004, -1.6423],\n",
      "        [-1.3343, -1.2151, -1.4004, -1.6426],\n",
      "        [-1.3344, -1.2153, -1.4003, -1.6423],\n",
      "        [-1.3343, -1.2155, -1.4004, -1.6421],\n",
      "        [-1.3343, -1.2153, -1.4004, -1.6424],\n",
      "        [-1.3341, -1.2148, -1.4006, -1.6432],\n",
      "        [-1.3341, -1.2149, -1.4005, -1.6429],\n",
      "        [-1.3341, -1.2149, -1.4005, -1.6429],\n",
      "        [-1.3341, -1.2149, -1.4005, -1.6430],\n",
      "        [-1.3343, -1.2154, -1.4004, -1.6421],\n",
      "        [-1.3343, -1.2152, -1.4004, -1.6424],\n",
      "        [-1.3343, -1.2152, -1.4004, -1.6425],\n",
      "        [-1.3343, -1.2154, -1.4004, -1.6422],\n",
      "        [-1.3343, -1.2152, -1.4004, -1.6424],\n",
      "        [-1.3343, -1.2153, -1.4004, -1.6424],\n",
      "        [-1.3343, -1.2152, -1.4004, -1.6423],\n",
      "        [-1.3342, -1.2152, -1.4005, -1.6424],\n",
      "        [-1.3343, -1.2152, -1.4005, -1.6424],\n",
      "        [-1.3343, -1.2152, -1.4005, -1.6425],\n",
      "        [-1.3343, -1.2153, -1.4004, -1.6423],\n",
      "        [-1.3342, -1.2152, -1.4005, -1.6424],\n",
      "        [-1.3342, -1.2153, -1.4004, -1.6424],\n",
      "        [-1.3342, -1.2152, -1.4004, -1.6425],\n",
      "        [-1.3342, -1.2152, -1.4004, -1.6425],\n",
      "        [-1.3342, -1.2152, -1.4005, -1.6426],\n",
      "        [-1.3343, -1.2153, -1.4004, -1.6422],\n",
      "        [-1.3344, -1.2155, -1.4004, -1.6419],\n",
      "        [-1.3343, -1.2153, -1.4004, -1.6422],\n",
      "        [-1.3343, -1.2154, -1.4004, -1.6422],\n",
      "        [-1.3343, -1.2151, -1.4004, -1.6427],\n",
      "        [-1.3343, -1.2153, -1.4004, -1.6424],\n",
      "        [-1.3343, -1.2153, -1.4004, -1.6422],\n",
      "        [-1.3343, -1.2151, -1.4004, -1.6425],\n",
      "        [-1.3341, -1.2151, -1.4004, -1.6428],\n",
      "        [-1.3342, -1.2150, -1.4005, -1.6428],\n",
      "        [-1.3342, -1.2150, -1.4005, -1.6429],\n",
      "        [-1.3342, -1.2152, -1.4004, -1.6426]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([0.0132, 0.0075, 0.0076, 0.0028, 0.0125, 0.0117, 0.0064, 0.0033, 0.0126,\n",
      "        0.0097, 0.0055, 0.0022, 0.0111, 0.0099, 0.0060, 0.0037, 0.0110, 0.0080,\n",
      "        0.0063, 0.0026, 0.0131, 0.0094, 0.0081, 0.0035, 0.0114, 0.0099, 0.0070,\n",
      "        0.0030, 0.0155, 0.0113, 0.0053, 0.0029, 0.0136, 0.0085, 0.0067, 0.0033,\n",
      "        0.0153, 0.0099, 0.0068, 0.0036, 0.0158, 0.0101, 0.0065, 0.0033, 0.0128,\n",
      "        0.0098, 0.0066, 0.0031, 0.0145, 0.0089, 0.0053, 0.0029, 0.0129, 0.0088,\n",
      "        0.0077, 0.0035, 0.0129, 0.0101, 0.0056, 0.0036, 0.0119, 0.0100, 0.0072,\n",
      "        0.0038], grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(-0.0081, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2634, 0.2967, 0.2465, 0.1934],\n",
      "        [0.2633, 0.2966, 0.2465, 0.1936],\n",
      "        [0.2634, 0.2967, 0.2465, 0.1935],\n",
      "        [0.2634, 0.2966, 0.2465, 0.1935],\n",
      "        [0.2633, 0.2967, 0.2465, 0.1935],\n",
      "        [0.2634, 0.2967, 0.2465, 0.1935],\n",
      "        [0.2634, 0.2967, 0.2465, 0.1935],\n",
      "        [0.2634, 0.2967, 0.2465, 0.1935],\n",
      "        [0.2634, 0.2967, 0.2465, 0.1934],\n",
      "        [0.2633, 0.2967, 0.2465, 0.1935],\n",
      "        [0.2633, 0.2966, 0.2465, 0.1936],\n",
      "        [0.2633, 0.2966, 0.2465, 0.1936],\n",
      "        [0.2633, 0.2967, 0.2465, 0.1935],\n",
      "        [0.2634, 0.2968, 0.2465, 0.1934],\n",
      "        [0.2633, 0.2967, 0.2465, 0.1934],\n",
      "        [0.2634, 0.2968, 0.2465, 0.1934],\n",
      "        [0.2633, 0.2966, 0.2465, 0.1935],\n",
      "        [0.2633, 0.2966, 0.2465, 0.1935],\n",
      "        [0.2634, 0.2967, 0.2465, 0.1935],\n",
      "        [0.2633, 0.2967, 0.2465, 0.1935],\n",
      "        [0.2634, 0.2967, 0.2465, 0.1935],\n",
      "        [0.2634, 0.2966, 0.2465, 0.1935],\n",
      "        [0.2634, 0.2967, 0.2465, 0.1935],\n",
      "        [0.2634, 0.2966, 0.2465, 0.1935],\n",
      "        [0.2633, 0.2966, 0.2465, 0.1936],\n",
      "        [0.2633, 0.2966, 0.2465, 0.1935],\n",
      "        [0.2633, 0.2966, 0.2465, 0.1936],\n",
      "        [0.2633, 0.2966, 0.2465, 0.1935],\n",
      "        [0.2634, 0.2967, 0.2465, 0.1935],\n",
      "        [0.2633, 0.2966, 0.2465, 0.1935],\n",
      "        [0.2633, 0.2966, 0.2465, 0.1936],\n",
      "        [0.2633, 0.2966, 0.2465, 0.1935],\n",
      "        [0.2634, 0.2968, 0.2465, 0.1934],\n",
      "        [0.2634, 0.2967, 0.2465, 0.1934],\n",
      "        [0.2634, 0.2967, 0.2465, 0.1934],\n",
      "        [0.2634, 0.2968, 0.2465, 0.1934],\n",
      "        [0.2633, 0.2966, 0.2465, 0.1936],\n",
      "        [0.2633, 0.2967, 0.2465, 0.1935],\n",
      "        [0.2633, 0.2967, 0.2465, 0.1935],\n",
      "        [0.2633, 0.2966, 0.2465, 0.1936],\n",
      "        [0.2634, 0.2966, 0.2465, 0.1935],\n",
      "        [0.2633, 0.2966, 0.2465, 0.1935],\n",
      "        [0.2633, 0.2966, 0.2465, 0.1935],\n",
      "        [0.2634, 0.2966, 0.2465, 0.1935],\n",
      "        [0.2633, 0.2967, 0.2465, 0.1935],\n",
      "        [0.2634, 0.2967, 0.2465, 0.1935],\n",
      "        [0.2633, 0.2966, 0.2465, 0.1935],\n",
      "        [0.2634, 0.2966, 0.2465, 0.1935],\n",
      "        [0.2634, 0.2966, 0.2465, 0.1935],\n",
      "        [0.2634, 0.2966, 0.2465, 0.1935],\n",
      "        [0.2634, 0.2966, 0.2465, 0.1935],\n",
      "        [0.2634, 0.2967, 0.2465, 0.1935],\n",
      "        [0.2633, 0.2966, 0.2465, 0.1935],\n",
      "        [0.2633, 0.2966, 0.2465, 0.1936],\n",
      "        [0.2633, 0.2966, 0.2465, 0.1936],\n",
      "        [0.2633, 0.2966, 0.2465, 0.1936],\n",
      "        [0.2633, 0.2967, 0.2465, 0.1935],\n",
      "        [0.2633, 0.2966, 0.2465, 0.1935],\n",
      "        [0.2633, 0.2966, 0.2465, 0.1935],\n",
      "        [0.2634, 0.2967, 0.2465, 0.1935],\n",
      "        [0.2634, 0.2967, 0.2465, 0.1934],\n",
      "        [0.2634, 0.2967, 0.2465, 0.1934],\n",
      "        [0.2634, 0.2967, 0.2465, 0.1934],\n",
      "        [0.2634, 0.2967, 0.2465, 0.1935]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3749, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(-0.0218, grad_fn=<AddBackward0>)\n",
      "batching here\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60: done 4 episodes, mean_reward=1.50, best_reward=2.00, speed=226.72\n",
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(0.0077, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.3373, -1.2235, -1.3993, -1.6272],\n",
      "        [-1.3374, -1.2238, -1.3993, -1.6266],\n",
      "        [-1.3373, -1.2236, -1.3993, -1.6271],\n",
      "        [-1.3374, -1.2237, -1.3993, -1.6268],\n",
      "        [-1.3374, -1.2236, -1.3993, -1.6270],\n",
      "        [-1.3374, -1.2236, -1.3992, -1.6269],\n",
      "        [-1.3374, -1.2237, -1.3992, -1.6269],\n",
      "        [-1.3375, -1.2237, -1.3992, -1.6268],\n",
      "        [-1.3374, -1.2236, -1.3992, -1.6270],\n",
      "        [-1.3374, -1.2237, -1.3993, -1.6268],\n",
      "        [-1.3374, -1.2238, -1.3992, -1.6266],\n",
      "        [-1.3374, -1.2239, -1.3992, -1.6266],\n",
      "        [-1.3374, -1.2235, -1.3992, -1.6272],\n",
      "        [-1.3374, -1.2234, -1.3993, -1.6272],\n",
      "        [-1.3374, -1.2235, -1.3993, -1.6271],\n",
      "        [-1.3374, -1.2235, -1.3992, -1.6271],\n",
      "        [-1.3373, -1.2236, -1.3992, -1.6271],\n",
      "        [-1.3373, -1.2236, -1.3993, -1.6271],\n",
      "        [-1.3374, -1.2237, -1.3992, -1.6269],\n",
      "        [-1.3373, -1.2235, -1.3993, -1.6272],\n",
      "        [-1.3374, -1.2237, -1.3992, -1.6270],\n",
      "        [-1.3374, -1.2237, -1.3992, -1.6269],\n",
      "        [-1.3373, -1.2236, -1.3993, -1.6270],\n",
      "        [-1.3374, -1.2236, -1.3992, -1.6271],\n",
      "        [-1.3375, -1.2239, -1.3992, -1.6266],\n",
      "        [-1.3374, -1.2237, -1.3991, -1.6269],\n",
      "        [-1.3375, -1.2238, -1.3992, -1.6267],\n",
      "        [-1.3375, -1.2241, -1.3991, -1.6262],\n",
      "        [-1.3374, -1.2237, -1.3992, -1.6269],\n",
      "        [-1.3374, -1.2239, -1.3992, -1.6266],\n",
      "        [-1.3375, -1.2239, -1.3992, -1.6265],\n",
      "        [-1.3374, -1.2237, -1.3992, -1.6269],\n",
      "        [-1.3373, -1.2234, -1.3993, -1.6274],\n",
      "        [-1.3373, -1.2234, -1.3993, -1.6273],\n",
      "        [-1.3373, -1.2234, -1.3993, -1.6273],\n",
      "        [-1.3373, -1.2235, -1.3993, -1.6271],\n",
      "        [-1.3375, -1.2239, -1.3992, -1.6265],\n",
      "        [-1.3374, -1.2237, -1.3992, -1.6269],\n",
      "        [-1.3374, -1.2238, -1.3992, -1.6268],\n",
      "        [-1.3373, -1.2237, -1.3992, -1.6269],\n",
      "        [-1.3374, -1.2238, -1.3993, -1.6266],\n",
      "        [-1.3374, -1.2238, -1.3992, -1.6268],\n",
      "        [-1.3374, -1.2237, -1.3993, -1.6268],\n",
      "        [-1.3374, -1.2238, -1.3992, -1.6267],\n",
      "        [-1.3374, -1.2237, -1.3993, -1.6267],\n",
      "        [-1.3374, -1.2236, -1.3993, -1.6270],\n",
      "        [-1.3375, -1.2239, -1.3993, -1.6264],\n",
      "        [-1.3374, -1.2236, -1.3993, -1.6270],\n",
      "        [-1.3373, -1.2236, -1.3992, -1.6271],\n",
      "        [-1.3373, -1.2237, -1.3992, -1.6270],\n",
      "        [-1.3373, -1.2237, -1.3992, -1.6270],\n",
      "        [-1.3373, -1.2237, -1.3992, -1.6269],\n",
      "        [-1.3374, -1.2239, -1.3992, -1.6266],\n",
      "        [-1.3374, -1.2237, -1.3992, -1.6269],\n",
      "        [-1.3374, -1.2235, -1.3993, -1.6271],\n",
      "        [-1.3374, -1.2237, -1.3992, -1.6269],\n",
      "        [-1.3374, -1.2237, -1.3992, -1.6269],\n",
      "        [-1.3373, -1.2236, -1.3993, -1.6270],\n",
      "        [-1.3374, -1.2237, -1.3993, -1.6268],\n",
      "        [-1.3374, -1.2237, -1.3993, -1.6269],\n",
      "        [-1.3374, -1.2238, -1.3992, -1.6268],\n",
      "        [-1.3374, -1.2236, -1.3993, -1.6271],\n",
      "        [-1.3374, -1.2235, -1.3992, -1.6272],\n",
      "        [-1.3374, -1.2237, -1.3991, -1.6269]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([0.0141, 0.0077, 0.0060, 0.0023, 0.2585, 0.0104, 0.0059, 0.0029, 0.0108,\n",
      "        0.0085, 0.0049, 0.0024, 0.2957, 0.2586, 0.2955, 0.0026, 0.0117, 0.0083,\n",
      "        0.0056, 0.0040, 0.0133, 0.0087, 0.0061, 0.0036, 0.0118, 0.0086, 0.0065,\n",
      "        0.0026, 0.0134, 0.0099, 0.0050, 0.0031, 0.2831, 0.2829, 0.3440, 0.0028,\n",
      "        0.0100, 0.0110, 0.0061, 0.0036, 0.0130, 0.0102, 0.0049, 0.0026, 0.0135,\n",
      "        0.0083, 0.0063, 0.0031, 0.2963, 0.3443, 0.3442, 0.3441, 0.0111, 0.0076,\n",
      "        0.0070, 0.0026, 0.0109, 0.0105, 0.0053, 0.0034, 0.0126, 0.0086, 0.0060,\n",
      "        0.0036], grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(-0.0583, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2625, 0.2942, 0.2468, 0.1965],\n",
      "        [0.2625, 0.2941, 0.2468, 0.1966],\n",
      "        [0.2626, 0.2942, 0.2468, 0.1965],\n",
      "        [0.2625, 0.2941, 0.2468, 0.1966],\n",
      "        [0.2625, 0.2942, 0.2468, 0.1965],\n",
      "        [0.2625, 0.2942, 0.2468, 0.1965],\n",
      "        [0.2625, 0.2941, 0.2468, 0.1965],\n",
      "        [0.2625, 0.2942, 0.2468, 0.1965],\n",
      "        [0.2625, 0.2942, 0.2468, 0.1965],\n",
      "        [0.2625, 0.2941, 0.2468, 0.1966],\n",
      "        [0.2625, 0.2941, 0.2468, 0.1966],\n",
      "        [0.2625, 0.2941, 0.2468, 0.1966],\n",
      "        [0.2625, 0.2942, 0.2468, 0.1965],\n",
      "        [0.2625, 0.2942, 0.2468, 0.1965],\n",
      "        [0.2625, 0.2942, 0.2468, 0.1965],\n",
      "        [0.2625, 0.2942, 0.2468, 0.1965],\n",
      "        [0.2625, 0.2942, 0.2468, 0.1965],\n",
      "        [0.2625, 0.2942, 0.2468, 0.1965],\n",
      "        [0.2625, 0.2941, 0.2468, 0.1965],\n",
      "        [0.2625, 0.2942, 0.2468, 0.1965],\n",
      "        [0.2625, 0.2941, 0.2468, 0.1965],\n",
      "        [0.2625, 0.2941, 0.2468, 0.1965],\n",
      "        [0.2625, 0.2942, 0.2468, 0.1965],\n",
      "        [0.2625, 0.2942, 0.2468, 0.1965],\n",
      "        [0.2625, 0.2941, 0.2468, 0.1966],\n",
      "        [0.2625, 0.2941, 0.2468, 0.1965],\n",
      "        [0.2625, 0.2941, 0.2468, 0.1966],\n",
      "        [0.2625, 0.2940, 0.2468, 0.1967],\n",
      "        [0.2625, 0.2941, 0.2468, 0.1965],\n",
      "        [0.2625, 0.2941, 0.2468, 0.1966],\n",
      "        [0.2625, 0.2941, 0.2468, 0.1966],\n",
      "        [0.2625, 0.2941, 0.2468, 0.1965],\n",
      "        [0.2626, 0.2942, 0.2468, 0.1964],\n",
      "        [0.2625, 0.2942, 0.2468, 0.1965],\n",
      "        [0.2625, 0.2942, 0.2468, 0.1965],\n",
      "        [0.2625, 0.2942, 0.2468, 0.1965],\n",
      "        [0.2625, 0.2941, 0.2468, 0.1966],\n",
      "        [0.2625, 0.2941, 0.2468, 0.1965],\n",
      "        [0.2625, 0.2941, 0.2468, 0.1966],\n",
      "        [0.2625, 0.2941, 0.2468, 0.1965],\n",
      "        [0.2625, 0.2941, 0.2468, 0.1966],\n",
      "        [0.2625, 0.2941, 0.2468, 0.1966],\n",
      "        [0.2625, 0.2941, 0.2468, 0.1966],\n",
      "        [0.2625, 0.2941, 0.2468, 0.1966],\n",
      "        [0.2625, 0.2941, 0.2468, 0.1966],\n",
      "        [0.2625, 0.2942, 0.2468, 0.1965],\n",
      "        [0.2625, 0.2941, 0.2468, 0.1966],\n",
      "        [0.2625, 0.2942, 0.2468, 0.1965],\n",
      "        [0.2626, 0.2942, 0.2468, 0.1965],\n",
      "        [0.2626, 0.2941, 0.2468, 0.1965],\n",
      "        [0.2625, 0.2941, 0.2468, 0.1965],\n",
      "        [0.2625, 0.2941, 0.2468, 0.1965],\n",
      "        [0.2625, 0.2941, 0.2468, 0.1966],\n",
      "        [0.2625, 0.2941, 0.2468, 0.1965],\n",
      "        [0.2625, 0.2942, 0.2468, 0.1965],\n",
      "        [0.2625, 0.2941, 0.2468, 0.1965],\n",
      "        [0.2625, 0.2941, 0.2468, 0.1965],\n",
      "        [0.2625, 0.2942, 0.2468, 0.1965],\n",
      "        [0.2625, 0.2941, 0.2468, 0.1966],\n",
      "        [0.2625, 0.2941, 0.2468, 0.1965],\n",
      "        [0.2625, 0.2941, 0.2468, 0.1966],\n",
      "        [0.2625, 0.2942, 0.2468, 0.1965],\n",
      "        [0.2625, 0.2942, 0.2468, 0.1965],\n",
      "        [0.2625, 0.2941, 0.2468, 0.1965]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3761, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(-0.0644, grad_fn=<AddBackward0>)\n",
      "batching here\n",
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(0.0155, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.3416, -1.2423, -1.3969, -1.5971],\n",
      "        [-1.3416, -1.2425, -1.3969, -1.5968],\n",
      "        [-1.3416, -1.2425, -1.3969, -1.5968],\n",
      "        [-1.3416, -1.2424, -1.3968, -1.5970],\n",
      "        [-1.3416, -1.2424, -1.3969, -1.5969],\n",
      "        [-1.3417, -1.2425, -1.3969, -1.5965],\n",
      "        [-1.3417, -1.2425, -1.3969, -1.5967],\n",
      "        [-1.3417, -1.2424, -1.3969, -1.5968],\n",
      "        [-1.3417, -1.2426, -1.3968, -1.5966],\n",
      "        [-1.3416, -1.2426, -1.3969, -1.5966],\n",
      "        [-1.3417, -1.2426, -1.3968, -1.5966],\n",
      "        [-1.3417, -1.2426, -1.3968, -1.5967],\n",
      "        [-1.3417, -1.2424, -1.3968, -1.5968],\n",
      "        [-1.3417, -1.2424, -1.3969, -1.5968],\n",
      "        [-1.3417, -1.2423, -1.3969, -1.5970],\n",
      "        [-1.3418, -1.2425, -1.3969, -1.5966],\n",
      "        [-1.3417, -1.2425, -1.3969, -1.5967],\n",
      "        [-1.3417, -1.2423, -1.3969, -1.5970],\n",
      "        [-1.3417, -1.2426, -1.3969, -1.5965],\n",
      "        [-1.3416, -1.2423, -1.3970, -1.5971],\n",
      "        [-1.3417, -1.2424, -1.3968, -1.5969],\n",
      "        [-1.3416, -1.2424, -1.3969, -1.5970],\n",
      "        [-1.3416, -1.2424, -1.3969, -1.5969],\n",
      "        [-1.3416, -1.2424, -1.3969, -1.5970],\n",
      "        [-1.3418, -1.2429, -1.3968, -1.5960],\n",
      "        [-1.3416, -1.2423, -1.3969, -1.5970],\n",
      "        [-1.3416, -1.2424, -1.3969, -1.5968],\n",
      "        [-1.3417, -1.2427, -1.3969, -1.5964],\n",
      "        [-1.3416, -1.2425, -1.3968, -1.5968],\n",
      "        [-1.3416, -1.2424, -1.3969, -1.5969],\n",
      "        [-1.3417, -1.2424, -1.3969, -1.5968],\n",
      "        [-1.3416, -1.2424, -1.3968, -1.5970],\n",
      "        [-1.3417, -1.2424, -1.3969, -1.5968],\n",
      "        [-1.3417, -1.2424, -1.3969, -1.5970],\n",
      "        [-1.3416, -1.2422, -1.3969, -1.5972],\n",
      "        [-1.3417, -1.2425, -1.3968, -1.5968],\n",
      "        [-1.3417, -1.2425, -1.3969, -1.5967],\n",
      "        [-1.3416, -1.2424, -1.3969, -1.5969],\n",
      "        [-1.3416, -1.2424, -1.3969, -1.5968],\n",
      "        [-1.3416, -1.2425, -1.3969, -1.5968],\n",
      "        [-1.3417, -1.2425, -1.3969, -1.5967],\n",
      "        [-1.3417, -1.2427, -1.3969, -1.5964],\n",
      "        [-1.3417, -1.2425, -1.3969, -1.5968],\n",
      "        [-1.3417, -1.2426, -1.3969, -1.5965],\n",
      "        [-1.3417, -1.2427, -1.3969, -1.5963],\n",
      "        [-1.3416, -1.2423, -1.3969, -1.5970],\n",
      "        [-1.3418, -1.2429, -1.3969, -1.5960],\n",
      "        [-1.3416, -1.2423, -1.3969, -1.5970],\n",
      "        [-1.3417, -1.2425, -1.3968, -1.5967],\n",
      "        [-1.3417, -1.2426, -1.3967, -1.5967],\n",
      "        [-1.3417, -1.2426, -1.3968, -1.5967],\n",
      "        [-1.3416, -1.2424, -1.3969, -1.5969],\n",
      "        [-1.3416, -1.2424, -1.3969, -1.5968],\n",
      "        [-1.3417, -1.2425, -1.3969, -1.5967],\n",
      "        [-1.3417, -1.2425, -1.3968, -1.5967],\n",
      "        [-1.3417, -1.2424, -1.3969, -1.5968],\n",
      "        [-1.3416, -1.2424, -1.3969, -1.5969],\n",
      "        [-1.3416, -1.2424, -1.3969, -1.5968],\n",
      "        [-1.3416, -1.2424, -1.3969, -1.5968],\n",
      "        [-1.3417, -1.2424, -1.3969, -1.5968],\n",
      "        [-1.3418, -1.2428, -1.3968, -1.5962],\n",
      "        [-1.3417, -1.2428, -1.3968, -1.5964],\n",
      "        [-1.3416, -1.2424, -1.3968, -1.5969],\n",
      "        [-1.3416, -1.2424, -1.3968, -1.5970]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([ 6.8060e-03,  4.8353e-03,  3.3177e-03,  1.7151e-03,  8.0532e-03,\n",
      "         5.8384e-03,  3.5264e-03,  1.5325e-03,  6.8706e-03,  4.8027e-03,\n",
      "         3.4919e-03,  1.8805e-03,  6.2292e-03,  5.0811e-03,  3.5450e-03,\n",
      "         1.5782e-03,  6.6281e-03,  4.9359e-03,  3.3956e-03,  1.9431e-03,\n",
      "         7.5823e-03,  4.7780e-03,  3.2848e-03,  1.7746e-03,  7.6240e-03,\n",
      "         5.7316e-03,  4.1895e-03,  1.7960e-03,  6.6486e-03,  4.7641e-03,\n",
      "         2.7848e-03,  1.8123e-03,  6.6284e-03,  4.6151e-03,  3.4271e-03,\n",
      "         1.2078e-03,  6.0958e-03,  5.5457e-03,  3.6345e-03,  2.2427e-03,\n",
      "         7.0157e-03,  4.6394e-03,  3.0588e-03,  1.1916e-03,  7.5780e-03,\n",
      "         6.5407e-03,  2.3696e-03,  2.7403e-03,  7.2990e-03,  4.9386e-03,\n",
      "         3.3112e-03,  1.8824e-03, -1.2363e+00,  5.2024e-03,  3.1154e-03,\n",
      "         1.7193e-03,  7.2278e-03,  4.8721e-03,  3.1781e-03,  1.6494e-03,\n",
      "         5.3764e-03,  4.3586e-03,  3.4961e-03,  1.9315e-03],\n",
      "       grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(0.0151, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2614, 0.2887, 0.2474, 0.2025],\n",
      "        [0.2614, 0.2887, 0.2474, 0.2026],\n",
      "        [0.2614, 0.2887, 0.2474, 0.2025],\n",
      "        [0.2614, 0.2887, 0.2474, 0.2025],\n",
      "        [0.2614, 0.2887, 0.2474, 0.2025],\n",
      "        [0.2614, 0.2886, 0.2474, 0.2026],\n",
      "        [0.2614, 0.2887, 0.2474, 0.2026],\n",
      "        [0.2614, 0.2887, 0.2474, 0.2025],\n",
      "        [0.2614, 0.2886, 0.2474, 0.2026],\n",
      "        [0.2614, 0.2886, 0.2474, 0.2026],\n",
      "        [0.2614, 0.2886, 0.2474, 0.2026],\n",
      "        [0.2614, 0.2886, 0.2474, 0.2026],\n",
      "        [0.2614, 0.2887, 0.2474, 0.2025],\n",
      "        [0.2614, 0.2887, 0.2474, 0.2025],\n",
      "        [0.2614, 0.2887, 0.2474, 0.2025],\n",
      "        [0.2614, 0.2887, 0.2474, 0.2026],\n",
      "        [0.2614, 0.2887, 0.2474, 0.2026],\n",
      "        [0.2614, 0.2887, 0.2474, 0.2025],\n",
      "        [0.2614, 0.2886, 0.2474, 0.2026],\n",
      "        [0.2614, 0.2887, 0.2473, 0.2025],\n",
      "        [0.2614, 0.2887, 0.2474, 0.2025],\n",
      "        [0.2614, 0.2887, 0.2474, 0.2025],\n",
      "        [0.2614, 0.2887, 0.2474, 0.2025],\n",
      "        [0.2614, 0.2887, 0.2474, 0.2025],\n",
      "        [0.2614, 0.2885, 0.2474, 0.2027],\n",
      "        [0.2614, 0.2887, 0.2474, 0.2025],\n",
      "        [0.2614, 0.2887, 0.2474, 0.2025],\n",
      "        [0.2614, 0.2886, 0.2474, 0.2026],\n",
      "        [0.2614, 0.2887, 0.2474, 0.2025],\n",
      "        [0.2614, 0.2887, 0.2474, 0.2025],\n",
      "        [0.2614, 0.2887, 0.2474, 0.2025],\n",
      "        [0.2614, 0.2887, 0.2474, 0.2025],\n",
      "        [0.2614, 0.2887, 0.2474, 0.2025],\n",
      "        [0.2614, 0.2887, 0.2474, 0.2025],\n",
      "        [0.2614, 0.2887, 0.2474, 0.2025],\n",
      "        [0.2614, 0.2887, 0.2474, 0.2025],\n",
      "        [0.2614, 0.2887, 0.2474, 0.2026],\n",
      "        [0.2614, 0.2887, 0.2474, 0.2025],\n",
      "        [0.2614, 0.2887, 0.2474, 0.2025],\n",
      "        [0.2614, 0.2887, 0.2474, 0.2026],\n",
      "        [0.2614, 0.2887, 0.2474, 0.2026],\n",
      "        [0.2614, 0.2886, 0.2474, 0.2026],\n",
      "        [0.2614, 0.2887, 0.2474, 0.2026],\n",
      "        [0.2614, 0.2886, 0.2474, 0.2026],\n",
      "        [0.2614, 0.2886, 0.2474, 0.2026],\n",
      "        [0.2614, 0.2887, 0.2474, 0.2025],\n",
      "        [0.2614, 0.2885, 0.2474, 0.2027],\n",
      "        [0.2614, 0.2887, 0.2474, 0.2025],\n",
      "        [0.2614, 0.2887, 0.2474, 0.2026],\n",
      "        [0.2614, 0.2886, 0.2474, 0.2026],\n",
      "        [0.2614, 0.2886, 0.2474, 0.2026],\n",
      "        [0.2614, 0.2887, 0.2474, 0.2025],\n",
      "        [0.2614, 0.2887, 0.2474, 0.2025],\n",
      "        [0.2614, 0.2887, 0.2474, 0.2026],\n",
      "        [0.2614, 0.2886, 0.2474, 0.2026],\n",
      "        [0.2614, 0.2887, 0.2474, 0.2025],\n",
      "        [0.2614, 0.2887, 0.2474, 0.2025],\n",
      "        [0.2614, 0.2887, 0.2474, 0.2025],\n",
      "        [0.2614, 0.2887, 0.2474, 0.2025],\n",
      "        [0.2614, 0.2887, 0.2474, 0.2026],\n",
      "        [0.2614, 0.2886, 0.2474, 0.2027],\n",
      "        [0.2614, 0.2886, 0.2474, 0.2026],\n",
      "        [0.2614, 0.2887, 0.2474, 0.2025],\n",
      "        [0.2614, 0.2887, 0.2474, 0.2025]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3784, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(0.0168, grad_fn=<AddBackward0>)\n",
      "batching here\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(2.2450e-05, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.3456, -1.1874, -1.4101, -1.6581],\n",
      "        [-1.3457, -1.1874, -1.4101, -1.6581],\n",
      "        [-1.3457, -1.1873, -1.4101, -1.6583],\n",
      "        [-1.3457, -1.1874, -1.4101, -1.6581],\n",
      "        [-1.3457, -1.1876, -1.4101, -1.6577],\n",
      "        [-1.3457, -1.1873, -1.4101, -1.6581],\n",
      "        [-1.3458, -1.1875, -1.4101, -1.6579],\n",
      "        [-1.3457, -1.1874, -1.4101, -1.6580],\n",
      "        [-1.3457, -1.1877, -1.4100, -1.6577],\n",
      "        [-1.3457, -1.1876, -1.4101, -1.6579],\n",
      "        [-1.3457, -1.1878, -1.4101, -1.6575],\n",
      "        [-1.3457, -1.1875, -1.4101, -1.6578],\n",
      "        [-1.3457, -1.1874, -1.4101, -1.6581],\n",
      "        [-1.3457, -1.1873, -1.4101, -1.6581],\n",
      "        [-1.3457, -1.1875, -1.4101, -1.6578],\n",
      "        [-1.3457, -1.1873, -1.4101, -1.6581],\n",
      "        [-1.3457, -1.1875, -1.4101, -1.6579],\n",
      "        [-1.3457, -1.1875, -1.4101, -1.6579],\n",
      "        [-1.3457, -1.1874, -1.4101, -1.6580],\n",
      "        [-1.3457, -1.1873, -1.4101, -1.6582],\n",
      "        [-1.3457, -1.1873, -1.4101, -1.6582],\n",
      "        [-1.3457, -1.1874, -1.4101, -1.6580],\n",
      "        [-1.3457, -1.1875, -1.4101, -1.6580],\n",
      "        [-1.3456, -1.1873, -1.4101, -1.6584],\n",
      "        [-1.3457, -1.1878, -1.4100, -1.6576],\n",
      "        [-1.3457, -1.1876, -1.4100, -1.6577],\n",
      "        [-1.3457, -1.1876, -1.4101, -1.6577],\n",
      "        [-1.3457, -1.1876, -1.4100, -1.6577],\n",
      "        [-1.3457, -1.1874, -1.4100, -1.6581],\n",
      "        [-1.3457, -1.1874, -1.4100, -1.6582],\n",
      "        [-1.3457, -1.1876, -1.4100, -1.6579],\n",
      "        [-1.3457, -1.1874, -1.4100, -1.6581],\n",
      "        [-1.3457, -1.1873, -1.4101, -1.6582],\n",
      "        [-1.3457, -1.1872, -1.4101, -1.6583],\n",
      "        [-1.3457, -1.1874, -1.4101, -1.6580],\n",
      "        [-1.3457, -1.1874, -1.4101, -1.6580],\n",
      "        [-1.3457, -1.1875, -1.4101, -1.6579],\n",
      "        [-1.3457, -1.1875, -1.4101, -1.6579],\n",
      "        [-1.3457, -1.1875, -1.4101, -1.6579],\n",
      "        [-1.3457, -1.1876, -1.4101, -1.6578],\n",
      "        [-1.3457, -1.1875, -1.4101, -1.6578],\n",
      "        [-1.3457, -1.1876, -1.4101, -1.6578],\n",
      "        [-1.3457, -1.1876, -1.4101, -1.6578],\n",
      "        [-1.3457, -1.1875, -1.4101, -1.6579],\n",
      "        [-1.3457, -1.1875, -1.4101, -1.6578],\n",
      "        [-1.3457, -1.1873, -1.4101, -1.6582],\n",
      "        [-1.3457, -1.1874, -1.4101, -1.6580],\n",
      "        [-1.3457, -1.1874, -1.4101, -1.6580],\n",
      "        [-1.3457, -1.1876, -1.4101, -1.6577],\n",
      "        [-1.3456, -1.1876, -1.4100, -1.6579],\n",
      "        [-1.3457, -1.1876, -1.4101, -1.6579],\n",
      "        [-1.3456, -1.1878, -1.4100, -1.6576],\n",
      "        [-1.3457, -1.1875, -1.4100, -1.6580],\n",
      "        [-1.3457, -1.1875, -1.4101, -1.6579],\n",
      "        [-1.3458, -1.1875, -1.4101, -1.6579],\n",
      "        [-1.3457, -1.1874, -1.4101, -1.6580],\n",
      "        [-1.3457, -1.1875, -1.4101, -1.6579],\n",
      "        [-1.3457, -1.1876, -1.4101, -1.6578],\n",
      "        [-1.3457, -1.1876, -1.4101, -1.6578],\n",
      "        [-1.3457, -1.1876, -1.4101, -1.6578],\n",
      "        [-1.3457, -1.1875, -1.4100, -1.6580],\n",
      "        [-1.3457, -1.1875, -1.4101, -1.6579],\n",
      "        [-1.3458, -1.1877, -1.4100, -1.6576],\n",
      "        [-1.3457, -1.1877, -1.4101, -1.6575]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([0.0081, 0.0069, 0.0043, 0.0021, 0.0093, 0.0079, 0.0052, 0.0027, 0.0116,\n",
      "        0.0075, 0.0049, 0.0025, 0.0084, 0.0062, 0.0040, 0.0026, 0.0098, 0.0062,\n",
      "        0.0049, 0.0026, 0.0096, 0.0061, 0.0041, 0.0029, 0.0078, 0.0085, 0.0040,\n",
      "        0.0023, 0.0098, 0.0073, 0.0046, 0.0027, 0.0095, 0.0059, 0.0043, 0.0016,\n",
      "        0.0114, 0.0062, 0.0049, 0.0021, 0.0093, 0.0060, 0.0045, 0.0028, 0.0079,\n",
      "        0.0075, 0.0041, 0.0022, 0.0095, 0.0071, 0.0045, 0.0020, 0.0115, 0.0089,\n",
      "        0.0044, 0.0032, 0.0083, 0.0062, 0.0041, 0.0025, 0.0082, 0.0072, 0.0047,\n",
      "        0.0020], grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(-0.0058, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2604, 0.3050, 0.2441, 0.1905],\n",
      "        [0.2604, 0.3050, 0.2441, 0.1905],\n",
      "        [0.2604, 0.3050, 0.2441, 0.1905],\n",
      "        [0.2604, 0.3050, 0.2441, 0.1905],\n",
      "        [0.2604, 0.3050, 0.2441, 0.1906],\n",
      "        [0.2603, 0.3050, 0.2441, 0.1905],\n",
      "        [0.2603, 0.3050, 0.2441, 0.1905],\n",
      "        [0.2603, 0.3050, 0.2441, 0.1905],\n",
      "        [0.2604, 0.3049, 0.2441, 0.1906],\n",
      "        [0.2604, 0.3050, 0.2441, 0.1905],\n",
      "        [0.2604, 0.3049, 0.2441, 0.1906],\n",
      "        [0.2604, 0.3050, 0.2441, 0.1906],\n",
      "        [0.2604, 0.3050, 0.2441, 0.1905],\n",
      "        [0.2604, 0.3050, 0.2441, 0.1905],\n",
      "        [0.2604, 0.3050, 0.2441, 0.1906],\n",
      "        [0.2604, 0.3050, 0.2441, 0.1905],\n",
      "        [0.2603, 0.3050, 0.2441, 0.1905],\n",
      "        [0.2604, 0.3050, 0.2441, 0.1905],\n",
      "        [0.2604, 0.3050, 0.2441, 0.1905],\n",
      "        [0.2604, 0.3050, 0.2441, 0.1905],\n",
      "        [0.2604, 0.3050, 0.2441, 0.1905],\n",
      "        [0.2604, 0.3050, 0.2441, 0.1905],\n",
      "        [0.2604, 0.3050, 0.2441, 0.1905],\n",
      "        [0.2604, 0.3051, 0.2441, 0.1905],\n",
      "        [0.2604, 0.3049, 0.2441, 0.1906],\n",
      "        [0.2603, 0.3049, 0.2441, 0.1906],\n",
      "        [0.2604, 0.3049, 0.2441, 0.1906],\n",
      "        [0.2604, 0.3049, 0.2441, 0.1906],\n",
      "        [0.2603, 0.3050, 0.2441, 0.1905],\n",
      "        [0.2604, 0.3050, 0.2441, 0.1905],\n",
      "        [0.2604, 0.3050, 0.2441, 0.1905],\n",
      "        [0.2604, 0.3050, 0.2441, 0.1905],\n",
      "        [0.2604, 0.3050, 0.2441, 0.1905],\n",
      "        [0.2604, 0.3051, 0.2441, 0.1905],\n",
      "        [0.2604, 0.3050, 0.2441, 0.1905],\n",
      "        [0.2604, 0.3050, 0.2441, 0.1905],\n",
      "        [0.2604, 0.3050, 0.2441, 0.1905],\n",
      "        [0.2604, 0.3050, 0.2441, 0.1905],\n",
      "        [0.2604, 0.3050, 0.2441, 0.1905],\n",
      "        [0.2604, 0.3050, 0.2441, 0.1906],\n",
      "        [0.2604, 0.3050, 0.2441, 0.1906],\n",
      "        [0.2604, 0.3050, 0.2441, 0.1906],\n",
      "        [0.2604, 0.3050, 0.2441, 0.1906],\n",
      "        [0.2604, 0.3050, 0.2441, 0.1905],\n",
      "        [0.2603, 0.3050, 0.2441, 0.1906],\n",
      "        [0.2604, 0.3050, 0.2441, 0.1905],\n",
      "        [0.2604, 0.3050, 0.2441, 0.1905],\n",
      "        [0.2604, 0.3050, 0.2441, 0.1905],\n",
      "        [0.2604, 0.3049, 0.2441, 0.1906],\n",
      "        [0.2604, 0.3049, 0.2441, 0.1905],\n",
      "        [0.2604, 0.3050, 0.2441, 0.1905],\n",
      "        [0.2604, 0.3049, 0.2441, 0.1906],\n",
      "        [0.2604, 0.3050, 0.2441, 0.1905],\n",
      "        [0.2604, 0.3050, 0.2441, 0.1905],\n",
      "        [0.2603, 0.3050, 0.2441, 0.1905],\n",
      "        [0.2603, 0.3050, 0.2441, 0.1905],\n",
      "        [0.2604, 0.3050, 0.2441, 0.1905],\n",
      "        [0.2604, 0.3050, 0.2441, 0.1906],\n",
      "        [0.2604, 0.3050, 0.2441, 0.1906],\n",
      "        [0.2604, 0.3050, 0.2441, 0.1906],\n",
      "        [0.2604, 0.3050, 0.2441, 0.1905],\n",
      "        [0.2604, 0.3050, 0.2441, 0.1905],\n",
      "        [0.2603, 0.3049, 0.2441, 0.1906],\n",
      "        [0.2604, 0.3049, 0.2441, 0.1906]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3727, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(-0.0195, grad_fn=<AddBackward0>)\n",
      "batching here\n",
      "63: done 4 episodes, mean_reward=0.00, best_reward=2.00, speed=222.28\n",
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(0.0024, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.3463, -1.1969, -1.4088, -1.6439],\n",
      "        [-1.3463, -1.1968, -1.4088, -1.6440],\n",
      "        [-1.3463, -1.1968, -1.4089, -1.6439],\n",
      "        [-1.3464, -1.1967, -1.4088, -1.6440],\n",
      "        [-1.3464, -1.1970, -1.4088, -1.6436],\n",
      "        [-1.3463, -1.1969, -1.4089, -1.6437],\n",
      "        [-1.3463, -1.1970, -1.4088, -1.6437],\n",
      "        [-1.3463, -1.1970, -1.4088, -1.6437],\n",
      "        [-1.3464, -1.1972, -1.4088, -1.6434],\n",
      "        [-1.3463, -1.1969, -1.4088, -1.6438],\n",
      "        [-1.3463, -1.1969, -1.4088, -1.6438],\n",
      "        [-1.3464, -1.1972, -1.4088, -1.6434],\n",
      "        [-1.3463, -1.1970, -1.4088, -1.6437],\n",
      "        [-1.3463, -1.1968, -1.4089, -1.6439],\n",
      "        [-1.3463, -1.1969, -1.4088, -1.6437],\n",
      "        [-1.3463, -1.1968, -1.4089, -1.6439],\n",
      "        [-1.3463, -1.1969, -1.4088, -1.6438],\n",
      "        [-1.3463, -1.1967, -1.4088, -1.6441],\n",
      "        [-1.3463, -1.1969, -1.4088, -1.6439],\n",
      "        [-1.3463, -1.1969, -1.4089, -1.6438],\n",
      "        [-1.3462, -1.1969, -1.4089, -1.6439],\n",
      "        [-1.3463, -1.1968, -1.4088, -1.6440],\n",
      "        [-1.3463, -1.1968, -1.4088, -1.6439],\n",
      "        [-1.3463, -1.1968, -1.4088, -1.6440],\n",
      "        [-1.3463, -1.1970, -1.4088, -1.6437],\n",
      "        [-1.3463, -1.1971, -1.4088, -1.6435],\n",
      "        [-1.3463, -1.1971, -1.4088, -1.6436],\n",
      "        [-1.3463, -1.1972, -1.4088, -1.6434],\n",
      "        [-1.3463, -1.1969, -1.4088, -1.6439],\n",
      "        [-1.3463, -1.1970, -1.4088, -1.6438],\n",
      "        [-1.3463, -1.1970, -1.4088, -1.6437],\n",
      "        [-1.3463, -1.1971, -1.4088, -1.6436],\n",
      "        [-1.3462, -1.1967, -1.4089, -1.6441],\n",
      "        [-1.3463, -1.1966, -1.4089, -1.6442],\n",
      "        [-1.3463, -1.1970, -1.4089, -1.6436],\n",
      "        [-1.3463, -1.1968, -1.4089, -1.6440],\n",
      "        [-1.3463, -1.1970, -1.4088, -1.6437],\n",
      "        [-1.3463, -1.1970, -1.4088, -1.6437],\n",
      "        [-1.3463, -1.1970, -1.4088, -1.6437],\n",
      "        [-1.3463, -1.1970, -1.4088, -1.6437],\n",
      "        [-1.3463, -1.1969, -1.4089, -1.6438],\n",
      "        [-1.3463, -1.1970, -1.4088, -1.6436],\n",
      "        [-1.3463, -1.1970, -1.4089, -1.6437],\n",
      "        [-1.3463, -1.1970, -1.4089, -1.6437],\n",
      "        [-1.3463, -1.1968, -1.4089, -1.6439],\n",
      "        [-1.3463, -1.1969, -1.4088, -1.6438],\n",
      "        [-1.3462, -1.1966, -1.4089, -1.6444],\n",
      "        [-1.3463, -1.1969, -1.4088, -1.6439],\n",
      "        [-1.3463, -1.1971, -1.4088, -1.6437],\n",
      "        [-1.3463, -1.1972, -1.4088, -1.6435],\n",
      "        [-1.3462, -1.1971, -1.4088, -1.6437],\n",
      "        [-1.3463, -1.1972, -1.4088, -1.6435],\n",
      "        [-1.3464, -1.1969, -1.4088, -1.6437],\n",
      "        [-1.3463, -1.1973, -1.4087, -1.6433],\n",
      "        [-1.3463, -1.1969, -1.4088, -1.6438],\n",
      "        [-1.3463, -1.1970, -1.4088, -1.6438],\n",
      "        [-1.3463, -1.1970, -1.4088, -1.6436],\n",
      "        [-1.3463, -1.1971, -1.4088, -1.6435],\n",
      "        [-1.3463, -1.1970, -1.4089, -1.6436],\n",
      "        [-1.3463, -1.1970, -1.4088, -1.6437],\n",
      "        [-1.3463, -1.1971, -1.4088, -1.6436],\n",
      "        [-1.3463, -1.1969, -1.4088, -1.6438],\n",
      "        [-1.3463, -1.1969, -1.4088, -1.6438],\n",
      "        [-1.3463, -1.1969, -1.4088, -1.6438]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([0.1923, 0.0085, 0.0043, 0.0027, 0.0084, 0.0058, 0.0038, 0.0026, 0.0076,\n",
      "        0.0072, 0.0051, 0.0018, 0.0088, 0.0068, 0.0042, 0.0021, 0.0087, 0.0075,\n",
      "        0.0047, 0.0023, 0.1924, 0.0086, 0.0050, 0.0027, 0.0076, 0.0062, 0.0039,\n",
      "        0.0022, 0.0092, 0.0066, 0.0053, 0.0025, 0.0106, 0.0064, 0.0038, 0.0019,\n",
      "        0.1920, 0.2259, 0.2636, 0.0022, 0.0086, 0.0078, 0.0053, 0.0023, 0.0083,\n",
      "        0.0065, 0.0042, 0.0018, 0.0076, 0.0062, 0.0038, 0.0019, 0.0090, 0.0068,\n",
      "        0.0057, 0.0027, 0.2636, 0.0061, 0.0042, 0.0018, 0.0087, 0.0056, 0.0037,\n",
      "        0.0018], grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(-0.0256, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2602, 0.3021, 0.2444, 0.1932],\n",
      "        [0.2602, 0.3022, 0.2444, 0.1932],\n",
      "        [0.2602, 0.3022, 0.2444, 0.1932],\n",
      "        [0.2602, 0.3022, 0.2444, 0.1932],\n",
      "        [0.2602, 0.3021, 0.2444, 0.1933],\n",
      "        [0.2602, 0.3021, 0.2444, 0.1933],\n",
      "        [0.2602, 0.3021, 0.2444, 0.1933],\n",
      "        [0.2602, 0.3021, 0.2444, 0.1933],\n",
      "        [0.2602, 0.3021, 0.2444, 0.1933],\n",
      "        [0.2602, 0.3021, 0.2444, 0.1932],\n",
      "        [0.2602, 0.3021, 0.2444, 0.1932],\n",
      "        [0.2602, 0.3021, 0.2444, 0.1933],\n",
      "        [0.2602, 0.3021, 0.2444, 0.1933],\n",
      "        [0.2602, 0.3021, 0.2444, 0.1932],\n",
      "        [0.2602, 0.3021, 0.2444, 0.1933],\n",
      "        [0.2602, 0.3021, 0.2444, 0.1932],\n",
      "        [0.2602, 0.3021, 0.2444, 0.1933],\n",
      "        [0.2602, 0.3022, 0.2444, 0.1932],\n",
      "        [0.2602, 0.3021, 0.2444, 0.1932],\n",
      "        [0.2602, 0.3021, 0.2444, 0.1932],\n",
      "        [0.2602, 0.3021, 0.2444, 0.1932],\n",
      "        [0.2602, 0.3022, 0.2444, 0.1932],\n",
      "        [0.2602, 0.3022, 0.2444, 0.1932],\n",
      "        [0.2602, 0.3022, 0.2444, 0.1932],\n",
      "        [0.2602, 0.3021, 0.2444, 0.1933],\n",
      "        [0.2602, 0.3021, 0.2444, 0.1933],\n",
      "        [0.2602, 0.3021, 0.2444, 0.1933],\n",
      "        [0.2602, 0.3020, 0.2444, 0.1933],\n",
      "        [0.2602, 0.3021, 0.2444, 0.1932],\n",
      "        [0.2602, 0.3021, 0.2444, 0.1933],\n",
      "        [0.2602, 0.3021, 0.2444, 0.1933],\n",
      "        [0.2602, 0.3021, 0.2444, 0.1933],\n",
      "        [0.2602, 0.3022, 0.2444, 0.1932],\n",
      "        [0.2602, 0.3022, 0.2444, 0.1932],\n",
      "        [0.2602, 0.3021, 0.2444, 0.1933],\n",
      "        [0.2602, 0.3022, 0.2444, 0.1932],\n",
      "        [0.2602, 0.3021, 0.2444, 0.1933],\n",
      "        [0.2602, 0.3021, 0.2444, 0.1933],\n",
      "        [0.2602, 0.3021, 0.2444, 0.1933],\n",
      "        [0.2602, 0.3021, 0.2444, 0.1933],\n",
      "        [0.2602, 0.3021, 0.2444, 0.1933],\n",
      "        [0.2602, 0.3021, 0.2444, 0.1933],\n",
      "        [0.2602, 0.3021, 0.2444, 0.1933],\n",
      "        [0.2602, 0.3021, 0.2444, 0.1933],\n",
      "        [0.2602, 0.3022, 0.2444, 0.1932],\n",
      "        [0.2602, 0.3021, 0.2444, 0.1932],\n",
      "        [0.2602, 0.3022, 0.2444, 0.1931],\n",
      "        [0.2602, 0.3021, 0.2444, 0.1932],\n",
      "        [0.2602, 0.3021, 0.2444, 0.1933],\n",
      "        [0.2602, 0.3020, 0.2444, 0.1933],\n",
      "        [0.2602, 0.3021, 0.2444, 0.1933],\n",
      "        [0.2602, 0.3020, 0.2444, 0.1933],\n",
      "        [0.2602, 0.3021, 0.2444, 0.1933],\n",
      "        [0.2602, 0.3020, 0.2445, 0.1933],\n",
      "        [0.2602, 0.3021, 0.2444, 0.1932],\n",
      "        [0.2602, 0.3021, 0.2444, 0.1932],\n",
      "        [0.2602, 0.3021, 0.2444, 0.1933],\n",
      "        [0.2602, 0.3021, 0.2444, 0.1933],\n",
      "        [0.2602, 0.3021, 0.2444, 0.1933],\n",
      "        [0.2602, 0.3021, 0.2444, 0.1933],\n",
      "        [0.2602, 0.3021, 0.2444, 0.1933],\n",
      "        [0.2602, 0.3021, 0.2444, 0.1933],\n",
      "        [0.2602, 0.3021, 0.2444, 0.1932],\n",
      "        [0.2602, 0.3021, 0.2444, 0.1932]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3739, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(-0.0369, grad_fn=<AddBackward0>)\n",
      "batching here\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64: done 3 episodes, mean_reward=0.00, best_reward=2.00, speed=222.90\n",
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(0.0012, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.3424, -1.2247, -1.4024, -1.6149],\n",
      "        [-1.3423, -1.2247, -1.4025, -1.6148],\n",
      "        [-1.3423, -1.2244, -1.4025, -1.6153],\n",
      "        [-1.3422, -1.2245, -1.4024, -1.6152],\n",
      "        [-1.3422, -1.2247, -1.4025, -1.6150],\n",
      "        [-1.3422, -1.2246, -1.4025, -1.6151],\n",
      "        [-1.3422, -1.2247, -1.4025, -1.6150],\n",
      "        [-1.3423, -1.2247, -1.4024, -1.6150],\n",
      "        [-1.3423, -1.2249, -1.4024, -1.6146],\n",
      "        [-1.3422, -1.2246, -1.4025, -1.6151],\n",
      "        [-1.3423, -1.2250, -1.4024, -1.6145],\n",
      "        [-1.3423, -1.2248, -1.4024, -1.6148],\n",
      "        [-1.3423, -1.2247, -1.4024, -1.6150],\n",
      "        [-1.3423, -1.2247, -1.4024, -1.6150],\n",
      "        [-1.3422, -1.2246, -1.4024, -1.6150],\n",
      "        [-1.3422, -1.2246, -1.4024, -1.6151],\n",
      "        [-1.3423, -1.2246, -1.4025, -1.6150],\n",
      "        [-1.3423, -1.2247, -1.4025, -1.6149],\n",
      "        [-1.3423, -1.2244, -1.4025, -1.6153],\n",
      "        [-1.3423, -1.2245, -1.4024, -1.6151],\n",
      "        [-1.3423, -1.2248, -1.4024, -1.6148],\n",
      "        [-1.3423, -1.2246, -1.4025, -1.6151],\n",
      "        [-1.3423, -1.2246, -1.4024, -1.6150],\n",
      "        [-1.3423, -1.2245, -1.4024, -1.6153],\n",
      "        [-1.3423, -1.2247, -1.4024, -1.6148],\n",
      "        [-1.3423, -1.2249, -1.4024, -1.6145],\n",
      "        [-1.3422, -1.2246, -1.4024, -1.6151],\n",
      "        [-1.3422, -1.2246, -1.4025, -1.6152],\n",
      "        [-1.3422, -1.2248, -1.4024, -1.6149],\n",
      "        [-1.3423, -1.2248, -1.4024, -1.6148],\n",
      "        [-1.3423, -1.2248, -1.4024, -1.6148],\n",
      "        [-1.3423, -1.2248, -1.4024, -1.6148],\n",
      "        [-1.3422, -1.2245, -1.4024, -1.6152],\n",
      "        [-1.3422, -1.2246, -1.4024, -1.6152],\n",
      "        [-1.3422, -1.2246, -1.4024, -1.6151],\n",
      "        [-1.3422, -1.2247, -1.4024, -1.6151],\n",
      "        [-1.3422, -1.2247, -1.4025, -1.6149],\n",
      "        [-1.3423, -1.2246, -1.4025, -1.6151],\n",
      "        [-1.3423, -1.2247, -1.4024, -1.6148],\n",
      "        [-1.3423, -1.2249, -1.4024, -1.6146],\n",
      "        [-1.3422, -1.2247, -1.4025, -1.6149],\n",
      "        [-1.3423, -1.2247, -1.4025, -1.6149],\n",
      "        [-1.3422, -1.2247, -1.4025, -1.6149],\n",
      "        [-1.3423, -1.2246, -1.4025, -1.6150],\n",
      "        [-1.3423, -1.2245, -1.4024, -1.6152],\n",
      "        [-1.3424, -1.2251, -1.4024, -1.6143],\n",
      "        [-1.3423, -1.2245, -1.4024, -1.6151],\n",
      "        [-1.3423, -1.2245, -1.4025, -1.6151],\n",
      "        [-1.3422, -1.2249, -1.4024, -1.6148],\n",
      "        [-1.3422, -1.2249, -1.4024, -1.6147],\n",
      "        [-1.3422, -1.2250, -1.4023, -1.6147],\n",
      "        [-1.3422, -1.2249, -1.4024, -1.6148],\n",
      "        [-1.3423, -1.2250, -1.4024, -1.6145],\n",
      "        [-1.3423, -1.2248, -1.4024, -1.6147],\n",
      "        [-1.3423, -1.2246, -1.4024, -1.6150],\n",
      "        [-1.3422, -1.2246, -1.4024, -1.6152],\n",
      "        [-1.3422, -1.2247, -1.4024, -1.6150],\n",
      "        [-1.3423, -1.2248, -1.4024, -1.6148],\n",
      "        [-1.3423, -1.2246, -1.4025, -1.6150],\n",
      "        [-1.3422, -1.2246, -1.4024, -1.6151],\n",
      "        [-1.3422, -1.2247, -1.4024, -1.6151],\n",
      "        [-1.3422, -1.2248, -1.4024, -1.6149],\n",
      "        [-1.3422, -1.2247, -1.4024, -1.6150],\n",
      "        [-1.3422, -1.2248, -1.4024, -1.6149]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([0.0052, 0.0039, 0.0035, 0.0019, 0.0070, 0.0054, 0.0029, 0.0018, 0.0063,\n",
      "        0.0053, 0.0030, 0.0017, 0.0062, 0.0044, 0.0030, 0.0015, 0.1363, 0.1493,\n",
      "        0.0033, 0.0017, 0.0054, 0.0049, 0.0039, 0.0023, 0.0066, 0.0044, 0.0032,\n",
      "        0.0020, 0.1495, 0.0041, 0.0031, 0.0017, 0.0072, 0.0047, 0.0029, 0.0012,\n",
      "        0.0056, 0.0052, 0.0028, 0.0011, 0.1362, 0.1492, 0.1560, 0.0016, 0.0077,\n",
      "        0.0036, 0.0039, 0.0018, 0.0072, 0.0039, 0.0026, 0.0015, 0.0050, 0.0034,\n",
      "        0.0030, 0.0016, 0.0074, 0.0046, 0.0030, 0.0016, 0.0056, 0.0045, 0.0028,\n",
      "        0.0015], grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(-0.0171, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2612, 0.2939, 0.2460, 0.1989],\n",
      "        [0.2613, 0.2938, 0.2460, 0.1989],\n",
      "        [0.2613, 0.2939, 0.2460, 0.1988],\n",
      "        [0.2613, 0.2939, 0.2460, 0.1988],\n",
      "        [0.2613, 0.2938, 0.2460, 0.1989],\n",
      "        [0.2613, 0.2939, 0.2460, 0.1989],\n",
      "        [0.2613, 0.2938, 0.2460, 0.1989],\n",
      "        [0.2613, 0.2939, 0.2460, 0.1989],\n",
      "        [0.2612, 0.2938, 0.2460, 0.1990],\n",
      "        [0.2613, 0.2939, 0.2460, 0.1989],\n",
      "        [0.2612, 0.2938, 0.2460, 0.1990],\n",
      "        [0.2613, 0.2938, 0.2460, 0.1989],\n",
      "        [0.2613, 0.2939, 0.2460, 0.1989],\n",
      "        [0.2613, 0.2939, 0.2460, 0.1989],\n",
      "        [0.2613, 0.2939, 0.2460, 0.1989],\n",
      "        [0.2613, 0.2939, 0.2460, 0.1989],\n",
      "        [0.2613, 0.2939, 0.2460, 0.1989],\n",
      "        [0.2613, 0.2939, 0.2460, 0.1989],\n",
      "        [0.2613, 0.2939, 0.2460, 0.1988],\n",
      "        [0.2612, 0.2939, 0.2460, 0.1989],\n",
      "        [0.2612, 0.2938, 0.2460, 0.1989],\n",
      "        [0.2613, 0.2939, 0.2460, 0.1989],\n",
      "        [0.2612, 0.2939, 0.2460, 0.1989],\n",
      "        [0.2612, 0.2939, 0.2460, 0.1988],\n",
      "        [0.2613, 0.2938, 0.2460, 0.1989],\n",
      "        [0.2612, 0.2938, 0.2460, 0.1990],\n",
      "        [0.2613, 0.2939, 0.2460, 0.1989],\n",
      "        [0.2613, 0.2939, 0.2460, 0.1989],\n",
      "        [0.2613, 0.2938, 0.2460, 0.1989],\n",
      "        [0.2612, 0.2938, 0.2460, 0.1989],\n",
      "        [0.2612, 0.2938, 0.2460, 0.1989],\n",
      "        [0.2612, 0.2938, 0.2460, 0.1989],\n",
      "        [0.2613, 0.2939, 0.2460, 0.1988],\n",
      "        [0.2613, 0.2939, 0.2460, 0.1989],\n",
      "        [0.2613, 0.2939, 0.2460, 0.1989],\n",
      "        [0.2613, 0.2939, 0.2460, 0.1989],\n",
      "        [0.2613, 0.2939, 0.2460, 0.1989],\n",
      "        [0.2613, 0.2939, 0.2460, 0.1989],\n",
      "        [0.2612, 0.2938, 0.2460, 0.1989],\n",
      "        [0.2613, 0.2938, 0.2460, 0.1990],\n",
      "        [0.2613, 0.2939, 0.2460, 0.1989],\n",
      "        [0.2613, 0.2938, 0.2460, 0.1989],\n",
      "        [0.2613, 0.2938, 0.2460, 0.1989],\n",
      "        [0.2613, 0.2939, 0.2460, 0.1989],\n",
      "        [0.2612, 0.2939, 0.2460, 0.1989],\n",
      "        [0.2612, 0.2937, 0.2460, 0.1990],\n",
      "        [0.2612, 0.2939, 0.2460, 0.1989],\n",
      "        [0.2612, 0.2939, 0.2460, 0.1989],\n",
      "        [0.2613, 0.2938, 0.2460, 0.1989],\n",
      "        [0.2613, 0.2938, 0.2460, 0.1989],\n",
      "        [0.2613, 0.2938, 0.2460, 0.1989],\n",
      "        [0.2613, 0.2938, 0.2460, 0.1989],\n",
      "        [0.2612, 0.2938, 0.2460, 0.1990],\n",
      "        [0.2612, 0.2938, 0.2460, 0.1989],\n",
      "        [0.2612, 0.2939, 0.2460, 0.1989],\n",
      "        [0.2613, 0.2939, 0.2460, 0.1989],\n",
      "        [0.2613, 0.2939, 0.2460, 0.1989],\n",
      "        [0.2612, 0.2938, 0.2460, 0.1989],\n",
      "        [0.2613, 0.2939, 0.2460, 0.1989],\n",
      "        [0.2613, 0.2939, 0.2460, 0.1989],\n",
      "        [0.2613, 0.2939, 0.2460, 0.1989],\n",
      "        [0.2613, 0.2938, 0.2460, 0.1989],\n",
      "        [0.2613, 0.2938, 0.2460, 0.1989],\n",
      "        [0.2613, 0.2938, 0.2460, 0.1989]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3768, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(-0.0297, grad_fn=<AddBackward0>)\n",
      "batching here\n",
      "65: done 1 episodes, mean_reward=0.00, best_reward=2.00, speed=221.17\n",
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(0.0159, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.3499, -1.2448, -1.3993, -1.5800],\n",
      "        [-1.3499, -1.2447, -1.3993, -1.5801],\n",
      "        [-1.3499, -1.2446, -1.3994, -1.5802],\n",
      "        [-1.3499, -1.2446, -1.3994, -1.5804],\n",
      "        [-1.3500, -1.2448, -1.3993, -1.5799],\n",
      "        [-1.3500, -1.2447, -1.3993, -1.5801],\n",
      "        [-1.3500, -1.2447, -1.3994, -1.5800],\n",
      "        [-1.3499, -1.2447, -1.3994, -1.5801],\n",
      "        [-1.3499, -1.2450, -1.3993, -1.5797],\n",
      "        [-1.3499, -1.2450, -1.3993, -1.5797],\n",
      "        [-1.3499, -1.2449, -1.3994, -1.5798],\n",
      "        [-1.3499, -1.2448, -1.3993, -1.5799],\n",
      "        [-1.3499, -1.2449, -1.3993, -1.5799],\n",
      "        [-1.3499, -1.2448, -1.3993, -1.5799],\n",
      "        [-1.3499, -1.2449, -1.3993, -1.5799],\n",
      "        [-1.3499, -1.2449, -1.3993, -1.5799],\n",
      "        [-1.3499, -1.2445, -1.3994, -1.5804],\n",
      "        [-1.3500, -1.2447, -1.3993, -1.5800],\n",
      "        [-1.3499, -1.2447, -1.3994, -1.5800],\n",
      "        [-1.3500, -1.2448, -1.3994, -1.5799],\n",
      "        [-1.3499, -1.2447, -1.3994, -1.5800],\n",
      "        [-1.3499, -1.2444, -1.3994, -1.5805],\n",
      "        [-1.3500, -1.2447, -1.3994, -1.5800],\n",
      "        [-1.3499, -1.2444, -1.3993, -1.5805],\n",
      "        [-1.3500, -1.2450, -1.3993, -1.5797],\n",
      "        [-1.3499, -1.2450, -1.3993, -1.5798],\n",
      "        [-1.3499, -1.2447, -1.3994, -1.5802],\n",
      "        [-1.3499, -1.2451, -1.3993, -1.5796],\n",
      "        [-1.3499, -1.2447, -1.3994, -1.5800],\n",
      "        [-1.3500, -1.2449, -1.3994, -1.5797],\n",
      "        [-1.3499, -1.2448, -1.3993, -1.5800],\n",
      "        [-1.3499, -1.2448, -1.3993, -1.5800],\n",
      "        [-1.3499, -1.2448, -1.3993, -1.5800],\n",
      "        [-1.3499, -1.2447, -1.3993, -1.5802],\n",
      "        [-1.3500, -1.2448, -1.3993, -1.5799],\n",
      "        [-1.3499, -1.2447, -1.3993, -1.5801],\n",
      "        [-1.3499, -1.2446, -1.3994, -1.5802],\n",
      "        [-1.3499, -1.2447, -1.3993, -1.5801],\n",
      "        [-1.3499, -1.2449, -1.3993, -1.5798],\n",
      "        [-1.3500, -1.2449, -1.3994, -1.5797],\n",
      "        [-1.3500, -1.2447, -1.3994, -1.5800],\n",
      "        [-1.3499, -1.2447, -1.3994, -1.5801],\n",
      "        [-1.3500, -1.2449, -1.3993, -1.5798],\n",
      "        [-1.3499, -1.2448, -1.3994, -1.5800],\n",
      "        [-1.3500, -1.2447, -1.3994, -1.5800],\n",
      "        [-1.3499, -1.2445, -1.3994, -1.5804],\n",
      "        [-1.3499, -1.2446, -1.3993, -1.5804],\n",
      "        [-1.3499, -1.2447, -1.3994, -1.5801],\n",
      "        [-1.3499, -1.2450, -1.3993, -1.5798],\n",
      "        [-1.3499, -1.2451, -1.3993, -1.5797],\n",
      "        [-1.3499, -1.2450, -1.3993, -1.5797],\n",
      "        [-1.3499, -1.2450, -1.3993, -1.5797],\n",
      "        [-1.3499, -1.2446, -1.3993, -1.5803],\n",
      "        [-1.3500, -1.2452, -1.3993, -1.5794],\n",
      "        [-1.3499, -1.2450, -1.3993, -1.5797],\n",
      "        [-1.3498, -1.2445, -1.3993, -1.5805],\n",
      "        [-1.3500, -1.2448, -1.3993, -1.5799],\n",
      "        [-1.3499, -1.2448, -1.3994, -1.5799],\n",
      "        [-1.3499, -1.2447, -1.3994, -1.5802],\n",
      "        [-1.3499, -1.2446, -1.3994, -1.5802],\n",
      "        [-1.3500, -1.2449, -1.3993, -1.5798],\n",
      "        [-1.3500, -1.2448, -1.3993, -1.5799],\n",
      "        [-1.3500, -1.2448, -1.3993, -1.5799],\n",
      "        [-1.3500, -1.2449, -1.3993, -1.5798]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([ 4.5413e-03,  2.7293e-03,  2.2954e-03,  9.0104e-04,  1.1216e-01,\n",
      "         1.1235e-01,  1.2686e-01,  9.9960e-02, -1.2412e+00,  3.0662e-03,\n",
      "         2.2230e-03,  1.1699e-03,  4.2284e-03,  2.9903e-03,  2.2278e-03,\n",
      "         1.1194e-03,  4.2914e-03,  3.3491e-03,  1.5992e-03,  6.2038e-04,\n",
      "         5.5675e-03,  4.3413e-03,  2.9483e-03,  1.9984e-03,  3.6382e-03,\n",
      "         2.7969e-03,  2.4439e-03,  7.2778e-04,  4.4877e-03,  4.0060e-03,\n",
      "         2.3165e-03,  1.3226e-03,  4.6738e-03,  3.2297e-03,  1.6294e-03,\n",
      "         1.0477e-03,  4.8105e-03,  2.8141e-03,  1.9719e-03,  4.7882e-04,\n",
      "         5.2137e-03,  3.9413e-03,  1.8887e-03,  9.9813e-04,  4.1967e-03,\n",
      "         3.6756e-03,  2.2590e-03,  9.7775e-04,  4.0382e-03,  3.1885e-03,\n",
      "         2.1872e-03,  9.9807e-04,  4.6106e-03,  2.5648e-03,  2.1850e-03,\n",
      "         1.8853e-03,  4.5902e-03,  3.6198e-03,  3.4616e-03,  1.9588e-03,\n",
      "         4.2540e-03,  3.3002e-03,  2.3058e-03,  1.1645e-03],\n",
      "       grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(0.0098, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2593, 0.2880, 0.2468, 0.2060],\n",
      "        [0.2593, 0.2880, 0.2468, 0.2060],\n",
      "        [0.2593, 0.2881, 0.2468, 0.2059],\n",
      "        [0.2593, 0.2881, 0.2468, 0.2059],\n",
      "        [0.2593, 0.2880, 0.2468, 0.2060],\n",
      "        [0.2592, 0.2880, 0.2468, 0.2060],\n",
      "        [0.2592, 0.2880, 0.2468, 0.2060],\n",
      "        [0.2593, 0.2880, 0.2468, 0.2060],\n",
      "        [0.2593, 0.2879, 0.2468, 0.2060],\n",
      "        [0.2593, 0.2879, 0.2468, 0.2060],\n",
      "        [0.2593, 0.2880, 0.2468, 0.2060],\n",
      "        [0.2593, 0.2880, 0.2468, 0.2060],\n",
      "        [0.2593, 0.2880, 0.2468, 0.2060],\n",
      "        [0.2593, 0.2880, 0.2468, 0.2060],\n",
      "        [0.2593, 0.2880, 0.2468, 0.2060],\n",
      "        [0.2593, 0.2880, 0.2468, 0.2060],\n",
      "        [0.2593, 0.2881, 0.2467, 0.2059],\n",
      "        [0.2593, 0.2880, 0.2468, 0.2060],\n",
      "        [0.2593, 0.2880, 0.2467, 0.2060],\n",
      "        [0.2592, 0.2880, 0.2467, 0.2060],\n",
      "        [0.2593, 0.2880, 0.2467, 0.2060],\n",
      "        [0.2593, 0.2881, 0.2468, 0.2059],\n",
      "        [0.2592, 0.2880, 0.2468, 0.2060],\n",
      "        [0.2593, 0.2881, 0.2468, 0.2059],\n",
      "        [0.2593, 0.2880, 0.2468, 0.2060],\n",
      "        [0.2593, 0.2880, 0.2468, 0.2060],\n",
      "        [0.2593, 0.2880, 0.2468, 0.2059],\n",
      "        [0.2593, 0.2879, 0.2468, 0.2061],\n",
      "        [0.2593, 0.2880, 0.2468, 0.2060],\n",
      "        [0.2592, 0.2880, 0.2468, 0.2060],\n",
      "        [0.2593, 0.2880, 0.2468, 0.2060],\n",
      "        [0.2593, 0.2880, 0.2468, 0.2060],\n",
      "        [0.2593, 0.2880, 0.2468, 0.2060],\n",
      "        [0.2593, 0.2880, 0.2468, 0.2059],\n",
      "        [0.2592, 0.2880, 0.2468, 0.2060],\n",
      "        [0.2593, 0.2880, 0.2468, 0.2059],\n",
      "        [0.2593, 0.2881, 0.2468, 0.2059],\n",
      "        [0.2593, 0.2880, 0.2468, 0.2059],\n",
      "        [0.2593, 0.2880, 0.2468, 0.2060],\n",
      "        [0.2593, 0.2880, 0.2468, 0.2060],\n",
      "        [0.2593, 0.2880, 0.2468, 0.2060],\n",
      "        [0.2593, 0.2880, 0.2468, 0.2060],\n",
      "        [0.2593, 0.2880, 0.2468, 0.2060],\n",
      "        [0.2593, 0.2880, 0.2467, 0.2060],\n",
      "        [0.2593, 0.2880, 0.2468, 0.2060],\n",
      "        [0.2593, 0.2881, 0.2468, 0.2059],\n",
      "        [0.2593, 0.2881, 0.2468, 0.2059],\n",
      "        [0.2593, 0.2880, 0.2468, 0.2059],\n",
      "        [0.2593, 0.2879, 0.2468, 0.2060],\n",
      "        [0.2593, 0.2879, 0.2468, 0.2060],\n",
      "        [0.2593, 0.2879, 0.2468, 0.2060],\n",
      "        [0.2593, 0.2879, 0.2468, 0.2060],\n",
      "        [0.2593, 0.2881, 0.2468, 0.2059],\n",
      "        [0.2592, 0.2879, 0.2468, 0.2061],\n",
      "        [0.2593, 0.2879, 0.2468, 0.2060],\n",
      "        [0.2593, 0.2881, 0.2468, 0.2059],\n",
      "        [0.2593, 0.2880, 0.2468, 0.2060],\n",
      "        [0.2593, 0.2880, 0.2468, 0.2060],\n",
      "        [0.2593, 0.2880, 0.2468, 0.2059],\n",
      "        [0.2593, 0.2880, 0.2467, 0.2059],\n",
      "        [0.2593, 0.2880, 0.2468, 0.2060],\n",
      "        [0.2593, 0.2880, 0.2468, 0.2060],\n",
      "        [0.2593, 0.2880, 0.2468, 0.2060],\n",
      "        [0.2593, 0.2880, 0.2468, 0.2060]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3792, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(0.0119, grad_fn=<AddBackward0>)\n",
      "batching here\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66: done 3 episodes, mean_reward=0.33, best_reward=2.00, speed=224.34\n",
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(0.0321, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.3528, -1.2001, -1.4117, -1.6266],\n",
      "        [-1.3529, -1.2002, -1.4117, -1.6264],\n",
      "        [-1.3528, -1.2002, -1.4116, -1.6265],\n",
      "        [-1.3528, -1.2001, -1.4117, -1.6266],\n",
      "        [-1.3529, -1.2003, -1.4116, -1.6263],\n",
      "        [-1.3529, -1.2003, -1.4116, -1.6263],\n",
      "        [-1.3529, -1.2003, -1.4116, -1.6262],\n",
      "        [-1.3528, -1.2002, -1.4117, -1.6264],\n",
      "        [-1.3529, -1.2003, -1.4116, -1.6263],\n",
      "        [-1.3529, -1.2007, -1.4116, -1.6257],\n",
      "        [-1.3528, -1.2002, -1.4117, -1.6264],\n",
      "        [-1.3529, -1.2004, -1.4116, -1.6261],\n",
      "        [-1.3528, -1.2003, -1.4116, -1.6263],\n",
      "        [-1.3528, -1.2003, -1.4116, -1.6263],\n",
      "        [-1.3529, -1.2004, -1.4116, -1.6262],\n",
      "        [-1.3528, -1.2003, -1.4116, -1.6263],\n",
      "        [-1.3528, -1.2002, -1.4117, -1.6265],\n",
      "        [-1.3529, -1.2005, -1.4117, -1.6259],\n",
      "        [-1.3528, -1.2000, -1.4117, -1.6267],\n",
      "        [-1.3529, -1.2004, -1.4117, -1.6261],\n",
      "        [-1.3529, -1.2006, -1.4116, -1.6257],\n",
      "        [-1.3528, -1.2001, -1.4117, -1.6267],\n",
      "        [-1.3529, -1.2003, -1.4116, -1.6263],\n",
      "        [-1.3528, -1.2001, -1.4117, -1.6267],\n",
      "        [-1.3528, -1.2005, -1.4116, -1.6261],\n",
      "        [-1.3528, -1.2005, -1.4116, -1.6260],\n",
      "        [-1.3528, -1.2006, -1.4116, -1.6260],\n",
      "        [-1.3528, -1.2004, -1.4117, -1.6261],\n",
      "        [-1.3528, -1.2006, -1.4116, -1.6259],\n",
      "        [-1.3528, -1.2003, -1.4116, -1.6263],\n",
      "        [-1.3528, -1.2005, -1.4116, -1.6260],\n",
      "        [-1.3528, -1.2004, -1.4116, -1.6263],\n",
      "        [-1.3529, -1.2003, -1.4116, -1.6264],\n",
      "        [-1.3529, -1.2004, -1.4116, -1.6262],\n",
      "        [-1.3528, -1.2000, -1.4117, -1.6268],\n",
      "        [-1.3529, -1.2007, -1.4116, -1.6256],\n",
      "        [-1.3528, -1.2002, -1.4117, -1.6264],\n",
      "        [-1.3528, -1.2004, -1.4116, -1.6263],\n",
      "        [-1.3528, -1.2002, -1.4117, -1.6265],\n",
      "        [-1.3529, -1.2002, -1.4117, -1.6263],\n",
      "        [-1.3529, -1.2004, -1.4117, -1.6261],\n",
      "        [-1.3528, -1.2002, -1.4117, -1.6265],\n",
      "        [-1.3529, -1.2006, -1.4117, -1.6259],\n",
      "        [-1.3528, -1.2001, -1.4117, -1.6266],\n",
      "        [-1.3528, -1.2002, -1.4117, -1.6264],\n",
      "        [-1.3528, -1.2003, -1.4117, -1.6264],\n",
      "        [-1.3528, -1.2003, -1.4117, -1.6263],\n",
      "        [-1.3528, -1.2002, -1.4116, -1.6266],\n",
      "        [-1.3528, -1.2006, -1.4116, -1.6260],\n",
      "        [-1.3528, -1.2006, -1.4116, -1.6260],\n",
      "        [-1.3528, -1.2006, -1.4116, -1.6260],\n",
      "        [-1.3528, -1.2006, -1.4116, -1.6261],\n",
      "        [-1.3528, -1.2003, -1.4117, -1.6263],\n",
      "        [-1.3529, -1.2003, -1.4117, -1.6263],\n",
      "        [-1.3528, -1.2004, -1.4116, -1.6263],\n",
      "        [-1.3528, -1.2002, -1.4117, -1.6265],\n",
      "        [-1.3528, -1.2005, -1.4117, -1.6260],\n",
      "        [-1.3528, -1.2003, -1.4117, -1.6262],\n",
      "        [-1.3528, -1.2002, -1.4117, -1.6264],\n",
      "        [-1.3528, -1.2003, -1.4117, -1.6263],\n",
      "        [-1.3528, -1.2004, -1.4116, -1.6262],\n",
      "        [-1.3528, -1.2004, -1.4116, -1.6261],\n",
      "        [-1.3528, -1.2004, -1.4116, -1.6261],\n",
      "        [-1.3529, -1.2001, -1.4117, -1.6266]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([ 6.0016e-03,  3.8405e-03,  2.7436e-03,  1.3412e-03,  6.7407e-03,\n",
      "         4.5056e-03,  2.5310e-03,  1.1845e-03,  4.9493e-03,  3.3333e-03,\n",
      "         1.9674e-03,  5.0658e-04,  1.2529e-01,  1.4735e-01,  3.4568e-03,\n",
      "         1.6055e-03,  5.4517e-03,  2.8780e-03,  2.9594e-03,  1.0800e-03,\n",
      "         6.2491e-03,  5.2349e-03,  2.6221e-03,  2.4038e-03, -1.3915e+00,\n",
      "        -1.3487e+00,  3.4958e-03,  1.3028e-03,  5.5434e-03,  4.5918e-03,\n",
      "         3.4623e-03,  1.9915e-03,  5.5255e-03,  4.2059e-03,  3.1213e-03,\n",
      "         5.9099e-04,  5.0799e-03,  3.5565e-03,  2.5177e-03,  1.1978e-03,\n",
      "         5.8791e-03,  5.6404e-03,  2.4988e-03,  1.7108e-03,  6.5099e-03,\n",
      "         4.7968e-03,  2.2346e-03,  1.3631e-03,  1.4145e-01,  1.4145e-01,\n",
      "         1.7002e-01,  1.3487e-03,  5.0474e-03,  5.0754e-03,  2.6851e-03,\n",
      "         1.3943e-03,  5.5775e-03,  5.1138e-03,  3.0413e-03,  1.6010e-03,\n",
      "         1.2543e-01,  1.4748e-01,  1.4134e-01,  1.4537e-03],\n",
      "       grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(0.0222, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2585, 0.3012, 0.2437, 0.1966],\n",
      "        [0.2585, 0.3011, 0.2437, 0.1966],\n",
      "        [0.2585, 0.3011, 0.2437, 0.1966],\n",
      "        [0.2585, 0.3012, 0.2437, 0.1966],\n",
      "        [0.2585, 0.3011, 0.2437, 0.1967],\n",
      "        [0.2585, 0.3011, 0.2437, 0.1967],\n",
      "        [0.2585, 0.3011, 0.2437, 0.1967],\n",
      "        [0.2585, 0.3011, 0.2437, 0.1966],\n",
      "        [0.2585, 0.3011, 0.2437, 0.1967],\n",
      "        [0.2585, 0.3010, 0.2438, 0.1968],\n",
      "        [0.2585, 0.3011, 0.2437, 0.1966],\n",
      "        [0.2585, 0.3011, 0.2438, 0.1967],\n",
      "        [0.2585, 0.3011, 0.2437, 0.1967],\n",
      "        [0.2585, 0.3011, 0.2437, 0.1967],\n",
      "        [0.2585, 0.3011, 0.2437, 0.1967],\n",
      "        [0.2585, 0.3011, 0.2437, 0.1967],\n",
      "        [0.2585, 0.3011, 0.2437, 0.1966],\n",
      "        [0.2585, 0.3010, 0.2437, 0.1967],\n",
      "        [0.2585, 0.3012, 0.2437, 0.1966],\n",
      "        [0.2585, 0.3011, 0.2437, 0.1967],\n",
      "        [0.2585, 0.3010, 0.2438, 0.1968],\n",
      "        [0.2585, 0.3012, 0.2437, 0.1966],\n",
      "        [0.2585, 0.3011, 0.2438, 0.1966],\n",
      "        [0.2585, 0.3012, 0.2437, 0.1966],\n",
      "        [0.2585, 0.3011, 0.2438, 0.1967],\n",
      "        [0.2585, 0.3010, 0.2437, 0.1967],\n",
      "        [0.2585, 0.3010, 0.2438, 0.1967],\n",
      "        [0.2585, 0.3011, 0.2437, 0.1967],\n",
      "        [0.2585, 0.3010, 0.2437, 0.1967],\n",
      "        [0.2585, 0.3011, 0.2437, 0.1967],\n",
      "        [0.2585, 0.3010, 0.2437, 0.1967],\n",
      "        [0.2585, 0.3011, 0.2437, 0.1967],\n",
      "        [0.2585, 0.3011, 0.2437, 0.1966],\n",
      "        [0.2585, 0.3011, 0.2438, 0.1967],\n",
      "        [0.2585, 0.3012, 0.2437, 0.1966],\n",
      "        [0.2585, 0.3010, 0.2438, 0.1968],\n",
      "        [0.2585, 0.3011, 0.2437, 0.1966],\n",
      "        [0.2585, 0.3011, 0.2437, 0.1967],\n",
      "        [0.2585, 0.3011, 0.2437, 0.1966],\n",
      "        [0.2585, 0.3011, 0.2437, 0.1967],\n",
      "        [0.2585, 0.3011, 0.2437, 0.1967],\n",
      "        [0.2585, 0.3011, 0.2437, 0.1966],\n",
      "        [0.2585, 0.3010, 0.2437, 0.1967],\n",
      "        [0.2585, 0.3012, 0.2437, 0.1966],\n",
      "        [0.2585, 0.3011, 0.2437, 0.1966],\n",
      "        [0.2585, 0.3011, 0.2437, 0.1966],\n",
      "        [0.2585, 0.3011, 0.2437, 0.1967],\n",
      "        [0.2585, 0.3011, 0.2437, 0.1966],\n",
      "        [0.2585, 0.3010, 0.2438, 0.1967],\n",
      "        [0.2585, 0.3010, 0.2438, 0.1967],\n",
      "        [0.2585, 0.3010, 0.2438, 0.1967],\n",
      "        [0.2585, 0.3010, 0.2438, 0.1967],\n",
      "        [0.2585, 0.3011, 0.2437, 0.1967],\n",
      "        [0.2585, 0.3011, 0.2437, 0.1967],\n",
      "        [0.2585, 0.3011, 0.2437, 0.1967],\n",
      "        [0.2585, 0.3011, 0.2437, 0.1966],\n",
      "        [0.2585, 0.3011, 0.2437, 0.1967],\n",
      "        [0.2585, 0.3011, 0.2437, 0.1967],\n",
      "        [0.2585, 0.3011, 0.2437, 0.1966],\n",
      "        [0.2585, 0.3011, 0.2437, 0.1966],\n",
      "        [0.2585, 0.3011, 0.2437, 0.1967],\n",
      "        [0.2585, 0.3011, 0.2437, 0.1967],\n",
      "        [0.2585, 0.3011, 0.2437, 0.1967],\n",
      "        [0.2585, 0.3012, 0.2437, 0.1966]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3750, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(0.0405, grad_fn=<AddBackward0>)\n",
      "batching here\n",
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(1.4201e-05, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.3380, -1.2108, -1.3947, -1.6515],\n",
      "        [-1.3380, -1.2109, -1.3947, -1.6513],\n",
      "        [-1.3380, -1.2108, -1.3947, -1.6514],\n",
      "        [-1.3380, -1.2108, -1.3947, -1.6514],\n",
      "        [-1.3381, -1.2109, -1.3947, -1.6511],\n",
      "        [-1.3381, -1.2109, -1.3947, -1.6512],\n",
      "        [-1.3380, -1.2109, -1.3947, -1.6513],\n",
      "        [-1.3381, -1.2111, -1.3947, -1.6510],\n",
      "        [-1.3380, -1.2107, -1.3947, -1.6517],\n",
      "        [-1.3381, -1.2111, -1.3947, -1.6509],\n",
      "        [-1.3381, -1.2111, -1.3947, -1.6509],\n",
      "        [-1.3381, -1.2110, -1.3947, -1.6509],\n",
      "        [-1.3381, -1.2108, -1.3947, -1.6514],\n",
      "        [-1.3380, -1.2107, -1.3947, -1.6516],\n",
      "        [-1.3381, -1.2109, -1.3946, -1.6514],\n",
      "        [-1.3381, -1.2108, -1.3947, -1.6515],\n",
      "        [-1.3380, -1.2107, -1.3947, -1.6516],\n",
      "        [-1.3382, -1.2113, -1.3947, -1.6506],\n",
      "        [-1.3380, -1.2108, -1.3947, -1.6515],\n",
      "        [-1.3381, -1.2110, -1.3947, -1.6511],\n",
      "        [-1.3381, -1.2110, -1.3946, -1.6511],\n",
      "        [-1.3380, -1.2109, -1.3947, -1.6514],\n",
      "        [-1.3380, -1.2108, -1.3947, -1.6514],\n",
      "        [-1.3380, -1.2109, -1.3947, -1.6513],\n",
      "        [-1.3380, -1.2109, -1.3947, -1.6513],\n",
      "        [-1.3381, -1.2109, -1.3947, -1.6513],\n",
      "        [-1.3382, -1.2112, -1.3947, -1.6507],\n",
      "        [-1.3380, -1.2108, -1.3948, -1.6514],\n",
      "        [-1.3381, -1.2111, -1.3947, -1.6510],\n",
      "        [-1.3380, -1.2109, -1.3947, -1.6513],\n",
      "        [-1.3381, -1.2109, -1.3947, -1.6512],\n",
      "        [-1.3381, -1.2109, -1.3947, -1.6513],\n",
      "        [-1.3381, -1.2108, -1.3947, -1.6514],\n",
      "        [-1.3380, -1.2108, -1.3947, -1.6515],\n",
      "        [-1.3380, -1.2107, -1.3947, -1.6517],\n",
      "        [-1.3380, -1.2108, -1.3946, -1.6516],\n",
      "        [-1.3380, -1.2109, -1.3947, -1.6513],\n",
      "        [-1.3380, -1.2109, -1.3947, -1.6513],\n",
      "        [-1.3380, -1.2109, -1.3947, -1.6513],\n",
      "        [-1.3380, -1.2109, -1.3947, -1.6512],\n",
      "        [-1.3381, -1.2111, -1.3947, -1.6509],\n",
      "        [-1.3380, -1.2108, -1.3947, -1.6515],\n",
      "        [-1.3382, -1.2113, -1.3947, -1.6504],\n",
      "        [-1.3380, -1.2109, -1.3947, -1.6513],\n",
      "        [-1.3381, -1.2109, -1.3946, -1.6514],\n",
      "        [-1.3380, -1.2107, -1.3947, -1.6516],\n",
      "        [-1.3380, -1.2110, -1.3947, -1.6512],\n",
      "        [-1.3380, -1.2111, -1.3947, -1.6510],\n",
      "        [-1.3380, -1.2110, -1.3947, -1.6511],\n",
      "        [-1.3381, -1.2109, -1.3946, -1.6513],\n",
      "        [-1.3381, -1.2111, -1.3946, -1.6510],\n",
      "        [-1.3380, -1.2113, -1.3946, -1.6508],\n",
      "        [-1.3379, -1.2109, -1.3947, -1.6513],\n",
      "        [-1.3380, -1.2110, -1.3947, -1.6512],\n",
      "        [-1.3381, -1.2111, -1.3946, -1.6511],\n",
      "        [-1.3380, -1.2109, -1.3947, -1.6513],\n",
      "        [-1.3380, -1.2110, -1.3947, -1.6512],\n",
      "        [-1.3380, -1.2110, -1.3947, -1.6511],\n",
      "        [-1.3380, -1.2109, -1.3947, -1.6512],\n",
      "        [-1.3380, -1.2109, -1.3947, -1.6514],\n",
      "        [-1.3381, -1.2108, -1.3947, -1.6514],\n",
      "        [-1.3381, -1.2107, -1.3947, -1.6515],\n",
      "        [-1.3381, -1.2109, -1.3947, -1.6512],\n",
      "        [-1.3380, -1.2108, -1.3947, -1.6513]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([0.0080, 0.0051, 0.0035, 0.0024, 0.0071, 0.0057, 0.0036, 0.0015, 0.0083,\n",
      "        0.0058, 0.0039, 0.0020, 0.0068, 0.0067, 0.0035, 0.0022, 0.0079, 0.0041,\n",
      "        0.0041, 0.0015, 0.0064, 0.0069, 0.0048, 0.0022, 0.0066, 0.0057, 0.0035,\n",
      "        0.0025, 0.0069, 0.0055, 0.0036, 0.0018, 0.0075, 0.0052, 0.0052, 0.0023,\n",
      "        0.0078, 0.0056, 0.0039, 0.0016, 0.0066, 0.0060, 0.0027, 0.0024, 0.0064,\n",
      "        0.0069, 0.0034, 0.0013, 0.0070, 0.0057, 0.0034, 0.0013, 0.0089, 0.0066,\n",
      "        0.0040, 0.0018, 0.0076, 0.0057, 0.0034, 0.0023, 0.0071, 0.0063, 0.0040,\n",
      "        0.0025], grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(-0.0047, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2624, 0.2979, 0.2479, 0.1918],\n",
      "        [0.2624, 0.2979, 0.2479, 0.1918],\n",
      "        [0.2624, 0.2980, 0.2479, 0.1918],\n",
      "        [0.2624, 0.2980, 0.2479, 0.1918],\n",
      "        [0.2623, 0.2979, 0.2479, 0.1918],\n",
      "        [0.2624, 0.2979, 0.2479, 0.1918],\n",
      "        [0.2624, 0.2979, 0.2479, 0.1918],\n",
      "        [0.2624, 0.2979, 0.2479, 0.1919],\n",
      "        [0.2624, 0.2980, 0.2479, 0.1917],\n",
      "        [0.2623, 0.2979, 0.2479, 0.1919],\n",
      "        [0.2623, 0.2979, 0.2479, 0.1919],\n",
      "        [0.2623, 0.2979, 0.2479, 0.1919],\n",
      "        [0.2624, 0.2979, 0.2479, 0.1918],\n",
      "        [0.2624, 0.2980, 0.2479, 0.1917],\n",
      "        [0.2624, 0.2979, 0.2479, 0.1918],\n",
      "        [0.2624, 0.2980, 0.2479, 0.1918],\n",
      "        [0.2624, 0.2980, 0.2479, 0.1917],\n",
      "        [0.2623, 0.2978, 0.2479, 0.1919],\n",
      "        [0.2624, 0.2980, 0.2479, 0.1918],\n",
      "        [0.2623, 0.2979, 0.2479, 0.1918],\n",
      "        [0.2623, 0.2979, 0.2479, 0.1918],\n",
      "        [0.2624, 0.2979, 0.2479, 0.1918],\n",
      "        [0.2624, 0.2980, 0.2479, 0.1918],\n",
      "        [0.2624, 0.2979, 0.2479, 0.1918],\n",
      "        [0.2624, 0.2979, 0.2479, 0.1918],\n",
      "        [0.2624, 0.2979, 0.2479, 0.1918],\n",
      "        [0.2623, 0.2978, 0.2479, 0.1919],\n",
      "        [0.2624, 0.2980, 0.2479, 0.1918],\n",
      "        [0.2623, 0.2979, 0.2479, 0.1919],\n",
      "        [0.2624, 0.2979, 0.2479, 0.1918],\n",
      "        [0.2623, 0.2979, 0.2479, 0.1918],\n",
      "        [0.2624, 0.2979, 0.2479, 0.1918],\n",
      "        [0.2623, 0.2980, 0.2479, 0.1918],\n",
      "        [0.2624, 0.2980, 0.2479, 0.1918],\n",
      "        [0.2624, 0.2980, 0.2479, 0.1917],\n",
      "        [0.2624, 0.2980, 0.2479, 0.1917],\n",
      "        [0.2624, 0.2979, 0.2479, 0.1918],\n",
      "        [0.2624, 0.2979, 0.2479, 0.1918],\n",
      "        [0.2624, 0.2979, 0.2479, 0.1918],\n",
      "        [0.2624, 0.2979, 0.2479, 0.1918],\n",
      "        [0.2623, 0.2979, 0.2479, 0.1919],\n",
      "        [0.2624, 0.2980, 0.2479, 0.1918],\n",
      "        [0.2623, 0.2978, 0.2479, 0.1920],\n",
      "        [0.2624, 0.2979, 0.2479, 0.1918],\n",
      "        [0.2623, 0.2979, 0.2479, 0.1918],\n",
      "        [0.2624, 0.2980, 0.2479, 0.1917],\n",
      "        [0.2624, 0.2979, 0.2479, 0.1918],\n",
      "        [0.2624, 0.2979, 0.2479, 0.1919],\n",
      "        [0.2624, 0.2979, 0.2479, 0.1918],\n",
      "        [0.2624, 0.2979, 0.2479, 0.1918],\n",
      "        [0.2623, 0.2979, 0.2479, 0.1919],\n",
      "        [0.2624, 0.2978, 0.2479, 0.1919],\n",
      "        [0.2624, 0.2979, 0.2479, 0.1918],\n",
      "        [0.2624, 0.2979, 0.2479, 0.1918],\n",
      "        [0.2624, 0.2979, 0.2479, 0.1918],\n",
      "        [0.2624, 0.2979, 0.2479, 0.1918],\n",
      "        [0.2624, 0.2979, 0.2479, 0.1918],\n",
      "        [0.2624, 0.2979, 0.2479, 0.1918],\n",
      "        [0.2624, 0.2979, 0.2479, 0.1918],\n",
      "        [0.2624, 0.2979, 0.2479, 0.1918],\n",
      "        [0.2623, 0.2980, 0.2479, 0.1918],\n",
      "        [0.2624, 0.2980, 0.2479, 0.1918],\n",
      "        [0.2623, 0.2979, 0.2479, 0.1918],\n",
      "        [0.2624, 0.2980, 0.2479, 0.1918]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3743, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(-0.0185, grad_fn=<AddBackward0>)\n",
      "batching here\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68: done 3 episodes, mean_reward=0.33, best_reward=2.00, speed=224.04\n",
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(0.0186, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.3394, -1.2154, -1.3946, -1.6426],\n",
      "        [-1.3394, -1.2155, -1.3946, -1.6425],\n",
      "        [-1.3394, -1.2154, -1.3946, -1.6427],\n",
      "        [-1.3394, -1.2153, -1.3946, -1.6427],\n",
      "        [-1.3394, -1.2154, -1.3946, -1.6427],\n",
      "        [-1.3395, -1.2155, -1.3946, -1.6424],\n",
      "        [-1.3394, -1.2154, -1.3946, -1.6426],\n",
      "        [-1.3395, -1.2155, -1.3946, -1.6424],\n",
      "        [-1.3394, -1.2156, -1.3946, -1.6422],\n",
      "        [-1.3395, -1.2156, -1.3946, -1.6422],\n",
      "        [-1.3394, -1.2153, -1.3946, -1.6428],\n",
      "        [-1.3394, -1.2154, -1.3946, -1.6426],\n",
      "        [-1.3394, -1.2152, -1.3945, -1.6430],\n",
      "        [-1.3395, -1.2153, -1.3946, -1.6427],\n",
      "        [-1.3394, -1.2154, -1.3946, -1.6426],\n",
      "        [-1.3394, -1.2153, -1.3946, -1.6428],\n",
      "        [-1.3394, -1.2153, -1.3946, -1.6428],\n",
      "        [-1.3395, -1.2156, -1.3946, -1.6422],\n",
      "        [-1.3394, -1.2155, -1.3946, -1.6426],\n",
      "        [-1.3394, -1.2154, -1.3946, -1.6426],\n",
      "        [-1.3394, -1.2154, -1.3946, -1.6426],\n",
      "        [-1.3393, -1.2153, -1.3945, -1.6430],\n",
      "        [-1.3393, -1.2153, -1.3945, -1.6429],\n",
      "        [-1.3394, -1.2155, -1.3945, -1.6426],\n",
      "        [-1.3395, -1.2155, -1.3945, -1.6424],\n",
      "        [-1.3393, -1.2152, -1.3946, -1.6431],\n",
      "        [-1.3395, -1.2156, -1.3946, -1.6423],\n",
      "        [-1.3395, -1.2156, -1.3946, -1.6423],\n",
      "        [-1.3394, -1.2154, -1.3946, -1.6426],\n",
      "        [-1.3394, -1.2153, -1.3945, -1.6428],\n",
      "        [-1.3394, -1.2155, -1.3945, -1.6424],\n",
      "        [-1.3394, -1.2154, -1.3945, -1.6427],\n",
      "        [-1.3395, -1.2155, -1.3945, -1.6425],\n",
      "        [-1.3395, -1.2155, -1.3945, -1.6424],\n",
      "        [-1.3395, -1.2154, -1.3945, -1.6426],\n",
      "        [-1.3394, -1.2154, -1.3945, -1.6426],\n",
      "        [-1.3394, -1.2154, -1.3946, -1.6426],\n",
      "        [-1.3394, -1.2154, -1.3946, -1.6426],\n",
      "        [-1.3394, -1.2155, -1.3946, -1.6425],\n",
      "        [-1.3394, -1.2155, -1.3946, -1.6425],\n",
      "        [-1.3394, -1.2156, -1.3945, -1.6424],\n",
      "        [-1.3394, -1.2154, -1.3946, -1.6427],\n",
      "        [-1.3395, -1.2156, -1.3946, -1.6422],\n",
      "        [-1.3394, -1.2155, -1.3946, -1.6426],\n",
      "        [-1.3394, -1.2154, -1.3946, -1.6427],\n",
      "        [-1.3394, -1.2155, -1.3945, -1.6426],\n",
      "        [-1.3394, -1.2158, -1.3944, -1.6422],\n",
      "        [-1.3394, -1.2154, -1.3945, -1.6427],\n",
      "        [-1.3394, -1.2155, -1.3945, -1.6426],\n",
      "        [-1.3394, -1.2155, -1.3945, -1.6426],\n",
      "        [-1.3394, -1.2157, -1.3945, -1.6423],\n",
      "        [-1.3395, -1.2157, -1.3945, -1.6421],\n",
      "        [-1.3393, -1.2154, -1.3945, -1.6428],\n",
      "        [-1.3394, -1.2154, -1.3945, -1.6426],\n",
      "        [-1.3394, -1.2155, -1.3946, -1.6425],\n",
      "        [-1.3394, -1.2155, -1.3945, -1.6425],\n",
      "        [-1.3395, -1.2155, -1.3946, -1.6424],\n",
      "        [-1.3394, -1.2154, -1.3946, -1.6426],\n",
      "        [-1.3394, -1.2154, -1.3946, -1.6426],\n",
      "        [-1.3394, -1.2155, -1.3945, -1.6424],\n",
      "        [-1.3395, -1.2154, -1.3946, -1.6425],\n",
      "        [-1.3394, -1.2153, -1.3946, -1.6428],\n",
      "        [-1.3395, -1.2157, -1.3946, -1.6421],\n",
      "        [-1.3394, -1.2153, -1.3946, -1.6428]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([ 2.1233e-01,  1.8015e-01,  2.1243e-01,  1.5726e-01,  6.2776e-03,\n",
      "         5.1157e-03,  3.3491e-03,  1.4097e-03,  6.4503e-03,  5.8368e-03,\n",
      "         3.3321e-03,  1.6575e-03,  7.2214e-03,  4.9721e-03,  3.9205e-03,\n",
      "         1.7285e-03,  8.6693e-03,  4.3463e-03,  3.1198e-03,  1.7608e-03,\n",
      "         1.8045e-01,  1.8078e-01,  1.5738e-01,  1.5712e-01,  8.3966e-03,\n",
      "         7.2754e-03,  3.2907e-03,  1.5485e-03,  6.9676e-03,  4.7270e-03,\n",
      "         2.9059e-03,  1.7471e-03,  8.4049e-03,  4.5951e-03,  3.5842e-03,\n",
      "         1.7820e-03,  8.4424e-03,  4.6880e-03,  3.4665e-03,  1.7302e-03,\n",
      "         6.8359e-03,  6.4853e-03,  2.7920e-03,  1.5378e-03, -1.6338e+00,\n",
      "         6.2524e-03,  3.7141e-03,  1.7751e-03,  6.9611e-03,  6.1946e-03,\n",
      "         3.0441e-03,  1.1936e-03,  2.1265e-01,  2.1259e-01,  1.8022e-01,\n",
      "         1.7314e-01,  5.5977e-03,  4.3372e-03,  2.7462e-03,  1.2146e-03,\n",
      "         8.4485e-03,  6.0104e-03,  3.1603e-03,  2.7464e-03],\n",
      "       grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(-0.0126, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2620, 0.2966, 0.2479, 0.1935],\n",
      "        [0.2620, 0.2966, 0.2479, 0.1935],\n",
      "        [0.2620, 0.2966, 0.2479, 0.1935],\n",
      "        [0.2620, 0.2966, 0.2479, 0.1935],\n",
      "        [0.2620, 0.2966, 0.2479, 0.1935],\n",
      "        [0.2620, 0.2966, 0.2479, 0.1935],\n",
      "        [0.2620, 0.2966, 0.2479, 0.1935],\n",
      "        [0.2620, 0.2966, 0.2479, 0.1935],\n",
      "        [0.2620, 0.2965, 0.2479, 0.1936],\n",
      "        [0.2620, 0.2965, 0.2479, 0.1936],\n",
      "        [0.2620, 0.2966, 0.2479, 0.1934],\n",
      "        [0.2620, 0.2966, 0.2479, 0.1935],\n",
      "        [0.2620, 0.2966, 0.2479, 0.1934],\n",
      "        [0.2620, 0.2966, 0.2479, 0.1935],\n",
      "        [0.2620, 0.2966, 0.2479, 0.1935],\n",
      "        [0.2620, 0.2966, 0.2479, 0.1934],\n",
      "        [0.2620, 0.2966, 0.2479, 0.1934],\n",
      "        [0.2620, 0.2965, 0.2479, 0.1935],\n",
      "        [0.2620, 0.2966, 0.2479, 0.1935],\n",
      "        [0.2620, 0.2966, 0.2479, 0.1935],\n",
      "        [0.2620, 0.2966, 0.2479, 0.1935],\n",
      "        [0.2620, 0.2966, 0.2479, 0.1934],\n",
      "        [0.2620, 0.2966, 0.2479, 0.1934],\n",
      "        [0.2620, 0.2966, 0.2480, 0.1935],\n",
      "        [0.2620, 0.2966, 0.2479, 0.1935],\n",
      "        [0.2620, 0.2967, 0.2479, 0.1934],\n",
      "        [0.2620, 0.2965, 0.2479, 0.1935],\n",
      "        [0.2620, 0.2965, 0.2479, 0.1935],\n",
      "        [0.2620, 0.2966, 0.2479, 0.1935],\n",
      "        [0.2620, 0.2966, 0.2479, 0.1934],\n",
      "        [0.2620, 0.2966, 0.2479, 0.1935],\n",
      "        [0.2620, 0.2966, 0.2480, 0.1935],\n",
      "        [0.2620, 0.2966, 0.2480, 0.1935],\n",
      "        [0.2620, 0.2966, 0.2480, 0.1935],\n",
      "        [0.2620, 0.2966, 0.2480, 0.1935],\n",
      "        [0.2620, 0.2966, 0.2480, 0.1935],\n",
      "        [0.2620, 0.2966, 0.2479, 0.1935],\n",
      "        [0.2620, 0.2966, 0.2479, 0.1935],\n",
      "        [0.2620, 0.2966, 0.2479, 0.1935],\n",
      "        [0.2620, 0.2966, 0.2479, 0.1935],\n",
      "        [0.2620, 0.2965, 0.2479, 0.1935],\n",
      "        [0.2620, 0.2966, 0.2479, 0.1935],\n",
      "        [0.2620, 0.2965, 0.2479, 0.1935],\n",
      "        [0.2620, 0.2966, 0.2479, 0.1935],\n",
      "        [0.2620, 0.2966, 0.2479, 0.1935],\n",
      "        [0.2620, 0.2966, 0.2480, 0.1935],\n",
      "        [0.2620, 0.2965, 0.2480, 0.1935],\n",
      "        [0.2620, 0.2966, 0.2479, 0.1935],\n",
      "        [0.2620, 0.2966, 0.2479, 0.1935],\n",
      "        [0.2620, 0.2966, 0.2480, 0.1935],\n",
      "        [0.2620, 0.2965, 0.2480, 0.1935],\n",
      "        [0.2620, 0.2965, 0.2480, 0.1936],\n",
      "        [0.2620, 0.2966, 0.2479, 0.1934],\n",
      "        [0.2620, 0.2966, 0.2479, 0.1935],\n",
      "        [0.2620, 0.2966, 0.2479, 0.1935],\n",
      "        [0.2620, 0.2966, 0.2479, 0.1935],\n",
      "        [0.2620, 0.2966, 0.2479, 0.1935],\n",
      "        [0.2620, 0.2966, 0.2479, 0.1935],\n",
      "        [0.2620, 0.2966, 0.2479, 0.1935],\n",
      "        [0.2620, 0.2966, 0.2479, 0.1935],\n",
      "        [0.2620, 0.2966, 0.2479, 0.1935],\n",
      "        [0.2620, 0.2966, 0.2479, 0.1934],\n",
      "        [0.2620, 0.2965, 0.2479, 0.1936],\n",
      "        [0.2620, 0.2966, 0.2479, 0.1934]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3750, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(-0.0077, grad_fn=<AddBackward0>)\n",
      "batching here\n",
      "69: done 2 episodes, mean_reward=0.00, best_reward=2.00, speed=223.30\n",
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.3435, -1.2502, -1.4028, -1.5764],\n",
      "        [-1.3434, -1.2501, -1.4028, -1.5765],\n",
      "        [-1.3435, -1.2501, -1.4029, -1.5765],\n",
      "        [-1.3434, -1.2500, -1.4029, -1.5767],\n",
      "        [-1.3435, -1.2502, -1.4029, -1.5764],\n",
      "        [-1.3435, -1.2502, -1.4029, -1.5763],\n",
      "        [-1.3435, -1.2502, -1.4028, -1.5764],\n",
      "        [-1.3435, -1.2502, -1.4029, -1.5764],\n",
      "        [-1.3435, -1.2502, -1.4029, -1.5764],\n",
      "        [-1.3435, -1.2503, -1.4028, -1.5762],\n",
      "        [-1.3435, -1.2502, -1.4029, -1.5763],\n",
      "        [-1.3435, -1.2502, -1.4029, -1.5763],\n",
      "        [-1.3434, -1.2501, -1.4029, -1.5766],\n",
      "        [-1.3435, -1.2503, -1.4029, -1.5763],\n",
      "        [-1.3434, -1.2501, -1.4028, -1.5765],\n",
      "        [-1.3435, -1.2501, -1.4028, -1.5766],\n",
      "        [-1.3434, -1.2502, -1.4029, -1.5764],\n",
      "        [-1.3434, -1.2503, -1.4028, -1.5763],\n",
      "        [-1.3435, -1.2503, -1.4028, -1.5763],\n",
      "        [-1.3435, -1.2503, -1.4028, -1.5763],\n",
      "        [-1.3435, -1.2502, -1.4028, -1.5764],\n",
      "        [-1.3435, -1.2501, -1.4028, -1.5766],\n",
      "        [-1.3435, -1.2501, -1.4029, -1.5765],\n",
      "        [-1.3434, -1.2501, -1.4029, -1.5766],\n",
      "        [-1.3435, -1.2502, -1.4029, -1.5762],\n",
      "        [-1.3435, -1.2503, -1.4029, -1.5762],\n",
      "        [-1.3436, -1.2502, -1.4029, -1.5762],\n",
      "        [-1.3435, -1.2499, -1.4029, -1.5768],\n",
      "        [-1.3434, -1.2502, -1.4028, -1.5765],\n",
      "        [-1.3434, -1.2502, -1.4028, -1.5764],\n",
      "        [-1.3434, -1.2502, -1.4028, -1.5764],\n",
      "        [-1.3434, -1.2502, -1.4028, -1.5764],\n",
      "        [-1.3435, -1.2503, -1.4028, -1.5763],\n",
      "        [-1.3435, -1.2501, -1.4028, -1.5766],\n",
      "        [-1.3434, -1.2499, -1.4029, -1.5769],\n",
      "        [-1.3435, -1.2502, -1.4028, -1.5765],\n",
      "        [-1.3435, -1.2503, -1.4028, -1.5763],\n",
      "        [-1.3435, -1.2503, -1.4028, -1.5763],\n",
      "        [-1.3435, -1.2500, -1.4029, -1.5766],\n",
      "        [-1.3435, -1.2501, -1.4029, -1.5765],\n",
      "        [-1.3434, -1.2502, -1.4029, -1.5764],\n",
      "        [-1.3434, -1.2502, -1.4029, -1.5764],\n",
      "        [-1.3434, -1.2503, -1.4028, -1.5763],\n",
      "        [-1.3435, -1.2503, -1.4028, -1.5763],\n",
      "        [-1.3436, -1.2502, -1.4028, -1.5763],\n",
      "        [-1.3436, -1.2503, -1.4028, -1.5762],\n",
      "        [-1.3434, -1.2499, -1.4029, -1.5768],\n",
      "        [-1.3434, -1.2500, -1.4028, -1.5768],\n",
      "        [-1.3434, -1.2503, -1.4028, -1.5764],\n",
      "        [-1.3434, -1.2504, -1.4027, -1.5763],\n",
      "        [-1.3434, -1.2503, -1.4028, -1.5765],\n",
      "        [-1.3435, -1.2503, -1.4028, -1.5763],\n",
      "        [-1.3435, -1.2500, -1.4029, -1.5766],\n",
      "        [-1.3435, -1.2501, -1.4029, -1.5765],\n",
      "        [-1.3435, -1.2500, -1.4029, -1.5766],\n",
      "        [-1.3435, -1.2502, -1.4028, -1.5764],\n",
      "        [-1.3434, -1.2500, -1.4029, -1.5767],\n",
      "        [-1.3436, -1.2506, -1.4029, -1.5757],\n",
      "        [-1.3435, -1.2502, -1.4028, -1.5764],\n",
      "        [-1.3434, -1.2502, -1.4028, -1.5765],\n",
      "        [-1.3435, -1.2503, -1.4029, -1.5761],\n",
      "        [-1.3434, -1.2500, -1.4029, -1.5767],\n",
      "        [-1.3436, -1.2506, -1.4028, -1.5757],\n",
      "        [-1.3434, -1.2500, -1.4029, -1.5767]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([0.0049, 0.0043, 0.0031, 0.0016, 0.0063, 0.0036, 0.0025, 0.0015, 0.0063,\n",
      "        0.0035, 0.0025, 0.0013, 0.0056, 0.0033, 0.0024, 0.0014, 0.0055, 0.0040,\n",
      "        0.0025, 0.0012, 0.0059, 0.0036, 0.0026, 0.0013, 0.0057, 0.0033, 0.0023,\n",
      "        0.0019, 0.1338, 0.1337, 0.1337, 0.1396, 0.0061, 0.0042, 0.0027, 0.0014,\n",
      "        0.1391, 0.1240, 0.0025, 0.0012, 0.0050, 0.0042, 0.0028, 0.0016, 0.0050,\n",
      "        0.0034, 0.0028, 0.0013, 0.0054, 0.0035, 0.0031, 0.0009, 0.0053, 0.0040,\n",
      "        0.0035, 0.0016, 0.0056, 0.0035, 0.0023, 0.0012, 0.0058, 0.0050, 0.0017,\n",
      "        0.0020], grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(-0.0156, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2609, 0.2865, 0.2459, 0.2067],\n",
      "        [0.2609, 0.2865, 0.2459, 0.2067],\n",
      "        [0.2609, 0.2865, 0.2459, 0.2067],\n",
      "        [0.2610, 0.2865, 0.2459, 0.2067],\n",
      "        [0.2609, 0.2865, 0.2459, 0.2067],\n",
      "        [0.2609, 0.2864, 0.2459, 0.2067],\n",
      "        [0.2609, 0.2864, 0.2459, 0.2067],\n",
      "        [0.2609, 0.2864, 0.2459, 0.2067],\n",
      "        [0.2609, 0.2865, 0.2459, 0.2067],\n",
      "        [0.2609, 0.2864, 0.2459, 0.2068],\n",
      "        [0.2609, 0.2864, 0.2459, 0.2067],\n",
      "        [0.2609, 0.2864, 0.2459, 0.2067],\n",
      "        [0.2609, 0.2865, 0.2459, 0.2067],\n",
      "        [0.2609, 0.2864, 0.2459, 0.2067],\n",
      "        [0.2609, 0.2865, 0.2459, 0.2067],\n",
      "        [0.2609, 0.2865, 0.2459, 0.2067],\n",
      "        [0.2609, 0.2864, 0.2459, 0.2067],\n",
      "        [0.2609, 0.2864, 0.2459, 0.2067],\n",
      "        [0.2609, 0.2864, 0.2459, 0.2067],\n",
      "        [0.2609, 0.2864, 0.2459, 0.2067],\n",
      "        [0.2609, 0.2865, 0.2459, 0.2067],\n",
      "        [0.2609, 0.2865, 0.2459, 0.2067],\n",
      "        [0.2609, 0.2865, 0.2459, 0.2067],\n",
      "        [0.2610, 0.2865, 0.2459, 0.2067],\n",
      "        [0.2609, 0.2864, 0.2459, 0.2068],\n",
      "        [0.2609, 0.2864, 0.2459, 0.2068],\n",
      "        [0.2609, 0.2864, 0.2459, 0.2068],\n",
      "        [0.2609, 0.2865, 0.2459, 0.2066],\n",
      "        [0.2609, 0.2865, 0.2459, 0.2067],\n",
      "        [0.2609, 0.2864, 0.2459, 0.2067],\n",
      "        [0.2609, 0.2864, 0.2459, 0.2067],\n",
      "        [0.2609, 0.2864, 0.2459, 0.2067],\n",
      "        [0.2609, 0.2864, 0.2459, 0.2067],\n",
      "        [0.2609, 0.2865, 0.2459, 0.2067],\n",
      "        [0.2610, 0.2865, 0.2459, 0.2066],\n",
      "        [0.2609, 0.2864, 0.2459, 0.2067],\n",
      "        [0.2609, 0.2864, 0.2459, 0.2067],\n",
      "        [0.2609, 0.2864, 0.2459, 0.2067],\n",
      "        [0.2609, 0.2865, 0.2459, 0.2067],\n",
      "        [0.2609, 0.2865, 0.2459, 0.2067],\n",
      "        [0.2610, 0.2864, 0.2459, 0.2067],\n",
      "        [0.2609, 0.2864, 0.2459, 0.2067],\n",
      "        [0.2609, 0.2864, 0.2459, 0.2067],\n",
      "        [0.2609, 0.2864, 0.2459, 0.2067],\n",
      "        [0.2609, 0.2864, 0.2459, 0.2067],\n",
      "        [0.2609, 0.2864, 0.2459, 0.2068],\n",
      "        [0.2610, 0.2865, 0.2459, 0.2066],\n",
      "        [0.2610, 0.2865, 0.2459, 0.2066],\n",
      "        [0.2609, 0.2864, 0.2459, 0.2067],\n",
      "        [0.2610, 0.2864, 0.2459, 0.2067],\n",
      "        [0.2610, 0.2864, 0.2459, 0.2067],\n",
      "        [0.2609, 0.2864, 0.2459, 0.2067],\n",
      "        [0.2609, 0.2865, 0.2459, 0.2067],\n",
      "        [0.2609, 0.2865, 0.2459, 0.2067],\n",
      "        [0.2609, 0.2865, 0.2459, 0.2067],\n",
      "        [0.2609, 0.2865, 0.2459, 0.2067],\n",
      "        [0.2610, 0.2865, 0.2459, 0.2067],\n",
      "        [0.2609, 0.2863, 0.2459, 0.2069],\n",
      "        [0.2609, 0.2864, 0.2459, 0.2067],\n",
      "        [0.2609, 0.2864, 0.2459, 0.2067],\n",
      "        [0.2609, 0.2864, 0.2459, 0.2068],\n",
      "        [0.2610, 0.2865, 0.2459, 0.2066],\n",
      "        [0.2609, 0.2863, 0.2459, 0.2069],\n",
      "        [0.2610, 0.2865, 0.2459, 0.2067]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3795, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(-0.0284, grad_fn=<AddBackward0>)\n",
      "batching here\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70: done 3 episodes, mean_reward=0.67, best_reward=2.00, speed=227.71\n",
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(0.0160, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.3490, -1.2598, -1.4028, -1.5565],\n",
      "        [-1.3490, -1.2597, -1.4028, -1.5565],\n",
      "        [-1.3489, -1.2596, -1.4029, -1.5567],\n",
      "        [-1.3490, -1.2598, -1.4029, -1.5564],\n",
      "        [-1.3490, -1.2598, -1.4029, -1.5564],\n",
      "        [-1.3490, -1.2599, -1.4029, -1.5563],\n",
      "        [-1.3490, -1.2599, -1.4029, -1.5563],\n",
      "        [-1.3490, -1.2599, -1.4028, -1.5564],\n",
      "        [-1.3490, -1.2599, -1.4029, -1.5563],\n",
      "        [-1.3490, -1.2599, -1.4029, -1.5563],\n",
      "        [-1.3490, -1.2599, -1.4028, -1.5563],\n",
      "        [-1.3490, -1.2599, -1.4028, -1.5564],\n",
      "        [-1.3490, -1.2598, -1.4028, -1.5564],\n",
      "        [-1.3490, -1.2599, -1.4029, -1.5563],\n",
      "        [-1.3490, -1.2600, -1.4028, -1.5563],\n",
      "        [-1.3490, -1.2599, -1.4028, -1.5563],\n",
      "        [-1.3490, -1.2599, -1.4029, -1.5563],\n",
      "        [-1.3490, -1.2599, -1.4029, -1.5563],\n",
      "        [-1.3490, -1.2599, -1.4028, -1.5564],\n",
      "        [-1.3489, -1.2599, -1.4029, -1.5564],\n",
      "        [-1.3489, -1.2597, -1.4028, -1.5567],\n",
      "        [-1.3490, -1.2598, -1.4028, -1.5565],\n",
      "        [-1.3489, -1.2596, -1.4029, -1.5567],\n",
      "        [-1.3490, -1.2596, -1.4029, -1.5566],\n",
      "        [-1.3490, -1.2597, -1.4029, -1.5566],\n",
      "        [-1.3489, -1.2597, -1.4029, -1.5565],\n",
      "        [-1.3490, -1.2599, -1.4028, -1.5563],\n",
      "        [-1.3489, -1.2597, -1.4029, -1.5566],\n",
      "        [-1.3490, -1.2600, -1.4028, -1.5562],\n",
      "        [-1.3490, -1.2599, -1.4028, -1.5564],\n",
      "        [-1.3490, -1.2600, -1.4028, -1.5563],\n",
      "        [-1.3489, -1.2598, -1.4028, -1.5565],\n",
      "        [-1.3489, -1.2598, -1.4028, -1.5565],\n",
      "        [-1.3490, -1.2599, -1.4028, -1.5564],\n",
      "        [-1.3490, -1.2600, -1.4028, -1.5563],\n",
      "        [-1.3489, -1.2598, -1.4028, -1.5565],\n",
      "        [-1.3489, -1.2596, -1.4029, -1.5567],\n",
      "        [-1.3490, -1.2598, -1.4029, -1.5563],\n",
      "        [-1.3490, -1.2597, -1.4029, -1.5565],\n",
      "        [-1.3490, -1.2598, -1.4029, -1.5563],\n",
      "        [-1.3490, -1.2599, -1.4029, -1.5563],\n",
      "        [-1.3490, -1.2599, -1.4029, -1.5563],\n",
      "        [-1.3490, -1.2600, -1.4028, -1.5562],\n",
      "        [-1.3490, -1.2599, -1.4029, -1.5563],\n",
      "        [-1.3490, -1.2597, -1.4028, -1.5565],\n",
      "        [-1.3491, -1.2599, -1.4028, -1.5562],\n",
      "        [-1.3489, -1.2597, -1.4029, -1.5566],\n",
      "        [-1.3490, -1.2596, -1.4029, -1.5567],\n",
      "        [-1.3489, -1.2600, -1.4028, -1.5563],\n",
      "        [-1.3489, -1.2600, -1.4028, -1.5563],\n",
      "        [-1.3489, -1.2600, -1.4028, -1.5563],\n",
      "        [-1.3489, -1.2600, -1.4028, -1.5562],\n",
      "        [-1.3490, -1.2597, -1.4029, -1.5565],\n",
      "        [-1.3490, -1.2597, -1.4029, -1.5565],\n",
      "        [-1.3490, -1.2596, -1.4029, -1.5567],\n",
      "        [-1.3490, -1.2599, -1.4030, -1.5562],\n",
      "        [-1.3489, -1.2598, -1.4028, -1.5565],\n",
      "        [-1.3489, -1.2598, -1.4029, -1.5565],\n",
      "        [-1.3490, -1.2600, -1.4028, -1.5562],\n",
      "        [-1.3490, -1.2600, -1.4028, -1.5560],\n",
      "        [-1.3490, -1.2598, -1.4028, -1.5564],\n",
      "        [-1.3489, -1.2596, -1.4029, -1.5567],\n",
      "        [-1.3490, -1.2599, -1.4029, -1.5562],\n",
      "        [-1.3489, -1.2597, -1.4029, -1.5566]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([ 4.4298e-03,  3.3925e-03,  2.3272e-03,  1.1581e-03,  4.6360e-03,\n",
      "         2.7249e-03,  1.7927e-03,  9.7221e-04,  1.2114e-01,  1.2110e-01,\n",
      "         2.3875e-03,  1.2599e-03,  4.2417e-03,  2.9358e-03,  1.7763e-03,\n",
      "         9.0476e-04,  1.0497e-01,  3.2531e-03,  2.2478e-03,  1.2206e-03,\n",
      "         4.5202e-03,  2.9497e-03,  2.1069e-03,  1.0040e-03,  5.0247e-03,\n",
      "         3.2801e-03,  2.0517e-03,  1.3155e-03,  4.3013e-03,  2.7145e-03,\n",
      "         1.5055e-03,  1.0915e-03, -1.2559e+00,  3.2723e-03,  2.3989e-03,\n",
      "         1.1828e-03,  4.3416e-03,  2.8291e-03,  1.6285e-03,  5.4566e-04,\n",
      "         1.0497e-01,  1.0916e-01,  2.4102e-03,  1.2299e-03,  4.2396e-03,\n",
      "         3.0305e-03,  2.1693e-03,  1.4695e-03,  4.1621e-03,  3.4001e-03,\n",
      "         2.2644e-03,  1.1924e-03,  3.2397e-03,  2.6763e-03,  1.9275e-03,\n",
      "         1.9515e-05,  5.3081e-03,  3.4822e-03,  2.1186e-03,  9.1459e-04,\n",
      "         3.6257e-03,  3.1454e-03,  1.6177e-03,  1.1210e-03],\n",
      "       grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(0.0086, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2595, 0.2837, 0.2459, 0.2109],\n",
      "        [0.2595, 0.2837, 0.2459, 0.2109],\n",
      "        [0.2595, 0.2838, 0.2459, 0.2108],\n",
      "        [0.2595, 0.2837, 0.2459, 0.2109],\n",
      "        [0.2595, 0.2837, 0.2459, 0.2109],\n",
      "        [0.2595, 0.2837, 0.2459, 0.2109],\n",
      "        [0.2595, 0.2837, 0.2459, 0.2109],\n",
      "        [0.2595, 0.2837, 0.2459, 0.2109],\n",
      "        [0.2595, 0.2837, 0.2459, 0.2109],\n",
      "        [0.2595, 0.2837, 0.2459, 0.2109],\n",
      "        [0.2595, 0.2837, 0.2459, 0.2109],\n",
      "        [0.2595, 0.2837, 0.2459, 0.2109],\n",
      "        [0.2595, 0.2837, 0.2459, 0.2109],\n",
      "        [0.2595, 0.2837, 0.2459, 0.2109],\n",
      "        [0.2595, 0.2837, 0.2459, 0.2109],\n",
      "        [0.2595, 0.2837, 0.2459, 0.2109],\n",
      "        [0.2595, 0.2837, 0.2459, 0.2109],\n",
      "        [0.2595, 0.2837, 0.2459, 0.2109],\n",
      "        [0.2595, 0.2837, 0.2459, 0.2109],\n",
      "        [0.2595, 0.2837, 0.2459, 0.2109],\n",
      "        [0.2595, 0.2837, 0.2459, 0.2108],\n",
      "        [0.2595, 0.2837, 0.2459, 0.2109],\n",
      "        [0.2595, 0.2838, 0.2459, 0.2108],\n",
      "        [0.2595, 0.2838, 0.2459, 0.2108],\n",
      "        [0.2595, 0.2837, 0.2459, 0.2109],\n",
      "        [0.2595, 0.2837, 0.2459, 0.2109],\n",
      "        [0.2595, 0.2837, 0.2459, 0.2109],\n",
      "        [0.2595, 0.2837, 0.2459, 0.2109],\n",
      "        [0.2595, 0.2837, 0.2459, 0.2109],\n",
      "        [0.2595, 0.2837, 0.2459, 0.2109],\n",
      "        [0.2595, 0.2837, 0.2459, 0.2109],\n",
      "        [0.2595, 0.2837, 0.2459, 0.2109],\n",
      "        [0.2595, 0.2837, 0.2459, 0.2109],\n",
      "        [0.2595, 0.2837, 0.2459, 0.2109],\n",
      "        [0.2595, 0.2837, 0.2459, 0.2109],\n",
      "        [0.2595, 0.2837, 0.2459, 0.2109],\n",
      "        [0.2595, 0.2838, 0.2459, 0.2108],\n",
      "        [0.2595, 0.2837, 0.2459, 0.2109],\n",
      "        [0.2595, 0.2837, 0.2459, 0.2109],\n",
      "        [0.2595, 0.2837, 0.2459, 0.2109],\n",
      "        [0.2595, 0.2837, 0.2459, 0.2109],\n",
      "        [0.2595, 0.2837, 0.2459, 0.2109],\n",
      "        [0.2595, 0.2837, 0.2459, 0.2109],\n",
      "        [0.2595, 0.2837, 0.2459, 0.2109],\n",
      "        [0.2595, 0.2837, 0.2459, 0.2109],\n",
      "        [0.2595, 0.2837, 0.2459, 0.2109],\n",
      "        [0.2595, 0.2837, 0.2459, 0.2108],\n",
      "        [0.2595, 0.2838, 0.2459, 0.2108],\n",
      "        [0.2595, 0.2837, 0.2459, 0.2109],\n",
      "        [0.2595, 0.2837, 0.2459, 0.2109],\n",
      "        [0.2595, 0.2836, 0.2459, 0.2109],\n",
      "        [0.2595, 0.2836, 0.2459, 0.2109],\n",
      "        [0.2595, 0.2837, 0.2459, 0.2109],\n",
      "        [0.2595, 0.2837, 0.2459, 0.2109],\n",
      "        [0.2595, 0.2838, 0.2459, 0.2108],\n",
      "        [0.2595, 0.2837, 0.2459, 0.2109],\n",
      "        [0.2595, 0.2837, 0.2459, 0.2109],\n",
      "        [0.2595, 0.2837, 0.2459, 0.2109],\n",
      "        [0.2595, 0.2837, 0.2459, 0.2109],\n",
      "        [0.2595, 0.2836, 0.2459, 0.2110],\n",
      "        [0.2595, 0.2837, 0.2459, 0.2109],\n",
      "        [0.2595, 0.2838, 0.2459, 0.2108],\n",
      "        [0.2595, 0.2837, 0.2459, 0.2109],\n",
      "        [0.2595, 0.2837, 0.2459, 0.2109]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3807, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(0.0108, grad_fn=<AddBackward0>)\n",
      "batching here\n",
      "71: done 2 episodes, mean_reward=1.00, best_reward=2.00, speed=231.00\n",
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.3529, -1.2203, -1.4115, -1.5965],\n",
      "        [-1.3528, -1.2203, -1.4115, -1.5967],\n",
      "        [-1.3529, -1.2202, -1.4115, -1.5967],\n",
      "        [-1.3529, -1.2204, -1.4115, -1.5964],\n",
      "        [-1.3529, -1.2203, -1.4115, -1.5966],\n",
      "        [-1.3529, -1.2203, -1.4115, -1.5967],\n",
      "        [-1.3529, -1.2203, -1.4115, -1.5967],\n",
      "        [-1.3529, -1.2203, -1.4115, -1.5966],\n",
      "        [-1.3529, -1.2204, -1.4115, -1.5966],\n",
      "        [-1.3528, -1.2202, -1.4115, -1.5968],\n",
      "        [-1.3529, -1.2203, -1.4115, -1.5966],\n",
      "        [-1.3529, -1.2202, -1.4115, -1.5967],\n",
      "        [-1.3529, -1.2203, -1.4115, -1.5967],\n",
      "        [-1.3529, -1.2204, -1.4114, -1.5965],\n",
      "        [-1.3528, -1.2203, -1.4114, -1.5967],\n",
      "        [-1.3529, -1.2203, -1.4115, -1.5967],\n",
      "        [-1.3528, -1.2203, -1.4115, -1.5967],\n",
      "        [-1.3529, -1.2203, -1.4115, -1.5965],\n",
      "        [-1.3529, -1.2203, -1.4115, -1.5966],\n",
      "        [-1.3529, -1.2202, -1.4115, -1.5967],\n",
      "        [-1.3528, -1.2202, -1.4115, -1.5967],\n",
      "        [-1.3528, -1.2201, -1.4115, -1.5969],\n",
      "        [-1.3529, -1.2201, -1.4115, -1.5969],\n",
      "        [-1.3529, -1.2203, -1.4115, -1.5966],\n",
      "        [-1.3529, -1.2203, -1.4115, -1.5966],\n",
      "        [-1.3529, -1.2203, -1.4115, -1.5966],\n",
      "        [-1.3529, -1.2202, -1.4115, -1.5967],\n",
      "        [-1.3529, -1.2202, -1.4115, -1.5968],\n",
      "        [-1.3529, -1.2204, -1.4114, -1.5967],\n",
      "        [-1.3529, -1.2203, -1.4114, -1.5966],\n",
      "        [-1.3529, -1.2203, -1.4115, -1.5967],\n",
      "        [-1.3529, -1.2203, -1.4114, -1.5966],\n",
      "        [-1.3529, -1.2202, -1.4114, -1.5968],\n",
      "        [-1.3529, -1.2204, -1.4114, -1.5965],\n",
      "        [-1.3529, -1.2202, -1.4114, -1.5968],\n",
      "        [-1.3529, -1.2201, -1.4115, -1.5969],\n",
      "        [-1.3529, -1.2201, -1.4115, -1.5968],\n",
      "        [-1.3529, -1.2205, -1.4115, -1.5962],\n",
      "        [-1.3529, -1.2201, -1.4115, -1.5968],\n",
      "        [-1.3529, -1.2205, -1.4115, -1.5963],\n",
      "        [-1.3529, -1.2204, -1.4115, -1.5965],\n",
      "        [-1.3529, -1.2203, -1.4114, -1.5967],\n",
      "        [-1.3529, -1.2204, -1.4114, -1.5966],\n",
      "        [-1.3529, -1.2203, -1.4115, -1.5966],\n",
      "        [-1.3529, -1.2202, -1.4115, -1.5967],\n",
      "        [-1.3529, -1.2201, -1.4114, -1.5969],\n",
      "        [-1.3530, -1.2203, -1.4114, -1.5965],\n",
      "        [-1.3529, -1.2202, -1.4115, -1.5967],\n",
      "        [-1.3528, -1.2205, -1.4114, -1.5965],\n",
      "        [-1.3528, -1.2205, -1.4114, -1.5964],\n",
      "        [-1.3528, -1.2205, -1.4114, -1.5964],\n",
      "        [-1.3528, -1.2206, -1.4114, -1.5964],\n",
      "        [-1.3529, -1.2199, -1.4116, -1.5971],\n",
      "        [-1.3529, -1.2203, -1.4115, -1.5965],\n",
      "        [-1.3528, -1.2201, -1.4115, -1.5970],\n",
      "        [-1.3530, -1.2207, -1.4114, -1.5959],\n",
      "        [-1.3529, -1.2205, -1.4114, -1.5963],\n",
      "        [-1.3529, -1.2204, -1.4115, -1.5964],\n",
      "        [-1.3529, -1.2206, -1.4114, -1.5962],\n",
      "        [-1.3529, -1.2204, -1.4114, -1.5965],\n",
      "        [-1.3528, -1.2202, -1.4115, -1.5968],\n",
      "        [-1.3529, -1.2202, -1.4115, -1.5967],\n",
      "        [-1.3529, -1.2203, -1.4115, -1.5966],\n",
      "        [-1.3529, -1.2202, -1.4115, -1.5967]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([ 5.4841e-03,  3.7285e-03,  2.7523e-03,  1.2256e-03,  1.3078e-01,\n",
      "         1.1800e-01,  1.3650e-01,  1.1521e-03,  4.5732e-03,  3.4375e-03,\n",
      "         2.4172e-03,  1.7528e-03,  4.6864e-03,  3.3513e-03,  2.4132e-03,\n",
      "         1.2780e-03,  6.1712e-03,  3.7163e-03,  3.4445e-03,  1.3898e-03,\n",
      "         5.6558e-03,  4.4932e-03,  3.5703e-03,  1.5324e-03,  1.3083e-01,\n",
      "         1.5431e-01,  1.1811e-01,  1.1735e-03,  5.8871e-03,  3.6736e-03,\n",
      "         2.5148e-03,  9.1977e-04,  5.2699e-03,  4.0247e-03,  2.2045e-03,\n",
      "         1.3137e-03,  4.7223e-03,  3.2470e-03,  2.3253e-03,  9.5128e-04,\n",
      "         5.5003e-03,  3.3554e-03,  2.1378e-03,  1.3723e-03,  5.9296e-03,\n",
      "         4.5832e-03,  2.6773e-03,  9.9921e-04,  5.2609e-03,  3.5435e-03,\n",
      "         2.3766e-03,  1.5428e-03,  6.1565e-03,  4.0222e-03,  2.3002e-03,\n",
      "        -2.9496e-05,  4.5205e-03,  3.6966e-03,  2.4634e-03,  1.1119e-03,\n",
      "         5.3224e-03,  3.8059e-03,  2.4360e-03,  1.4126e-03],\n",
      "       grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(-0.0152, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2585, 0.2951, 0.2438, 0.2026],\n",
      "        [0.2585, 0.2951, 0.2438, 0.2026],\n",
      "        [0.2585, 0.2952, 0.2438, 0.2026],\n",
      "        [0.2585, 0.2951, 0.2438, 0.2026],\n",
      "        [0.2585, 0.2951, 0.2438, 0.2026],\n",
      "        [0.2585, 0.2952, 0.2438, 0.2026],\n",
      "        [0.2585, 0.2952, 0.2438, 0.2026],\n",
      "        [0.2585, 0.2951, 0.2438, 0.2026],\n",
      "        [0.2585, 0.2951, 0.2438, 0.2026],\n",
      "        [0.2585, 0.2952, 0.2438, 0.2026],\n",
      "        [0.2585, 0.2951, 0.2438, 0.2026],\n",
      "        [0.2585, 0.2952, 0.2438, 0.2026],\n",
      "        [0.2585, 0.2952, 0.2438, 0.2026],\n",
      "        [0.2585, 0.2951, 0.2438, 0.2026],\n",
      "        [0.2585, 0.2951, 0.2438, 0.2026],\n",
      "        [0.2585, 0.2952, 0.2438, 0.2026],\n",
      "        [0.2585, 0.2951, 0.2438, 0.2026],\n",
      "        [0.2585, 0.2951, 0.2438, 0.2026],\n",
      "        [0.2585, 0.2951, 0.2438, 0.2026],\n",
      "        [0.2585, 0.2952, 0.2438, 0.2026],\n",
      "        [0.2585, 0.2952, 0.2438, 0.2026],\n",
      "        [0.2585, 0.2952, 0.2438, 0.2025],\n",
      "        [0.2585, 0.2952, 0.2438, 0.2025],\n",
      "        [0.2585, 0.2951, 0.2438, 0.2026],\n",
      "        [0.2585, 0.2951, 0.2438, 0.2026],\n",
      "        [0.2585, 0.2951, 0.2438, 0.2026],\n",
      "        [0.2585, 0.2952, 0.2438, 0.2026],\n",
      "        [0.2585, 0.2952, 0.2438, 0.2025],\n",
      "        [0.2585, 0.2951, 0.2438, 0.2026],\n",
      "        [0.2585, 0.2951, 0.2438, 0.2026],\n",
      "        [0.2585, 0.2951, 0.2438, 0.2026],\n",
      "        [0.2585, 0.2951, 0.2438, 0.2026],\n",
      "        [0.2585, 0.2952, 0.2438, 0.2025],\n",
      "        [0.2585, 0.2951, 0.2438, 0.2026],\n",
      "        [0.2585, 0.2952, 0.2438, 0.2025],\n",
      "        [0.2585, 0.2952, 0.2438, 0.2025],\n",
      "        [0.2585, 0.2952, 0.2438, 0.2025],\n",
      "        [0.2585, 0.2951, 0.2438, 0.2027],\n",
      "        [0.2585, 0.2952, 0.2438, 0.2025],\n",
      "        [0.2585, 0.2951, 0.2438, 0.2026],\n",
      "        [0.2585, 0.2951, 0.2438, 0.2026],\n",
      "        [0.2585, 0.2951, 0.2438, 0.2026],\n",
      "        [0.2585, 0.2951, 0.2438, 0.2026],\n",
      "        [0.2585, 0.2951, 0.2438, 0.2026],\n",
      "        [0.2585, 0.2952, 0.2438, 0.2026],\n",
      "        [0.2585, 0.2952, 0.2438, 0.2025],\n",
      "        [0.2585, 0.2951, 0.2438, 0.2026],\n",
      "        [0.2585, 0.2952, 0.2438, 0.2026],\n",
      "        [0.2585, 0.2951, 0.2438, 0.2026],\n",
      "        [0.2585, 0.2951, 0.2438, 0.2026],\n",
      "        [0.2585, 0.2951, 0.2438, 0.2026],\n",
      "        [0.2585, 0.2951, 0.2438, 0.2026],\n",
      "        [0.2585, 0.2953, 0.2438, 0.2025],\n",
      "        [0.2585, 0.2951, 0.2438, 0.2026],\n",
      "        [0.2585, 0.2952, 0.2438, 0.2025],\n",
      "        [0.2585, 0.2950, 0.2438, 0.2027],\n",
      "        [0.2585, 0.2951, 0.2438, 0.2026],\n",
      "        [0.2585, 0.2951, 0.2438, 0.2026],\n",
      "        [0.2585, 0.2950, 0.2438, 0.2027],\n",
      "        [0.2585, 0.2951, 0.2438, 0.2026],\n",
      "        [0.2585, 0.2952, 0.2438, 0.2025],\n",
      "        [0.2585, 0.2952, 0.2438, 0.2026],\n",
      "        [0.2585, 0.2951, 0.2438, 0.2026],\n",
      "        [0.2585, 0.2952, 0.2438, 0.2026]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3774, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(-0.0281, grad_fn=<AddBackward0>)\n",
      "batching here\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72: done 2 episodes, mean_reward=0.00, best_reward=2.00, speed=230.33\n",
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(0.0461, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.3550, -1.2313, -1.4087, -1.5816],\n",
      "        [-1.3550, -1.2312, -1.4086, -1.5816],\n",
      "        [-1.3550, -1.2312, -1.4086, -1.5817],\n",
      "        [-1.3550, -1.2313, -1.4086, -1.5815],\n",
      "        [-1.3550, -1.2313, -1.4086, -1.5816],\n",
      "        [-1.3550, -1.2312, -1.4086, -1.5816],\n",
      "        [-1.3549, -1.2312, -1.4087, -1.5817],\n",
      "        [-1.3550, -1.2313, -1.4086, -1.5815],\n",
      "        [-1.3550, -1.2311, -1.4087, -1.5817],\n",
      "        [-1.3551, -1.2314, -1.4086, -1.5814],\n",
      "        [-1.3550, -1.2309, -1.4087, -1.5820],\n",
      "        [-1.3550, -1.2312, -1.4086, -1.5816],\n",
      "        [-1.3550, -1.2313, -1.4086, -1.5816],\n",
      "        [-1.3549, -1.2310, -1.4086, -1.5820],\n",
      "        [-1.3551, -1.2317, -1.4086, -1.5809],\n",
      "        [-1.3550, -1.2313, -1.4086, -1.5816],\n",
      "        [-1.3550, -1.2314, -1.4086, -1.5814],\n",
      "        [-1.3550, -1.2311, -1.4087, -1.5818],\n",
      "        [-1.3550, -1.2313, -1.4086, -1.5815],\n",
      "        [-1.3550, -1.2312, -1.4086, -1.5817],\n",
      "        [-1.3550, -1.2312, -1.4087, -1.5816],\n",
      "        [-1.3550, -1.2311, -1.4086, -1.5819],\n",
      "        [-1.3550, -1.2312, -1.4087, -1.5817],\n",
      "        [-1.3550, -1.2312, -1.4087, -1.5817],\n",
      "        [-1.3550, -1.2311, -1.4087, -1.5818],\n",
      "        [-1.3550, -1.2310, -1.4086, -1.5819],\n",
      "        [-1.3551, -1.2312, -1.4086, -1.5817],\n",
      "        [-1.3550, -1.2313, -1.4086, -1.5815],\n",
      "        [-1.3550, -1.2312, -1.4086, -1.5817],\n",
      "        [-1.3549, -1.2311, -1.4086, -1.5819],\n",
      "        [-1.3550, -1.2311, -1.4086, -1.5819],\n",
      "        [-1.3550, -1.2313, -1.4086, -1.5815],\n",
      "        [-1.3550, -1.2312, -1.4085, -1.5817],\n",
      "        [-1.3550, -1.2314, -1.4085, -1.5815],\n",
      "        [-1.3550, -1.2312, -1.4085, -1.5818],\n",
      "        [-1.3550, -1.2314, -1.4085, -1.5816],\n",
      "        [-1.3549, -1.2311, -1.4087, -1.5819],\n",
      "        [-1.3551, -1.2317, -1.4086, -1.5809],\n",
      "        [-1.3549, -1.2311, -1.4087, -1.5818],\n",
      "        [-1.3550, -1.2314, -1.4086, -1.5814],\n",
      "        [-1.3549, -1.2311, -1.4087, -1.5818],\n",
      "        [-1.3550, -1.2313, -1.4086, -1.5815],\n",
      "        [-1.3550, -1.2312, -1.4087, -1.5816],\n",
      "        [-1.3549, -1.2311, -1.4087, -1.5818],\n",
      "        [-1.3550, -1.2311, -1.4086, -1.5818],\n",
      "        [-1.3550, -1.2311, -1.4086, -1.5818],\n",
      "        [-1.3550, -1.2310, -1.4086, -1.5819],\n",
      "        [-1.3550, -1.2311, -1.4086, -1.5819],\n",
      "        [-1.3550, -1.2315, -1.4085, -1.5813],\n",
      "        [-1.3550, -1.2315, -1.4085, -1.5813],\n",
      "        [-1.3550, -1.2311, -1.4086, -1.5818],\n",
      "        [-1.3550, -1.2311, -1.4087, -1.5817],\n",
      "        [-1.3549, -1.2309, -1.4087, -1.5821],\n",
      "        [-1.3550, -1.2312, -1.4086, -1.5818],\n",
      "        [-1.3549, -1.2310, -1.4086, -1.5821],\n",
      "        [-1.3549, -1.2311, -1.4086, -1.5819],\n",
      "        [-1.3549, -1.2311, -1.4087, -1.5819],\n",
      "        [-1.3550, -1.2314, -1.4086, -1.5814],\n",
      "        [-1.3550, -1.2312, -1.4087, -1.5816],\n",
      "        [-1.3550, -1.2314, -1.4086, -1.5815],\n",
      "        [-1.3550, -1.2312, -1.4087, -1.5817],\n",
      "        [-1.3550, -1.2313, -1.4086, -1.5815],\n",
      "        [-1.3550, -1.2311, -1.4086, -1.5817],\n",
      "        [-1.3550, -1.2312, -1.4086, -1.5817]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([ 3.7581e-03,  3.6977e-03,  2.2759e-03,  8.0341e-04,  4.2573e-03,\n",
      "         3.0154e-03,  2.2220e-03,  1.1657e-03,  3.4698e-03,  2.3174e-03,\n",
      "         2.0095e-03,  9.1484e-04,  4.1091e-03,  2.9351e-03,  1.0895e-03,\n",
      "         9.1576e-04,  4.2952e-03,  3.2942e-03,  2.2195e-03,  1.3514e-03,\n",
      "         3.9470e-03,  3.4055e-03,  2.1077e-03,  9.5432e-04,  3.7711e-03,\n",
      "         3.4070e-03,  2.0325e-03,  6.3350e-04,  4.6783e-03,  3.3756e-03,\n",
      "         2.2278e-03,  7.4624e-04,  5.1599e-03,  3.1281e-03,  2.4845e-03,\n",
      "         1.2935e-03,  4.8977e-03,  2.3725e-03,  2.2379e-03,  4.8812e-04,\n",
      "         4.2439e-03,  2.9787e-03,  2.0331e-03,  1.3521e-03,  4.0931e-03,\n",
      "         3.4018e-03,  2.2355e-03,  1.7927e-03,  9.5211e-02,  9.5211e-02,\n",
      "         2.0772e-03,  8.9520e-04,  4.0658e-03,  2.9544e-03,  2.1881e-03,\n",
      "         8.6328e-04, -1.3761e+00, -1.3912e+00, -1.5791e+00,  1.2390e-03,\n",
      "         1.0910e-01,  1.0475e-01,  2.3062e-03,  1.2183e-03],\n",
      "       grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(0.0594, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2579, 0.2919, 0.2445, 0.2057],\n",
      "        [0.2579, 0.2919, 0.2445, 0.2056],\n",
      "        [0.2580, 0.2919, 0.2445, 0.2056],\n",
      "        [0.2579, 0.2919, 0.2445, 0.2057],\n",
      "        [0.2580, 0.2919, 0.2445, 0.2056],\n",
      "        [0.2579, 0.2919, 0.2445, 0.2056],\n",
      "        [0.2580, 0.2920, 0.2445, 0.2056],\n",
      "        [0.2579, 0.2919, 0.2445, 0.2057],\n",
      "        [0.2579, 0.2920, 0.2445, 0.2056],\n",
      "        [0.2579, 0.2919, 0.2445, 0.2057],\n",
      "        [0.2579, 0.2920, 0.2445, 0.2056],\n",
      "        [0.2579, 0.2919, 0.2445, 0.2056],\n",
      "        [0.2579, 0.2919, 0.2445, 0.2056],\n",
      "        [0.2580, 0.2920, 0.2445, 0.2056],\n",
      "        [0.2579, 0.2918, 0.2445, 0.2058],\n",
      "        [0.2579, 0.2919, 0.2445, 0.2056],\n",
      "        [0.2579, 0.2919, 0.2445, 0.2057],\n",
      "        [0.2580, 0.2920, 0.2445, 0.2056],\n",
      "        [0.2579, 0.2919, 0.2445, 0.2057],\n",
      "        [0.2579, 0.2919, 0.2445, 0.2056],\n",
      "        [0.2580, 0.2919, 0.2445, 0.2057],\n",
      "        [0.2579, 0.2920, 0.2445, 0.2056],\n",
      "        [0.2580, 0.2920, 0.2445, 0.2056],\n",
      "        [0.2580, 0.2919, 0.2445, 0.2056],\n",
      "        [0.2579, 0.2920, 0.2445, 0.2056],\n",
      "        [0.2579, 0.2920, 0.2445, 0.2056],\n",
      "        [0.2579, 0.2920, 0.2445, 0.2056],\n",
      "        [0.2580, 0.2919, 0.2445, 0.2057],\n",
      "        [0.2580, 0.2919, 0.2445, 0.2056],\n",
      "        [0.2580, 0.2920, 0.2445, 0.2056],\n",
      "        [0.2580, 0.2920, 0.2445, 0.2056],\n",
      "        [0.2580, 0.2919, 0.2445, 0.2057],\n",
      "        [0.2579, 0.2919, 0.2445, 0.2056],\n",
      "        [0.2579, 0.2919, 0.2445, 0.2057],\n",
      "        [0.2580, 0.2919, 0.2445, 0.2056],\n",
      "        [0.2580, 0.2919, 0.2445, 0.2057],\n",
      "        [0.2580, 0.2920, 0.2445, 0.2056],\n",
      "        [0.2579, 0.2918, 0.2445, 0.2058],\n",
      "        [0.2580, 0.2920, 0.2445, 0.2056],\n",
      "        [0.2579, 0.2919, 0.2445, 0.2057],\n",
      "        [0.2580, 0.2920, 0.2445, 0.2056],\n",
      "        [0.2579, 0.2919, 0.2445, 0.2057],\n",
      "        [0.2580, 0.2919, 0.2445, 0.2056],\n",
      "        [0.2580, 0.2920, 0.2445, 0.2056],\n",
      "        [0.2579, 0.2920, 0.2445, 0.2056],\n",
      "        [0.2580, 0.2920, 0.2445, 0.2056],\n",
      "        [0.2579, 0.2920, 0.2445, 0.2056],\n",
      "        [0.2580, 0.2920, 0.2445, 0.2056],\n",
      "        [0.2580, 0.2918, 0.2445, 0.2057],\n",
      "        [0.2580, 0.2918, 0.2445, 0.2057],\n",
      "        [0.2579, 0.2920, 0.2445, 0.2056],\n",
      "        [0.2579, 0.2920, 0.2445, 0.2056],\n",
      "        [0.2580, 0.2920, 0.2445, 0.2055],\n",
      "        [0.2580, 0.2920, 0.2445, 0.2056],\n",
      "        [0.2580, 0.2920, 0.2445, 0.2055],\n",
      "        [0.2580, 0.2920, 0.2445, 0.2056],\n",
      "        [0.2580, 0.2920, 0.2445, 0.2056],\n",
      "        [0.2579, 0.2919, 0.2445, 0.2057],\n",
      "        [0.2580, 0.2919, 0.2445, 0.2056],\n",
      "        [0.2580, 0.2919, 0.2445, 0.2057],\n",
      "        [0.2580, 0.2919, 0.2445, 0.2056],\n",
      "        [0.2579, 0.2919, 0.2445, 0.2057],\n",
      "        [0.2579, 0.2920, 0.2445, 0.2056],\n",
      "        [0.2580, 0.2919, 0.2445, 0.2056]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3786, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(0.0917, grad_fn=<AddBackward0>)\n",
      "batching here\n",
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(1.1675e-05, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.3661, -1.2576, -1.3842, -1.5604],\n",
      "        [-1.3661, -1.2576, -1.3842, -1.5605],\n",
      "        [-1.3661, -1.2576, -1.3842, -1.5605],\n",
      "        [-1.3661, -1.2576, -1.3842, -1.5604],\n",
      "        [-1.3661, -1.2575, -1.3842, -1.5605],\n",
      "        [-1.3661, -1.2575, -1.3843, -1.5605],\n",
      "        [-1.3661, -1.2576, -1.3843, -1.5603],\n",
      "        [-1.3661, -1.2574, -1.3842, -1.5606],\n",
      "        [-1.3661, -1.2574, -1.3842, -1.5607],\n",
      "        [-1.3661, -1.2575, -1.3842, -1.5605],\n",
      "        [-1.3661, -1.2574, -1.3842, -1.5607],\n",
      "        [-1.3661, -1.2574, -1.3843, -1.5606],\n",
      "        [-1.3661, -1.2575, -1.3842, -1.5607],\n",
      "        [-1.3660, -1.2575, -1.3842, -1.5607],\n",
      "        [-1.3660, -1.2575, -1.3842, -1.5607],\n",
      "        [-1.3661, -1.2577, -1.3842, -1.5604],\n",
      "        [-1.3661, -1.2577, -1.3842, -1.5603],\n",
      "        [-1.3660, -1.2576, -1.3842, -1.5604],\n",
      "        [-1.3661, -1.2577, -1.3842, -1.5603],\n",
      "        [-1.3661, -1.2577, -1.3842, -1.5603],\n",
      "        [-1.3660, -1.2576, -1.3842, -1.5605],\n",
      "        [-1.3661, -1.2576, -1.3842, -1.5605],\n",
      "        [-1.3661, -1.2576, -1.3842, -1.5605],\n",
      "        [-1.3661, -1.2576, -1.3842, -1.5605],\n",
      "        [-1.3661, -1.2574, -1.3842, -1.5606],\n",
      "        [-1.3661, -1.2575, -1.3842, -1.5606],\n",
      "        [-1.3661, -1.2576, -1.3842, -1.5603],\n",
      "        [-1.3661, -1.2576, -1.3843, -1.5603],\n",
      "        [-1.3660, -1.2576, -1.3842, -1.5606],\n",
      "        [-1.3661, -1.2576, -1.3842, -1.5606],\n",
      "        [-1.3660, -1.2576, -1.3842, -1.5606],\n",
      "        [-1.3661, -1.2577, -1.3842, -1.5604],\n",
      "        [-1.3661, -1.2575, -1.3842, -1.5606],\n",
      "        [-1.3661, -1.2574, -1.3842, -1.5608],\n",
      "        [-1.3661, -1.2572, -1.3842, -1.5610],\n",
      "        [-1.3660, -1.2572, -1.3842, -1.5611],\n",
      "        [-1.3660, -1.2576, -1.3842, -1.5606],\n",
      "        [-1.3661, -1.2577, -1.3842, -1.5603],\n",
      "        [-1.3661, -1.2576, -1.3842, -1.5605],\n",
      "        [-1.3660, -1.2576, -1.3842, -1.5605],\n",
      "        [-1.3661, -1.2575, -1.3843, -1.5605],\n",
      "        [-1.3661, -1.2578, -1.3843, -1.5602],\n",
      "        [-1.3661, -1.2577, -1.3842, -1.5604],\n",
      "        [-1.3661, -1.2576, -1.3842, -1.5605],\n",
      "        [-1.3661, -1.2575, -1.3842, -1.5606],\n",
      "        [-1.3661, -1.2574, -1.3842, -1.5607],\n",
      "        [-1.3661, -1.2575, -1.3842, -1.5605],\n",
      "        [-1.3661, -1.2574, -1.3842, -1.5608],\n",
      "        [-1.3661, -1.2575, -1.3842, -1.5606],\n",
      "        [-1.3661, -1.2576, -1.3842, -1.5604],\n",
      "        [-1.3661, -1.2578, -1.3842, -1.5602],\n",
      "        [-1.3661, -1.2576, -1.3842, -1.5604],\n",
      "        [-1.3660, -1.2575, -1.3842, -1.5607],\n",
      "        [-1.3660, -1.2576, -1.3843, -1.5605],\n",
      "        [-1.3661, -1.2576, -1.3842, -1.5605],\n",
      "        [-1.3660, -1.2574, -1.3843, -1.5607],\n",
      "        [-1.3661, -1.2578, -1.3842, -1.5603],\n",
      "        [-1.3661, -1.2576, -1.3842, -1.5604],\n",
      "        [-1.3661, -1.2577, -1.3842, -1.5603],\n",
      "        [-1.3661, -1.2577, -1.3842, -1.5603],\n",
      "        [-1.3661, -1.2576, -1.3842, -1.5604],\n",
      "        [-1.3660, -1.2575, -1.3842, -1.5606],\n",
      "        [-1.3661, -1.2576, -1.3843, -1.5604],\n",
      "        [-1.3661, -1.2576, -1.3842, -1.5604]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([0.0067, 0.0058, 0.0040, 0.0014, 0.0064, 0.0050, 0.0033, 0.0020, 0.0065,\n",
      "        0.0047, 0.0035, 0.0017, 0.0082, 0.0057, 0.0039, 0.0022, 0.0060, 0.0052,\n",
      "        0.0034, 0.0016, 0.0071, 0.0054, 0.0041, 0.0019, 0.0067, 0.0051, 0.0030,\n",
      "        0.0013, 0.0063, 0.0060, 0.0037, 0.0015, 0.0055, 0.0045, 0.0031, 0.0019,\n",
      "        0.0071, 0.0055, 0.0035, 0.0020, 0.0069, 0.0043, 0.0033, 0.0020, 0.0066,\n",
      "        0.0052, 0.0031, 0.0018, 0.0070, 0.0051, 0.0031, 0.0016, 0.0069, 0.0048,\n",
      "        0.0031, 0.0024, 0.0076, 0.0047, 0.0031, 0.0022, 0.0080, 0.0049, 0.0041,\n",
      "        0.0023], grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(-0.0043, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2551, 0.2843, 0.2505, 0.2100],\n",
      "        [0.2551, 0.2843, 0.2505, 0.2100],\n",
      "        [0.2551, 0.2843, 0.2505, 0.2100],\n",
      "        [0.2551, 0.2843, 0.2505, 0.2101],\n",
      "        [0.2551, 0.2844, 0.2505, 0.2100],\n",
      "        [0.2551, 0.2844, 0.2505, 0.2100],\n",
      "        [0.2551, 0.2843, 0.2505, 0.2101],\n",
      "        [0.2551, 0.2844, 0.2505, 0.2100],\n",
      "        [0.2551, 0.2844, 0.2505, 0.2100],\n",
      "        [0.2551, 0.2844, 0.2505, 0.2100],\n",
      "        [0.2551, 0.2844, 0.2505, 0.2100],\n",
      "        [0.2551, 0.2844, 0.2505, 0.2100],\n",
      "        [0.2551, 0.2844, 0.2505, 0.2100],\n",
      "        [0.2551, 0.2844, 0.2505, 0.2100],\n",
      "        [0.2551, 0.2844, 0.2505, 0.2100],\n",
      "        [0.2551, 0.2843, 0.2505, 0.2101],\n",
      "        [0.2551, 0.2843, 0.2505, 0.2101],\n",
      "        [0.2551, 0.2843, 0.2505, 0.2100],\n",
      "        [0.2551, 0.2843, 0.2505, 0.2101],\n",
      "        [0.2551, 0.2843, 0.2505, 0.2101],\n",
      "        [0.2551, 0.2843, 0.2505, 0.2100],\n",
      "        [0.2551, 0.2843, 0.2505, 0.2100],\n",
      "        [0.2551, 0.2843, 0.2505, 0.2100],\n",
      "        [0.2551, 0.2843, 0.2505, 0.2100],\n",
      "        [0.2551, 0.2844, 0.2505, 0.2100],\n",
      "        [0.2551, 0.2844, 0.2505, 0.2100],\n",
      "        [0.2551, 0.2843, 0.2505, 0.2101],\n",
      "        [0.2551, 0.2843, 0.2505, 0.2101],\n",
      "        [0.2551, 0.2843, 0.2505, 0.2100],\n",
      "        [0.2551, 0.2843, 0.2505, 0.2100],\n",
      "        [0.2551, 0.2843, 0.2505, 0.2100],\n",
      "        [0.2551, 0.2843, 0.2505, 0.2101],\n",
      "        [0.2551, 0.2844, 0.2505, 0.2100],\n",
      "        [0.2551, 0.2844, 0.2505, 0.2100],\n",
      "        [0.2551, 0.2844, 0.2505, 0.2099],\n",
      "        [0.2551, 0.2844, 0.2505, 0.2099],\n",
      "        [0.2551, 0.2843, 0.2505, 0.2100],\n",
      "        [0.2551, 0.2843, 0.2505, 0.2101],\n",
      "        [0.2551, 0.2843, 0.2505, 0.2100],\n",
      "        [0.2551, 0.2843, 0.2505, 0.2100],\n",
      "        [0.2551, 0.2844, 0.2505, 0.2100],\n",
      "        [0.2551, 0.2843, 0.2505, 0.2101],\n",
      "        [0.2551, 0.2843, 0.2505, 0.2101],\n",
      "        [0.2551, 0.2843, 0.2505, 0.2100],\n",
      "        [0.2551, 0.2844, 0.2505, 0.2100],\n",
      "        [0.2551, 0.2844, 0.2505, 0.2100],\n",
      "        [0.2551, 0.2844, 0.2505, 0.2100],\n",
      "        [0.2551, 0.2844, 0.2505, 0.2100],\n",
      "        [0.2551, 0.2844, 0.2505, 0.2100],\n",
      "        [0.2551, 0.2843, 0.2505, 0.2101],\n",
      "        [0.2551, 0.2843, 0.2505, 0.2101],\n",
      "        [0.2551, 0.2843, 0.2505, 0.2100],\n",
      "        [0.2551, 0.2844, 0.2505, 0.2100],\n",
      "        [0.2551, 0.2843, 0.2505, 0.2100],\n",
      "        [0.2551, 0.2843, 0.2505, 0.2100],\n",
      "        [0.2551, 0.2844, 0.2505, 0.2100],\n",
      "        [0.2551, 0.2843, 0.2505, 0.2101],\n",
      "        [0.2551, 0.2843, 0.2505, 0.2100],\n",
      "        [0.2551, 0.2843, 0.2505, 0.2101],\n",
      "        [0.2551, 0.2843, 0.2505, 0.2101],\n",
      "        [0.2551, 0.2843, 0.2505, 0.2100],\n",
      "        [0.2551, 0.2844, 0.2505, 0.2100],\n",
      "        [0.2551, 0.2843, 0.2505, 0.2101],\n",
      "        [0.2551, 0.2843, 0.2505, 0.2100]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3806, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(-0.0181, grad_fn=<AddBackward0>)\n",
      "batching here\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74: done 2 episodes, mean_reward=0.00, best_reward=2.00, speed=226.68\n",
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(0.0013, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.3665, -1.2608, -1.3841, -1.5557],\n",
      "        [-1.3665, -1.2609, -1.3841, -1.5557],\n",
      "        [-1.3664, -1.2609, -1.3842, -1.5557],\n",
      "        [-1.3664, -1.2608, -1.3842, -1.5558],\n",
      "        [-1.3666, -1.2608, -1.3842, -1.5556],\n",
      "        [-1.3666, -1.2607, -1.3842, -1.5558],\n",
      "        [-1.3666, -1.2608, -1.3842, -1.5557],\n",
      "        [-1.3665, -1.2607, -1.3842, -1.5559],\n",
      "        [-1.3666, -1.2607, -1.3841, -1.5558],\n",
      "        [-1.3665, -1.2607, -1.3842, -1.5558],\n",
      "        [-1.3665, -1.2606, -1.3842, -1.5558],\n",
      "        [-1.3666, -1.2607, -1.3842, -1.5557],\n",
      "        [-1.3665, -1.2610, -1.3842, -1.5554],\n",
      "        [-1.3665, -1.2609, -1.3842, -1.5556],\n",
      "        [-1.3665, -1.2609, -1.3842, -1.5556],\n",
      "        [-1.3665, -1.2609, -1.3842, -1.5555],\n",
      "        [-1.3665, -1.2609, -1.3842, -1.5556],\n",
      "        [-1.3665, -1.2609, -1.3842, -1.5557],\n",
      "        [-1.3665, -1.2609, -1.3842, -1.5555],\n",
      "        [-1.3665, -1.2609, -1.3842, -1.5556],\n",
      "        [-1.3665, -1.2609, -1.3842, -1.5556],\n",
      "        [-1.3665, -1.2609, -1.3842, -1.5556],\n",
      "        [-1.3665, -1.2609, -1.3842, -1.5556],\n",
      "        [-1.3665, -1.2609, -1.3841, -1.5555],\n",
      "        [-1.3665, -1.2607, -1.3842, -1.5558],\n",
      "        [-1.3665, -1.2608, -1.3841, -1.5558],\n",
      "        [-1.3665, -1.2606, -1.3842, -1.5559],\n",
      "        [-1.3665, -1.2607, -1.3842, -1.5557],\n",
      "        [-1.3665, -1.2609, -1.3841, -1.5557],\n",
      "        [-1.3665, -1.2608, -1.3841, -1.5558],\n",
      "        [-1.3665, -1.2609, -1.3841, -1.5557],\n",
      "        [-1.3665, -1.2608, -1.3841, -1.5557],\n",
      "        [-1.3665, -1.2604, -1.3841, -1.5563],\n",
      "        [-1.3665, -1.2608, -1.3841, -1.5557],\n",
      "        [-1.3665, -1.2608, -1.3842, -1.5557],\n",
      "        [-1.3665, -1.2604, -1.3842, -1.5563],\n",
      "        [-1.3665, -1.2608, -1.3842, -1.5557],\n",
      "        [-1.3665, -1.2609, -1.3842, -1.5556],\n",
      "        [-1.3665, -1.2609, -1.3841, -1.5556],\n",
      "        [-1.3665, -1.2608, -1.3841, -1.5557],\n",
      "        [-1.3665, -1.2609, -1.3842, -1.5556],\n",
      "        [-1.3665, -1.2609, -1.3842, -1.5556],\n",
      "        [-1.3665, -1.2609, -1.3842, -1.5555],\n",
      "        [-1.3665, -1.2609, -1.3842, -1.5556],\n",
      "        [-1.3664, -1.2607, -1.3841, -1.5559],\n",
      "        [-1.3665, -1.2606, -1.3841, -1.5561],\n",
      "        [-1.3664, -1.2607, -1.3842, -1.5559],\n",
      "        [-1.3665, -1.2607, -1.3842, -1.5559],\n",
      "        [-1.3665, -1.2609, -1.3841, -1.5557],\n",
      "        [-1.3665, -1.2610, -1.3841, -1.5554],\n",
      "        [-1.3665, -1.2610, -1.3842, -1.5554],\n",
      "        [-1.3665, -1.2608, -1.3842, -1.5557],\n",
      "        [-1.3664, -1.2609, -1.3842, -1.5556],\n",
      "        [-1.3664, -1.2607, -1.3842, -1.5558],\n",
      "        [-1.3665, -1.2611, -1.3841, -1.5554],\n",
      "        [-1.3665, -1.2608, -1.3842, -1.5558],\n",
      "        [-1.3665, -1.2608, -1.3842, -1.5557],\n",
      "        [-1.3665, -1.2608, -1.3842, -1.5557],\n",
      "        [-1.3666, -1.2608, -1.3841, -1.5557],\n",
      "        [-1.3665, -1.2610, -1.3842, -1.5554],\n",
      "        [-1.3665, -1.2608, -1.3842, -1.5556],\n",
      "        [-1.3665, -1.2609, -1.3841, -1.5556],\n",
      "        [-1.3665, -1.2607, -1.3842, -1.5559],\n",
      "        [-1.3665, -1.2608, -1.3842, -1.5557]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([0.1630, 0.1856, 0.1650, 0.0018, 0.0063, 0.0049, 0.0036, 0.0022, 0.0065,\n",
      "        0.0046, 0.0033, 0.0016, 0.0060, 0.0047, 0.0029, 0.0014, 0.0065, 0.0049,\n",
      "        0.0032, 0.0017, 0.1649, 0.1501, 0.1852, 0.0016, 0.0060, 0.0043, 0.0033,\n",
      "        0.0014, 0.0064, 0.0050, 0.0033, 0.0016, 0.0065, 0.0041, 0.0025, 0.0019,\n",
      "        0.0071, 0.0046, 0.0027, 0.0015, 0.0058, 0.0046, 0.0030, 0.0015, 0.0056,\n",
      "        0.0048, 0.0030, 0.0012, 0.0066, 0.0043, 0.0031, 0.0017, 0.0065, 0.0051,\n",
      "        0.0033, 0.0018, 0.0059, 0.0046, 0.0034, 0.0012, 0.0069, 0.0047, 0.0034,\n",
      "        0.0018], grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(-0.0193, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2550, 0.2834, 0.2505, 0.2110],\n",
      "        [0.2550, 0.2834, 0.2505, 0.2110],\n",
      "        [0.2550, 0.2834, 0.2505, 0.2110],\n",
      "        [0.2550, 0.2834, 0.2505, 0.2110],\n",
      "        [0.2550, 0.2834, 0.2505, 0.2111],\n",
      "        [0.2550, 0.2835, 0.2505, 0.2110],\n",
      "        [0.2550, 0.2834, 0.2505, 0.2111],\n",
      "        [0.2550, 0.2835, 0.2505, 0.2110],\n",
      "        [0.2550, 0.2835, 0.2505, 0.2110],\n",
      "        [0.2550, 0.2835, 0.2505, 0.2110],\n",
      "        [0.2550, 0.2835, 0.2505, 0.2110],\n",
      "        [0.2550, 0.2834, 0.2505, 0.2110],\n",
      "        [0.2550, 0.2834, 0.2505, 0.2111],\n",
      "        [0.2550, 0.2834, 0.2505, 0.2111],\n",
      "        [0.2550, 0.2834, 0.2505, 0.2111],\n",
      "        [0.2550, 0.2834, 0.2505, 0.2111],\n",
      "        [0.2550, 0.2834, 0.2505, 0.2111],\n",
      "        [0.2550, 0.2834, 0.2505, 0.2111],\n",
      "        [0.2550, 0.2834, 0.2505, 0.2111],\n",
      "        [0.2550, 0.2834, 0.2505, 0.2111],\n",
      "        [0.2550, 0.2834, 0.2505, 0.2111],\n",
      "        [0.2550, 0.2834, 0.2505, 0.2111],\n",
      "        [0.2550, 0.2834, 0.2505, 0.2111],\n",
      "        [0.2550, 0.2834, 0.2505, 0.2111],\n",
      "        [0.2550, 0.2835, 0.2505, 0.2110],\n",
      "        [0.2550, 0.2834, 0.2505, 0.2110],\n",
      "        [0.2550, 0.2835, 0.2505, 0.2110],\n",
      "        [0.2550, 0.2835, 0.2505, 0.2110],\n",
      "        [0.2550, 0.2834, 0.2505, 0.2110],\n",
      "        [0.2550, 0.2834, 0.2505, 0.2110],\n",
      "        [0.2550, 0.2834, 0.2505, 0.2110],\n",
      "        [0.2550, 0.2834, 0.2505, 0.2110],\n",
      "        [0.2550, 0.2835, 0.2505, 0.2109],\n",
      "        [0.2550, 0.2834, 0.2505, 0.2110],\n",
      "        [0.2550, 0.2834, 0.2505, 0.2110],\n",
      "        [0.2550, 0.2836, 0.2505, 0.2109],\n",
      "        [0.2550, 0.2834, 0.2505, 0.2110],\n",
      "        [0.2550, 0.2834, 0.2505, 0.2111],\n",
      "        [0.2550, 0.2834, 0.2505, 0.2111],\n",
      "        [0.2550, 0.2834, 0.2505, 0.2111],\n",
      "        [0.2550, 0.2834, 0.2505, 0.2111],\n",
      "        [0.2550, 0.2834, 0.2505, 0.2111],\n",
      "        [0.2550, 0.2834, 0.2505, 0.2111],\n",
      "        [0.2550, 0.2834, 0.2505, 0.2111],\n",
      "        [0.2550, 0.2834, 0.2505, 0.2110],\n",
      "        [0.2550, 0.2835, 0.2505, 0.2110],\n",
      "        [0.2550, 0.2835, 0.2505, 0.2110],\n",
      "        [0.2550, 0.2835, 0.2505, 0.2110],\n",
      "        [0.2550, 0.2834, 0.2505, 0.2110],\n",
      "        [0.2550, 0.2834, 0.2505, 0.2111],\n",
      "        [0.2550, 0.2834, 0.2505, 0.2111],\n",
      "        [0.2550, 0.2834, 0.2505, 0.2111],\n",
      "        [0.2550, 0.2834, 0.2505, 0.2111],\n",
      "        [0.2550, 0.2834, 0.2505, 0.2110],\n",
      "        [0.2550, 0.2834, 0.2505, 0.2111],\n",
      "        [0.2550, 0.2834, 0.2505, 0.2110],\n",
      "        [0.2550, 0.2834, 0.2505, 0.2110],\n",
      "        [0.2550, 0.2834, 0.2505, 0.2110],\n",
      "        [0.2550, 0.2834, 0.2505, 0.2110],\n",
      "        [0.2550, 0.2834, 0.2505, 0.2111],\n",
      "        [0.2550, 0.2834, 0.2505, 0.2111],\n",
      "        [0.2550, 0.2834, 0.2505, 0.2111],\n",
      "        [0.2550, 0.2834, 0.2505, 0.2110],\n",
      "        [0.2550, 0.2834, 0.2505, 0.2111]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3809, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(-0.0318, grad_fn=<AddBackward0>)\n",
      "batching here\n",
      "75: done 3 episodes, mean_reward=0.00, best_reward=2.00, speed=227.21\n",
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(0.0615, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.3668, -1.2669, -1.3859, -1.5452],\n",
      "        [-1.3668, -1.2669, -1.3859, -1.5452],\n",
      "        [-1.3668, -1.2667, -1.3859, -1.5454],\n",
      "        [-1.3669, -1.2668, -1.3859, -1.5452],\n",
      "        [-1.3669, -1.2668, -1.3859, -1.5452],\n",
      "        [-1.3669, -1.2668, -1.3859, -1.5451],\n",
      "        [-1.3669, -1.2667, -1.3859, -1.5453],\n",
      "        [-1.3669, -1.2667, -1.3859, -1.5454],\n",
      "        [-1.3669, -1.2668, -1.3859, -1.5452],\n",
      "        [-1.3669, -1.2668, -1.3859, -1.5452],\n",
      "        [-1.3669, -1.2668, -1.3859, -1.5451],\n",
      "        [-1.3669, -1.2668, -1.3859, -1.5451],\n",
      "        [-1.3669, -1.2668, -1.3859, -1.5452],\n",
      "        [-1.3668, -1.2667, -1.3859, -1.5455],\n",
      "        [-1.3669, -1.2669, -1.3859, -1.5451],\n",
      "        [-1.3668, -1.2667, -1.3859, -1.5454],\n",
      "        [-1.3669, -1.2670, -1.3859, -1.5450],\n",
      "        [-1.3669, -1.2670, -1.3859, -1.5450],\n",
      "        [-1.3669, -1.2670, -1.3859, -1.5450],\n",
      "        [-1.3669, -1.2670, -1.3859, -1.5450],\n",
      "        [-1.3669, -1.2670, -1.3858, -1.5451],\n",
      "        [-1.3668, -1.2670, -1.3859, -1.5450],\n",
      "        [-1.3668, -1.2669, -1.3859, -1.5452],\n",
      "        [-1.3669, -1.2670, -1.3859, -1.5450],\n",
      "        [-1.3669, -1.2668, -1.3859, -1.5452],\n",
      "        [-1.3669, -1.2668, -1.3859, -1.5453],\n",
      "        [-1.3669, -1.2668, -1.3859, -1.5453],\n",
      "        [-1.3669, -1.2668, -1.3859, -1.5452],\n",
      "        [-1.3668, -1.2669, -1.3858, -1.5452],\n",
      "        [-1.3668, -1.2669, -1.3858, -1.5452],\n",
      "        [-1.3668, -1.2670, -1.3858, -1.5451],\n",
      "        [-1.3668, -1.2670, -1.3859, -1.5451],\n",
      "        [-1.3668, -1.2664, -1.3859, -1.5458],\n",
      "        [-1.3669, -1.2666, -1.3859, -1.5455],\n",
      "        [-1.3668, -1.2667, -1.3859, -1.5454],\n",
      "        [-1.3669, -1.2666, -1.3859, -1.5456],\n",
      "        [-1.3669, -1.2668, -1.3859, -1.5452],\n",
      "        [-1.3668, -1.2669, -1.3859, -1.5452],\n",
      "        [-1.3669, -1.2669, -1.3859, -1.5452],\n",
      "        [-1.3669, -1.2669, -1.3859, -1.5451],\n",
      "        [-1.3668, -1.2669, -1.3859, -1.5452],\n",
      "        [-1.3669, -1.2670, -1.3859, -1.5450],\n",
      "        [-1.3668, -1.2669, -1.3859, -1.5452],\n",
      "        [-1.3669, -1.2669, -1.3859, -1.5452],\n",
      "        [-1.3669, -1.2666, -1.3858, -1.5455],\n",
      "        [-1.3669, -1.2668, -1.3859, -1.5453],\n",
      "        [-1.3669, -1.2669, -1.3859, -1.5450],\n",
      "        [-1.3669, -1.2669, -1.3859, -1.5451],\n",
      "        [-1.3668, -1.2670, -1.3858, -1.5451],\n",
      "        [-1.3669, -1.2668, -1.3859, -1.5452],\n",
      "        [-1.3669, -1.2669, -1.3859, -1.5451],\n",
      "        [-1.3669, -1.2669, -1.3859, -1.5452],\n",
      "        [-1.3669, -1.2668, -1.3859, -1.5451],\n",
      "        [-1.3668, -1.2666, -1.3859, -1.5456],\n",
      "        [-1.3668, -1.2668, -1.3859, -1.5453],\n",
      "        [-1.3669, -1.2668, -1.3859, -1.5452],\n",
      "        [-1.3668, -1.2668, -1.3858, -1.5453],\n",
      "        [-1.3668, -1.2671, -1.3859, -1.5450],\n",
      "        [-1.3669, -1.2672, -1.3859, -1.5448],\n",
      "        [-1.3669, -1.2670, -1.3859, -1.5450],\n",
      "        [-1.3668, -1.2668, -1.3859, -1.5453],\n",
      "        [-1.3668, -1.2669, -1.3859, -1.5452],\n",
      "        [-1.3668, -1.2669, -1.3859, -1.5452],\n",
      "        [-1.3668, -1.2668, -1.3859, -1.5452]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([ 4.8651e-03,  4.1071e-03,  2.5483e-03,  1.7003e-03,  4.9464e-03,\n",
      "         3.5660e-03,  2.6016e-03,  1.4745e-03,  5.4271e-03,  4.0051e-03,\n",
      "         2.6031e-03,  1.2239e-03, -1.2243e+00, -1.2376e+00, -1.5268e+00,\n",
      "        -1.2655e+00,  1.4936e-01,  1.3397e-01,  1.3397e-01,  1.2248e-01,\n",
      "         5.2468e-03,  4.3723e-03,  2.5922e-03,  1.4381e-03,  5.4085e-03,\n",
      "         4.5780e-03,  2.7648e-03,  1.3134e-03,  1.3430e-01,  1.2265e-01,\n",
      "         1.2257e-01,  1.4355e-03,  5.5015e-03,  3.4920e-03,  2.7294e-03,\n",
      "         1.5882e-03,  1.3254e-01,  4.0499e-03,  2.7541e-03,  1.2666e-03,\n",
      "         5.3253e-03,  4.3038e-03,  2.5983e-03,  1.4362e-03,  5.8608e-03,\n",
      "         4.3931e-03,  2.4797e-03,  1.4830e-03,  5.7806e-03,  4.0985e-03,\n",
      "         2.3604e-03,  1.2975e-03,  5.5279e-03,  5.1625e-03,  2.8970e-03,\n",
      "         1.3675e-03,  5.2605e-03,  3.3470e-03,  1.7311e-03,  7.7981e-04,\n",
      "         6.0555e-03,  3.9929e-03,  2.7316e-03,  1.4470e-03],\n",
      "       grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(0.0630, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2549, 0.2817, 0.2501, 0.2133],\n",
      "        [0.2549, 0.2817, 0.2501, 0.2133],\n",
      "        [0.2549, 0.2817, 0.2501, 0.2132],\n",
      "        [0.2549, 0.2817, 0.2501, 0.2133],\n",
      "        [0.2549, 0.2817, 0.2501, 0.2133],\n",
      "        [0.2549, 0.2817, 0.2501, 0.2133],\n",
      "        [0.2549, 0.2818, 0.2501, 0.2133],\n",
      "        [0.2549, 0.2818, 0.2501, 0.2132],\n",
      "        [0.2549, 0.2817, 0.2501, 0.2133],\n",
      "        [0.2549, 0.2817, 0.2501, 0.2133],\n",
      "        [0.2549, 0.2817, 0.2501, 0.2133],\n",
      "        [0.2549, 0.2817, 0.2501, 0.2133],\n",
      "        [0.2549, 0.2817, 0.2501, 0.2133],\n",
      "        [0.2549, 0.2818, 0.2501, 0.2132],\n",
      "        [0.2549, 0.2817, 0.2501, 0.2133],\n",
      "        [0.2549, 0.2817, 0.2501, 0.2132],\n",
      "        [0.2549, 0.2817, 0.2501, 0.2133],\n",
      "        [0.2549, 0.2817, 0.2501, 0.2133],\n",
      "        [0.2549, 0.2817, 0.2501, 0.2133],\n",
      "        [0.2549, 0.2817, 0.2501, 0.2133],\n",
      "        [0.2549, 0.2817, 0.2501, 0.2133],\n",
      "        [0.2549, 0.2817, 0.2501, 0.2133],\n",
      "        [0.2549, 0.2817, 0.2501, 0.2133],\n",
      "        [0.2549, 0.2817, 0.2501, 0.2133],\n",
      "        [0.2549, 0.2817, 0.2501, 0.2133],\n",
      "        [0.2549, 0.2817, 0.2501, 0.2133],\n",
      "        [0.2549, 0.2817, 0.2501, 0.2133],\n",
      "        [0.2549, 0.2817, 0.2501, 0.2133],\n",
      "        [0.2549, 0.2817, 0.2501, 0.2133],\n",
      "        [0.2549, 0.2817, 0.2501, 0.2133],\n",
      "        [0.2549, 0.2817, 0.2501, 0.2133],\n",
      "        [0.2549, 0.2817, 0.2501, 0.2133],\n",
      "        [0.2549, 0.2818, 0.2501, 0.2132],\n",
      "        [0.2549, 0.2818, 0.2501, 0.2132],\n",
      "        [0.2549, 0.2818, 0.2501, 0.2132],\n",
      "        [0.2549, 0.2818, 0.2501, 0.2132],\n",
      "        [0.2549, 0.2817, 0.2501, 0.2133],\n",
      "        [0.2549, 0.2817, 0.2501, 0.2133],\n",
      "        [0.2549, 0.2817, 0.2501, 0.2133],\n",
      "        [0.2549, 0.2817, 0.2501, 0.2133],\n",
      "        [0.2549, 0.2817, 0.2501, 0.2133],\n",
      "        [0.2549, 0.2817, 0.2501, 0.2133],\n",
      "        [0.2549, 0.2817, 0.2501, 0.2133],\n",
      "        [0.2549, 0.2817, 0.2501, 0.2133],\n",
      "        [0.2549, 0.2818, 0.2501, 0.2132],\n",
      "        [0.2549, 0.2817, 0.2501, 0.2133],\n",
      "        [0.2549, 0.2817, 0.2501, 0.2133],\n",
      "        [0.2549, 0.2817, 0.2501, 0.2133],\n",
      "        [0.2549, 0.2817, 0.2501, 0.2133],\n",
      "        [0.2549, 0.2817, 0.2501, 0.2133],\n",
      "        [0.2549, 0.2817, 0.2501, 0.2133],\n",
      "        [0.2549, 0.2817, 0.2501, 0.2133],\n",
      "        [0.2549, 0.2817, 0.2501, 0.2133],\n",
      "        [0.2549, 0.2818, 0.2501, 0.2132],\n",
      "        [0.2549, 0.2817, 0.2501, 0.2132],\n",
      "        [0.2549, 0.2817, 0.2501, 0.2133],\n",
      "        [0.2549, 0.2817, 0.2501, 0.2133],\n",
      "        [0.2549, 0.2817, 0.2501, 0.2133],\n",
      "        [0.2549, 0.2816, 0.2501, 0.2134],\n",
      "        [0.2549, 0.2817, 0.2501, 0.2133],\n",
      "        [0.2549, 0.2817, 0.2501, 0.2133],\n",
      "        [0.2549, 0.2817, 0.2501, 0.2133],\n",
      "        [0.2549, 0.2817, 0.2501, 0.2133],\n",
      "        [0.2549, 0.2817, 0.2501, 0.2133]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3815, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(0.1107, grad_fn=<AddBackward0>)\n",
      "batching here\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76: done 1 episodes, mean_reward=0.00, best_reward=2.00, speed=225.42\n",
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(0.0310, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.3759, -1.2223, -1.3989, -1.5801],\n",
      "        [-1.3759, -1.2223, -1.3989, -1.5800],\n",
      "        [-1.3759, -1.2225, -1.3989, -1.5797],\n",
      "        [-1.3759, -1.2223, -1.3989, -1.5800],\n",
      "        [-1.3759, -1.2221, -1.3989, -1.5802],\n",
      "        [-1.3760, -1.2221, -1.3989, -1.5802],\n",
      "        [-1.3760, -1.2222, -1.3989, -1.5801],\n",
      "        [-1.3760, -1.2222, -1.3989, -1.5801],\n",
      "        [-1.3759, -1.2223, -1.3989, -1.5800],\n",
      "        [-1.3760, -1.2223, -1.3989, -1.5800],\n",
      "        [-1.3759, -1.2223, -1.3989, -1.5801],\n",
      "        [-1.3760, -1.2223, -1.3989, -1.5800],\n",
      "        [-1.3759, -1.2223, -1.3989, -1.5801],\n",
      "        [-1.3759, -1.2224, -1.3989, -1.5800],\n",
      "        [-1.3759, -1.2222, -1.3989, -1.5802],\n",
      "        [-1.3759, -1.2223, -1.3989, -1.5801],\n",
      "        [-1.3759, -1.2224, -1.3989, -1.5800],\n",
      "        [-1.3759, -1.2224, -1.3988, -1.5800],\n",
      "        [-1.3759, -1.2224, -1.3989, -1.5799],\n",
      "        [-1.3759, -1.2223, -1.3989, -1.5800],\n",
      "        [-1.3759, -1.2223, -1.3989, -1.5800],\n",
      "        [-1.3759, -1.2224, -1.3989, -1.5800],\n",
      "        [-1.3759, -1.2225, -1.3989, -1.5798],\n",
      "        [-1.3759, -1.2223, -1.3989, -1.5801],\n",
      "        [-1.3759, -1.2223, -1.3989, -1.5801],\n",
      "        [-1.3759, -1.2223, -1.3989, -1.5800],\n",
      "        [-1.3759, -1.2223, -1.3989, -1.5800],\n",
      "        [-1.3759, -1.2223, -1.3989, -1.5800],\n",
      "        [-1.3759, -1.2224, -1.3989, -1.5800],\n",
      "        [-1.3759, -1.2223, -1.3989, -1.5801],\n",
      "        [-1.3759, -1.2225, -1.3988, -1.5799],\n",
      "        [-1.3759, -1.2226, -1.3989, -1.5797],\n",
      "        [-1.3759, -1.2220, -1.3989, -1.5804],\n",
      "        [-1.3759, -1.2220, -1.3989, -1.5804],\n",
      "        [-1.3759, -1.2220, -1.3989, -1.5804],\n",
      "        [-1.3759, -1.2221, -1.3989, -1.5803],\n",
      "        [-1.3759, -1.2223, -1.3989, -1.5801],\n",
      "        [-1.3759, -1.2224, -1.3989, -1.5798],\n",
      "        [-1.3759, -1.2224, -1.3989, -1.5800],\n",
      "        [-1.3759, -1.2224, -1.3989, -1.5800],\n",
      "        [-1.3759, -1.2224, -1.3989, -1.5799],\n",
      "        [-1.3759, -1.2221, -1.3989, -1.5804],\n",
      "        [-1.3759, -1.2228, -1.3989, -1.5793],\n",
      "        [-1.3759, -1.2224, -1.3989, -1.5799],\n",
      "        [-1.3759, -1.2224, -1.3989, -1.5799],\n",
      "        [-1.3760, -1.2223, -1.3989, -1.5800],\n",
      "        [-1.3760, -1.2222, -1.3989, -1.5800],\n",
      "        [-1.3759, -1.2221, -1.3989, -1.5803],\n",
      "        [-1.3759, -1.2223, -1.3989, -1.5800],\n",
      "        [-1.3759, -1.2223, -1.3989, -1.5801],\n",
      "        [-1.3759, -1.2224, -1.3989, -1.5800],\n",
      "        [-1.3759, -1.2224, -1.3989, -1.5800],\n",
      "        [-1.3759, -1.2224, -1.3989, -1.5799],\n",
      "        [-1.3759, -1.2223, -1.3989, -1.5801],\n",
      "        [-1.3759, -1.2224, -1.3989, -1.5800],\n",
      "        [-1.3759, -1.2221, -1.3989, -1.5803],\n",
      "        [-1.3759, -1.2223, -1.3988, -1.5801],\n",
      "        [-1.3759, -1.2223, -1.3988, -1.5801],\n",
      "        [-1.3760, -1.2223, -1.3989, -1.5800],\n",
      "        [-1.3760, -1.2227, -1.3988, -1.5795],\n",
      "        [-1.3759, -1.2223, -1.3989, -1.5801],\n",
      "        [-1.3759, -1.2222, -1.3989, -1.5802],\n",
      "        [-1.3759, -1.2222, -1.3989, -1.5802],\n",
      "        [-1.3759, -1.2223, -1.3989, -1.5801]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([ 0.0069,  0.0065,  0.0035,  0.0017,  0.0070,  0.0061,  0.0045,  0.0020,\n",
      "         0.1729,  0.0054,  0.0036,  0.0014,  0.0087,  0.0050,  0.0039,  0.0021,\n",
      "         0.0078,  0.0068,  0.0039,  0.0020,  0.0081,  0.0052,  0.0034,  0.0026,\n",
      "         0.0077,  0.0058,  0.0045,  0.0017,  0.0076,  0.0059,  0.0037,  0.0018,\n",
      "         0.0077,  0.0057,  0.0039,  0.0015,  0.0081,  0.0059,  0.0042,  0.0023,\n",
      "         0.0075,  0.0072,  0.0033,  0.0016,  0.0085,  0.0066,  0.0039,  0.0025,\n",
      "         0.0080,  0.0068,  0.0039,  0.0023, -1.3774, -1.2172,  0.0034,  0.0021,\n",
      "         0.0098,  0.0058,  0.0046,  0.0019,  0.0075,  0.0059,  0.0044,  0.0019],\n",
      "       grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(0.0332, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2526, 0.2946, 0.2469, 0.2060],\n",
      "        [0.2526, 0.2946, 0.2469, 0.2060],\n",
      "        [0.2526, 0.2945, 0.2469, 0.2060],\n",
      "        [0.2526, 0.2946, 0.2469, 0.2060],\n",
      "        [0.2526, 0.2946, 0.2469, 0.2059],\n",
      "        [0.2526, 0.2946, 0.2469, 0.2059],\n",
      "        [0.2526, 0.2946, 0.2469, 0.2059],\n",
      "        [0.2526, 0.2946, 0.2469, 0.2060],\n",
      "        [0.2526, 0.2946, 0.2469, 0.2060],\n",
      "        [0.2526, 0.2945, 0.2469, 0.2060],\n",
      "        [0.2526, 0.2946, 0.2469, 0.2060],\n",
      "        [0.2526, 0.2946, 0.2469, 0.2060],\n",
      "        [0.2526, 0.2946, 0.2469, 0.2060],\n",
      "        [0.2526, 0.2945, 0.2469, 0.2060],\n",
      "        [0.2526, 0.2946, 0.2469, 0.2059],\n",
      "        [0.2526, 0.2946, 0.2469, 0.2060],\n",
      "        [0.2526, 0.2945, 0.2469, 0.2060],\n",
      "        [0.2526, 0.2945, 0.2469, 0.2060],\n",
      "        [0.2526, 0.2945, 0.2469, 0.2060],\n",
      "        [0.2526, 0.2945, 0.2469, 0.2060],\n",
      "        [0.2526, 0.2945, 0.2469, 0.2060],\n",
      "        [0.2526, 0.2945, 0.2469, 0.2060],\n",
      "        [0.2526, 0.2945, 0.2469, 0.2060],\n",
      "        [0.2526, 0.2945, 0.2469, 0.2060],\n",
      "        [0.2526, 0.2946, 0.2469, 0.2060],\n",
      "        [0.2526, 0.2946, 0.2469, 0.2060],\n",
      "        [0.2526, 0.2946, 0.2469, 0.2060],\n",
      "        [0.2526, 0.2946, 0.2469, 0.2060],\n",
      "        [0.2526, 0.2945, 0.2469, 0.2060],\n",
      "        [0.2526, 0.2946, 0.2469, 0.2059],\n",
      "        [0.2526, 0.2945, 0.2469, 0.2060],\n",
      "        [0.2526, 0.2945, 0.2469, 0.2060],\n",
      "        [0.2526, 0.2946, 0.2469, 0.2059],\n",
      "        [0.2526, 0.2946, 0.2469, 0.2059],\n",
      "        [0.2526, 0.2946, 0.2469, 0.2059],\n",
      "        [0.2526, 0.2946, 0.2469, 0.2059],\n",
      "        [0.2526, 0.2946, 0.2469, 0.2060],\n",
      "        [0.2526, 0.2945, 0.2469, 0.2060],\n",
      "        [0.2526, 0.2945, 0.2469, 0.2060],\n",
      "        [0.2526, 0.2945, 0.2469, 0.2060],\n",
      "        [0.2526, 0.2945, 0.2469, 0.2060],\n",
      "        [0.2526, 0.2946, 0.2469, 0.2059],\n",
      "        [0.2526, 0.2944, 0.2469, 0.2061],\n",
      "        [0.2526, 0.2945, 0.2469, 0.2060],\n",
      "        [0.2526, 0.2945, 0.2469, 0.2060],\n",
      "        [0.2526, 0.2946, 0.2469, 0.2060],\n",
      "        [0.2526, 0.2946, 0.2469, 0.2060],\n",
      "        [0.2526, 0.2946, 0.2469, 0.2059],\n",
      "        [0.2526, 0.2945, 0.2469, 0.2060],\n",
      "        [0.2526, 0.2945, 0.2469, 0.2060],\n",
      "        [0.2526, 0.2945, 0.2469, 0.2060],\n",
      "        [0.2526, 0.2945, 0.2469, 0.2060],\n",
      "        [0.2526, 0.2945, 0.2469, 0.2060],\n",
      "        [0.2526, 0.2946, 0.2469, 0.2060],\n",
      "        [0.2526, 0.2945, 0.2469, 0.2060],\n",
      "        [0.2526, 0.2946, 0.2469, 0.2059],\n",
      "        [0.2526, 0.2946, 0.2469, 0.2060],\n",
      "        [0.2526, 0.2945, 0.2469, 0.2060],\n",
      "        [0.2526, 0.2946, 0.2469, 0.2060],\n",
      "        [0.2526, 0.2944, 0.2469, 0.2061],\n",
      "        [0.2526, 0.2945, 0.2469, 0.2060],\n",
      "        [0.2526, 0.2946, 0.2469, 0.2059],\n",
      "        [0.2526, 0.2946, 0.2469, 0.2059],\n",
      "        [0.2526, 0.2945, 0.2469, 0.2060]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3784, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(0.0504, grad_fn=<AddBackward0>)\n",
      "batching here\n",
      "77: done 2 episodes, mean_reward=0.00, best_reward=2.00, speed=224.93\n",
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(0.0024, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.3873, -1.1880, -1.3889, -1.6292],\n",
      "        [-1.3874, -1.1880, -1.3889, -1.6292],\n",
      "        [-1.3874, -1.1881, -1.3889, -1.6290],\n",
      "        [-1.3873, -1.1881, -1.3889, -1.6290],\n",
      "        [-1.3874, -1.1880, -1.3889, -1.6291],\n",
      "        [-1.3874, -1.1881, -1.3889, -1.6291],\n",
      "        [-1.3874, -1.1879, -1.3889, -1.6292],\n",
      "        [-1.3874, -1.1879, -1.3889, -1.6292],\n",
      "        [-1.3874, -1.1880, -1.3889, -1.6292],\n",
      "        [-1.3874, -1.1881, -1.3888, -1.6291],\n",
      "        [-1.3874, -1.1880, -1.3889, -1.6292],\n",
      "        [-1.3874, -1.1880, -1.3889, -1.6292],\n",
      "        [-1.3874, -1.1881, -1.3889, -1.6291],\n",
      "        [-1.3874, -1.1880, -1.3889, -1.6293],\n",
      "        [-1.3874, -1.1879, -1.3889, -1.6293],\n",
      "        [-1.3875, -1.1880, -1.3888, -1.6292],\n",
      "        [-1.3874, -1.1882, -1.3889, -1.6289],\n",
      "        [-1.3873, -1.1881, -1.3889, -1.6291],\n",
      "        [-1.3873, -1.1881, -1.3889, -1.6290],\n",
      "        [-1.3873, -1.1883, -1.3889, -1.6288],\n",
      "        [-1.3873, -1.1882, -1.3889, -1.6289],\n",
      "        [-1.3873, -1.1882, -1.3889, -1.6290],\n",
      "        [-1.3873, -1.1883, -1.3889, -1.6289],\n",
      "        [-1.3873, -1.1882, -1.3889, -1.6290],\n",
      "        [-1.3874, -1.1880, -1.3889, -1.6291],\n",
      "        [-1.3874, -1.1880, -1.3889, -1.6291],\n",
      "        [-1.3874, -1.1880, -1.3889, -1.6291],\n",
      "        [-1.3874, -1.1880, -1.3889, -1.6291],\n",
      "        [-1.3873, -1.1881, -1.3889, -1.6291],\n",
      "        [-1.3873, -1.1881, -1.3888, -1.6293],\n",
      "        [-1.3873, -1.1883, -1.3888, -1.6289],\n",
      "        [-1.3874, -1.1883, -1.3888, -1.6288],\n",
      "        [-1.3874, -1.1877, -1.3889, -1.6296],\n",
      "        [-1.3874, -1.1879, -1.3889, -1.6293],\n",
      "        [-1.3874, -1.1878, -1.3889, -1.6295],\n",
      "        [-1.3875, -1.1879, -1.3889, -1.6292],\n",
      "        [-1.3873, -1.1883, -1.3889, -1.6288],\n",
      "        [-1.3874, -1.1881, -1.3889, -1.6291],\n",
      "        [-1.3873, -1.1882, -1.3889, -1.6289],\n",
      "        [-1.3873, -1.1882, -1.3889, -1.6290],\n",
      "        [-1.3874, -1.1882, -1.3888, -1.6290],\n",
      "        [-1.3873, -1.1881, -1.3888, -1.6293],\n",
      "        [-1.3874, -1.1881, -1.3888, -1.6291],\n",
      "        [-1.3874, -1.1883, -1.3888, -1.6288],\n",
      "        [-1.3874, -1.1880, -1.3889, -1.6291],\n",
      "        [-1.3874, -1.1879, -1.3889, -1.6293],\n",
      "        [-1.3874, -1.1880, -1.3889, -1.6291],\n",
      "        [-1.3874, -1.1880, -1.3889, -1.6291],\n",
      "        [-1.3873, -1.1882, -1.3889, -1.6290],\n",
      "        [-1.3873, -1.1881, -1.3889, -1.6291],\n",
      "        [-1.3873, -1.1882, -1.3889, -1.6290],\n",
      "        [-1.3873, -1.1881, -1.3889, -1.6291],\n",
      "        [-1.3873, -1.1881, -1.3889, -1.6291],\n",
      "        [-1.3874, -1.1880, -1.3889, -1.6292],\n",
      "        [-1.3874, -1.1880, -1.3889, -1.6292],\n",
      "        [-1.3874, -1.1881, -1.3889, -1.6291],\n",
      "        [-1.3874, -1.1883, -1.3889, -1.6287],\n",
      "        [-1.3873, -1.1880, -1.3889, -1.6293],\n",
      "        [-1.3873, -1.1879, -1.3889, -1.6294],\n",
      "        [-1.3874, -1.1883, -1.3889, -1.6289],\n",
      "        [-1.3874, -1.1880, -1.3889, -1.6292],\n",
      "        [-1.3873, -1.1881, -1.3889, -1.6291],\n",
      "        [-1.3873, -1.1881, -1.3889, -1.6291],\n",
      "        [-1.3873, -1.1881, -1.3889, -1.6291]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([0.0092, 0.0080, 0.0053, 0.0023, 0.2688, 0.2686, 0.0064, 0.0028, 0.0109,\n",
      "        0.0095, 0.0048, 0.0024, 0.0107, 0.0067, 0.0047, 0.0026, 0.0103, 0.0069,\n",
      "        0.0051, 0.0020, 0.0090, 0.0068, 0.0061, 0.0028, 0.2690, 0.2687, 0.0050,\n",
      "        0.0021, 0.0104, 0.0079, 0.0057, 0.0020, 0.0125, 0.0075, 0.0061, 0.0018,\n",
      "        0.0105, 0.0083, 0.0054, 0.0024, 0.0111, 0.0088, 0.0060, 0.0025, 0.0105,\n",
      "        0.0081, 0.0052, 0.0026, 0.0121, 0.0077, 0.0050, 0.0022, 0.0092, 0.0069,\n",
      "        0.0047, 0.0024, 0.0103, 0.0089, 0.0052, 0.0028, 0.0110, 0.0070, 0.0054,\n",
      "        0.0033], grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(-0.0228, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2497, 0.3048, 0.2494, 0.1961],\n",
      "        [0.2497, 0.3048, 0.2494, 0.1961],\n",
      "        [0.2497, 0.3048, 0.2494, 0.1961],\n",
      "        [0.2497, 0.3048, 0.2493, 0.1961],\n",
      "        [0.2497, 0.3048, 0.2493, 0.1961],\n",
      "        [0.2497, 0.3048, 0.2494, 0.1961],\n",
      "        [0.2497, 0.3048, 0.2493, 0.1961],\n",
      "        [0.2497, 0.3049, 0.2493, 0.1961],\n",
      "        [0.2497, 0.3048, 0.2494, 0.1961],\n",
      "        [0.2497, 0.3048, 0.2494, 0.1961],\n",
      "        [0.2497, 0.3048, 0.2494, 0.1961],\n",
      "        [0.2497, 0.3048, 0.2494, 0.1961],\n",
      "        [0.2497, 0.3048, 0.2494, 0.1961],\n",
      "        [0.2497, 0.3048, 0.2494, 0.1961],\n",
      "        [0.2497, 0.3049, 0.2494, 0.1961],\n",
      "        [0.2497, 0.3048, 0.2494, 0.1961],\n",
      "        [0.2497, 0.3048, 0.2494, 0.1962],\n",
      "        [0.2497, 0.3048, 0.2494, 0.1961],\n",
      "        [0.2497, 0.3048, 0.2494, 0.1961],\n",
      "        [0.2497, 0.3047, 0.2494, 0.1962],\n",
      "        [0.2497, 0.3048, 0.2494, 0.1961],\n",
      "        [0.2497, 0.3048, 0.2494, 0.1961],\n",
      "        [0.2497, 0.3047, 0.2494, 0.1962],\n",
      "        [0.2497, 0.3048, 0.2493, 0.1961],\n",
      "        [0.2497, 0.3048, 0.2494, 0.1961],\n",
      "        [0.2497, 0.3048, 0.2494, 0.1961],\n",
      "        [0.2497, 0.3048, 0.2493, 0.1961],\n",
      "        [0.2497, 0.3048, 0.2493, 0.1961],\n",
      "        [0.2497, 0.3048, 0.2494, 0.1961],\n",
      "        [0.2497, 0.3048, 0.2494, 0.1961],\n",
      "        [0.2497, 0.3047, 0.2494, 0.1962],\n",
      "        [0.2497, 0.3047, 0.2494, 0.1962],\n",
      "        [0.2497, 0.3049, 0.2493, 0.1960],\n",
      "        [0.2497, 0.3049, 0.2493, 0.1961],\n",
      "        [0.2497, 0.3049, 0.2493, 0.1960],\n",
      "        [0.2497, 0.3049, 0.2494, 0.1961],\n",
      "        [0.2497, 0.3047, 0.2494, 0.1962],\n",
      "        [0.2497, 0.3048, 0.2494, 0.1961],\n",
      "        [0.2497, 0.3048, 0.2494, 0.1961],\n",
      "        [0.2497, 0.3048, 0.2494, 0.1961],\n",
      "        [0.2497, 0.3048, 0.2494, 0.1961],\n",
      "        [0.2497, 0.3048, 0.2494, 0.1961],\n",
      "        [0.2497, 0.3048, 0.2494, 0.1961],\n",
      "        [0.2497, 0.3047, 0.2494, 0.1962],\n",
      "        [0.2497, 0.3048, 0.2494, 0.1961],\n",
      "        [0.2497, 0.3049, 0.2494, 0.1961],\n",
      "        [0.2497, 0.3048, 0.2493, 0.1961],\n",
      "        [0.2497, 0.3048, 0.2494, 0.1961],\n",
      "        [0.2497, 0.3048, 0.2494, 0.1961],\n",
      "        [0.2497, 0.3048, 0.2494, 0.1961],\n",
      "        [0.2497, 0.3048, 0.2494, 0.1961],\n",
      "        [0.2497, 0.3048, 0.2494, 0.1961],\n",
      "        [0.2497, 0.3048, 0.2494, 0.1961],\n",
      "        [0.2497, 0.3048, 0.2494, 0.1961],\n",
      "        [0.2497, 0.3048, 0.2494, 0.1961],\n",
      "        [0.2497, 0.3048, 0.2494, 0.1961],\n",
      "        [0.2497, 0.3047, 0.2494, 0.1962],\n",
      "        [0.2497, 0.3048, 0.2494, 0.1961],\n",
      "        [0.2497, 0.3048, 0.2494, 0.1961],\n",
      "        [0.2497, 0.3048, 0.2494, 0.1962],\n",
      "        [0.2497, 0.3048, 0.2494, 0.1961],\n",
      "        [0.2497, 0.3048, 0.2494, 0.1961],\n",
      "        [0.2497, 0.3048, 0.2494, 0.1961],\n",
      "        [0.2497, 0.3048, 0.2494, 0.1961]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3744, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(-0.0341, grad_fn=<AddBackward0>)\n",
      "batching here\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78: done 2 episodes, mean_reward=0.00, best_reward=2.00, speed=224.46\n",
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.3985, -1.1945, -1.3895, -1.6045],\n",
      "        [-1.3985, -1.1946, -1.3895, -1.6044],\n",
      "        [-1.3985, -1.1946, -1.3895, -1.6044],\n",
      "        [-1.3985, -1.1946, -1.3895, -1.6045],\n",
      "        [-1.3985, -1.1945, -1.3896, -1.6045],\n",
      "        [-1.3986, -1.1946, -1.3895, -1.6042],\n",
      "        [-1.3985, -1.1945, -1.3896, -1.6045],\n",
      "        [-1.3986, -1.1945, -1.3896, -1.6044],\n",
      "        [-1.3986, -1.1945, -1.3895, -1.6044],\n",
      "        [-1.3985, -1.1946, -1.3896, -1.6043],\n",
      "        [-1.3985, -1.1944, -1.3896, -1.6046],\n",
      "        [-1.3986, -1.1944, -1.3896, -1.6046],\n",
      "        [-1.3985, -1.1947, -1.3895, -1.6044],\n",
      "        [-1.3985, -1.1946, -1.3895, -1.6044],\n",
      "        [-1.3985, -1.1946, -1.3895, -1.6044],\n",
      "        [-1.3985, -1.1948, -1.3895, -1.6042],\n",
      "        [-1.3985, -1.1946, -1.3895, -1.6045],\n",
      "        [-1.3985, -1.1947, -1.3895, -1.6043],\n",
      "        [-1.3985, -1.1946, -1.3895, -1.6044],\n",
      "        [-1.3985, -1.1947, -1.3895, -1.6043],\n",
      "        [-1.3985, -1.1946, -1.3895, -1.6044],\n",
      "        [-1.3985, -1.1947, -1.3895, -1.6044],\n",
      "        [-1.3985, -1.1946, -1.3895, -1.6044],\n",
      "        [-1.3985, -1.1946, -1.3895, -1.6045],\n",
      "        [-1.3986, -1.1944, -1.3895, -1.6046],\n",
      "        [-1.3986, -1.1946, -1.3895, -1.6043],\n",
      "        [-1.3985, -1.1947, -1.3895, -1.6041],\n",
      "        [-1.3986, -1.1945, -1.3895, -1.6045],\n",
      "        [-1.3985, -1.1946, -1.3895, -1.6045],\n",
      "        [-1.3984, -1.1947, -1.3894, -1.6044],\n",
      "        [-1.3985, -1.1945, -1.3895, -1.6046],\n",
      "        [-1.3985, -1.1946, -1.3895, -1.6044],\n",
      "        [-1.3986, -1.1941, -1.3895, -1.6051],\n",
      "        [-1.3986, -1.1941, -1.3896, -1.6050],\n",
      "        [-1.3986, -1.1944, -1.3895, -1.6046],\n",
      "        [-1.3986, -1.1943, -1.3896, -1.6047],\n",
      "        [-1.3985, -1.1947, -1.3895, -1.6043],\n",
      "        [-1.3985, -1.1946, -1.3895, -1.6044],\n",
      "        [-1.3985, -1.1946, -1.3895, -1.6044],\n",
      "        [-1.3985, -1.1947, -1.3895, -1.6044],\n",
      "        [-1.3985, -1.1949, -1.3895, -1.6040],\n",
      "        [-1.3985, -1.1947, -1.3895, -1.6042],\n",
      "        [-1.3985, -1.1948, -1.3895, -1.6042],\n",
      "        [-1.3984, -1.1948, -1.3895, -1.6041],\n",
      "        [-1.3985, -1.1945, -1.3895, -1.6046],\n",
      "        [-1.3985, -1.1945, -1.3895, -1.6045],\n",
      "        [-1.3986, -1.1945, -1.3895, -1.6045],\n",
      "        [-1.3985, -1.1944, -1.3895, -1.6047],\n",
      "        [-1.3985, -1.1945, -1.3895, -1.6045],\n",
      "        [-1.3985, -1.1946, -1.3896, -1.6044],\n",
      "        [-1.3985, -1.1946, -1.3896, -1.6044],\n",
      "        [-1.3985, -1.1946, -1.3895, -1.6044],\n",
      "        [-1.3985, -1.1945, -1.3895, -1.6045],\n",
      "        [-1.3985, -1.1944, -1.3896, -1.6045],\n",
      "        [-1.3985, -1.1945, -1.3895, -1.6045],\n",
      "        [-1.3985, -1.1945, -1.3896, -1.6044],\n",
      "        [-1.3985, -1.1948, -1.3895, -1.6042],\n",
      "        [-1.3985, -1.1945, -1.3895, -1.6045],\n",
      "        [-1.3985, -1.1946, -1.3895, -1.6044],\n",
      "        [-1.3985, -1.1946, -1.3895, -1.6044],\n",
      "        [-1.3985, -1.1946, -1.3895, -1.6044],\n",
      "        [-1.3985, -1.1945, -1.3895, -1.6046],\n",
      "        [-1.3985, -1.1945, -1.3895, -1.6045],\n",
      "        [-1.3986, -1.1944, -1.3895, -1.6047]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([0.0075, 0.0065, 0.0037, 0.0020, 0.0082, 0.0050, 0.0040, 0.0014, 0.0075,\n",
      "        0.0067, 0.0042, 0.0023, 0.0075, 0.0078, 0.0046, 0.0019, 0.0073, 0.0053,\n",
      "        0.0041, 0.0016, 0.0085, 0.0055, 0.0044, 0.0020, 0.0072, 0.0052, 0.0038,\n",
      "        0.0021, 0.0086, 0.0061, 0.0037, 0.0015, 0.0097, 0.0064, 0.0042, 0.0020,\n",
      "        0.0083, 0.0056, 0.0036, 0.0020, 0.0080, 0.0072, 0.0033, 0.0021, 0.0076,\n",
      "        0.0065, 0.0046, 0.0031, 0.1844, 0.0074, 0.0037, 0.0020, 0.0074, 0.0066,\n",
      "        0.0046, 0.0025, 0.0096, 0.0067, 0.0044, 0.0026, 0.2475, 0.0066, 0.0038,\n",
      "        0.0021], grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(-0.0116, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2470, 0.3028, 0.2492, 0.2010],\n",
      "        [0.2470, 0.3028, 0.2492, 0.2010],\n",
      "        [0.2470, 0.3028, 0.2492, 0.2010],\n",
      "        [0.2470, 0.3028, 0.2492, 0.2010],\n",
      "        [0.2470, 0.3029, 0.2492, 0.2010],\n",
      "        [0.2469, 0.3028, 0.2492, 0.2010],\n",
      "        [0.2470, 0.3029, 0.2492, 0.2010],\n",
      "        [0.2470, 0.3028, 0.2492, 0.2010],\n",
      "        [0.2469, 0.3028, 0.2492, 0.2010],\n",
      "        [0.2470, 0.3028, 0.2492, 0.2010],\n",
      "        [0.2470, 0.3029, 0.2492, 0.2010],\n",
      "        [0.2470, 0.3029, 0.2492, 0.2010],\n",
      "        [0.2470, 0.3028, 0.2492, 0.2010],\n",
      "        [0.2470, 0.3028, 0.2492, 0.2010],\n",
      "        [0.2470, 0.3028, 0.2492, 0.2010],\n",
      "        [0.2470, 0.3028, 0.2492, 0.2011],\n",
      "        [0.2470, 0.3028, 0.2492, 0.2010],\n",
      "        [0.2470, 0.3028, 0.2492, 0.2010],\n",
      "        [0.2470, 0.3028, 0.2492, 0.2010],\n",
      "        [0.2470, 0.3028, 0.2492, 0.2010],\n",
      "        [0.2470, 0.3028, 0.2492, 0.2010],\n",
      "        [0.2470, 0.3028, 0.2492, 0.2010],\n",
      "        [0.2470, 0.3028, 0.2492, 0.2010],\n",
      "        [0.2470, 0.3028, 0.2492, 0.2010],\n",
      "        [0.2469, 0.3029, 0.2492, 0.2010],\n",
      "        [0.2469, 0.3028, 0.2492, 0.2010],\n",
      "        [0.2470, 0.3028, 0.2492, 0.2011],\n",
      "        [0.2470, 0.3029, 0.2492, 0.2010],\n",
      "        [0.2470, 0.3028, 0.2492, 0.2010],\n",
      "        [0.2470, 0.3028, 0.2492, 0.2010],\n",
      "        [0.2470, 0.3028, 0.2492, 0.2010],\n",
      "        [0.2470, 0.3028, 0.2492, 0.2010],\n",
      "        [0.2470, 0.3030, 0.2492, 0.2009],\n",
      "        [0.2469, 0.3030, 0.2492, 0.2009],\n",
      "        [0.2469, 0.3029, 0.2492, 0.2010],\n",
      "        [0.2470, 0.3029, 0.2492, 0.2010],\n",
      "        [0.2470, 0.3028, 0.2492, 0.2010],\n",
      "        [0.2470, 0.3028, 0.2492, 0.2010],\n",
      "        [0.2470, 0.3028, 0.2492, 0.2010],\n",
      "        [0.2470, 0.3028, 0.2492, 0.2010],\n",
      "        [0.2470, 0.3027, 0.2492, 0.2011],\n",
      "        [0.2470, 0.3028, 0.2492, 0.2011],\n",
      "        [0.2470, 0.3028, 0.2492, 0.2011],\n",
      "        [0.2470, 0.3028, 0.2492, 0.2011],\n",
      "        [0.2470, 0.3029, 0.2492, 0.2010],\n",
      "        [0.2470, 0.3029, 0.2492, 0.2010],\n",
      "        [0.2470, 0.3029, 0.2492, 0.2010],\n",
      "        [0.2470, 0.3029, 0.2492, 0.2010],\n",
      "        [0.2470, 0.3028, 0.2492, 0.2010],\n",
      "        [0.2470, 0.3028, 0.2492, 0.2010],\n",
      "        [0.2470, 0.3028, 0.2492, 0.2010],\n",
      "        [0.2470, 0.3028, 0.2492, 0.2010],\n",
      "        [0.2470, 0.3029, 0.2492, 0.2010],\n",
      "        [0.2470, 0.3029, 0.2492, 0.2010],\n",
      "        [0.2470, 0.3029, 0.2492, 0.2010],\n",
      "        [0.2470, 0.3028, 0.2492, 0.2010],\n",
      "        [0.2470, 0.3028, 0.2492, 0.2010],\n",
      "        [0.2470, 0.3028, 0.2492, 0.2010],\n",
      "        [0.2470, 0.3028, 0.2492, 0.2010],\n",
      "        [0.2470, 0.3028, 0.2492, 0.2010],\n",
      "        [0.2470, 0.3028, 0.2492, 0.2010],\n",
      "        [0.2470, 0.3029, 0.2492, 0.2010],\n",
      "        [0.2470, 0.3029, 0.2492, 0.2010],\n",
      "        [0.2470, 0.3029, 0.2492, 0.2010]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3759, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(-0.0246, grad_fn=<AddBackward0>)\n",
      "batching here\n",
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(0.0603, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.3958, -1.2046, -1.3861, -1.5970],\n",
      "        [-1.3958, -1.2045, -1.3861, -1.5970],\n",
      "        [-1.3958, -1.2045, -1.3861, -1.5970],\n",
      "        [-1.3958, -1.2046, -1.3861, -1.5970],\n",
      "        [-1.3959, -1.2042, -1.3861, -1.5973],\n",
      "        [-1.3959, -1.2046, -1.3862, -1.5967],\n",
      "        [-1.3959, -1.2042, -1.3862, -1.5974],\n",
      "        [-1.3959, -1.2045, -1.3862, -1.5969],\n",
      "        [-1.3959, -1.2046, -1.3862, -1.5968],\n",
      "        [-1.3959, -1.2045, -1.3861, -1.5970],\n",
      "        [-1.3959, -1.2043, -1.3861, -1.5972],\n",
      "        [-1.3959, -1.2044, -1.3862, -1.5971],\n",
      "        [-1.3959, -1.2045, -1.3861, -1.5970],\n",
      "        [-1.3959, -1.2044, -1.3861, -1.5972],\n",
      "        [-1.3958, -1.2044, -1.3861, -1.5973],\n",
      "        [-1.3959, -1.2044, -1.3861, -1.5972],\n",
      "        [-1.3958, -1.2045, -1.3861, -1.5971],\n",
      "        [-1.3958, -1.2045, -1.3861, -1.5970],\n",
      "        [-1.3958, -1.2046, -1.3861, -1.5970],\n",
      "        [-1.3958, -1.2045, -1.3861, -1.5970],\n",
      "        [-1.3958, -1.2046, -1.3861, -1.5970],\n",
      "        [-1.3958, -1.2045, -1.3861, -1.5970],\n",
      "        [-1.3958, -1.2045, -1.3861, -1.5970],\n",
      "        [-1.3958, -1.2045, -1.3861, -1.5970],\n",
      "        [-1.3959, -1.2043, -1.3861, -1.5972],\n",
      "        [-1.3959, -1.2046, -1.3861, -1.5969],\n",
      "        [-1.3959, -1.2046, -1.3861, -1.5968],\n",
      "        [-1.3959, -1.2044, -1.3862, -1.5971],\n",
      "        [-1.3958, -1.2045, -1.3861, -1.5971],\n",
      "        [-1.3958, -1.2045, -1.3861, -1.5971],\n",
      "        [-1.3958, -1.2045, -1.3861, -1.5971],\n",
      "        [-1.3958, -1.2045, -1.3861, -1.5971],\n",
      "        [-1.3959, -1.2040, -1.3861, -1.5977],\n",
      "        [-1.3959, -1.2043, -1.3862, -1.5972],\n",
      "        [-1.3959, -1.2041, -1.3862, -1.5975],\n",
      "        [-1.3959, -1.2043, -1.3862, -1.5972],\n",
      "        [-1.3958, -1.2045, -1.3861, -1.5970],\n",
      "        [-1.3958, -1.2045, -1.3861, -1.5971],\n",
      "        [-1.3958, -1.2046, -1.3861, -1.5970],\n",
      "        [-1.3958, -1.2045, -1.3861, -1.5970],\n",
      "        [-1.3959, -1.2045, -1.3861, -1.5971],\n",
      "        [-1.3959, -1.2043, -1.3862, -1.5973],\n",
      "        [-1.3959, -1.2046, -1.3861, -1.5969],\n",
      "        [-1.3959, -1.2044, -1.3862, -1.5971],\n",
      "        [-1.3959, -1.2046, -1.3861, -1.5969],\n",
      "        [-1.3959, -1.2046, -1.3861, -1.5969],\n",
      "        [-1.3959, -1.2042, -1.3861, -1.5974],\n",
      "        [-1.3958, -1.2048, -1.3861, -1.5967],\n",
      "        [-1.3959, -1.2046, -1.3861, -1.5969],\n",
      "        [-1.3958, -1.2044, -1.3862, -1.5972],\n",
      "        [-1.3959, -1.2044, -1.3862, -1.5971],\n",
      "        [-1.3959, -1.2043, -1.3861, -1.5974],\n",
      "        [-1.3959, -1.2045, -1.3862, -1.5970],\n",
      "        [-1.3959, -1.2044, -1.3862, -1.5972],\n",
      "        [-1.3959, -1.2046, -1.3862, -1.5969],\n",
      "        [-1.3958, -1.2043, -1.3862, -1.5972],\n",
      "        [-1.3958, -1.2046, -1.3861, -1.5969],\n",
      "        [-1.3959, -1.2045, -1.3861, -1.5970],\n",
      "        [-1.3959, -1.2045, -1.3861, -1.5970],\n",
      "        [-1.3959, -1.2045, -1.3862, -1.5970],\n",
      "        [-1.3959, -1.2045, -1.3861, -1.5970],\n",
      "        [-1.3958, -1.2047, -1.3861, -1.5968],\n",
      "        [-1.3958, -1.2045, -1.3861, -1.5971],\n",
      "        [-1.3958, -1.2044, -1.3861, -1.5972]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([ 6.3557e-03,  6.3660e-03,  3.6973e-03,  1.4944e-03,  7.5632e-03,\n",
      "         4.8165e-03,  3.8123e-03,  1.1563e-03,  6.1881e-03,  6.4628e-03,\n",
      "         3.9706e-03,  2.3014e-03,  7.6378e-03,  6.3483e-03,  5.2242e-03,\n",
      "         2.9014e-03,  7.5852e-03,  5.5584e-03,  3.1723e-03,  1.8516e-03,\n",
      "         7.5722e-03,  6.6057e-03,  3.8988e-03,  1.9565e-03,  7.7837e-03,\n",
      "         4.8151e-03,  3.5881e-03,  2.5199e-03,  7.7346e-03,  6.6831e-03,\n",
      "         4.5227e-03,  1.6418e-03,  6.7697e-03,  6.2570e-03,  4.0595e-03,\n",
      "         2.1104e-03,  7.6180e-03,  6.6496e-03,  4.3373e-03,  1.9540e-03,\n",
      "        -1.1622e+00, -1.3622e+00, -1.5765e+00, -1.3939e+00,  7.5752e-03,\n",
      "         5.8486e-03,  4.8005e-03,  1.6601e-03,  7.9718e-03,  6.4123e-03,\n",
      "         4.3268e-03,  2.8524e-03,  7.6639e-03,  6.0999e-03,  3.8830e-03,\n",
      "         2.4050e-03,  6.3742e-03,  4.8045e-03,  4.2668e-03,  1.8371e-03,\n",
      "         6.6437e-03,  5.5006e-03,  4.0395e-03,  2.2560e-03],\n",
      "       grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(0.0813, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2476, 0.2998, 0.2500, 0.2025],\n",
      "        [0.2476, 0.2998, 0.2500, 0.2025],\n",
      "        [0.2476, 0.2998, 0.2500, 0.2025],\n",
      "        [0.2476, 0.2998, 0.2500, 0.2025],\n",
      "        [0.2476, 0.2999, 0.2500, 0.2024],\n",
      "        [0.2476, 0.2998, 0.2500, 0.2026],\n",
      "        [0.2476, 0.2999, 0.2500, 0.2024],\n",
      "        [0.2476, 0.2998, 0.2500, 0.2025],\n",
      "        [0.2476, 0.2998, 0.2500, 0.2025],\n",
      "        [0.2476, 0.2999, 0.2500, 0.2025],\n",
      "        [0.2476, 0.2999, 0.2500, 0.2025],\n",
      "        [0.2476, 0.2999, 0.2500, 0.2025],\n",
      "        [0.2476, 0.2998, 0.2500, 0.2025],\n",
      "        [0.2476, 0.2999, 0.2500, 0.2025],\n",
      "        [0.2476, 0.2999, 0.2501, 0.2024],\n",
      "        [0.2476, 0.2999, 0.2500, 0.2025],\n",
      "        [0.2476, 0.2998, 0.2500, 0.2025],\n",
      "        [0.2476, 0.2998, 0.2500, 0.2025],\n",
      "        [0.2476, 0.2998, 0.2500, 0.2025],\n",
      "        [0.2476, 0.2998, 0.2500, 0.2025],\n",
      "        [0.2476, 0.2998, 0.2500, 0.2025],\n",
      "        [0.2476, 0.2998, 0.2500, 0.2025],\n",
      "        [0.2476, 0.2998, 0.2500, 0.2025],\n",
      "        [0.2476, 0.2998, 0.2500, 0.2025],\n",
      "        [0.2476, 0.2999, 0.2500, 0.2025],\n",
      "        [0.2476, 0.2998, 0.2500, 0.2025],\n",
      "        [0.2476, 0.2998, 0.2500, 0.2025],\n",
      "        [0.2476, 0.2999, 0.2500, 0.2025],\n",
      "        [0.2476, 0.2998, 0.2500, 0.2025],\n",
      "        [0.2476, 0.2998, 0.2500, 0.2025],\n",
      "        [0.2476, 0.2998, 0.2501, 0.2025],\n",
      "        [0.2476, 0.2998, 0.2500, 0.2025],\n",
      "        [0.2476, 0.3000, 0.2500, 0.2024],\n",
      "        [0.2476, 0.2999, 0.2500, 0.2025],\n",
      "        [0.2476, 0.3000, 0.2500, 0.2024],\n",
      "        [0.2476, 0.2999, 0.2500, 0.2025],\n",
      "        [0.2476, 0.2998, 0.2500, 0.2025],\n",
      "        [0.2476, 0.2998, 0.2500, 0.2025],\n",
      "        [0.2476, 0.2998, 0.2500, 0.2025],\n",
      "        [0.2476, 0.2998, 0.2500, 0.2025],\n",
      "        [0.2476, 0.2998, 0.2500, 0.2025],\n",
      "        [0.2476, 0.2999, 0.2500, 0.2024],\n",
      "        [0.2476, 0.2998, 0.2500, 0.2025],\n",
      "        [0.2476, 0.2999, 0.2500, 0.2025],\n",
      "        [0.2476, 0.2998, 0.2500, 0.2025],\n",
      "        [0.2476, 0.2998, 0.2500, 0.2025],\n",
      "        [0.2476, 0.2999, 0.2500, 0.2024],\n",
      "        [0.2476, 0.2998, 0.2501, 0.2026],\n",
      "        [0.2476, 0.2998, 0.2500, 0.2025],\n",
      "        [0.2476, 0.2999, 0.2500, 0.2025],\n",
      "        [0.2476, 0.2999, 0.2500, 0.2025],\n",
      "        [0.2476, 0.2999, 0.2500, 0.2024],\n",
      "        [0.2476, 0.2999, 0.2500, 0.2025],\n",
      "        [0.2476, 0.2999, 0.2500, 0.2025],\n",
      "        [0.2476, 0.2998, 0.2500, 0.2025],\n",
      "        [0.2476, 0.2999, 0.2500, 0.2025],\n",
      "        [0.2476, 0.2998, 0.2500, 0.2025],\n",
      "        [0.2476, 0.2998, 0.2500, 0.2025],\n",
      "        [0.2476, 0.2998, 0.2500, 0.2025],\n",
      "        [0.2476, 0.2998, 0.2500, 0.2025],\n",
      "        [0.2476, 0.2998, 0.2500, 0.2025],\n",
      "        [0.2476, 0.2998, 0.2500, 0.2026],\n",
      "        [0.2476, 0.2999, 0.2500, 0.2025],\n",
      "        [0.2476, 0.2999, 0.2500, 0.2025]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3768, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(0.1278, grad_fn=<AddBackward0>)\n",
      "batching here\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80: done 4 episodes, mean_reward=0.25, best_reward=2.00, speed=225.66\n",
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(0.0517, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.3833, -1.1974, -1.3989, -1.6074],\n",
      "        [-1.3833, -1.1974, -1.3989, -1.6074],\n",
      "        [-1.3833, -1.1974, -1.3990, -1.6073],\n",
      "        [-1.3833, -1.1974, -1.3990, -1.6073],\n",
      "        [-1.3834, -1.1971, -1.3990, -1.6077],\n",
      "        [-1.3834, -1.1976, -1.3989, -1.6069],\n",
      "        [-1.3833, -1.1972, -1.3990, -1.6075],\n",
      "        [-1.3834, -1.1973, -1.3989, -1.6074],\n",
      "        [-1.3834, -1.1973, -1.3990, -1.6074],\n",
      "        [-1.3834, -1.1973, -1.3990, -1.6074],\n",
      "        [-1.3834, -1.1973, -1.3990, -1.6074],\n",
      "        [-1.3834, -1.1971, -1.3990, -1.6077],\n",
      "        [-1.3834, -1.1975, -1.3989, -1.6072],\n",
      "        [-1.3834, -1.1975, -1.3989, -1.6071],\n",
      "        [-1.3833, -1.1972, -1.3990, -1.6077],\n",
      "        [-1.3833, -1.1972, -1.3990, -1.6077],\n",
      "        [-1.3833, -1.1974, -1.3989, -1.6074],\n",
      "        [-1.3833, -1.1975, -1.3989, -1.6073],\n",
      "        [-1.3833, -1.1975, -1.3989, -1.6073],\n",
      "        [-1.3833, -1.1975, -1.3989, -1.6073],\n",
      "        [-1.3833, -1.1975, -1.3989, -1.6073],\n",
      "        [-1.3833, -1.1975, -1.3989, -1.6073],\n",
      "        [-1.3833, -1.1974, -1.3990, -1.6073],\n",
      "        [-1.3833, -1.1974, -1.3990, -1.6073],\n",
      "        [-1.3833, -1.1974, -1.3989, -1.6074],\n",
      "        [-1.3834, -1.1972, -1.3990, -1.6076],\n",
      "        [-1.3834, -1.1972, -1.3990, -1.6074],\n",
      "        [-1.3834, -1.1973, -1.3990, -1.6075],\n",
      "        [-1.3833, -1.1975, -1.3989, -1.6074],\n",
      "        [-1.3833, -1.1975, -1.3989, -1.6074],\n",
      "        [-1.3833, -1.1975, -1.3989, -1.6074],\n",
      "        [-1.3833, -1.1975, -1.3989, -1.6073],\n",
      "        [-1.3833, -1.1970, -1.3990, -1.6079],\n",
      "        [-1.3834, -1.1970, -1.3989, -1.6079],\n",
      "        [-1.3833, -1.1970, -1.3990, -1.6079],\n",
      "        [-1.3833, -1.1970, -1.3990, -1.6078],\n",
      "        [-1.3833, -1.1975, -1.3989, -1.6073],\n",
      "        [-1.3833, -1.1975, -1.3989, -1.6073],\n",
      "        [-1.3833, -1.1975, -1.3989, -1.6073],\n",
      "        [-1.3833, -1.1975, -1.3989, -1.6073],\n",
      "        [-1.3833, -1.1974, -1.3989, -1.6073],\n",
      "        [-1.3833, -1.1974, -1.3989, -1.6074],\n",
      "        [-1.3833, -1.1974, -1.3989, -1.6074],\n",
      "        [-1.3833, -1.1975, -1.3989, -1.6074],\n",
      "        [-1.3834, -1.1975, -1.3989, -1.6072],\n",
      "        [-1.3834, -1.1974, -1.3989, -1.6073],\n",
      "        [-1.3833, -1.1972, -1.3991, -1.6075],\n",
      "        [-1.3834, -1.1972, -1.3990, -1.6076],\n",
      "        [-1.3833, -1.1976, -1.3989, -1.6070],\n",
      "        [-1.3833, -1.1973, -1.3990, -1.6075],\n",
      "        [-1.3833, -1.1975, -1.3989, -1.6072],\n",
      "        [-1.3833, -1.1973, -1.3989, -1.6076],\n",
      "        [-1.3833, -1.1975, -1.3990, -1.6072],\n",
      "        [-1.3833, -1.1972, -1.3990, -1.6076],\n",
      "        [-1.3833, -1.1974, -1.3990, -1.6074],\n",
      "        [-1.3833, -1.1974, -1.3990, -1.6074],\n",
      "        [-1.3833, -1.1975, -1.3989, -1.6073],\n",
      "        [-1.3833, -1.1974, -1.3989, -1.6074],\n",
      "        [-1.3833, -1.1974, -1.3989, -1.6074],\n",
      "        [-1.3833, -1.1974, -1.3989, -1.6074],\n",
      "        [-1.3833, -1.1976, -1.3989, -1.6071],\n",
      "        [-1.3833, -1.1976, -1.3989, -1.6070],\n",
      "        [-1.3833, -1.1975, -1.3989, -1.6073],\n",
      "        [-1.3832, -1.1976, -1.3989, -1.6073]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([ 0.2676,  0.2646,  0.0053,  0.0026,  0.0108,  0.0079,  0.0047,  0.0020,\n",
      "         0.0091,  0.0069,  0.0055,  0.0027,  0.0084,  0.0081,  0.0063,  0.0025,\n",
      "         0.0091,  0.0068,  0.0046,  0.0027,  0.2289,  0.2645,  0.0053,  0.0023,\n",
      "         0.0119,  0.0093,  0.0050,  0.0024,  0.0105,  0.0092,  0.0063,  0.0032,\n",
      "        -1.3728,  0.0081,  0.0056,  0.0024,  0.2674,  0.2644,  0.2289,  0.2644,\n",
      "         0.0104,  0.0080,  0.0052,  0.0022, -1.3745, -1.1906,  0.0046,  0.0026,\n",
      "         0.0110,  0.0092,  0.0069,  0.0040,  0.0088,  0.0083,  0.0046,  0.0029,\n",
      "         0.2647,  0.2647,  0.0066,  0.0036,  0.0116,  0.0074,  0.0053,  0.0027],\n",
      "       grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(0.0163, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2508, 0.3020, 0.2469, 0.2004],\n",
      "        [0.2508, 0.3020, 0.2469, 0.2004],\n",
      "        [0.2508, 0.3020, 0.2469, 0.2004],\n",
      "        [0.2508, 0.3020, 0.2469, 0.2004],\n",
      "        [0.2507, 0.3021, 0.2468, 0.2003],\n",
      "        [0.2507, 0.3019, 0.2469, 0.2005],\n",
      "        [0.2507, 0.3020, 0.2468, 0.2004],\n",
      "        [0.2507, 0.3020, 0.2469, 0.2004],\n",
      "        [0.2507, 0.3020, 0.2468, 0.2004],\n",
      "        [0.2507, 0.3020, 0.2469, 0.2004],\n",
      "        [0.2507, 0.3020, 0.2469, 0.2004],\n",
      "        [0.2507, 0.3021, 0.2469, 0.2004],\n",
      "        [0.2507, 0.3019, 0.2469, 0.2004],\n",
      "        [0.2507, 0.3019, 0.2469, 0.2005],\n",
      "        [0.2507, 0.3021, 0.2469, 0.2004],\n",
      "        [0.2507, 0.3020, 0.2469, 0.2004],\n",
      "        [0.2508, 0.3020, 0.2469, 0.2004],\n",
      "        [0.2508, 0.3019, 0.2469, 0.2004],\n",
      "        [0.2508, 0.3020, 0.2469, 0.2004],\n",
      "        [0.2508, 0.3020, 0.2469, 0.2004],\n",
      "        [0.2508, 0.3019, 0.2469, 0.2004],\n",
      "        [0.2508, 0.3020, 0.2469, 0.2004],\n",
      "        [0.2508, 0.3020, 0.2469, 0.2004],\n",
      "        [0.2508, 0.3020, 0.2469, 0.2004],\n",
      "        [0.2507, 0.3020, 0.2469, 0.2004],\n",
      "        [0.2507, 0.3020, 0.2469, 0.2004],\n",
      "        [0.2507, 0.3020, 0.2468, 0.2004],\n",
      "        [0.2507, 0.3020, 0.2469, 0.2004],\n",
      "        [0.2508, 0.3020, 0.2469, 0.2004],\n",
      "        [0.2508, 0.3020, 0.2469, 0.2004],\n",
      "        [0.2508, 0.3020, 0.2469, 0.2004],\n",
      "        [0.2508, 0.3020, 0.2469, 0.2004],\n",
      "        [0.2507, 0.3021, 0.2468, 0.2003],\n",
      "        [0.2507, 0.3021, 0.2469, 0.2003],\n",
      "        [0.2507, 0.3021, 0.2468, 0.2003],\n",
      "        [0.2507, 0.3021, 0.2468, 0.2003],\n",
      "        [0.2508, 0.3020, 0.2469, 0.2004],\n",
      "        [0.2508, 0.3020, 0.2469, 0.2004],\n",
      "        [0.2508, 0.3020, 0.2469, 0.2004],\n",
      "        [0.2508, 0.3020, 0.2469, 0.2004],\n",
      "        [0.2507, 0.3020, 0.2469, 0.2004],\n",
      "        [0.2507, 0.3020, 0.2469, 0.2004],\n",
      "        [0.2507, 0.3020, 0.2469, 0.2004],\n",
      "        [0.2508, 0.3020, 0.2469, 0.2004],\n",
      "        [0.2507, 0.3020, 0.2469, 0.2004],\n",
      "        [0.2507, 0.3020, 0.2469, 0.2004],\n",
      "        [0.2507, 0.3020, 0.2468, 0.2004],\n",
      "        [0.2507, 0.3021, 0.2469, 0.2004],\n",
      "        [0.2507, 0.3019, 0.2469, 0.2005],\n",
      "        [0.2508, 0.3020, 0.2469, 0.2004],\n",
      "        [0.2508, 0.3019, 0.2469, 0.2005],\n",
      "        [0.2508, 0.3020, 0.2469, 0.2004],\n",
      "        [0.2507, 0.3019, 0.2469, 0.2005],\n",
      "        [0.2508, 0.3020, 0.2468, 0.2004],\n",
      "        [0.2508, 0.3020, 0.2469, 0.2004],\n",
      "        [0.2508, 0.3020, 0.2469, 0.2004],\n",
      "        [0.2508, 0.3020, 0.2469, 0.2004],\n",
      "        [0.2507, 0.3020, 0.2469, 0.2004],\n",
      "        [0.2507, 0.3020, 0.2469, 0.2004],\n",
      "        [0.2507, 0.3020, 0.2469, 0.2004],\n",
      "        [0.2508, 0.3019, 0.2469, 0.2005],\n",
      "        [0.2507, 0.3019, 0.2469, 0.2005],\n",
      "        [0.2508, 0.3020, 0.2469, 0.2004],\n",
      "        [0.2508, 0.3019, 0.2469, 0.2004]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3759, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(0.0542, grad_fn=<AddBackward0>)\n",
      "batching here\n",
      "81: done 3 episodes, mean_reward=0.33, best_reward=2.00, speed=227.41\n",
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(0.0064, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.3968, -1.1718, -1.3912, -1.6398],\n",
      "        [-1.3968, -1.1719, -1.3912, -1.6396],\n",
      "        [-1.3967, -1.1718, -1.3912, -1.6397],\n",
      "        [-1.3968, -1.1719, -1.3912, -1.6397],\n",
      "        [-1.3969, -1.1716, -1.3912, -1.6400],\n",
      "        [-1.3969, -1.1718, -1.3912, -1.6397],\n",
      "        [-1.3968, -1.1717, -1.3912, -1.6398],\n",
      "        [-1.3968, -1.1716, -1.3912, -1.6400],\n",
      "        [-1.3969, -1.1717, -1.3912, -1.6398],\n",
      "        [-1.3968, -1.1716, -1.3912, -1.6400],\n",
      "        [-1.3969, -1.1717, -1.3912, -1.6398],\n",
      "        [-1.3968, -1.1718, -1.3912, -1.6396],\n",
      "        [-1.3969, -1.1715, -1.3912, -1.6401],\n",
      "        [-1.3968, -1.1717, -1.3912, -1.6399],\n",
      "        [-1.3969, -1.1717, -1.3912, -1.6399],\n",
      "        [-1.3969, -1.1715, -1.3912, -1.6402],\n",
      "        [-1.3968, -1.1719, -1.3912, -1.6397],\n",
      "        [-1.3968, -1.1719, -1.3912, -1.6397],\n",
      "        [-1.3968, -1.1719, -1.3912, -1.6397],\n",
      "        [-1.3968, -1.1718, -1.3912, -1.6397],\n",
      "        [-1.3968, -1.1718, -1.3912, -1.6398],\n",
      "        [-1.3968, -1.1719, -1.3912, -1.6396],\n",
      "        [-1.3967, -1.1718, -1.3912, -1.6397],\n",
      "        [-1.3968, -1.1719, -1.3912, -1.6397],\n",
      "        [-1.3968, -1.1717, -1.3912, -1.6399],\n",
      "        [-1.3968, -1.1717, -1.3912, -1.6399],\n",
      "        [-1.3968, -1.1717, -1.3912, -1.6398],\n",
      "        [-1.3968, -1.1717, -1.3912, -1.6399],\n",
      "        [-1.3968, -1.1719, -1.3912, -1.6397],\n",
      "        [-1.3968, -1.1718, -1.3912, -1.6398],\n",
      "        [-1.3968, -1.1717, -1.3912, -1.6399],\n",
      "        [-1.3968, -1.1716, -1.3912, -1.6400],\n",
      "        [-1.3968, -1.1715, -1.3912, -1.6402],\n",
      "        [-1.3968, -1.1713, -1.3913, -1.6404],\n",
      "        [-1.3969, -1.1715, -1.3912, -1.6403],\n",
      "        [-1.3969, -1.1713, -1.3912, -1.6404],\n",
      "        [-1.3968, -1.1718, -1.3912, -1.6397],\n",
      "        [-1.3968, -1.1718, -1.3912, -1.6397],\n",
      "        [-1.3968, -1.1717, -1.3912, -1.6399],\n",
      "        [-1.3968, -1.1719, -1.3912, -1.6396],\n",
      "        [-1.3968, -1.1719, -1.3912, -1.6397],\n",
      "        [-1.3968, -1.1718, -1.3912, -1.6397],\n",
      "        [-1.3968, -1.1718, -1.3912, -1.6398],\n",
      "        [-1.3968, -1.1718, -1.3912, -1.6397],\n",
      "        [-1.3968, -1.1715, -1.3914, -1.6400],\n",
      "        [-1.3969, -1.1715, -1.3912, -1.6401],\n",
      "        [-1.3969, -1.1717, -1.3912, -1.6397],\n",
      "        [-1.3969, -1.1716, -1.3913, -1.6399],\n",
      "        [-1.3968, -1.1722, -1.3912, -1.6392],\n",
      "        [-1.3968, -1.1718, -1.3912, -1.6398],\n",
      "        [-1.3968, -1.1719, -1.3912, -1.6397],\n",
      "        [-1.3968, -1.1718, -1.3912, -1.6398],\n",
      "        [-1.3968, -1.1718, -1.3912, -1.6398],\n",
      "        [-1.3968, -1.1718, -1.3912, -1.6398],\n",
      "        [-1.3968, -1.1718, -1.3912, -1.6398],\n",
      "        [-1.3968, -1.1717, -1.3912, -1.6398],\n",
      "        [-1.3968, -1.1718, -1.3912, -1.6397],\n",
      "        [-1.3968, -1.1718, -1.3912, -1.6399],\n",
      "        [-1.3968, -1.1718, -1.3911, -1.6399],\n",
      "        [-1.3968, -1.1717, -1.3912, -1.6400],\n",
      "        [-1.3968, -1.1718, -1.3912, -1.6399],\n",
      "        [-1.3968, -1.1718, -1.3912, -1.6397],\n",
      "        [-1.3968, -1.1718, -1.3912, -1.6398],\n",
      "        [-1.3968, -1.1718, -1.3912, -1.6398]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([0.0121, 0.0088, 0.0050, 0.0026, 0.0147, 0.0075, 0.0072, 0.0028, 0.0097,\n",
      "        0.0074, 0.0055, 0.0024, 0.0123, 0.0089, 0.0058, 0.0028, 0.3148, 0.3135,\n",
      "        0.3148, 0.0031, 0.0102, 0.0074, 0.0070, 0.0022, 0.0105, 0.0095, 0.0052,\n",
      "        0.0032, 0.2640, 0.3151, 0.0079, 0.0031, 0.0124, 0.0098, 0.0056, 0.0035,\n",
      "        0.0148, 0.0096, 0.0057, 0.0028, 0.0106, 0.0092, 0.0064, 0.0031, 0.0144,\n",
      "        0.0092, 0.0051, 0.0028, 0.0099, 0.0100, 0.0064, 0.0037, 0.3134, 0.2641,\n",
      "        0.3695, 0.0031, 0.0104, 0.0098, 0.0079, 0.0045, 0.0125, 0.0105, 0.0053,\n",
      "        0.0026], grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(-0.0449, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2474, 0.3098, 0.2488, 0.1940],\n",
      "        [0.2474, 0.3098, 0.2488, 0.1941],\n",
      "        [0.2474, 0.3098, 0.2488, 0.1940],\n",
      "        [0.2474, 0.3098, 0.2488, 0.1940],\n",
      "        [0.2474, 0.3099, 0.2488, 0.1940],\n",
      "        [0.2474, 0.3098, 0.2488, 0.1940],\n",
      "        [0.2474, 0.3098, 0.2488, 0.1940],\n",
      "        [0.2474, 0.3099, 0.2488, 0.1940],\n",
      "        [0.2474, 0.3098, 0.2488, 0.1940],\n",
      "        [0.2474, 0.3099, 0.2488, 0.1940],\n",
      "        [0.2474, 0.3098, 0.2488, 0.1940],\n",
      "        [0.2474, 0.3098, 0.2488, 0.1940],\n",
      "        [0.2474, 0.3099, 0.2488, 0.1940],\n",
      "        [0.2474, 0.3098, 0.2488, 0.1940],\n",
      "        [0.2474, 0.3098, 0.2488, 0.1940],\n",
      "        [0.2474, 0.3099, 0.2488, 0.1939],\n",
      "        [0.2474, 0.3098, 0.2488, 0.1940],\n",
      "        [0.2474, 0.3098, 0.2488, 0.1940],\n",
      "        [0.2474, 0.3098, 0.2488, 0.1940],\n",
      "        [0.2474, 0.3098, 0.2488, 0.1940],\n",
      "        [0.2474, 0.3098, 0.2488, 0.1940],\n",
      "        [0.2474, 0.3098, 0.2488, 0.1941],\n",
      "        [0.2474, 0.3098, 0.2488, 0.1940],\n",
      "        [0.2474, 0.3098, 0.2488, 0.1940],\n",
      "        [0.2474, 0.3099, 0.2488, 0.1940],\n",
      "        [0.2474, 0.3099, 0.2488, 0.1940],\n",
      "        [0.2474, 0.3098, 0.2488, 0.1940],\n",
      "        [0.2474, 0.3098, 0.2488, 0.1940],\n",
      "        [0.2474, 0.3098, 0.2488, 0.1940],\n",
      "        [0.2474, 0.3098, 0.2488, 0.1940],\n",
      "        [0.2474, 0.3098, 0.2488, 0.1940],\n",
      "        [0.2474, 0.3099, 0.2488, 0.1940],\n",
      "        [0.2474, 0.3099, 0.2488, 0.1939],\n",
      "        [0.2474, 0.3100, 0.2488, 0.1939],\n",
      "        [0.2474, 0.3099, 0.2488, 0.1939],\n",
      "        [0.2474, 0.3100, 0.2488, 0.1939],\n",
      "        [0.2474, 0.3098, 0.2488, 0.1940],\n",
      "        [0.2474, 0.3098, 0.2488, 0.1940],\n",
      "        [0.2474, 0.3098, 0.2488, 0.1940],\n",
      "        [0.2474, 0.3098, 0.2488, 0.1941],\n",
      "        [0.2474, 0.3098, 0.2488, 0.1940],\n",
      "        [0.2474, 0.3098, 0.2488, 0.1940],\n",
      "        [0.2474, 0.3098, 0.2488, 0.1940],\n",
      "        [0.2474, 0.3098, 0.2488, 0.1940],\n",
      "        [0.2474, 0.3099, 0.2487, 0.1940],\n",
      "        [0.2474, 0.3099, 0.2488, 0.1940],\n",
      "        [0.2474, 0.3098, 0.2488, 0.1940],\n",
      "        [0.2474, 0.3099, 0.2488, 0.1940],\n",
      "        [0.2474, 0.3097, 0.2488, 0.1941],\n",
      "        [0.2474, 0.3098, 0.2488, 0.1940],\n",
      "        [0.2474, 0.3098, 0.2488, 0.1940],\n",
      "        [0.2474, 0.3098, 0.2488, 0.1940],\n",
      "        [0.2474, 0.3098, 0.2488, 0.1940],\n",
      "        [0.2474, 0.3098, 0.2488, 0.1940],\n",
      "        [0.2474, 0.3098, 0.2488, 0.1940],\n",
      "        [0.2474, 0.3098, 0.2488, 0.1940],\n",
      "        [0.2474, 0.3098, 0.2488, 0.1940],\n",
      "        [0.2474, 0.3098, 0.2488, 0.1940],\n",
      "        [0.2474, 0.3098, 0.2488, 0.1940],\n",
      "        [0.2474, 0.3099, 0.2488, 0.1940],\n",
      "        [0.2474, 0.3098, 0.2488, 0.1940],\n",
      "        [0.2474, 0.3098, 0.2488, 0.1940],\n",
      "        [0.2474, 0.3098, 0.2488, 0.1940],\n",
      "        [0.2474, 0.3098, 0.2488, 0.1940]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3728, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(-0.0523, grad_fn=<AddBackward0>)\n",
      "batching here\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(2.0996e-05, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.3993, -1.1879, -1.3904, -1.6126],\n",
      "        [-1.3992, -1.1882, -1.3904, -1.6121],\n",
      "        [-1.3992, -1.1879, -1.3904, -1.6126],\n",
      "        [-1.3992, -1.1881, -1.3904, -1.6123],\n",
      "        [-1.3994, -1.1878, -1.3904, -1.6125],\n",
      "        [-1.3994, -1.1878, -1.3904, -1.6125],\n",
      "        [-1.3994, -1.1877, -1.3904, -1.6126],\n",
      "        [-1.3994, -1.1877, -1.3904, -1.6126],\n",
      "        [-1.3994, -1.1875, -1.3904, -1.6129],\n",
      "        [-1.3992, -1.1883, -1.3904, -1.6118],\n",
      "        [-1.3993, -1.1879, -1.3904, -1.6124],\n",
      "        [-1.3993, -1.1879, -1.3904, -1.6125],\n",
      "        [-1.3993, -1.1876, -1.3904, -1.6129],\n",
      "        [-1.3993, -1.1877, -1.3904, -1.6128],\n",
      "        [-1.3993, -1.1877, -1.3904, -1.6128],\n",
      "        [-1.3993, -1.1877, -1.3904, -1.6127],\n",
      "        [-1.3992, -1.1880, -1.3904, -1.6124],\n",
      "        [-1.3993, -1.1879, -1.3904, -1.6126],\n",
      "        [-1.3993, -1.1881, -1.3904, -1.6123],\n",
      "        [-1.3992, -1.1882, -1.3904, -1.6121],\n",
      "        [-1.3993, -1.1879, -1.3904, -1.6126],\n",
      "        [-1.3992, -1.1882, -1.3904, -1.6121],\n",
      "        [-1.3992, -1.1879, -1.3904, -1.6126],\n",
      "        [-1.3992, -1.1881, -1.3904, -1.6123],\n",
      "        [-1.3993, -1.1878, -1.3904, -1.6125],\n",
      "        [-1.3993, -1.1879, -1.3904, -1.6125],\n",
      "        [-1.3993, -1.1879, -1.3904, -1.6125],\n",
      "        [-1.3993, -1.1879, -1.3904, -1.6125],\n",
      "        [-1.3993, -1.1879, -1.3904, -1.6124],\n",
      "        [-1.3993, -1.1878, -1.3904, -1.6127],\n",
      "        [-1.3993, -1.1879, -1.3904, -1.6126],\n",
      "        [-1.3992, -1.1878, -1.3904, -1.6127],\n",
      "        [-1.3994, -1.1875, -1.3904, -1.6129],\n",
      "        [-1.3993, -1.1876, -1.3904, -1.6128],\n",
      "        [-1.3994, -1.1875, -1.3904, -1.6129],\n",
      "        [-1.3994, -1.1870, -1.3905, -1.6137],\n",
      "        [-1.3992, -1.1882, -1.3904, -1.6121],\n",
      "        [-1.3992, -1.1880, -1.3904, -1.6124],\n",
      "        [-1.3992, -1.1879, -1.3904, -1.6125],\n",
      "        [-1.3992, -1.1881, -1.3904, -1.6122],\n",
      "        [-1.3992, -1.1882, -1.3904, -1.6122],\n",
      "        [-1.3993, -1.1881, -1.3904, -1.6123],\n",
      "        [-1.3992, -1.1882, -1.3904, -1.6122],\n",
      "        [-1.3993, -1.1882, -1.3903, -1.6121],\n",
      "        [-1.3994, -1.1874, -1.3904, -1.6130],\n",
      "        [-1.3994, -1.1876, -1.3904, -1.6128],\n",
      "        [-1.3994, -1.1877, -1.3904, -1.6127],\n",
      "        [-1.3994, -1.1877, -1.3904, -1.6126],\n",
      "        [-1.3992, -1.1882, -1.3904, -1.6122],\n",
      "        [-1.3992, -1.1880, -1.3904, -1.6124],\n",
      "        [-1.3992, -1.1880, -1.3904, -1.6124],\n",
      "        [-1.3992, -1.1880, -1.3904, -1.6124],\n",
      "        [-1.3992, -1.1879, -1.3904, -1.6125],\n",
      "        [-1.3993, -1.1880, -1.3904, -1.6124],\n",
      "        [-1.3992, -1.1878, -1.3904, -1.6126],\n",
      "        [-1.3992, -1.1880, -1.3904, -1.6125],\n",
      "        [-1.3993, -1.1880, -1.3904, -1.6124],\n",
      "        [-1.3993, -1.1880, -1.3904, -1.6125],\n",
      "        [-1.3992, -1.1880, -1.3904, -1.6124],\n",
      "        [-1.3992, -1.1879, -1.3904, -1.6126],\n",
      "        [-1.3992, -1.1880, -1.3904, -1.6125],\n",
      "        [-1.3992, -1.1880, -1.3904, -1.6124],\n",
      "        [-1.3992, -1.1880, -1.3904, -1.6124],\n",
      "        [-1.3992, -1.1880, -1.3904, -1.6124]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([0.0080, 0.0063, 0.0047, 0.0022, 0.0093, 0.0070, 0.0042, 0.0025, 0.0082,\n",
      "        0.0059, 0.0044, 0.0022, 0.0081, 0.0061, 0.0056, 0.0023, 0.0092, 0.0071,\n",
      "        0.0046, 0.0021, 0.0080, 0.0054, 0.0047, 0.0016, 0.0093, 0.0070, 0.0047,\n",
      "        0.0027, 0.0077, 0.0061, 0.0054, 0.0022, 0.0079, 0.0072, 0.0056, 0.0037,\n",
      "        0.0080, 0.0085, 0.0044, 0.0025, 0.0110, 0.0085, 0.0042, 0.0018, 0.0095,\n",
      "        0.0060, 0.0046, 0.0019, 0.0076, 0.0081, 0.0055, 0.0027, 0.0092, 0.0067,\n",
      "        0.0056, 0.0022, 0.0079, 0.0070, 0.0047, 0.0025, 0.0081, 0.0070, 0.0048,\n",
      "        0.0020], grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(-0.0057, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2468, 0.3049, 0.2490, 0.1994],\n",
      "        [0.2468, 0.3048, 0.2490, 0.1995],\n",
      "        [0.2468, 0.3049, 0.2490, 0.1994],\n",
      "        [0.2468, 0.3048, 0.2490, 0.1994],\n",
      "        [0.2468, 0.3049, 0.2490, 0.1994],\n",
      "        [0.2468, 0.3049, 0.2490, 0.1994],\n",
      "        [0.2468, 0.3049, 0.2490, 0.1994],\n",
      "        [0.2468, 0.3049, 0.2490, 0.1994],\n",
      "        [0.2468, 0.3050, 0.2490, 0.1993],\n",
      "        [0.2468, 0.3047, 0.2490, 0.1995],\n",
      "        [0.2468, 0.3049, 0.2490, 0.1994],\n",
      "        [0.2468, 0.3049, 0.2490, 0.1994],\n",
      "        [0.2468, 0.3049, 0.2490, 0.1993],\n",
      "        [0.2468, 0.3049, 0.2490, 0.1993],\n",
      "        [0.2468, 0.3049, 0.2490, 0.1993],\n",
      "        [0.2468, 0.3049, 0.2490, 0.1993],\n",
      "        [0.2468, 0.3048, 0.2490, 0.1994],\n",
      "        [0.2468, 0.3049, 0.2490, 0.1994],\n",
      "        [0.2468, 0.3048, 0.2490, 0.1994],\n",
      "        [0.2468, 0.3048, 0.2490, 0.1995],\n",
      "        [0.2468, 0.3049, 0.2490, 0.1994],\n",
      "        [0.2468, 0.3048, 0.2490, 0.1995],\n",
      "        [0.2468, 0.3049, 0.2490, 0.1994],\n",
      "        [0.2468, 0.3048, 0.2490, 0.1994],\n",
      "        [0.2468, 0.3049, 0.2490, 0.1994],\n",
      "        [0.2468, 0.3049, 0.2490, 0.1994],\n",
      "        [0.2468, 0.3049, 0.2490, 0.1994],\n",
      "        [0.2468, 0.3049, 0.2490, 0.1994],\n",
      "        [0.2468, 0.3049, 0.2490, 0.1994],\n",
      "        [0.2468, 0.3049, 0.2490, 0.1994],\n",
      "        [0.2468, 0.3049, 0.2490, 0.1994],\n",
      "        [0.2468, 0.3049, 0.2490, 0.1994],\n",
      "        [0.2467, 0.3050, 0.2490, 0.1993],\n",
      "        [0.2468, 0.3050, 0.2490, 0.1993],\n",
      "        [0.2467, 0.3050, 0.2490, 0.1993],\n",
      "        [0.2467, 0.3051, 0.2490, 0.1992],\n",
      "        [0.2468, 0.3048, 0.2490, 0.1995],\n",
      "        [0.2468, 0.3048, 0.2490, 0.1994],\n",
      "        [0.2468, 0.3049, 0.2490, 0.1994],\n",
      "        [0.2468, 0.3048, 0.2490, 0.1994],\n",
      "        [0.2468, 0.3048, 0.2490, 0.1994],\n",
      "        [0.2468, 0.3048, 0.2490, 0.1994],\n",
      "        [0.2468, 0.3048, 0.2490, 0.1994],\n",
      "        [0.2468, 0.3048, 0.2490, 0.1995],\n",
      "        [0.2467, 0.3050, 0.2490, 0.1993],\n",
      "        [0.2467, 0.3049, 0.2490, 0.1993],\n",
      "        [0.2467, 0.3049, 0.2490, 0.1994],\n",
      "        [0.2468, 0.3049, 0.2490, 0.1994],\n",
      "        [0.2468, 0.3048, 0.2490, 0.1995],\n",
      "        [0.2468, 0.3048, 0.2490, 0.1994],\n",
      "        [0.2468, 0.3048, 0.2490, 0.1994],\n",
      "        [0.2468, 0.3048, 0.2490, 0.1994],\n",
      "        [0.2468, 0.3049, 0.2490, 0.1994],\n",
      "        [0.2468, 0.3048, 0.2490, 0.1994],\n",
      "        [0.2468, 0.3049, 0.2490, 0.1994],\n",
      "        [0.2468, 0.3048, 0.2490, 0.1994],\n",
      "        [0.2468, 0.3048, 0.2490, 0.1994],\n",
      "        [0.2468, 0.3048, 0.2490, 0.1994],\n",
      "        [0.2468, 0.3048, 0.2490, 0.1994],\n",
      "        [0.2468, 0.3049, 0.2490, 0.1994],\n",
      "        [0.2468, 0.3048, 0.2490, 0.1994],\n",
      "        [0.2468, 0.3048, 0.2490, 0.1994],\n",
      "        [0.2468, 0.3048, 0.2490, 0.1994],\n",
      "        [0.2468, 0.3048, 0.2490, 0.1994]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3751, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(-0.0194, grad_fn=<AddBackward0>)\n",
      "batching here\n",
      "83: done 5 episodes, mean_reward=0.20, best_reward=2.00, speed=229.37\n",
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(0.0053, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.3980, -1.1941, -1.3900, -1.6051],\n",
      "        [-1.3980, -1.1947, -1.3900, -1.6043],\n",
      "        [-1.3980, -1.1942, -1.3901, -1.6050],\n",
      "        [-1.3980, -1.1943, -1.3900, -1.6048],\n",
      "        [-1.3981, -1.1940, -1.3901, -1.6051],\n",
      "        [-1.3980, -1.1941, -1.3900, -1.6051],\n",
      "        [-1.3980, -1.1941, -1.3900, -1.6051],\n",
      "        [-1.3980, -1.1942, -1.3900, -1.6049],\n",
      "        [-1.3980, -1.1940, -1.3900, -1.6052],\n",
      "        [-1.3981, -1.1941, -1.3900, -1.6051],\n",
      "        [-1.3981, -1.1943, -1.3900, -1.6048],\n",
      "        [-1.3981, -1.1944, -1.3900, -1.6046],\n",
      "        [-1.3981, -1.1939, -1.3900, -1.6053],\n",
      "        [-1.3981, -1.1939, -1.3900, -1.6053],\n",
      "        [-1.3981, -1.1939, -1.3900, -1.6053],\n",
      "        [-1.3980, -1.1941, -1.3900, -1.6051],\n",
      "        [-1.3980, -1.1943, -1.3900, -1.6049],\n",
      "        [-1.3980, -1.1942, -1.3900, -1.6050],\n",
      "        [-1.3980, -1.1944, -1.3900, -1.6047],\n",
      "        [-1.3980, -1.1945, -1.3900, -1.6046],\n",
      "        [-1.3980, -1.1941, -1.3900, -1.6051],\n",
      "        [-1.3980, -1.1947, -1.3900, -1.6043],\n",
      "        [-1.3980, -1.1942, -1.3901, -1.6050],\n",
      "        [-1.3980, -1.1943, -1.3900, -1.6048],\n",
      "        [-1.3980, -1.1941, -1.3901, -1.6050],\n",
      "        [-1.3980, -1.1941, -1.3900, -1.6050],\n",
      "        [-1.3980, -1.1941, -1.3901, -1.6050],\n",
      "        [-1.3981, -1.1940, -1.3900, -1.6052],\n",
      "        [-1.3980, -1.1941, -1.3900, -1.6051],\n",
      "        [-1.3980, -1.1942, -1.3900, -1.6050],\n",
      "        [-1.3979, -1.1943, -1.3901, -1.6049],\n",
      "        [-1.3979, -1.1941, -1.3901, -1.6051],\n",
      "        [-1.3981, -1.1939, -1.3900, -1.6054],\n",
      "        [-1.3981, -1.1936, -1.3901, -1.6056],\n",
      "        [-1.3981, -1.1937, -1.3901, -1.6054],\n",
      "        [-1.3981, -1.1937, -1.3901, -1.6056],\n",
      "        [-1.3980, -1.1945, -1.3900, -1.6046],\n",
      "        [-1.3980, -1.1943, -1.3901, -1.6049],\n",
      "        [-1.3979, -1.1944, -1.3900, -1.6049],\n",
      "        [-1.3980, -1.1942, -1.3900, -1.6050],\n",
      "        [-1.3980, -1.1943, -1.3900, -1.6048],\n",
      "        [-1.3980, -1.1942, -1.3900, -1.6050],\n",
      "        [-1.3980, -1.1942, -1.3900, -1.6051],\n",
      "        [-1.3980, -1.1943, -1.3900, -1.6048],\n",
      "        [-1.3981, -1.1939, -1.3901, -1.6053],\n",
      "        [-1.3981, -1.1939, -1.3900, -1.6053],\n",
      "        [-1.3981, -1.1940, -1.3901, -1.6052],\n",
      "        [-1.3981, -1.1939, -1.3901, -1.6052],\n",
      "        [-1.3980, -1.1943, -1.3900, -1.6049],\n",
      "        [-1.3980, -1.1943, -1.3900, -1.6049],\n",
      "        [-1.3980, -1.1943, -1.3900, -1.6049],\n",
      "        [-1.3980, -1.1942, -1.3900, -1.6050],\n",
      "        [-1.3980, -1.1941, -1.3901, -1.6051],\n",
      "        [-1.3980, -1.1941, -1.3900, -1.6051],\n",
      "        [-1.3980, -1.1942, -1.3901, -1.6050],\n",
      "        [-1.3980, -1.1942, -1.3901, -1.6049],\n",
      "        [-1.3979, -1.1942, -1.3900, -1.6050],\n",
      "        [-1.3979, -1.1943, -1.3900, -1.6049],\n",
      "        [-1.3979, -1.1942, -1.3900, -1.6050],\n",
      "        [-1.3980, -1.1942, -1.3900, -1.6050],\n",
      "        [-1.3980, -1.1943, -1.3900, -1.6049],\n",
      "        [-1.3980, -1.1943, -1.3900, -1.6049],\n",
      "        [-1.3980, -1.1943, -1.3900, -1.6049],\n",
      "        [-1.3980, -1.1943, -1.3900, -1.6049]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([0.0089, 0.0065, 0.0040, 0.0022, 0.1922, 0.0080, 0.0055, 0.0026, 0.0092,\n",
      "        0.0069, 0.0043, 0.0017, 0.2252, 0.2586, 0.1923, 0.0023, 0.0100, 0.0057,\n",
      "        0.0041, 0.0018, 0.0077, 0.0057, 0.0047, 0.0022, 0.2236, 0.0058, 0.0039,\n",
      "        0.0023, 0.0075, 0.0064, 0.0035, 0.0019, 0.0087, 0.0079, 0.0044, 0.0024,\n",
      "        0.0075, 0.0061, 0.0047, 0.0023, 0.0078, 0.0074, 0.0061, 0.0025, 0.0076,\n",
      "        0.0077, 0.0043, 0.0022, 0.2250, 0.2583, 0.2250, 0.1924, 0.0075, 0.0075,\n",
      "        0.0042, 0.0022, 0.0073, 0.0072, 0.0041, 0.0019, 0.2250, 0.1922, 0.2583,\n",
      "        0.2250], grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(-0.0494, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2471, 0.3030, 0.2491, 0.2009],\n",
      "        [0.2471, 0.3028, 0.2491, 0.2010],\n",
      "        [0.2471, 0.3030, 0.2491, 0.2009],\n",
      "        [0.2471, 0.3029, 0.2491, 0.2009],\n",
      "        [0.2471, 0.3030, 0.2491, 0.2009],\n",
      "        [0.2471, 0.3030, 0.2491, 0.2009],\n",
      "        [0.2471, 0.3030, 0.2491, 0.2009],\n",
      "        [0.2471, 0.3029, 0.2491, 0.2009],\n",
      "        [0.2471, 0.3030, 0.2491, 0.2009],\n",
      "        [0.2471, 0.3030, 0.2491, 0.2009],\n",
      "        [0.2471, 0.3029, 0.2491, 0.2009],\n",
      "        [0.2471, 0.3029, 0.2491, 0.2010],\n",
      "        [0.2471, 0.3030, 0.2491, 0.2008],\n",
      "        [0.2471, 0.3030, 0.2491, 0.2008],\n",
      "        [0.2471, 0.3030, 0.2491, 0.2008],\n",
      "        [0.2471, 0.3030, 0.2491, 0.2009],\n",
      "        [0.2471, 0.3029, 0.2491, 0.2009],\n",
      "        [0.2471, 0.3030, 0.2491, 0.2009],\n",
      "        [0.2471, 0.3029, 0.2491, 0.2009],\n",
      "        [0.2471, 0.3029, 0.2491, 0.2010],\n",
      "        [0.2471, 0.3030, 0.2491, 0.2009],\n",
      "        [0.2471, 0.3028, 0.2491, 0.2010],\n",
      "        [0.2471, 0.3030, 0.2491, 0.2009],\n",
      "        [0.2471, 0.3029, 0.2491, 0.2009],\n",
      "        [0.2471, 0.3030, 0.2491, 0.2009],\n",
      "        [0.2471, 0.3030, 0.2491, 0.2009],\n",
      "        [0.2471, 0.3030, 0.2491, 0.2009],\n",
      "        [0.2471, 0.3030, 0.2491, 0.2008],\n",
      "        [0.2471, 0.3030, 0.2491, 0.2009],\n",
      "        [0.2471, 0.3029, 0.2491, 0.2009],\n",
      "        [0.2471, 0.3029, 0.2491, 0.2009],\n",
      "        [0.2471, 0.3030, 0.2491, 0.2009],\n",
      "        [0.2471, 0.3031, 0.2491, 0.2008],\n",
      "        [0.2471, 0.3031, 0.2490, 0.2008],\n",
      "        [0.2471, 0.3031, 0.2490, 0.2008],\n",
      "        [0.2471, 0.3031, 0.2491, 0.2008],\n",
      "        [0.2471, 0.3029, 0.2491, 0.2010],\n",
      "        [0.2471, 0.3029, 0.2491, 0.2009],\n",
      "        [0.2471, 0.3029, 0.2491, 0.2009],\n",
      "        [0.2471, 0.3029, 0.2491, 0.2009],\n",
      "        [0.2471, 0.3029, 0.2491, 0.2009],\n",
      "        [0.2471, 0.3029, 0.2491, 0.2009],\n",
      "        [0.2471, 0.3030, 0.2491, 0.2009],\n",
      "        [0.2471, 0.3029, 0.2491, 0.2009],\n",
      "        [0.2471, 0.3030, 0.2491, 0.2008],\n",
      "        [0.2471, 0.3030, 0.2491, 0.2008],\n",
      "        [0.2471, 0.3030, 0.2491, 0.2009],\n",
      "        [0.2471, 0.3030, 0.2491, 0.2008],\n",
      "        [0.2471, 0.3029, 0.2491, 0.2009],\n",
      "        [0.2471, 0.3029, 0.2491, 0.2009],\n",
      "        [0.2471, 0.3029, 0.2491, 0.2009],\n",
      "        [0.2471, 0.3029, 0.2491, 0.2009],\n",
      "        [0.2471, 0.3030, 0.2491, 0.2009],\n",
      "        [0.2471, 0.3030, 0.2491, 0.2009],\n",
      "        [0.2471, 0.3029, 0.2491, 0.2009],\n",
      "        [0.2471, 0.3029, 0.2491, 0.2009],\n",
      "        [0.2471, 0.3029, 0.2491, 0.2009],\n",
      "        [0.2471, 0.3029, 0.2491, 0.2009],\n",
      "        [0.2471, 0.3029, 0.2491, 0.2009],\n",
      "        [0.2471, 0.3029, 0.2491, 0.2009],\n",
      "        [0.2471, 0.3029, 0.2491, 0.2009],\n",
      "        [0.2471, 0.3029, 0.2491, 0.2009],\n",
      "        [0.2471, 0.3029, 0.2491, 0.2009],\n",
      "        [0.2471, 0.3029, 0.2491, 0.2009]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3759, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(-0.0579, grad_fn=<AddBackward0>)\n",
      "batching here\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(9.8078e-06, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.4010, -1.2102, -1.3829, -1.5863],\n",
      "        [-1.4010, -1.2105, -1.3830, -1.5859],\n",
      "        [-1.4010, -1.2103, -1.3829, -1.5862],\n",
      "        [-1.4010, -1.2103, -1.3829, -1.5862],\n",
      "        [-1.4010, -1.2101, -1.3829, -1.5864],\n",
      "        [-1.4011, -1.2103, -1.3830, -1.5861],\n",
      "        [-1.4010, -1.2100, -1.3829, -1.5866],\n",
      "        [-1.4010, -1.2101, -1.3829, -1.5864],\n",
      "        [-1.4011, -1.2104, -1.3829, -1.5860],\n",
      "        [-1.4011, -1.2103, -1.3829, -1.5860],\n",
      "        [-1.4010, -1.2104, -1.3830, -1.5860],\n",
      "        [-1.4011, -1.2102, -1.3829, -1.5862],\n",
      "        [-1.4011, -1.2102, -1.3829, -1.5863],\n",
      "        [-1.4011, -1.2102, -1.3829, -1.5862],\n",
      "        [-1.4010, -1.2101, -1.3830, -1.5864],\n",
      "        [-1.4011, -1.2103, -1.3830, -1.5861],\n",
      "        [-1.4010, -1.2102, -1.3830, -1.5862],\n",
      "        [-1.4010, -1.2104, -1.3829, -1.5862],\n",
      "        [-1.4010, -1.2102, -1.3829, -1.5863],\n",
      "        [-1.4010, -1.2103, -1.3830, -1.5861],\n",
      "        [-1.4010, -1.2102, -1.3829, -1.5863],\n",
      "        [-1.4010, -1.2105, -1.3830, -1.5859],\n",
      "        [-1.4010, -1.2103, -1.3829, -1.5862],\n",
      "        [-1.4010, -1.2103, -1.3829, -1.5862],\n",
      "        [-1.4011, -1.2102, -1.3829, -1.5862],\n",
      "        [-1.4011, -1.2104, -1.3829, -1.5859],\n",
      "        [-1.4011, -1.2102, -1.3829, -1.5863],\n",
      "        [-1.4011, -1.2102, -1.3830, -1.5863],\n",
      "        [-1.4011, -1.2100, -1.3830, -1.5865],\n",
      "        [-1.4010, -1.2103, -1.3830, -1.5861],\n",
      "        [-1.4011, -1.2102, -1.3829, -1.5862],\n",
      "        [-1.4011, -1.2101, -1.3829, -1.5864],\n",
      "        [-1.4011, -1.2099, -1.3830, -1.5866],\n",
      "        [-1.4012, -1.2098, -1.3829, -1.5868],\n",
      "        [-1.4012, -1.2098, -1.3829, -1.5868],\n",
      "        [-1.4012, -1.2098, -1.3829, -1.5867],\n",
      "        [-1.4010, -1.2103, -1.3830, -1.5861],\n",
      "        [-1.4010, -1.2102, -1.3829, -1.5863],\n",
      "        [-1.4010, -1.2102, -1.3830, -1.5862],\n",
      "        [-1.4010, -1.2103, -1.3829, -1.5863],\n",
      "        [-1.4010, -1.2107, -1.3829, -1.5856],\n",
      "        [-1.4010, -1.2105, -1.3830, -1.5859],\n",
      "        [-1.4010, -1.2099, -1.3830, -1.5867],\n",
      "        [-1.4010, -1.2101, -1.3830, -1.5864],\n",
      "        [-1.4011, -1.2100, -1.3829, -1.5865],\n",
      "        [-1.4012, -1.2100, -1.3830, -1.5865],\n",
      "        [-1.4012, -1.2100, -1.3829, -1.5864],\n",
      "        [-1.4011, -1.2099, -1.3830, -1.5865],\n",
      "        [-1.4011, -1.2101, -1.3829, -1.5864],\n",
      "        [-1.4011, -1.2101, -1.3830, -1.5864],\n",
      "        [-1.4011, -1.2100, -1.3829, -1.5865],\n",
      "        [-1.4011, -1.2102, -1.3829, -1.5863],\n",
      "        [-1.4010, -1.2101, -1.3830, -1.5864],\n",
      "        [-1.4010, -1.2101, -1.3830, -1.5864],\n",
      "        [-1.4010, -1.2104, -1.3830, -1.5860],\n",
      "        [-1.4010, -1.2102, -1.3830, -1.5862],\n",
      "        [-1.4010, -1.2101, -1.3830, -1.5865],\n",
      "        [-1.4010, -1.2103, -1.3830, -1.5862],\n",
      "        [-1.4010, -1.2103, -1.3829, -1.5862],\n",
      "        [-1.4010, -1.2103, -1.3829, -1.5862],\n",
      "        [-1.4010, -1.2104, -1.3829, -1.5861],\n",
      "        [-1.4010, -1.2103, -1.3829, -1.5861],\n",
      "        [-1.4011, -1.2104, -1.3829, -1.5860],\n",
      "        [-1.4010, -1.2102, -1.3829, -1.5864]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([0.0056, 0.0044, 0.0032, 0.0014, 0.0057, 0.0055, 0.0044, 0.0018, 0.0060,\n",
      "        0.0044, 0.0029, 0.0015, 0.0065, 0.0047, 0.0037, 0.0015, 0.0055, 0.0052,\n",
      "        0.0037, 0.0012, 0.0056, 0.0051, 0.0032, 0.0016, 0.0055, 0.0052, 0.0037,\n",
      "        0.0020, 0.0065, 0.0045, 0.0027, 0.0016, 0.0071, 0.0049, 0.0032, 0.0014,\n",
      "        0.0053, 0.0043, 0.0034, 0.0017, 0.0060, 0.0041, 0.0041, 0.0020, 0.0073,\n",
      "        0.0048, 0.0032, 0.0015, 0.0057, 0.0050, 0.0031, 0.0016, 0.0064, 0.0048,\n",
      "        0.0031, 0.0014, 0.0066, 0.0047, 0.0032, 0.0018, 0.0070, 0.0046, 0.0029,\n",
      "        0.0017], grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(-0.0040, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2463, 0.2981, 0.2508, 0.2047],\n",
      "        [0.2463, 0.2981, 0.2508, 0.2048],\n",
      "        [0.2463, 0.2981, 0.2508, 0.2047],\n",
      "        [0.2463, 0.2981, 0.2508, 0.2047],\n",
      "        [0.2463, 0.2982, 0.2508, 0.2047],\n",
      "        [0.2463, 0.2981, 0.2508, 0.2047],\n",
      "        [0.2463, 0.2982, 0.2508, 0.2046],\n",
      "        [0.2463, 0.2982, 0.2508, 0.2047],\n",
      "        [0.2463, 0.2981, 0.2508, 0.2047],\n",
      "        [0.2463, 0.2981, 0.2508, 0.2047],\n",
      "        [0.2463, 0.2981, 0.2508, 0.2047],\n",
      "        [0.2463, 0.2981, 0.2508, 0.2047],\n",
      "        [0.2463, 0.2981, 0.2509, 0.2047],\n",
      "        [0.2463, 0.2981, 0.2508, 0.2047],\n",
      "        [0.2463, 0.2982, 0.2508, 0.2047],\n",
      "        [0.2463, 0.2981, 0.2508, 0.2047],\n",
      "        [0.2463, 0.2981, 0.2508, 0.2047],\n",
      "        [0.2464, 0.2981, 0.2508, 0.2047],\n",
      "        [0.2463, 0.2981, 0.2508, 0.2047],\n",
      "        [0.2463, 0.2981, 0.2508, 0.2047],\n",
      "        [0.2463, 0.2981, 0.2508, 0.2047],\n",
      "        [0.2463, 0.2981, 0.2508, 0.2048],\n",
      "        [0.2463, 0.2981, 0.2508, 0.2047],\n",
      "        [0.2463, 0.2981, 0.2508, 0.2047],\n",
      "        [0.2463, 0.2981, 0.2508, 0.2047],\n",
      "        [0.2463, 0.2981, 0.2508, 0.2048],\n",
      "        [0.2463, 0.2981, 0.2508, 0.2047],\n",
      "        [0.2463, 0.2981, 0.2508, 0.2047],\n",
      "        [0.2463, 0.2982, 0.2508, 0.2046],\n",
      "        [0.2463, 0.2981, 0.2508, 0.2047],\n",
      "        [0.2463, 0.2981, 0.2508, 0.2047],\n",
      "        [0.2463, 0.2982, 0.2509, 0.2047],\n",
      "        [0.2463, 0.2982, 0.2508, 0.2046],\n",
      "        [0.2463, 0.2983, 0.2509, 0.2046],\n",
      "        [0.2463, 0.2983, 0.2508, 0.2046],\n",
      "        [0.2463, 0.2983, 0.2508, 0.2046],\n",
      "        [0.2463, 0.2981, 0.2508, 0.2047],\n",
      "        [0.2463, 0.2981, 0.2508, 0.2047],\n",
      "        [0.2463, 0.2981, 0.2508, 0.2047],\n",
      "        [0.2463, 0.2981, 0.2508, 0.2047],\n",
      "        [0.2463, 0.2980, 0.2508, 0.2048],\n",
      "        [0.2463, 0.2980, 0.2508, 0.2048],\n",
      "        [0.2463, 0.2982, 0.2508, 0.2046],\n",
      "        [0.2463, 0.2982, 0.2508, 0.2047],\n",
      "        [0.2463, 0.2982, 0.2508, 0.2046],\n",
      "        [0.2463, 0.2982, 0.2508, 0.2046],\n",
      "        [0.2463, 0.2982, 0.2508, 0.2047],\n",
      "        [0.2463, 0.2982, 0.2508, 0.2046],\n",
      "        [0.2463, 0.2982, 0.2508, 0.2047],\n",
      "        [0.2463, 0.2982, 0.2508, 0.2047],\n",
      "        [0.2463, 0.2982, 0.2508, 0.2046],\n",
      "        [0.2463, 0.2981, 0.2508, 0.2047],\n",
      "        [0.2463, 0.2982, 0.2508, 0.2047],\n",
      "        [0.2463, 0.2982, 0.2508, 0.2047],\n",
      "        [0.2463, 0.2981, 0.2508, 0.2047],\n",
      "        [0.2463, 0.2981, 0.2508, 0.2047],\n",
      "        [0.2464, 0.2982, 0.2508, 0.2046],\n",
      "        [0.2464, 0.2981, 0.2508, 0.2047],\n",
      "        [0.2463, 0.2981, 0.2508, 0.2047],\n",
      "        [0.2463, 0.2981, 0.2508, 0.2047],\n",
      "        [0.2463, 0.2981, 0.2508, 0.2047],\n",
      "        [0.2463, 0.2981, 0.2508, 0.2047],\n",
      "        [0.2463, 0.2981, 0.2508, 0.2047],\n",
      "        [0.2463, 0.2981, 0.2508, 0.2047]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3775, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(-0.0177, grad_fn=<AddBackward0>)\n",
      "batching here\n",
      "85: done 1 episodes, mean_reward=2.00, best_reward=2.00, speed=228.67\n",
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(0.0609, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.4004, -1.2133, -1.3831, -1.5823],\n",
      "        [-1.4004, -1.2133, -1.3831, -1.5822],\n",
      "        [-1.4004, -1.2133, -1.3831, -1.5822],\n",
      "        [-1.4004, -1.2133, -1.3831, -1.5822],\n",
      "        [-1.4004, -1.2134, -1.3832, -1.5822],\n",
      "        [-1.4004, -1.2131, -1.3831, -1.5826],\n",
      "        [-1.4004, -1.2133, -1.3831, -1.5824],\n",
      "        [-1.4004, -1.2134, -1.3832, -1.5822],\n",
      "        [-1.4005, -1.2130, -1.3832, -1.5826],\n",
      "        [-1.4005, -1.2133, -1.3832, -1.5822],\n",
      "        [-1.4005, -1.2131, -1.3832, -1.5824],\n",
      "        [-1.4005, -1.2132, -1.3832, -1.5823],\n",
      "        [-1.4004, -1.2133, -1.3831, -1.5823],\n",
      "        [-1.4004, -1.2133, -1.3832, -1.5823],\n",
      "        [-1.4004, -1.2135, -1.3832, -1.5820],\n",
      "        [-1.4005, -1.2132, -1.3831, -1.5824],\n",
      "        [-1.4004, -1.2132, -1.3831, -1.5824],\n",
      "        [-1.4005, -1.2132, -1.3832, -1.5823],\n",
      "        [-1.4004, -1.2133, -1.3831, -1.5824],\n",
      "        [-1.4004, -1.2133, -1.3832, -1.5823],\n",
      "        [-1.4004, -1.2133, -1.3831, -1.5823],\n",
      "        [-1.4004, -1.2133, -1.3831, -1.5822],\n",
      "        [-1.4004, -1.2133, -1.3831, -1.5822],\n",
      "        [-1.4004, -1.2133, -1.3831, -1.5822],\n",
      "        [-1.4005, -1.2133, -1.3831, -1.5823],\n",
      "        [-1.4005, -1.2134, -1.3832, -1.5821],\n",
      "        [-1.4004, -1.2130, -1.3832, -1.5827],\n",
      "        [-1.4004, -1.2131, -1.3831, -1.5827],\n",
      "        [-1.4005, -1.2131, -1.3831, -1.5824],\n",
      "        [-1.4005, -1.2133, -1.3832, -1.5823],\n",
      "        [-1.4004, -1.2134, -1.3831, -1.5822],\n",
      "        [-1.4004, -1.2133, -1.3831, -1.5823],\n",
      "        [-1.4006, -1.2128, -1.3831, -1.5828],\n",
      "        [-1.4006, -1.2128, -1.3831, -1.5828],\n",
      "        [-1.4006, -1.2128, -1.3831, -1.5828],\n",
      "        [-1.4005, -1.2131, -1.3831, -1.5825],\n",
      "        [-1.4004, -1.2133, -1.3832, -1.5823],\n",
      "        [-1.4004, -1.2133, -1.3832, -1.5823],\n",
      "        [-1.4004, -1.2133, -1.3832, -1.5822],\n",
      "        [-1.4004, -1.2133, -1.3831, -1.5822],\n",
      "        [-1.4005, -1.2133, -1.3831, -1.5822],\n",
      "        [-1.4004, -1.2135, -1.3832, -1.5820],\n",
      "        [-1.4005, -1.2132, -1.3831, -1.5824],\n",
      "        [-1.4004, -1.2133, -1.3831, -1.5823],\n",
      "        [-1.4006, -1.2130, -1.3832, -1.5825],\n",
      "        [-1.4005, -1.2130, -1.3831, -1.5826],\n",
      "        [-1.4005, -1.2129, -1.3832, -1.5827],\n",
      "        [-1.4006, -1.2128, -1.3832, -1.5828],\n",
      "        [-1.4004, -1.2133, -1.3831, -1.5822],\n",
      "        [-1.4005, -1.2132, -1.3831, -1.5824],\n",
      "        [-1.4005, -1.2131, -1.3832, -1.5825],\n",
      "        [-1.4005, -1.2134, -1.3831, -1.5821],\n",
      "        [-1.4005, -1.2131, -1.3831, -1.5825],\n",
      "        [-1.4005, -1.2132, -1.3832, -1.5824],\n",
      "        [-1.4004, -1.2133, -1.3832, -1.5823],\n",
      "        [-1.4004, -1.2133, -1.3832, -1.5823],\n",
      "        [-1.4005, -1.2133, -1.3831, -1.5823],\n",
      "        [-1.4005, -1.2132, -1.3831, -1.5823],\n",
      "        [-1.4005, -1.2133, -1.3831, -1.5823],\n",
      "        [-1.4005, -1.2131, -1.3831, -1.5825],\n",
      "        [-1.4004, -1.2133, -1.3831, -1.5823],\n",
      "        [-1.4005, -1.2132, -1.3831, -1.5824],\n",
      "        [-1.4005, -1.2132, -1.3831, -1.5823],\n",
      "        [-1.4005, -1.2133, -1.3831, -1.5822]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([ 5.3302e-03,  4.5504e-03,  2.6706e-03,  1.7500e-03,  5.2874e-03,\n",
      "         4.3782e-03,  3.2652e-03,  1.5491e-03, -1.1715e+00, -1.1851e+00,\n",
      "        -1.5631e+00, -1.3989e+00,  7.1726e-03,  4.5749e-03,  2.9565e-03,\n",
      "         2.0535e-03,  6.2227e-03,  5.3508e-03,  2.7773e-03,  1.3120e-03,\n",
      "         6.0764e-03,  4.5504e-03,  3.4826e-03,  1.5490e-03,  6.5978e-03,\n",
      "         3.6196e-03,  2.9074e-03,  1.4347e-03,  6.1167e-03,  4.9957e-03,\n",
      "         2.4289e-03,  1.2005e-03,  1.5344e-01,  1.3450e-01,  1.3446e-01,\n",
      "         1.5770e-03,  6.0651e-03,  5.2800e-03,  3.0988e-03,  1.3567e-03,\n",
      "         5.9782e-03,  4.8557e-03,  2.8178e-03,  1.3618e-03,  5.1739e-03,\n",
      "         4.4117e-03,  3.1153e-03,  1.7699e-03,  6.2515e-03,  4.3130e-03,\n",
      "         3.0433e-03,  1.5906e-03,  6.2078e-03,  4.6390e-03,  2.5711e-03,\n",
      "         1.2578e-03,  6.8791e-03,  5.1412e-03,  2.9618e-03,  1.6971e-03,\n",
      "         6.0982e-03,  4.1150e-03,  2.7679e-03,  1.4504e-03],\n",
      "       grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(0.0732, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2465, 0.2972, 0.2508, 0.2055],\n",
      "        [0.2465, 0.2972, 0.2508, 0.2055],\n",
      "        [0.2465, 0.2972, 0.2508, 0.2055],\n",
      "        [0.2465, 0.2972, 0.2508, 0.2055],\n",
      "        [0.2465, 0.2972, 0.2508, 0.2055],\n",
      "        [0.2465, 0.2973, 0.2508, 0.2054],\n",
      "        [0.2465, 0.2972, 0.2508, 0.2055],\n",
      "        [0.2465, 0.2972, 0.2508, 0.2055],\n",
      "        [0.2465, 0.2973, 0.2508, 0.2054],\n",
      "        [0.2465, 0.2972, 0.2508, 0.2055],\n",
      "        [0.2465, 0.2973, 0.2508, 0.2055],\n",
      "        [0.2465, 0.2972, 0.2508, 0.2055],\n",
      "        [0.2465, 0.2972, 0.2508, 0.2055],\n",
      "        [0.2465, 0.2972, 0.2508, 0.2055],\n",
      "        [0.2465, 0.2972, 0.2508, 0.2056],\n",
      "        [0.2465, 0.2972, 0.2508, 0.2055],\n",
      "        [0.2465, 0.2972, 0.2508, 0.2055],\n",
      "        [0.2465, 0.2972, 0.2508, 0.2055],\n",
      "        [0.2465, 0.2972, 0.2508, 0.2055],\n",
      "        [0.2465, 0.2972, 0.2508, 0.2055],\n",
      "        [0.2465, 0.2972, 0.2508, 0.2055],\n",
      "        [0.2465, 0.2972, 0.2508, 0.2055],\n",
      "        [0.2465, 0.2972, 0.2508, 0.2055],\n",
      "        [0.2465, 0.2972, 0.2508, 0.2055],\n",
      "        [0.2465, 0.2972, 0.2508, 0.2055],\n",
      "        [0.2465, 0.2972, 0.2508, 0.2055],\n",
      "        [0.2465, 0.2973, 0.2508, 0.2054],\n",
      "        [0.2465, 0.2973, 0.2508, 0.2054],\n",
      "        [0.2465, 0.2973, 0.2508, 0.2055],\n",
      "        [0.2465, 0.2972, 0.2508, 0.2055],\n",
      "        [0.2465, 0.2972, 0.2508, 0.2055],\n",
      "        [0.2465, 0.2972, 0.2508, 0.2055],\n",
      "        [0.2465, 0.2974, 0.2508, 0.2054],\n",
      "        [0.2464, 0.2974, 0.2508, 0.2054],\n",
      "        [0.2465, 0.2974, 0.2508, 0.2054],\n",
      "        [0.2465, 0.2973, 0.2508, 0.2055],\n",
      "        [0.2465, 0.2972, 0.2508, 0.2055],\n",
      "        [0.2465, 0.2972, 0.2508, 0.2055],\n",
      "        [0.2465, 0.2972, 0.2508, 0.2055],\n",
      "        [0.2465, 0.2972, 0.2508, 0.2055],\n",
      "        [0.2465, 0.2972, 0.2508, 0.2055],\n",
      "        [0.2465, 0.2972, 0.2508, 0.2056],\n",
      "        [0.2465, 0.2972, 0.2508, 0.2055],\n",
      "        [0.2465, 0.2972, 0.2508, 0.2055],\n",
      "        [0.2465, 0.2973, 0.2508, 0.2055],\n",
      "        [0.2465, 0.2973, 0.2508, 0.2054],\n",
      "        [0.2465, 0.2973, 0.2508, 0.2054],\n",
      "        [0.2465, 0.2974, 0.2508, 0.2054],\n",
      "        [0.2465, 0.2972, 0.2508, 0.2055],\n",
      "        [0.2465, 0.2973, 0.2508, 0.2055],\n",
      "        [0.2465, 0.2973, 0.2508, 0.2055],\n",
      "        [0.2465, 0.2972, 0.2508, 0.2055],\n",
      "        [0.2465, 0.2973, 0.2508, 0.2055],\n",
      "        [0.2465, 0.2972, 0.2508, 0.2055],\n",
      "        [0.2465, 0.2972, 0.2508, 0.2055],\n",
      "        [0.2465, 0.2972, 0.2508, 0.2055],\n",
      "        [0.2465, 0.2972, 0.2508, 0.2055],\n",
      "        [0.2465, 0.2972, 0.2508, 0.2055],\n",
      "        [0.2465, 0.2972, 0.2508, 0.2055],\n",
      "        [0.2465, 0.2973, 0.2508, 0.2055],\n",
      "        [0.2465, 0.2972, 0.2508, 0.2055],\n",
      "        [0.2465, 0.2972, 0.2508, 0.2055],\n",
      "        [0.2465, 0.2972, 0.2508, 0.2055],\n",
      "        [0.2465, 0.2972, 0.2508, 0.2055]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3778, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(0.1203, grad_fn=<AddBackward0>)\n",
      "batching here\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86: done 4 episodes, mean_reward=0.00, best_reward=2.00, speed=230.09\n",
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(0.0022, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.4015, -1.1904, -1.3972, -1.5976],\n",
      "        [-1.4015, -1.1904, -1.3972, -1.5976],\n",
      "        [-1.4015, -1.1904, -1.3972, -1.5976],\n",
      "        [-1.4015, -1.1903, -1.3972, -1.5978],\n",
      "        [-1.4014, -1.1905, -1.3972, -1.5976],\n",
      "        [-1.4014, -1.1903, -1.3973, -1.5978],\n",
      "        [-1.4014, -1.1903, -1.3972, -1.5978],\n",
      "        [-1.4014, -1.1903, -1.3972, -1.5978],\n",
      "        [-1.4015, -1.1904, -1.3972, -1.5976],\n",
      "        [-1.4015, -1.1903, -1.3972, -1.5976],\n",
      "        [-1.4015, -1.1901, -1.3973, -1.5978],\n",
      "        [-1.4015, -1.1903, -1.3972, -1.5977],\n",
      "        [-1.4014, -1.1905, -1.3972, -1.5975],\n",
      "        [-1.4015, -1.1904, -1.3972, -1.5976],\n",
      "        [-1.4014, -1.1905, -1.3972, -1.5975],\n",
      "        [-1.4014, -1.1904, -1.3972, -1.5977],\n",
      "        [-1.4014, -1.1904, -1.3972, -1.5976],\n",
      "        [-1.4014, -1.1904, -1.3972, -1.5976],\n",
      "        [-1.4015, -1.1904, -1.3972, -1.5976],\n",
      "        [-1.4015, -1.1904, -1.3972, -1.5976],\n",
      "        [-1.4015, -1.1904, -1.3972, -1.5976],\n",
      "        [-1.4015, -1.1904, -1.3972, -1.5976],\n",
      "        [-1.4015, -1.1904, -1.3972, -1.5976],\n",
      "        [-1.4015, -1.1904, -1.3972, -1.5976],\n",
      "        [-1.4014, -1.1902, -1.3972, -1.5980],\n",
      "        [-1.4014, -1.1903, -1.3973, -1.5977],\n",
      "        [-1.4014, -1.1903, -1.3972, -1.5979],\n",
      "        [-1.4014, -1.1903, -1.3972, -1.5978],\n",
      "        [-1.4015, -1.1903, -1.3972, -1.5978],\n",
      "        [-1.4015, -1.1905, -1.3972, -1.5975],\n",
      "        [-1.4014, -1.1903, -1.3972, -1.5977],\n",
      "        [-1.4015, -1.1904, -1.3972, -1.5976],\n",
      "        [-1.4015, -1.1903, -1.3972, -1.5977],\n",
      "        [-1.4015, -1.1902, -1.3972, -1.5978],\n",
      "        [-1.4015, -1.1903, -1.3972, -1.5976],\n",
      "        [-1.4014, -1.1906, -1.3972, -1.5974],\n",
      "        [-1.4015, -1.1904, -1.3972, -1.5976],\n",
      "        [-1.4015, -1.1904, -1.3972, -1.5976],\n",
      "        [-1.4015, -1.1904, -1.3972, -1.5976],\n",
      "        [-1.4015, -1.1903, -1.3973, -1.5976],\n",
      "        [-1.4015, -1.1904, -1.3972, -1.5976],\n",
      "        [-1.4015, -1.1904, -1.3972, -1.5976],\n",
      "        [-1.4015, -1.1904, -1.3972, -1.5976],\n",
      "        [-1.4015, -1.1905, -1.3972, -1.5975],\n",
      "        [-1.4016, -1.1901, -1.3972, -1.5979],\n",
      "        [-1.4016, -1.1901, -1.3973, -1.5979],\n",
      "        [-1.4016, -1.1899, -1.3973, -1.5981],\n",
      "        [-1.4016, -1.1899, -1.3973, -1.5981],\n",
      "        [-1.4015, -1.1906, -1.3972, -1.5973],\n",
      "        [-1.4014, -1.1904, -1.3972, -1.5977],\n",
      "        [-1.4014, -1.1905, -1.3972, -1.5976],\n",
      "        [-1.4015, -1.1903, -1.3972, -1.5978],\n",
      "        [-1.4015, -1.1903, -1.3973, -1.5977],\n",
      "        [-1.4015, -1.1902, -1.3972, -1.5979],\n",
      "        [-1.4015, -1.1904, -1.3972, -1.5976],\n",
      "        [-1.4015, -1.1903, -1.3972, -1.5978],\n",
      "        [-1.4014, -1.1904, -1.3972, -1.5977],\n",
      "        [-1.4015, -1.1904, -1.3972, -1.5975],\n",
      "        [-1.4015, -1.1904, -1.3973, -1.5976],\n",
      "        [-1.4015, -1.1905, -1.3972, -1.5975],\n",
      "        [-1.4014, -1.1904, -1.3972, -1.5976],\n",
      "        [-1.4014, -1.1903, -1.3972, -1.5977],\n",
      "        [-1.4015, -1.1903, -1.3972, -1.5978],\n",
      "        [-1.4014, -1.1906, -1.3972, -1.5974]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([0.2158, 0.0063, 0.0049, 0.0021, 0.0069, 0.0055, 0.0036, 0.0018, 0.0085,\n",
      "        0.0064, 0.0044, 0.0019, 0.0096, 0.0064, 0.0035, 0.0020, 0.0085, 0.0064,\n",
      "        0.0043, 0.0025, 0.2152, 0.0065, 0.0038, 0.0025, 0.0088, 0.0052, 0.0051,\n",
      "        0.0018, 0.0088, 0.0055, 0.0053, 0.0023, 0.0086, 0.0067, 0.0043, 0.0019,\n",
      "        0.1833, 0.1833, 0.2152, 0.0022, 0.0085, 0.0072, 0.0047, 0.0017, 0.0096,\n",
      "        0.0062, 0.0051, 0.0019, 0.0072, 0.0068, 0.0038, 0.0030, 0.0072, 0.0056,\n",
      "        0.0036, 0.0023, 0.1835, 0.0074, 0.0037, 0.0022, 0.0096, 0.0075, 0.0038,\n",
      "        0.0020], grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(-0.0233, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2462, 0.3041, 0.2473, 0.2024],\n",
      "        [0.2462, 0.3041, 0.2473, 0.2024],\n",
      "        [0.2462, 0.3041, 0.2473, 0.2024],\n",
      "        [0.2462, 0.3041, 0.2473, 0.2023],\n",
      "        [0.2462, 0.3041, 0.2473, 0.2024],\n",
      "        [0.2463, 0.3041, 0.2473, 0.2023],\n",
      "        [0.2462, 0.3041, 0.2473, 0.2023],\n",
      "        [0.2462, 0.3041, 0.2473, 0.2023],\n",
      "        [0.2462, 0.3041, 0.2473, 0.2024],\n",
      "        [0.2462, 0.3041, 0.2473, 0.2024],\n",
      "        [0.2462, 0.3042, 0.2473, 0.2023],\n",
      "        [0.2462, 0.3041, 0.2473, 0.2024],\n",
      "        [0.2462, 0.3041, 0.2473, 0.2024],\n",
      "        [0.2462, 0.3041, 0.2473, 0.2024],\n",
      "        [0.2462, 0.3041, 0.2473, 0.2024],\n",
      "        [0.2462, 0.3041, 0.2473, 0.2024],\n",
      "        [0.2462, 0.3041, 0.2473, 0.2024],\n",
      "        [0.2462, 0.3041, 0.2473, 0.2024],\n",
      "        [0.2462, 0.3041, 0.2473, 0.2024],\n",
      "        [0.2462, 0.3041, 0.2473, 0.2024],\n",
      "        [0.2462, 0.3041, 0.2473, 0.2024],\n",
      "        [0.2462, 0.3041, 0.2473, 0.2024],\n",
      "        [0.2462, 0.3041, 0.2473, 0.2024],\n",
      "        [0.2462, 0.3041, 0.2473, 0.2024],\n",
      "        [0.2462, 0.3042, 0.2473, 0.2023],\n",
      "        [0.2462, 0.3041, 0.2473, 0.2024],\n",
      "        [0.2462, 0.3041, 0.2473, 0.2023],\n",
      "        [0.2462, 0.3041, 0.2473, 0.2024],\n",
      "        [0.2462, 0.3041, 0.2473, 0.2023],\n",
      "        [0.2462, 0.3041, 0.2473, 0.2024],\n",
      "        [0.2462, 0.3041, 0.2473, 0.2024],\n",
      "        [0.2462, 0.3041, 0.2473, 0.2024],\n",
      "        [0.2462, 0.3041, 0.2473, 0.2024],\n",
      "        [0.2462, 0.3042, 0.2473, 0.2023],\n",
      "        [0.2462, 0.3041, 0.2473, 0.2024],\n",
      "        [0.2462, 0.3040, 0.2473, 0.2024],\n",
      "        [0.2462, 0.3041, 0.2473, 0.2024],\n",
      "        [0.2462, 0.3041, 0.2473, 0.2024],\n",
      "        [0.2462, 0.3041, 0.2473, 0.2024],\n",
      "        [0.2462, 0.3041, 0.2473, 0.2024],\n",
      "        [0.2462, 0.3041, 0.2473, 0.2024],\n",
      "        [0.2462, 0.3041, 0.2473, 0.2024],\n",
      "        [0.2462, 0.3041, 0.2473, 0.2024],\n",
      "        [0.2462, 0.3041, 0.2473, 0.2024],\n",
      "        [0.2462, 0.3042, 0.2473, 0.2023],\n",
      "        [0.2462, 0.3042, 0.2473, 0.2023],\n",
      "        [0.2462, 0.3042, 0.2473, 0.2023],\n",
      "        [0.2462, 0.3042, 0.2473, 0.2023],\n",
      "        [0.2462, 0.3040, 0.2473, 0.2024],\n",
      "        [0.2462, 0.3041, 0.2473, 0.2024],\n",
      "        [0.2462, 0.3041, 0.2473, 0.2024],\n",
      "        [0.2462, 0.3041, 0.2473, 0.2023],\n",
      "        [0.2462, 0.3041, 0.2473, 0.2024],\n",
      "        [0.2462, 0.3042, 0.2473, 0.2023],\n",
      "        [0.2462, 0.3041, 0.2473, 0.2024],\n",
      "        [0.2462, 0.3041, 0.2473, 0.2023],\n",
      "        [0.2463, 0.3041, 0.2473, 0.2024],\n",
      "        [0.2462, 0.3041, 0.2473, 0.2024],\n",
      "        [0.2462, 0.3041, 0.2473, 0.2024],\n",
      "        [0.2462, 0.3041, 0.2473, 0.2024],\n",
      "        [0.2462, 0.3041, 0.2473, 0.2024],\n",
      "        [0.2462, 0.3041, 0.2473, 0.2024],\n",
      "        [0.2462, 0.3041, 0.2473, 0.2023],\n",
      "        [0.2462, 0.3040, 0.2473, 0.2024]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3759, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(-0.0348, grad_fn=<AddBackward0>)\n",
      "batching here\n",
      "87: done 2 episodes, mean_reward=0.50, best_reward=2.00, speed=231.52\n",
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(0.0012, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.3965, -1.2148, -1.3976, -1.5675],\n",
      "        [-1.3964, -1.2149, -1.3976, -1.5674],\n",
      "        [-1.3964, -1.2147, -1.3976, -1.5676],\n",
      "        [-1.3964, -1.2146, -1.3976, -1.5677],\n",
      "        [-1.3964, -1.2146, -1.3976, -1.5678],\n",
      "        [-1.3964, -1.2146, -1.3976, -1.5678],\n",
      "        [-1.3964, -1.2147, -1.3976, -1.5677],\n",
      "        [-1.3964, -1.2146, -1.3976, -1.5678],\n",
      "        [-1.3965, -1.2145, -1.3977, -1.5677],\n",
      "        [-1.3965, -1.2147, -1.3976, -1.5675],\n",
      "        [-1.3965, -1.2146, -1.3976, -1.5677],\n",
      "        [-1.3965, -1.2145, -1.3977, -1.5678],\n",
      "        [-1.3964, -1.2147, -1.3976, -1.5676],\n",
      "        [-1.3964, -1.2147, -1.3976, -1.5676],\n",
      "        [-1.3964, -1.2147, -1.3976, -1.5677],\n",
      "        [-1.3964, -1.2147, -1.3976, -1.5677],\n",
      "        [-1.3964, -1.2147, -1.3976, -1.5676],\n",
      "        [-1.3964, -1.2147, -1.3976, -1.5676],\n",
      "        [-1.3964, -1.2147, -1.3976, -1.5676],\n",
      "        [-1.3964, -1.2147, -1.3976, -1.5676],\n",
      "        [-1.3964, -1.2147, -1.3976, -1.5677],\n",
      "        [-1.3964, -1.2148, -1.3976, -1.5675],\n",
      "        [-1.3964, -1.2147, -1.3976, -1.5676],\n",
      "        [-1.3964, -1.2147, -1.3976, -1.5676],\n",
      "        [-1.3964, -1.2147, -1.3976, -1.5677],\n",
      "        [-1.3964, -1.2147, -1.3976, -1.5677],\n",
      "        [-1.3964, -1.2146, -1.3976, -1.5677],\n",
      "        [-1.3964, -1.2146, -1.3976, -1.5677],\n",
      "        [-1.3964, -1.2148, -1.3976, -1.5675],\n",
      "        [-1.3965, -1.2144, -1.3976, -1.5680],\n",
      "        [-1.3964, -1.2152, -1.3976, -1.5670],\n",
      "        [-1.3964, -1.2147, -1.3976, -1.5676],\n",
      "        [-1.3964, -1.2147, -1.3976, -1.5676],\n",
      "        [-1.3964, -1.2146, -1.3976, -1.5677],\n",
      "        [-1.3964, -1.2148, -1.3976, -1.5675],\n",
      "        [-1.3965, -1.2148, -1.3976, -1.5674],\n",
      "        [-1.3964, -1.2147, -1.3976, -1.5676],\n",
      "        [-1.3964, -1.2147, -1.3976, -1.5676],\n",
      "        [-1.3964, -1.2148, -1.3976, -1.5675],\n",
      "        [-1.3964, -1.2147, -1.3976, -1.5677],\n",
      "        [-1.3965, -1.2147, -1.3976, -1.5676],\n",
      "        [-1.3965, -1.2147, -1.3976, -1.5676],\n",
      "        [-1.3965, -1.2147, -1.3976, -1.5676],\n",
      "        [-1.3965, -1.2146, -1.3976, -1.5678],\n",
      "        [-1.3965, -1.2143, -1.3976, -1.5681],\n",
      "        [-1.3965, -1.2143, -1.3977, -1.5680],\n",
      "        [-1.3965, -1.2143, -1.3977, -1.5680],\n",
      "        [-1.3965, -1.2143, -1.3977, -1.5680],\n",
      "        [-1.3964, -1.2147, -1.3977, -1.5676],\n",
      "        [-1.3964, -1.2147, -1.3976, -1.5677],\n",
      "        [-1.3964, -1.2146, -1.3976, -1.5677],\n",
      "        [-1.3964, -1.2147, -1.3976, -1.5677],\n",
      "        [-1.3965, -1.2146, -1.3976, -1.5677],\n",
      "        [-1.3964, -1.2147, -1.3976, -1.5676],\n",
      "        [-1.3965, -1.2143, -1.3977, -1.5681],\n",
      "        [-1.3964, -1.2151, -1.3976, -1.5671],\n",
      "        [-1.3964, -1.2147, -1.3976, -1.5676],\n",
      "        [-1.3964, -1.2146, -1.3976, -1.5678],\n",
      "        [-1.3964, -1.2146, -1.3977, -1.5678],\n",
      "        [-1.3964, -1.2146, -1.3976, -1.5678],\n",
      "        [-1.3964, -1.2147, -1.3976, -1.5677],\n",
      "        [-1.3964, -1.2147, -1.3976, -1.5677],\n",
      "        [-1.3965, -1.2146, -1.3976, -1.5677],\n",
      "        [-1.3965, -1.2146, -1.3976, -1.5677]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([0.0076, 0.0050, 0.0031, 0.0018, 0.0059, 0.0044, 0.0033, 0.0016, 0.0072,\n",
      "        0.0047, 0.0032, 0.0017, 0.0059, 0.0051, 0.0035, 0.0018, 0.1710, 0.1488,\n",
      "        0.0027, 0.0013, 0.0069, 0.0052, 0.0037, 0.0016, 0.0060, 0.0058, 0.0030,\n",
      "        0.0017, 0.0066, 0.0047, 0.0026, 0.0016, 0.0058, 0.0057, 0.0027, 0.0012,\n",
      "        0.0060, 0.0052, 0.0030, 0.0019, 0.1713, 0.1712, 0.1714, 0.0018, 0.0059,\n",
      "        0.0051, 0.0034, 0.0019, 0.0066, 0.0053, 0.0031, 0.0016, 0.0068, 0.0057,\n",
      "        0.0033, 0.0009, 0.0066, 0.0052, 0.0036, 0.0018, 0.0074, 0.0050, 0.0039,\n",
      "        0.0016], grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(-0.0167, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2475, 0.2968, 0.2472, 0.2086],\n",
      "        [0.2475, 0.2967, 0.2472, 0.2086],\n",
      "        [0.2475, 0.2968, 0.2472, 0.2085],\n",
      "        [0.2475, 0.2968, 0.2472, 0.2085],\n",
      "        [0.2475, 0.2968, 0.2472, 0.2085],\n",
      "        [0.2475, 0.2968, 0.2472, 0.2085],\n",
      "        [0.2475, 0.2968, 0.2472, 0.2085],\n",
      "        [0.2475, 0.2968, 0.2472, 0.2085],\n",
      "        [0.2475, 0.2968, 0.2472, 0.2085],\n",
      "        [0.2475, 0.2968, 0.2472, 0.2086],\n",
      "        [0.2475, 0.2968, 0.2472, 0.2085],\n",
      "        [0.2475, 0.2969, 0.2472, 0.2085],\n",
      "        [0.2475, 0.2968, 0.2472, 0.2085],\n",
      "        [0.2475, 0.2968, 0.2472, 0.2085],\n",
      "        [0.2475, 0.2968, 0.2472, 0.2085],\n",
      "        [0.2475, 0.2968, 0.2472, 0.2085],\n",
      "        [0.2475, 0.2968, 0.2472, 0.2085],\n",
      "        [0.2475, 0.2968, 0.2472, 0.2085],\n",
      "        [0.2475, 0.2968, 0.2472, 0.2085],\n",
      "        [0.2475, 0.2968, 0.2472, 0.2085],\n",
      "        [0.2475, 0.2968, 0.2472, 0.2085],\n",
      "        [0.2475, 0.2968, 0.2472, 0.2086],\n",
      "        [0.2475, 0.2968, 0.2472, 0.2085],\n",
      "        [0.2475, 0.2968, 0.2472, 0.2085],\n",
      "        [0.2475, 0.2968, 0.2472, 0.2085],\n",
      "        [0.2475, 0.2968, 0.2472, 0.2085],\n",
      "        [0.2475, 0.2968, 0.2472, 0.2085],\n",
      "        [0.2475, 0.2968, 0.2472, 0.2085],\n",
      "        [0.2475, 0.2968, 0.2472, 0.2086],\n",
      "        [0.2475, 0.2969, 0.2472, 0.2085],\n",
      "        [0.2475, 0.2966, 0.2472, 0.2087],\n",
      "        [0.2475, 0.2968, 0.2472, 0.2085],\n",
      "        [0.2475, 0.2968, 0.2472, 0.2085],\n",
      "        [0.2475, 0.2968, 0.2472, 0.2085],\n",
      "        [0.2475, 0.2968, 0.2472, 0.2086],\n",
      "        [0.2475, 0.2968, 0.2472, 0.2086],\n",
      "        [0.2475, 0.2968, 0.2472, 0.2085],\n",
      "        [0.2475, 0.2968, 0.2472, 0.2085],\n",
      "        [0.2475, 0.2968, 0.2472, 0.2086],\n",
      "        [0.2475, 0.2968, 0.2472, 0.2085],\n",
      "        [0.2475, 0.2968, 0.2472, 0.2085],\n",
      "        [0.2475, 0.2968, 0.2472, 0.2085],\n",
      "        [0.2475, 0.2968, 0.2472, 0.2085],\n",
      "        [0.2475, 0.2968, 0.2472, 0.2085],\n",
      "        [0.2474, 0.2969, 0.2472, 0.2084],\n",
      "        [0.2475, 0.2969, 0.2472, 0.2085],\n",
      "        [0.2475, 0.2969, 0.2472, 0.2085],\n",
      "        [0.2475, 0.2969, 0.2472, 0.2085],\n",
      "        [0.2475, 0.2968, 0.2472, 0.2085],\n",
      "        [0.2475, 0.2968, 0.2472, 0.2085],\n",
      "        [0.2475, 0.2968, 0.2472, 0.2085],\n",
      "        [0.2475, 0.2968, 0.2472, 0.2085],\n",
      "        [0.2475, 0.2968, 0.2472, 0.2085],\n",
      "        [0.2475, 0.2968, 0.2472, 0.2085],\n",
      "        [0.2475, 0.2969, 0.2472, 0.2084],\n",
      "        [0.2475, 0.2967, 0.2472, 0.2086],\n",
      "        [0.2475, 0.2968, 0.2472, 0.2085],\n",
      "        [0.2475, 0.2968, 0.2472, 0.2085],\n",
      "        [0.2475, 0.2968, 0.2472, 0.2085],\n",
      "        [0.2475, 0.2968, 0.2472, 0.2085],\n",
      "        [0.2475, 0.2968, 0.2472, 0.2085],\n",
      "        [0.2475, 0.2968, 0.2472, 0.2085],\n",
      "        [0.2475, 0.2968, 0.2472, 0.2085],\n",
      "        [0.2475, 0.2968, 0.2472, 0.2085]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3785, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(-0.0293, grad_fn=<AddBackward0>)\n",
      "batching here\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88: done 3 episodes, mean_reward=0.67, best_reward=2.00, speed=235.94\n",
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(0.0016, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.3999, -1.2226, -1.3967, -1.5536],\n",
      "        [-1.3999, -1.2225, -1.3968, -1.5536],\n",
      "        [-1.3999, -1.2223, -1.3968, -1.5538],\n",
      "        [-1.3998, -1.2224, -1.3967, -1.5538],\n",
      "        [-1.3998, -1.2223, -1.3968, -1.5539],\n",
      "        [-1.3998, -1.2223, -1.3968, -1.5540],\n",
      "        [-1.3998, -1.2224, -1.3968, -1.5539],\n",
      "        [-1.3998, -1.2224, -1.3968, -1.5538],\n",
      "        [-1.4000, -1.2222, -1.3968, -1.5540],\n",
      "        [-1.4000, -1.2220, -1.3968, -1.5541],\n",
      "        [-1.3999, -1.2222, -1.3968, -1.5539],\n",
      "        [-1.4000, -1.2225, -1.3967, -1.5536],\n",
      "        [-1.3999, -1.2225, -1.3967, -1.5537],\n",
      "        [-1.3999, -1.2224, -1.3967, -1.5537],\n",
      "        [-1.3999, -1.2224, -1.3967, -1.5537],\n",
      "        [-1.3999, -1.2224, -1.3967, -1.5537],\n",
      "        [-1.3999, -1.2223, -1.3968, -1.5539],\n",
      "        [-1.3999, -1.2225, -1.3967, -1.5537],\n",
      "        [-1.3998, -1.2226, -1.3968, -1.5535],\n",
      "        [-1.3999, -1.2224, -1.3967, -1.5538],\n",
      "        [-1.3999, -1.2226, -1.3967, -1.5535],\n",
      "        [-1.3999, -1.2223, -1.3967, -1.5539],\n",
      "        [-1.3999, -1.2225, -1.3967, -1.5536],\n",
      "        [-1.3999, -1.2224, -1.3968, -1.5538],\n",
      "        [-1.3999, -1.2224, -1.3968, -1.5538],\n",
      "        [-1.3999, -1.2224, -1.3968, -1.5538],\n",
      "        [-1.3998, -1.2224, -1.3968, -1.5538],\n",
      "        [-1.3998, -1.2225, -1.3968, -1.5536],\n",
      "        [-1.3999, -1.2224, -1.3967, -1.5538],\n",
      "        [-1.3998, -1.2224, -1.3967, -1.5539],\n",
      "        [-1.3999, -1.2224, -1.3967, -1.5539],\n",
      "        [-1.3999, -1.2226, -1.3967, -1.5536],\n",
      "        [-1.3999, -1.2224, -1.3968, -1.5538],\n",
      "        [-1.3998, -1.2224, -1.3967, -1.5538],\n",
      "        [-1.3999, -1.2223, -1.3967, -1.5539],\n",
      "        [-1.3999, -1.2223, -1.3968, -1.5538],\n",
      "        [-1.3999, -1.2224, -1.3968, -1.5537],\n",
      "        [-1.3999, -1.2223, -1.3967, -1.5539],\n",
      "        [-1.3998, -1.2227, -1.3968, -1.5534],\n",
      "        [-1.3999, -1.2222, -1.3968, -1.5540],\n",
      "        [-1.3999, -1.2224, -1.3967, -1.5538],\n",
      "        [-1.3999, -1.2223, -1.3967, -1.5538],\n",
      "        [-1.3999, -1.2223, -1.3968, -1.5539],\n",
      "        [-1.3999, -1.2224, -1.3968, -1.5538],\n",
      "        [-1.4000, -1.2220, -1.3968, -1.5542],\n",
      "        [-1.3999, -1.2223, -1.3967, -1.5539],\n",
      "        [-1.3999, -1.2224, -1.3967, -1.5538],\n",
      "        [-1.3999, -1.2223, -1.3967, -1.5538],\n",
      "        [-1.3999, -1.2224, -1.3968, -1.5538],\n",
      "        [-1.3998, -1.2224, -1.3968, -1.5538],\n",
      "        [-1.3998, -1.2224, -1.3968, -1.5538],\n",
      "        [-1.3999, -1.2224, -1.3967, -1.5537],\n",
      "        [-1.3999, -1.2223, -1.3968, -1.5538],\n",
      "        [-1.3999, -1.2223, -1.3968, -1.5539],\n",
      "        [-1.3998, -1.2223, -1.3968, -1.5539],\n",
      "        [-1.3999, -1.2223, -1.3967, -1.5540],\n",
      "        [-1.3999, -1.2224, -1.3968, -1.5538],\n",
      "        [-1.3998, -1.2223, -1.3968, -1.5539],\n",
      "        [-1.3998, -1.2222, -1.3968, -1.5540],\n",
      "        [-1.3999, -1.2223, -1.3968, -1.5538],\n",
      "        [-1.3999, -1.2223, -1.3968, -1.5539],\n",
      "        [-1.3999, -1.2223, -1.3967, -1.5539],\n",
      "        [-1.3999, -1.2224, -1.3968, -1.5538],\n",
      "        [-1.3999, -1.2223, -1.3968, -1.5539]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([0.0060, 0.0044, 0.0025, 0.0011, 0.1480, 0.1292, 0.1642, 0.1291, 0.0062,\n",
      "        0.0050, 0.0031, 0.0013, 0.0058, 0.0038, 0.0026, 0.0015, 0.0050, 0.0042,\n",
      "        0.0025, 0.0015, 0.0050, 0.0052, 0.0026, 0.0014, 0.1641, 0.1475, 0.1475,\n",
      "        0.1639, 0.0062, 0.0042, 0.0029, 0.0016, 0.0051, 0.0042, 0.0033, 0.0010,\n",
      "        0.0065, 0.0047, 0.0029, 0.0017, 0.0052, 0.0039, 0.0030, 0.0013, 0.1478,\n",
      "        0.0046, 0.0031, 0.0016, 0.0065, 0.0044, 0.0033, 0.0015, 0.0060, 0.0046,\n",
      "        0.0033, 0.0020, 0.0054, 0.0049, 0.0036, 0.0019, 0.0051, 0.0044, 0.0026,\n",
      "        0.0014], grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(-0.0241, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2466, 0.2945, 0.2474, 0.2115],\n",
      "        [0.2466, 0.2945, 0.2474, 0.2115],\n",
      "        [0.2466, 0.2945, 0.2474, 0.2114],\n",
      "        [0.2466, 0.2945, 0.2474, 0.2114],\n",
      "        [0.2466, 0.2945, 0.2474, 0.2114],\n",
      "        [0.2466, 0.2946, 0.2474, 0.2114],\n",
      "        [0.2466, 0.2945, 0.2474, 0.2114],\n",
      "        [0.2466, 0.2945, 0.2474, 0.2114],\n",
      "        [0.2466, 0.2946, 0.2474, 0.2114],\n",
      "        [0.2466, 0.2946, 0.2474, 0.2114],\n",
      "        [0.2466, 0.2946, 0.2474, 0.2114],\n",
      "        [0.2466, 0.2945, 0.2474, 0.2115],\n",
      "        [0.2466, 0.2945, 0.2474, 0.2115],\n",
      "        [0.2466, 0.2945, 0.2474, 0.2115],\n",
      "        [0.2466, 0.2945, 0.2474, 0.2115],\n",
      "        [0.2466, 0.2945, 0.2474, 0.2115],\n",
      "        [0.2466, 0.2946, 0.2474, 0.2114],\n",
      "        [0.2466, 0.2945, 0.2474, 0.2115],\n",
      "        [0.2466, 0.2945, 0.2474, 0.2115],\n",
      "        [0.2466, 0.2945, 0.2474, 0.2114],\n",
      "        [0.2466, 0.2945, 0.2474, 0.2115],\n",
      "        [0.2466, 0.2945, 0.2474, 0.2114],\n",
      "        [0.2466, 0.2945, 0.2474, 0.2115],\n",
      "        [0.2466, 0.2945, 0.2474, 0.2115],\n",
      "        [0.2466, 0.2945, 0.2474, 0.2114],\n",
      "        [0.2466, 0.2945, 0.2474, 0.2114],\n",
      "        [0.2466, 0.2945, 0.2474, 0.2114],\n",
      "        [0.2466, 0.2945, 0.2474, 0.2115],\n",
      "        [0.2466, 0.2945, 0.2474, 0.2114],\n",
      "        [0.2466, 0.2945, 0.2474, 0.2114],\n",
      "        [0.2466, 0.2945, 0.2474, 0.2114],\n",
      "        [0.2466, 0.2945, 0.2474, 0.2115],\n",
      "        [0.2466, 0.2945, 0.2474, 0.2114],\n",
      "        [0.2466, 0.2945, 0.2474, 0.2114],\n",
      "        [0.2466, 0.2945, 0.2474, 0.2114],\n",
      "        [0.2466, 0.2945, 0.2474, 0.2114],\n",
      "        [0.2466, 0.2945, 0.2474, 0.2115],\n",
      "        [0.2466, 0.2946, 0.2474, 0.2114],\n",
      "        [0.2466, 0.2944, 0.2474, 0.2115],\n",
      "        [0.2466, 0.2946, 0.2474, 0.2114],\n",
      "        [0.2466, 0.2945, 0.2474, 0.2114],\n",
      "        [0.2466, 0.2945, 0.2474, 0.2114],\n",
      "        [0.2466, 0.2946, 0.2474, 0.2114],\n",
      "        [0.2466, 0.2945, 0.2474, 0.2115],\n",
      "        [0.2466, 0.2946, 0.2474, 0.2114],\n",
      "        [0.2466, 0.2946, 0.2474, 0.2114],\n",
      "        [0.2466, 0.2945, 0.2474, 0.2114],\n",
      "        [0.2466, 0.2945, 0.2474, 0.2114],\n",
      "        [0.2466, 0.2945, 0.2474, 0.2115],\n",
      "        [0.2466, 0.2945, 0.2474, 0.2114],\n",
      "        [0.2466, 0.2945, 0.2474, 0.2114],\n",
      "        [0.2466, 0.2945, 0.2474, 0.2115],\n",
      "        [0.2466, 0.2945, 0.2474, 0.2114],\n",
      "        [0.2466, 0.2946, 0.2474, 0.2114],\n",
      "        [0.2466, 0.2946, 0.2474, 0.2114],\n",
      "        [0.2466, 0.2946, 0.2474, 0.2114],\n",
      "        [0.2466, 0.2945, 0.2474, 0.2114],\n",
      "        [0.2466, 0.2945, 0.2474, 0.2114],\n",
      "        [0.2466, 0.2946, 0.2474, 0.2114],\n",
      "        [0.2466, 0.2945, 0.2474, 0.2114],\n",
      "        [0.2466, 0.2946, 0.2474, 0.2114],\n",
      "        [0.2466, 0.2946, 0.2474, 0.2114],\n",
      "        [0.2466, 0.2945, 0.2474, 0.2114],\n",
      "        [0.2466, 0.2946, 0.2474, 0.2114]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3794, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(-0.0363, grad_fn=<AddBackward0>)\n",
      "batching here\n",
      "89: done 3 episodes, mean_reward=0.00, best_reward=2.00, speed=236.30\n",
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.3988, -1.2271, -1.3958, -1.5496],\n",
      "        [-1.3988, -1.2271, -1.3958, -1.5495],\n",
      "        [-1.3988, -1.2272, -1.3958, -1.5495],\n",
      "        [-1.3988, -1.2271, -1.3958, -1.5496],\n",
      "        [-1.3987, -1.2271, -1.3959, -1.5496],\n",
      "        [-1.3987, -1.2271, -1.3959, -1.5497],\n",
      "        [-1.3988, -1.2269, -1.3959, -1.5498],\n",
      "        [-1.3988, -1.2272, -1.3958, -1.5495],\n",
      "        [-1.3988, -1.2271, -1.3958, -1.5496],\n",
      "        [-1.3988, -1.2272, -1.3958, -1.5495],\n",
      "        [-1.3988, -1.2270, -1.3958, -1.5498],\n",
      "        [-1.3989, -1.2270, -1.3958, -1.5496],\n",
      "        [-1.3988, -1.2273, -1.3958, -1.5494],\n",
      "        [-1.3988, -1.2273, -1.3958, -1.5494],\n",
      "        [-1.3988, -1.2272, -1.3958, -1.5495],\n",
      "        [-1.3988, -1.2272, -1.3958, -1.5494],\n",
      "        [-1.3988, -1.2272, -1.3958, -1.5495],\n",
      "        [-1.3988, -1.2274, -1.3958, -1.5493],\n",
      "        [-1.3988, -1.2274, -1.3958, -1.5492],\n",
      "        [-1.3988, -1.2272, -1.3958, -1.5495],\n",
      "        [-1.3987, -1.2273, -1.3958, -1.5493],\n",
      "        [-1.3988, -1.2272, -1.3958, -1.5495],\n",
      "        [-1.3988, -1.2273, -1.3958, -1.5494],\n",
      "        [-1.3988, -1.2273, -1.3958, -1.5494],\n",
      "        [-1.3987, -1.2271, -1.3958, -1.5497],\n",
      "        [-1.3987, -1.2272, -1.3958, -1.5496],\n",
      "        [-1.3987, -1.2272, -1.3958, -1.5496],\n",
      "        [-1.3987, -1.2270, -1.3958, -1.5497],\n",
      "        [-1.3988, -1.2275, -1.3958, -1.5491],\n",
      "        [-1.3987, -1.2275, -1.3958, -1.5492],\n",
      "        [-1.3988, -1.2274, -1.3958, -1.5492],\n",
      "        [-1.3987, -1.2275, -1.3958, -1.5491],\n",
      "        [-1.3988, -1.2272, -1.3958, -1.5495],\n",
      "        [-1.3988, -1.2271, -1.3958, -1.5496],\n",
      "        [-1.3988, -1.2272, -1.3958, -1.5495],\n",
      "        [-1.3988, -1.2271, -1.3958, -1.5495],\n",
      "        [-1.3988, -1.2273, -1.3958, -1.5493],\n",
      "        [-1.3988, -1.2271, -1.3958, -1.5497],\n",
      "        [-1.3987, -1.2276, -1.3958, -1.5490],\n",
      "        [-1.3988, -1.2271, -1.3958, -1.5496],\n",
      "        [-1.3987, -1.2273, -1.3958, -1.5494],\n",
      "        [-1.3988, -1.2272, -1.3958, -1.5494],\n",
      "        [-1.3988, -1.2274, -1.3958, -1.5492],\n",
      "        [-1.3988, -1.2272, -1.3958, -1.5495],\n",
      "        [-1.3988, -1.2271, -1.3958, -1.5496],\n",
      "        [-1.3988, -1.2272, -1.3958, -1.5494],\n",
      "        [-1.3987, -1.2273, -1.3958, -1.5494],\n",
      "        [-1.3988, -1.2272, -1.3958, -1.5495],\n",
      "        [-1.3988, -1.2273, -1.3958, -1.5494],\n",
      "        [-1.3988, -1.2273, -1.3958, -1.5494],\n",
      "        [-1.3988, -1.2273, -1.3958, -1.5494],\n",
      "        [-1.3988, -1.2274, -1.3958, -1.5493],\n",
      "        [-1.3988, -1.2274, -1.3958, -1.5493],\n",
      "        [-1.3988, -1.2274, -1.3958, -1.5492],\n",
      "        [-1.3987, -1.2274, -1.3958, -1.5492],\n",
      "        [-1.3988, -1.2273, -1.3958, -1.5493],\n",
      "        [-1.3987, -1.2274, -1.3958, -1.5493],\n",
      "        [-1.3988, -1.2273, -1.3958, -1.5494],\n",
      "        [-1.3988, -1.2271, -1.3958, -1.5496],\n",
      "        [-1.3988, -1.2272, -1.3958, -1.5495],\n",
      "        [-1.3988, -1.2272, -1.3958, -1.5495],\n",
      "        [-1.3988, -1.2272, -1.3958, -1.5495],\n",
      "        [-1.3988, -1.2272, -1.3958, -1.5495],\n",
      "        [-1.3988, -1.2273, -1.3958, -1.5494]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([0.0047, 0.0036, 0.0023, 0.0012, 0.0050, 0.0038, 0.0029, 0.0014, 0.0039,\n",
      "        0.0033, 0.0025, 0.0011, 0.1336, 0.1206, 0.0022, 0.0013, 0.0049, 0.0034,\n",
      "        0.0022, 0.0013, 0.0046, 0.0037, 0.0024, 0.0011, 0.0049, 0.0036, 0.0023,\n",
      "        0.0014, 0.0045, 0.0037, 0.0021, 0.0010, 0.0053, 0.0032, 0.0026, 0.0010,\n",
      "        0.0042, 0.0045, 0.0020, 0.0018, 0.0049, 0.0036, 0.0023, 0.0013, 0.0051,\n",
      "        0.0034, 0.0030, 0.0014, 0.1058, 0.1206, 0.1058, 0.0012, 0.0042, 0.0031,\n",
      "        0.0021, 0.0012, 0.0047, 0.0033, 0.0027, 0.0013, 0.1060, 0.1205, 0.1209,\n",
      "        0.0011], grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(-0.0171, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2469, 0.2931, 0.2476, 0.2123],\n",
      "        [0.2469, 0.2931, 0.2476, 0.2124],\n",
      "        [0.2469, 0.2931, 0.2476, 0.2124],\n",
      "        [0.2469, 0.2931, 0.2476, 0.2123],\n",
      "        [0.2469, 0.2931, 0.2476, 0.2123],\n",
      "        [0.2469, 0.2932, 0.2476, 0.2123],\n",
      "        [0.2469, 0.2932, 0.2476, 0.2123],\n",
      "        [0.2469, 0.2931, 0.2476, 0.2124],\n",
      "        [0.2469, 0.2931, 0.2476, 0.2123],\n",
      "        [0.2469, 0.2931, 0.2476, 0.2124],\n",
      "        [0.2469, 0.2932, 0.2476, 0.2123],\n",
      "        [0.2469, 0.2932, 0.2476, 0.2123],\n",
      "        [0.2469, 0.2931, 0.2476, 0.2124],\n",
      "        [0.2469, 0.2931, 0.2476, 0.2124],\n",
      "        [0.2469, 0.2931, 0.2476, 0.2124],\n",
      "        [0.2469, 0.2931, 0.2476, 0.2124],\n",
      "        [0.2469, 0.2931, 0.2476, 0.2123],\n",
      "        [0.2469, 0.2931, 0.2476, 0.2124],\n",
      "        [0.2469, 0.2930, 0.2476, 0.2124],\n",
      "        [0.2469, 0.2931, 0.2476, 0.2124],\n",
      "        [0.2469, 0.2931, 0.2476, 0.2124],\n",
      "        [0.2469, 0.2931, 0.2476, 0.2124],\n",
      "        [0.2469, 0.2931, 0.2476, 0.2124],\n",
      "        [0.2469, 0.2931, 0.2476, 0.2124],\n",
      "        [0.2469, 0.2931, 0.2476, 0.2123],\n",
      "        [0.2469, 0.2931, 0.2476, 0.2123],\n",
      "        [0.2469, 0.2931, 0.2476, 0.2123],\n",
      "        [0.2469, 0.2932, 0.2476, 0.2123],\n",
      "        [0.2469, 0.2930, 0.2476, 0.2124],\n",
      "        [0.2469, 0.2930, 0.2476, 0.2124],\n",
      "        [0.2469, 0.2930, 0.2476, 0.2124],\n",
      "        [0.2469, 0.2930, 0.2476, 0.2124],\n",
      "        [0.2469, 0.2931, 0.2476, 0.2124],\n",
      "        [0.2469, 0.2931, 0.2476, 0.2123],\n",
      "        [0.2469, 0.2931, 0.2476, 0.2124],\n",
      "        [0.2469, 0.2931, 0.2476, 0.2123],\n",
      "        [0.2469, 0.2931, 0.2476, 0.2124],\n",
      "        [0.2469, 0.2932, 0.2476, 0.2123],\n",
      "        [0.2469, 0.2930, 0.2476, 0.2125],\n",
      "        [0.2469, 0.2932, 0.2476, 0.2123],\n",
      "        [0.2469, 0.2931, 0.2476, 0.2124],\n",
      "        [0.2469, 0.2931, 0.2476, 0.2124],\n",
      "        [0.2469, 0.2930, 0.2476, 0.2124],\n",
      "        [0.2469, 0.2931, 0.2476, 0.2123],\n",
      "        [0.2469, 0.2931, 0.2476, 0.2123],\n",
      "        [0.2469, 0.2931, 0.2476, 0.2124],\n",
      "        [0.2469, 0.2931, 0.2476, 0.2124],\n",
      "        [0.2469, 0.2931, 0.2476, 0.2124],\n",
      "        [0.2469, 0.2931, 0.2476, 0.2124],\n",
      "        [0.2469, 0.2931, 0.2476, 0.2124],\n",
      "        [0.2469, 0.2931, 0.2476, 0.2124],\n",
      "        [0.2469, 0.2931, 0.2476, 0.2124],\n",
      "        [0.2469, 0.2931, 0.2476, 0.2124],\n",
      "        [0.2469, 0.2930, 0.2476, 0.2124],\n",
      "        [0.2469, 0.2931, 0.2476, 0.2124],\n",
      "        [0.2469, 0.2931, 0.2476, 0.2124],\n",
      "        [0.2469, 0.2931, 0.2476, 0.2124],\n",
      "        [0.2469, 0.2931, 0.2476, 0.2124],\n",
      "        [0.2469, 0.2931, 0.2476, 0.2123],\n",
      "        [0.2469, 0.2931, 0.2476, 0.2124],\n",
      "        [0.2469, 0.2931, 0.2476, 0.2124],\n",
      "        [0.2469, 0.2931, 0.2476, 0.2124],\n",
      "        [0.2469, 0.2931, 0.2476, 0.2123],\n",
      "        [0.2469, 0.2931, 0.2476, 0.2124]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3798, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(-0.0300, grad_fn=<AddBackward0>)\n",
      "batching here\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(0.0604, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.3996, -1.2361, -1.3939, -1.5386],\n",
      "        [-1.3996, -1.2361, -1.3939, -1.5386],\n",
      "        [-1.3996, -1.2362, -1.3939, -1.5386],\n",
      "        [-1.3996, -1.2362, -1.3939, -1.5385],\n",
      "        [-1.3995, -1.2364, -1.3939, -1.5382],\n",
      "        [-1.3996, -1.2362, -1.3939, -1.5385],\n",
      "        [-1.3996, -1.2362, -1.3939, -1.5385],\n",
      "        [-1.3996, -1.2363, -1.3939, -1.5384],\n",
      "        [-1.3996, -1.2359, -1.3939, -1.5388],\n",
      "        [-1.3996, -1.2361, -1.3939, -1.5386],\n",
      "        [-1.3996, -1.2363, -1.3939, -1.5384],\n",
      "        [-1.3997, -1.2359, -1.3939, -1.5388],\n",
      "        [-1.3996, -1.2363, -1.3939, -1.5384],\n",
      "        [-1.3996, -1.2364, -1.3939, -1.5383],\n",
      "        [-1.3996, -1.2362, -1.3939, -1.5385],\n",
      "        [-1.3996, -1.2363, -1.3939, -1.5383],\n",
      "        [-1.3995, -1.2363, -1.3938, -1.5384],\n",
      "        [-1.3996, -1.2362, -1.3938, -1.5385],\n",
      "        [-1.3996, -1.2363, -1.3939, -1.5384],\n",
      "        [-1.3996, -1.2363, -1.3939, -1.5384],\n",
      "        [-1.3996, -1.2363, -1.3939, -1.5384],\n",
      "        [-1.3996, -1.2363, -1.3939, -1.5385],\n",
      "        [-1.3996, -1.2363, -1.3939, -1.5384],\n",
      "        [-1.3996, -1.2363, -1.3939, -1.5384],\n",
      "        [-1.3995, -1.2363, -1.3939, -1.5384],\n",
      "        [-1.3996, -1.2361, -1.3939, -1.5386],\n",
      "        [-1.3996, -1.2361, -1.3939, -1.5386],\n",
      "        [-1.3996, -1.2362, -1.3939, -1.5385],\n",
      "        [-1.3996, -1.2363, -1.3939, -1.5384],\n",
      "        [-1.3996, -1.2360, -1.3939, -1.5387],\n",
      "        [-1.3996, -1.2364, -1.3939, -1.5383],\n",
      "        [-1.3996, -1.2362, -1.3939, -1.5385],\n",
      "        [-1.3996, -1.2362, -1.3939, -1.5385],\n",
      "        [-1.3995, -1.2362, -1.3939, -1.5385],\n",
      "        [-1.3996, -1.2362, -1.3939, -1.5385],\n",
      "        [-1.3995, -1.2363, -1.3939, -1.5385],\n",
      "        [-1.3996, -1.2364, -1.3939, -1.5383],\n",
      "        [-1.3996, -1.2362, -1.3939, -1.5385],\n",
      "        [-1.3996, -1.2364, -1.3939, -1.5382],\n",
      "        [-1.3996, -1.2363, -1.3939, -1.5384],\n",
      "        [-1.3996, -1.2364, -1.3939, -1.5383],\n",
      "        [-1.3996, -1.2363, -1.3939, -1.5384],\n",
      "        [-1.3995, -1.2364, -1.3939, -1.5383],\n",
      "        [-1.3996, -1.2363, -1.3939, -1.5385],\n",
      "        [-1.3996, -1.2365, -1.3939, -1.5382],\n",
      "        [-1.3996, -1.2362, -1.3939, -1.5385],\n",
      "        [-1.3996, -1.2363, -1.3939, -1.5383],\n",
      "        [-1.3996, -1.2362, -1.3939, -1.5385],\n",
      "        [-1.3996, -1.2364, -1.3939, -1.5383],\n",
      "        [-1.3996, -1.2365, -1.3938, -1.5382],\n",
      "        [-1.3996, -1.2362, -1.3938, -1.5385],\n",
      "        [-1.3995, -1.2363, -1.3938, -1.5385],\n",
      "        [-1.3995, -1.2365, -1.3939, -1.5382],\n",
      "        [-1.3996, -1.2362, -1.3939, -1.5385],\n",
      "        [-1.3996, -1.2360, -1.3939, -1.5388],\n",
      "        [-1.3996, -1.2363, -1.3939, -1.5384],\n",
      "        [-1.3996, -1.2364, -1.3939, -1.5383],\n",
      "        [-1.3995, -1.2364, -1.3939, -1.5383],\n",
      "        [-1.3996, -1.2364, -1.3939, -1.5383],\n",
      "        [-1.3996, -1.2363, -1.3938, -1.5385],\n",
      "        [-1.3996, -1.2362, -1.3939, -1.5384],\n",
      "        [-1.3996, -1.2361, -1.3939, -1.5385],\n",
      "        [-1.3996, -1.2363, -1.3939, -1.5383],\n",
      "        [-1.3995, -1.2365, -1.3939, -1.5381]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([ 4.1899e-03,  3.4486e-03,  1.8345e-03,  1.0202e-03,  4.2579e-03,\n",
      "         3.1386e-03,  1.8544e-03,  1.0073e-03,  4.5503e-03,  3.0625e-03,\n",
      "         1.8351e-03,  1.2558e-03,  3.8209e-03,  2.7516e-03,  1.7126e-03,\n",
      "         7.6832e-04,  4.3871e-03,  3.1174e-03,  1.7320e-03,  9.2831e-04,\n",
      "         3.6410e-03,  3.1024e-03,  2.0177e-03,  9.1877e-04,  3.4433e-03,\n",
      "         2.7209e-03,  2.0529e-03,  8.5209e-04, -1.1960e+00, -1.2086e+00,\n",
      "        -1.3779e+00, -1.2353e+00,  3.7384e-03,  2.8107e-03,  2.3988e-03,\n",
      "         1.2191e-03,  3.8270e-03,  2.8050e-03,  1.9961e-03,  1.0429e-03,\n",
      "         3.9461e-03,  3.0819e-03,  1.9070e-03,  1.1562e-03,  4.0062e-03,\n",
      "         2.9644e-03,  2.1907e-03,  1.2831e-03,  4.3380e-03,  2.7026e-03,\n",
      "         2.1212e-03,  1.0560e-03,  3.8537e-03,  3.0819e-03,  2.1817e-03,\n",
      "         1.0397e-03,  3.7076e-03,  2.8312e-03,  1.9924e-03,  1.2309e-03,\n",
      "         4.0871e-03,  3.2263e-03,  2.0022e-03,  5.2717e-04],\n",
      "       grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(0.0761, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2467, 0.2905, 0.2481, 0.2147],\n",
      "        [0.2467, 0.2905, 0.2481, 0.2147],\n",
      "        [0.2467, 0.2905, 0.2481, 0.2147],\n",
      "        [0.2467, 0.2905, 0.2481, 0.2147],\n",
      "        [0.2467, 0.2904, 0.2481, 0.2148],\n",
      "        [0.2467, 0.2905, 0.2481, 0.2147],\n",
      "        [0.2467, 0.2905, 0.2481, 0.2147],\n",
      "        [0.2467, 0.2905, 0.2481, 0.2147],\n",
      "        [0.2467, 0.2906, 0.2481, 0.2146],\n",
      "        [0.2467, 0.2905, 0.2481, 0.2147],\n",
      "        [0.2467, 0.2905, 0.2481, 0.2147],\n",
      "        [0.2467, 0.2906, 0.2481, 0.2146],\n",
      "        [0.2467, 0.2905, 0.2481, 0.2147],\n",
      "        [0.2467, 0.2904, 0.2481, 0.2148],\n",
      "        [0.2467, 0.2905, 0.2481, 0.2147],\n",
      "        [0.2467, 0.2905, 0.2481, 0.2147],\n",
      "        [0.2467, 0.2905, 0.2481, 0.2147],\n",
      "        [0.2467, 0.2905, 0.2481, 0.2147],\n",
      "        [0.2467, 0.2905, 0.2481, 0.2147],\n",
      "        [0.2467, 0.2905, 0.2481, 0.2147],\n",
      "        [0.2467, 0.2905, 0.2481, 0.2147],\n",
      "        [0.2467, 0.2905, 0.2481, 0.2147],\n",
      "        [0.2467, 0.2904, 0.2481, 0.2147],\n",
      "        [0.2467, 0.2905, 0.2481, 0.2147],\n",
      "        [0.2467, 0.2905, 0.2481, 0.2147],\n",
      "        [0.2467, 0.2905, 0.2481, 0.2147],\n",
      "        [0.2467, 0.2905, 0.2481, 0.2147],\n",
      "        [0.2467, 0.2905, 0.2481, 0.2147],\n",
      "        [0.2467, 0.2905, 0.2481, 0.2147],\n",
      "        [0.2467, 0.2905, 0.2481, 0.2147],\n",
      "        [0.2467, 0.2904, 0.2481, 0.2148],\n",
      "        [0.2467, 0.2905, 0.2481, 0.2147],\n",
      "        [0.2467, 0.2905, 0.2481, 0.2147],\n",
      "        [0.2467, 0.2905, 0.2481, 0.2147],\n",
      "        [0.2467, 0.2905, 0.2481, 0.2147],\n",
      "        [0.2467, 0.2905, 0.2481, 0.2147],\n",
      "        [0.2467, 0.2904, 0.2481, 0.2147],\n",
      "        [0.2467, 0.2905, 0.2481, 0.2147],\n",
      "        [0.2467, 0.2904, 0.2481, 0.2148],\n",
      "        [0.2467, 0.2905, 0.2481, 0.2147],\n",
      "        [0.2467, 0.2904, 0.2481, 0.2148],\n",
      "        [0.2467, 0.2905, 0.2481, 0.2147],\n",
      "        [0.2467, 0.2904, 0.2481, 0.2147],\n",
      "        [0.2467, 0.2905, 0.2481, 0.2147],\n",
      "        [0.2467, 0.2904, 0.2481, 0.2148],\n",
      "        [0.2467, 0.2905, 0.2481, 0.2147],\n",
      "        [0.2467, 0.2905, 0.2481, 0.2147],\n",
      "        [0.2467, 0.2905, 0.2481, 0.2147],\n",
      "        [0.2467, 0.2904, 0.2481, 0.2148],\n",
      "        [0.2467, 0.2904, 0.2481, 0.2148],\n",
      "        [0.2467, 0.2905, 0.2481, 0.2147],\n",
      "        [0.2467, 0.2905, 0.2481, 0.2147],\n",
      "        [0.2467, 0.2904, 0.2481, 0.2148],\n",
      "        [0.2467, 0.2905, 0.2481, 0.2147],\n",
      "        [0.2467, 0.2906, 0.2481, 0.2146],\n",
      "        [0.2467, 0.2905, 0.2481, 0.2147],\n",
      "        [0.2467, 0.2904, 0.2481, 0.2147],\n",
      "        [0.2467, 0.2904, 0.2481, 0.2147],\n",
      "        [0.2467, 0.2904, 0.2481, 0.2147],\n",
      "        [0.2467, 0.2905, 0.2481, 0.2147],\n",
      "        [0.2467, 0.2905, 0.2481, 0.2147],\n",
      "        [0.2467, 0.2905, 0.2481, 0.2147],\n",
      "        [0.2467, 0.2904, 0.2481, 0.2147],\n",
      "        [0.2467, 0.2904, 0.2481, 0.2148]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3805, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(0.1227, grad_fn=<AddBackward0>)\n",
      "batching here\n",
      "91: done 3 episodes, mean_reward=0.00, best_reward=2.00, speed=234.34\n",
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(0.0172, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.4136, -1.1916, -1.3954, -1.5833],\n",
      "        [-1.4136, -1.1917, -1.3954, -1.5833],\n",
      "        [-1.4136, -1.1917, -1.3954, -1.5833],\n",
      "        [-1.4136, -1.1917, -1.3954, -1.5833],\n",
      "        [-1.4136, -1.1917, -1.3954, -1.5833],\n",
      "        [-1.4136, -1.1916, -1.3954, -1.5835],\n",
      "        [-1.4136, -1.1917, -1.3954, -1.5834],\n",
      "        [-1.4136, -1.1915, -1.3954, -1.5836],\n",
      "        [-1.4137, -1.1915, -1.3954, -1.5834],\n",
      "        [-1.4137, -1.1916, -1.3954, -1.5834],\n",
      "        [-1.4138, -1.1914, -1.3954, -1.5836],\n",
      "        [-1.4138, -1.1914, -1.3954, -1.5836],\n",
      "        [-1.4136, -1.1916, -1.3954, -1.5834],\n",
      "        [-1.4135, -1.1920, -1.3954, -1.5829],\n",
      "        [-1.4136, -1.1915, -1.3954, -1.5835],\n",
      "        [-1.4136, -1.1918, -1.3954, -1.5831],\n",
      "        [-1.4136, -1.1917, -1.3954, -1.5833],\n",
      "        [-1.4136, -1.1917, -1.3954, -1.5833],\n",
      "        [-1.4136, -1.1917, -1.3954, -1.5832],\n",
      "        [-1.4136, -1.1917, -1.3954, -1.5833],\n",
      "        [-1.4136, -1.1918, -1.3954, -1.5832],\n",
      "        [-1.4136, -1.1918, -1.3954, -1.5832],\n",
      "        [-1.4136, -1.1918, -1.3954, -1.5832],\n",
      "        [-1.4136, -1.1918, -1.3954, -1.5832],\n",
      "        [-1.4136, -1.1917, -1.3954, -1.5833],\n",
      "        [-1.4136, -1.1915, -1.3954, -1.5835],\n",
      "        [-1.4137, -1.1915, -1.3954, -1.5835],\n",
      "        [-1.4136, -1.1919, -1.3954, -1.5831],\n",
      "        [-1.4136, -1.1918, -1.3954, -1.5832],\n",
      "        [-1.4135, -1.1919, -1.3954, -1.5831],\n",
      "        [-1.4136, -1.1918, -1.3954, -1.5832],\n",
      "        [-1.4136, -1.1918, -1.3954, -1.5832],\n",
      "        [-1.4136, -1.1917, -1.3954, -1.5832],\n",
      "        [-1.4136, -1.1916, -1.3955, -1.5834],\n",
      "        [-1.4136, -1.1917, -1.3954, -1.5834],\n",
      "        [-1.4136, -1.1916, -1.3954, -1.5835],\n",
      "        [-1.4136, -1.1917, -1.3954, -1.5833],\n",
      "        [-1.4136, -1.1917, -1.3954, -1.5832],\n",
      "        [-1.4136, -1.1918, -1.3954, -1.5832],\n",
      "        [-1.4136, -1.1918, -1.3954, -1.5832],\n",
      "        [-1.4136, -1.1918, -1.3954, -1.5832],\n",
      "        [-1.4136, -1.1918, -1.3954, -1.5832],\n",
      "        [-1.4136, -1.1917, -1.3954, -1.5833],\n",
      "        [-1.4136, -1.1917, -1.3954, -1.5833],\n",
      "        [-1.4136, -1.1918, -1.3954, -1.5831],\n",
      "        [-1.4136, -1.1917, -1.3954, -1.5833],\n",
      "        [-1.4136, -1.1918, -1.3954, -1.5832],\n",
      "        [-1.4136, -1.1918, -1.3954, -1.5832],\n",
      "        [-1.4136, -1.1917, -1.3954, -1.5833],\n",
      "        [-1.4136, -1.1916, -1.3954, -1.5833],\n",
      "        [-1.4136, -1.1918, -1.3954, -1.5832],\n",
      "        [-1.4136, -1.1918, -1.3955, -1.5832],\n",
      "        [-1.4136, -1.1916, -1.3955, -1.5834],\n",
      "        [-1.4136, -1.1917, -1.3954, -1.5833],\n",
      "        [-1.4136, -1.1918, -1.3954, -1.5832],\n",
      "        [-1.4136, -1.1917, -1.3954, -1.5833],\n",
      "        [-1.4135, -1.1920, -1.3954, -1.5830],\n",
      "        [-1.4136, -1.1918, -1.3954, -1.5832],\n",
      "        [-1.4136, -1.1918, -1.3954, -1.5832],\n",
      "        [-1.4136, -1.1918, -1.3954, -1.5832],\n",
      "        [-1.4137, -1.1916, -1.3954, -1.5834],\n",
      "        [-1.4137, -1.1914, -1.3954, -1.5836],\n",
      "        [-1.4136, -1.1917, -1.3954, -1.5834],\n",
      "        [-1.4136, -1.1918, -1.3954, -1.5831]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([ 1.2312e-01,  1.4410e-01,  1.4415e-01,  1.4600e-01,  5.7733e-03,\n",
      "         5.2824e-03,  2.6836e-03,  1.9625e-03,  4.6621e-03,  4.0459e-03,\n",
      "         2.4690e-03,  1.4786e-03,  4.8395e-03,  3.7450e-03,  2.9199e-03,\n",
      "         1.2528e-03,  6.6135e-03,  4.4113e-03,  2.8839e-03,  1.2694e-03,\n",
      "         1.4394e-01,  1.4394e-01,  1.2293e-01,  1.2293e-01,  5.0128e-03,\n",
      "         4.7072e-03,  3.1896e-03,  1.3168e-03,  4.7638e-03,  3.5700e-03,\n",
      "         2.3453e-03,  1.3193e-03,  1.2294e-01,  1.6362e-01,  3.3980e-03,\n",
      "         1.6970e-03,  4.9095e-03,  4.8884e-03,  2.9045e-03,  1.4581e-03,\n",
      "         5.7099e-03,  4.3336e-03,  3.3742e-03,  1.5374e-03,  6.1988e-03,\n",
      "         4.4063e-03,  3.1713e-03,  1.3512e-03,  5.6180e-03,  4.7781e-03,\n",
      "         2.2055e-03,  1.2122e-03, -1.3897e+00,  4.3535e-03,  2.9238e-03,\n",
      "         1.6336e-03,  4.3389e-03,  3.3796e-03,  2.6110e-03,  1.1563e-03,\n",
      "         5.5896e-03,  4.5250e-03,  2.7875e-03,  1.2752e-03],\n",
      "       grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(-0.0026, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2433, 0.3037, 0.2477, 0.2053],\n",
      "        [0.2433, 0.3037, 0.2477, 0.2053],\n",
      "        [0.2433, 0.3037, 0.2477, 0.2053],\n",
      "        [0.2433, 0.3037, 0.2477, 0.2053],\n",
      "        [0.2433, 0.3037, 0.2477, 0.2053],\n",
      "        [0.2433, 0.3037, 0.2477, 0.2053],\n",
      "        [0.2433, 0.3037, 0.2477, 0.2053],\n",
      "        [0.2433, 0.3038, 0.2477, 0.2052],\n",
      "        [0.2432, 0.3038, 0.2477, 0.2053],\n",
      "        [0.2433, 0.3037, 0.2477, 0.2053],\n",
      "        [0.2432, 0.3038, 0.2477, 0.2052],\n",
      "        [0.2432, 0.3038, 0.2477, 0.2052],\n",
      "        [0.2433, 0.3037, 0.2477, 0.2053],\n",
      "        [0.2433, 0.3036, 0.2477, 0.2054],\n",
      "        [0.2433, 0.3038, 0.2477, 0.2053],\n",
      "        [0.2433, 0.3037, 0.2477, 0.2053],\n",
      "        [0.2433, 0.3037, 0.2477, 0.2053],\n",
      "        [0.2433, 0.3037, 0.2477, 0.2053],\n",
      "        [0.2433, 0.3037, 0.2477, 0.2053],\n",
      "        [0.2433, 0.3037, 0.2477, 0.2053],\n",
      "        [0.2433, 0.3037, 0.2477, 0.2053],\n",
      "        [0.2433, 0.3037, 0.2477, 0.2053],\n",
      "        [0.2433, 0.3037, 0.2477, 0.2053],\n",
      "        [0.2433, 0.3037, 0.2477, 0.2053],\n",
      "        [0.2433, 0.3037, 0.2477, 0.2053],\n",
      "        [0.2433, 0.3038, 0.2477, 0.2053],\n",
      "        [0.2433, 0.3038, 0.2477, 0.2053],\n",
      "        [0.2433, 0.3037, 0.2477, 0.2053],\n",
      "        [0.2433, 0.3037, 0.2477, 0.2053],\n",
      "        [0.2433, 0.3036, 0.2477, 0.2053],\n",
      "        [0.2433, 0.3037, 0.2477, 0.2053],\n",
      "        [0.2433, 0.3037, 0.2477, 0.2053],\n",
      "        [0.2433, 0.3037, 0.2477, 0.2053],\n",
      "        [0.2433, 0.3037, 0.2477, 0.2053],\n",
      "        [0.2433, 0.3037, 0.2477, 0.2053],\n",
      "        [0.2433, 0.3037, 0.2477, 0.2053],\n",
      "        [0.2433, 0.3037, 0.2477, 0.2053],\n",
      "        [0.2433, 0.3037, 0.2477, 0.2053],\n",
      "        [0.2433, 0.3037, 0.2477, 0.2053],\n",
      "        [0.2433, 0.3037, 0.2477, 0.2053],\n",
      "        [0.2433, 0.3037, 0.2477, 0.2053],\n",
      "        [0.2433, 0.3037, 0.2477, 0.2053],\n",
      "        [0.2433, 0.3037, 0.2477, 0.2053],\n",
      "        [0.2433, 0.3037, 0.2477, 0.2053],\n",
      "        [0.2433, 0.3037, 0.2477, 0.2053],\n",
      "        [0.2433, 0.3037, 0.2477, 0.2053],\n",
      "        [0.2433, 0.3037, 0.2477, 0.2053],\n",
      "        [0.2433, 0.3037, 0.2477, 0.2053],\n",
      "        [0.2433, 0.3037, 0.2477, 0.2053],\n",
      "        [0.2433, 0.3037, 0.2477, 0.2053],\n",
      "        [0.2433, 0.3037, 0.2477, 0.2053],\n",
      "        [0.2433, 0.3037, 0.2477, 0.2053],\n",
      "        [0.2433, 0.3037, 0.2477, 0.2053],\n",
      "        [0.2433, 0.3037, 0.2477, 0.2053],\n",
      "        [0.2433, 0.3037, 0.2477, 0.2053],\n",
      "        [0.2433, 0.3037, 0.2477, 0.2053],\n",
      "        [0.2433, 0.3036, 0.2477, 0.2054],\n",
      "        [0.2433, 0.3037, 0.2477, 0.2053],\n",
      "        [0.2433, 0.3037, 0.2477, 0.2053],\n",
      "        [0.2433, 0.3037, 0.2477, 0.2053],\n",
      "        [0.2433, 0.3037, 0.2477, 0.2053],\n",
      "        [0.2433, 0.3038, 0.2477, 0.2052],\n",
      "        [0.2433, 0.3037, 0.2477, 0.2053],\n",
      "        [0.2433, 0.3037, 0.2477, 0.2053]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3765, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(0.0008, grad_fn=<AddBackward0>)\n",
      "batching here\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92: done 1 episodes, mean_reward=0.00, best_reward=2.00, speed=232.81\n",
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.4132, -1.2088, -1.3813, -1.5759],\n",
      "        [-1.4133, -1.2087, -1.3813, -1.5759],\n",
      "        [-1.4133, -1.2086, -1.3813, -1.5760],\n",
      "        [-1.4133, -1.2088, -1.3813, -1.5758],\n",
      "        [-1.4132, -1.2087, -1.3813, -1.5760],\n",
      "        [-1.4132, -1.2087, -1.3813, -1.5760],\n",
      "        [-1.4132, -1.2087, -1.3813, -1.5760],\n",
      "        [-1.4132, -1.2087, -1.3813, -1.5760],\n",
      "        [-1.4134, -1.2084, -1.3812, -1.5763],\n",
      "        [-1.4134, -1.2084, -1.3812, -1.5763],\n",
      "        [-1.4134, -1.2085, -1.3812, -1.5762],\n",
      "        [-1.4134, -1.2085, -1.3812, -1.5762],\n",
      "        [-1.4133, -1.2086, -1.3812, -1.5762],\n",
      "        [-1.4132, -1.2092, -1.3813, -1.5754],\n",
      "        [-1.4133, -1.2086, -1.3813, -1.5761],\n",
      "        [-1.4132, -1.2088, -1.3812, -1.5759],\n",
      "        [-1.4132, -1.2088, -1.3813, -1.5759],\n",
      "        [-1.4132, -1.2088, -1.3813, -1.5759],\n",
      "        [-1.4132, -1.2088, -1.3813, -1.5759],\n",
      "        [-1.4132, -1.2088, -1.3813, -1.5759],\n",
      "        [-1.4132, -1.2088, -1.3813, -1.5758],\n",
      "        [-1.4132, -1.2088, -1.3813, -1.5759],\n",
      "        [-1.4133, -1.2088, -1.3812, -1.5758],\n",
      "        [-1.4133, -1.2087, -1.3812, -1.5760],\n",
      "        [-1.4133, -1.2087, -1.3813, -1.5759],\n",
      "        [-1.4133, -1.2087, -1.3812, -1.5760],\n",
      "        [-1.4133, -1.2087, -1.3812, -1.5760],\n",
      "        [-1.4132, -1.2089, -1.3813, -1.5758],\n",
      "        [-1.4133, -1.2088, -1.3813, -1.5758],\n",
      "        [-1.4133, -1.2087, -1.3813, -1.5760],\n",
      "        [-1.4133, -1.2086, -1.3813, -1.5760],\n",
      "        [-1.4133, -1.2088, -1.3812, -1.5759],\n",
      "        [-1.4133, -1.2087, -1.3812, -1.5760],\n",
      "        [-1.4133, -1.2085, -1.3812, -1.5763],\n",
      "        [-1.4132, -1.2087, -1.3812, -1.5761],\n",
      "        [-1.4132, -1.2087, -1.3813, -1.5760],\n",
      "        [-1.4132, -1.2088, -1.3813, -1.5759],\n",
      "        [-1.4133, -1.2087, -1.3813, -1.5759],\n",
      "        [-1.4132, -1.2088, -1.3813, -1.5759],\n",
      "        [-1.4133, -1.2087, -1.3813, -1.5759],\n",
      "        [-1.4132, -1.2088, -1.3813, -1.5759],\n",
      "        [-1.4132, -1.2088, -1.3813, -1.5759],\n",
      "        [-1.4132, -1.2088, -1.3813, -1.5759],\n",
      "        [-1.4132, -1.2088, -1.3813, -1.5759],\n",
      "        [-1.4132, -1.2087, -1.3813, -1.5760],\n",
      "        [-1.4132, -1.2087, -1.3813, -1.5760],\n",
      "        [-1.4132, -1.2088, -1.3813, -1.5759],\n",
      "        [-1.4132, -1.2088, -1.3813, -1.5759],\n",
      "        [-1.4132, -1.2087, -1.3813, -1.5760],\n",
      "        [-1.4133, -1.2086, -1.3813, -1.5761],\n",
      "        [-1.4132, -1.2089, -1.3813, -1.5757],\n",
      "        [-1.4133, -1.2088, -1.3813, -1.5759],\n",
      "        [-1.4132, -1.2087, -1.3813, -1.5760],\n",
      "        [-1.4133, -1.2088, -1.3813, -1.5759],\n",
      "        [-1.4133, -1.2086, -1.3813, -1.5761],\n",
      "        [-1.4133, -1.2086, -1.3813, -1.5761],\n",
      "        [-1.4133, -1.2085, -1.3812, -1.5762],\n",
      "        [-1.4131, -1.2093, -1.3813, -1.5753],\n",
      "        [-1.4133, -1.2088, -1.3812, -1.5758],\n",
      "        [-1.4132, -1.2088, -1.3812, -1.5759],\n",
      "        [-1.4133, -1.2086, -1.3813, -1.5760],\n",
      "        [-1.4132, -1.2087, -1.3812, -1.5760],\n",
      "        [-1.4133, -1.2086, -1.3812, -1.5761],\n",
      "        [-1.4133, -1.2087, -1.3813, -1.5759]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([0.0054, 0.0036, 0.0026, 0.0017, 0.0045, 0.0041, 0.0027, 0.0015, 0.0056,\n",
      "        0.0042, 0.0024, 0.0012, 0.0056, 0.0038, 0.0026, 0.0010, 0.0047, 0.0036,\n",
      "        0.0031, 0.0012, 0.0046, 0.0035, 0.0022, 0.0014, 0.0047, 0.0047, 0.0024,\n",
      "        0.0014, 0.0056, 0.0034, 0.0032, 0.0011, 0.0045, 0.0044, 0.0028, 0.0014,\n",
      "        0.1560, 0.1400, 0.0025, 0.0013, 0.0061, 0.0046, 0.0023, 0.0015, 0.0048,\n",
      "        0.0048, 0.0032, 0.0015, 0.0062, 0.0046, 0.0020, 0.0014, 0.0047, 0.0043,\n",
      "        0.0023, 0.0013, 0.0064, 0.0036, 0.0030, 0.0014, 0.0055, 0.0035, 0.0028,\n",
      "        0.0009], grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(-0.0078, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2434, 0.2986, 0.2513, 0.2068],\n",
      "        [0.2434, 0.2986, 0.2513, 0.2068],\n",
      "        [0.2433, 0.2986, 0.2513, 0.2068],\n",
      "        [0.2433, 0.2986, 0.2513, 0.2068],\n",
      "        [0.2434, 0.2986, 0.2513, 0.2068],\n",
      "        [0.2434, 0.2986, 0.2513, 0.2068],\n",
      "        [0.2434, 0.2986, 0.2512, 0.2068],\n",
      "        [0.2434, 0.2986, 0.2513, 0.2068],\n",
      "        [0.2433, 0.2987, 0.2513, 0.2067],\n",
      "        [0.2433, 0.2987, 0.2513, 0.2067],\n",
      "        [0.2433, 0.2987, 0.2513, 0.2068],\n",
      "        [0.2433, 0.2987, 0.2513, 0.2068],\n",
      "        [0.2433, 0.2986, 0.2513, 0.2068],\n",
      "        [0.2434, 0.2985, 0.2513, 0.2069],\n",
      "        [0.2433, 0.2986, 0.2513, 0.2068],\n",
      "        [0.2434, 0.2986, 0.2513, 0.2068],\n",
      "        [0.2434, 0.2986, 0.2513, 0.2068],\n",
      "        [0.2434, 0.2986, 0.2513, 0.2068],\n",
      "        [0.2434, 0.2986, 0.2513, 0.2068],\n",
      "        [0.2434, 0.2986, 0.2513, 0.2068],\n",
      "        [0.2434, 0.2985, 0.2513, 0.2068],\n",
      "        [0.2434, 0.2986, 0.2513, 0.2068],\n",
      "        [0.2433, 0.2985, 0.2513, 0.2068],\n",
      "        [0.2433, 0.2986, 0.2513, 0.2068],\n",
      "        [0.2433, 0.2986, 0.2513, 0.2068],\n",
      "        [0.2433, 0.2986, 0.2513, 0.2068],\n",
      "        [0.2433, 0.2986, 0.2513, 0.2068],\n",
      "        [0.2434, 0.2985, 0.2513, 0.2068],\n",
      "        [0.2433, 0.2985, 0.2513, 0.2068],\n",
      "        [0.2433, 0.2986, 0.2513, 0.2068],\n",
      "        [0.2433, 0.2986, 0.2513, 0.2068],\n",
      "        [0.2433, 0.2986, 0.2513, 0.2068],\n",
      "        [0.2433, 0.2986, 0.2513, 0.2068],\n",
      "        [0.2433, 0.2986, 0.2513, 0.2067],\n",
      "        [0.2434, 0.2986, 0.2513, 0.2068],\n",
      "        [0.2434, 0.2986, 0.2513, 0.2068],\n",
      "        [0.2434, 0.2986, 0.2513, 0.2068],\n",
      "        [0.2434, 0.2986, 0.2513, 0.2068],\n",
      "        [0.2434, 0.2986, 0.2513, 0.2068],\n",
      "        [0.2434, 0.2986, 0.2513, 0.2068],\n",
      "        [0.2434, 0.2986, 0.2513, 0.2068],\n",
      "        [0.2434, 0.2986, 0.2513, 0.2068],\n",
      "        [0.2434, 0.2986, 0.2513, 0.2068],\n",
      "        [0.2434, 0.2986, 0.2513, 0.2068],\n",
      "        [0.2434, 0.2986, 0.2513, 0.2068],\n",
      "        [0.2434, 0.2986, 0.2513, 0.2068],\n",
      "        [0.2434, 0.2986, 0.2513, 0.2068],\n",
      "        [0.2434, 0.2986, 0.2513, 0.2068],\n",
      "        [0.2434, 0.2986, 0.2513, 0.2068],\n",
      "        [0.2433, 0.2986, 0.2513, 0.2068],\n",
      "        [0.2434, 0.2985, 0.2513, 0.2069],\n",
      "        [0.2433, 0.2986, 0.2513, 0.2068],\n",
      "        [0.2434, 0.2986, 0.2513, 0.2068],\n",
      "        [0.2433, 0.2986, 0.2513, 0.2068],\n",
      "        [0.2433, 0.2986, 0.2513, 0.2068],\n",
      "        [0.2433, 0.2986, 0.2513, 0.2068],\n",
      "        [0.2433, 0.2986, 0.2513, 0.2068],\n",
      "        [0.2434, 0.2984, 0.2513, 0.2069],\n",
      "        [0.2433, 0.2986, 0.2513, 0.2068],\n",
      "        [0.2434, 0.2985, 0.2513, 0.2068],\n",
      "        [0.2433, 0.2986, 0.2513, 0.2068],\n",
      "        [0.2434, 0.2986, 0.2513, 0.2068],\n",
      "        [0.2433, 0.2986, 0.2513, 0.2068],\n",
      "        [0.2433, 0.2986, 0.2513, 0.2068]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3778, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(-0.0213, grad_fn=<AddBackward0>)\n",
      "batching here\n",
      "93: done 3 episodes, mean_reward=0.00, best_reward=2.00, speed=233.17\n",
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(0.0010, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.4139, -1.2096, -1.3798, -1.5758],\n",
      "        [-1.4140, -1.2094, -1.3797, -1.5760],\n",
      "        [-1.4140, -1.2093, -1.3797, -1.5762],\n",
      "        [-1.4139, -1.2095, -1.3797, -1.5758],\n",
      "        [-1.4140, -1.2092, -1.3798, -1.5762],\n",
      "        [-1.4140, -1.2094, -1.3797, -1.5760],\n",
      "        [-1.4139, -1.2094, -1.3798, -1.5760],\n",
      "        [-1.4140, -1.2094, -1.3798, -1.5760],\n",
      "        [-1.4141, -1.2091, -1.3797, -1.5763],\n",
      "        [-1.4140, -1.2092, -1.3797, -1.5762],\n",
      "        [-1.4141, -1.2091, -1.3797, -1.5763],\n",
      "        [-1.4141, -1.2092, -1.3797, -1.5763],\n",
      "        [-1.4140, -1.2093, -1.3797, -1.5761],\n",
      "        [-1.4139, -1.2095, -1.3798, -1.5758],\n",
      "        [-1.4140, -1.2094, -1.3798, -1.5760],\n",
      "        [-1.4140, -1.2094, -1.3798, -1.5760],\n",
      "        [-1.4140, -1.2094, -1.3798, -1.5760],\n",
      "        [-1.4140, -1.2093, -1.3797, -1.5761],\n",
      "        [-1.4140, -1.2093, -1.3797, -1.5761],\n",
      "        [-1.4140, -1.2093, -1.3797, -1.5760],\n",
      "        [-1.4139, -1.2094, -1.3797, -1.5760],\n",
      "        [-1.4140, -1.2094, -1.3797, -1.5760],\n",
      "        [-1.4140, -1.2093, -1.3798, -1.5761],\n",
      "        [-1.4140, -1.2094, -1.3798, -1.5759],\n",
      "        [-1.4140, -1.2093, -1.3797, -1.5760],\n",
      "        [-1.4140, -1.2094, -1.3797, -1.5760],\n",
      "        [-1.4140, -1.2093, -1.3797, -1.5761],\n",
      "        [-1.4140, -1.2093, -1.3797, -1.5761],\n",
      "        [-1.4139, -1.2096, -1.3798, -1.5758],\n",
      "        [-1.4139, -1.2095, -1.3797, -1.5759],\n",
      "        [-1.4139, -1.2096, -1.3797, -1.5758],\n",
      "        [-1.4139, -1.2097, -1.3797, -1.5756],\n",
      "        [-1.4139, -1.2093, -1.3798, -1.5761],\n",
      "        [-1.4139, -1.2094, -1.3798, -1.5760],\n",
      "        [-1.4139, -1.2092, -1.3798, -1.5763],\n",
      "        [-1.4139, -1.2092, -1.3798, -1.5763],\n",
      "        [-1.4140, -1.2093, -1.3798, -1.5761],\n",
      "        [-1.4140, -1.2094, -1.3798, -1.5759],\n",
      "        [-1.4139, -1.2096, -1.3798, -1.5758],\n",
      "        [-1.4140, -1.2094, -1.3797, -1.5760],\n",
      "        [-1.4140, -1.2094, -1.3798, -1.5760],\n",
      "        [-1.4140, -1.2094, -1.3798, -1.5759],\n",
      "        [-1.4139, -1.2095, -1.3798, -1.5759],\n",
      "        [-1.4139, -1.2094, -1.3798, -1.5760],\n",
      "        [-1.4140, -1.2094, -1.3798, -1.5759],\n",
      "        [-1.4140, -1.2093, -1.3798, -1.5760],\n",
      "        [-1.4140, -1.2093, -1.3797, -1.5761],\n",
      "        [-1.4140, -1.2093, -1.3797, -1.5761],\n",
      "        [-1.4140, -1.2093, -1.3797, -1.5761],\n",
      "        [-1.4140, -1.2093, -1.3798, -1.5761],\n",
      "        [-1.4140, -1.2094, -1.3798, -1.5759],\n",
      "        [-1.4139, -1.2095, -1.3797, -1.5759],\n",
      "        [-1.4140, -1.2093, -1.3798, -1.5760],\n",
      "        [-1.4139, -1.2095, -1.3798, -1.5758],\n",
      "        [-1.4139, -1.2095, -1.3797, -1.5759],\n",
      "        [-1.4139, -1.2095, -1.3797, -1.5759],\n",
      "        [-1.4139, -1.2094, -1.3797, -1.5760],\n",
      "        [-1.4140, -1.2093, -1.3797, -1.5762],\n",
      "        [-1.4140, -1.2095, -1.3797, -1.5759],\n",
      "        [-1.4140, -1.2095, -1.3797, -1.5759],\n",
      "        [-1.4140, -1.2093, -1.3797, -1.5761],\n",
      "        [-1.4140, -1.2093, -1.3797, -1.5761],\n",
      "        [-1.4140, -1.2093, -1.3797, -1.5761],\n",
      "        [-1.4140, -1.2094, -1.3798, -1.5760]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([0.0051, 0.0045, 0.0025, 0.0013, 0.0053, 0.0033, 0.0026, 0.0014, 0.0054,\n",
      "        0.0034, 0.0029, 0.0014, 0.0052, 0.0032, 0.0030, 0.0012, 0.1126, 0.0046,\n",
      "        0.0025, 0.0014, 0.0058, 0.0044, 0.0027, 0.0012, 0.0049, 0.0041, 0.0025,\n",
      "        0.0014, 0.0055, 0.0041, 0.0028, 0.0012, 0.0043, 0.0031, 0.0026, 0.0016,\n",
      "        0.0055, 0.0030, 0.0024, 0.0009, 0.1126, 0.1284, 0.0023, 0.0013, 0.1316,\n",
      "        0.1286, 0.1470, 0.1471, 0.0051, 0.0044, 0.0027, 0.0011, 0.0061, 0.0037,\n",
      "        0.0031, 0.0016, 0.0046, 0.0042, 0.0028, 0.0015, 0.0046, 0.0040, 0.0030,\n",
      "        0.0012], grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(-0.0170, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2432, 0.2983, 0.2516, 0.2068],\n",
      "        [0.2432, 0.2984, 0.2516, 0.2068],\n",
      "        [0.2432, 0.2984, 0.2516, 0.2068],\n",
      "        [0.2432, 0.2983, 0.2516, 0.2068],\n",
      "        [0.2432, 0.2984, 0.2516, 0.2068],\n",
      "        [0.2432, 0.2984, 0.2516, 0.2068],\n",
      "        [0.2432, 0.2984, 0.2516, 0.2068],\n",
      "        [0.2432, 0.2984, 0.2516, 0.2068],\n",
      "        [0.2431, 0.2985, 0.2516, 0.2067],\n",
      "        [0.2432, 0.2984, 0.2516, 0.2068],\n",
      "        [0.2432, 0.2985, 0.2517, 0.2067],\n",
      "        [0.2432, 0.2985, 0.2516, 0.2067],\n",
      "        [0.2432, 0.2984, 0.2516, 0.2068],\n",
      "        [0.2432, 0.2983, 0.2516, 0.2068],\n",
      "        [0.2432, 0.2984, 0.2516, 0.2068],\n",
      "        [0.2432, 0.2984, 0.2516, 0.2068],\n",
      "        [0.2432, 0.2984, 0.2516, 0.2068],\n",
      "        [0.2432, 0.2984, 0.2516, 0.2068],\n",
      "        [0.2432, 0.2984, 0.2517, 0.2068],\n",
      "        [0.2432, 0.2984, 0.2516, 0.2068],\n",
      "        [0.2432, 0.2984, 0.2516, 0.2068],\n",
      "        [0.2432, 0.2984, 0.2516, 0.2068],\n",
      "        [0.2432, 0.2984, 0.2516, 0.2068],\n",
      "        [0.2432, 0.2984, 0.2516, 0.2068],\n",
      "        [0.2432, 0.2984, 0.2516, 0.2068],\n",
      "        [0.2432, 0.2984, 0.2516, 0.2068],\n",
      "        [0.2432, 0.2984, 0.2516, 0.2068],\n",
      "        [0.2432, 0.2984, 0.2516, 0.2068],\n",
      "        [0.2432, 0.2983, 0.2516, 0.2069],\n",
      "        [0.2432, 0.2983, 0.2517, 0.2068],\n",
      "        [0.2432, 0.2983, 0.2516, 0.2068],\n",
      "        [0.2432, 0.2983, 0.2516, 0.2069],\n",
      "        [0.2432, 0.2984, 0.2516, 0.2068],\n",
      "        [0.2432, 0.2984, 0.2516, 0.2068],\n",
      "        [0.2432, 0.2984, 0.2516, 0.2067],\n",
      "        [0.2432, 0.2984, 0.2516, 0.2067],\n",
      "        [0.2432, 0.2984, 0.2516, 0.2068],\n",
      "        [0.2432, 0.2984, 0.2516, 0.2068],\n",
      "        [0.2432, 0.2983, 0.2516, 0.2069],\n",
      "        [0.2432, 0.2984, 0.2516, 0.2068],\n",
      "        [0.2432, 0.2984, 0.2516, 0.2068],\n",
      "        [0.2432, 0.2984, 0.2516, 0.2068],\n",
      "        [0.2432, 0.2984, 0.2516, 0.2068],\n",
      "        [0.2432, 0.2984, 0.2516, 0.2068],\n",
      "        [0.2432, 0.2984, 0.2516, 0.2068],\n",
      "        [0.2432, 0.2984, 0.2516, 0.2068],\n",
      "        [0.2432, 0.2984, 0.2516, 0.2068],\n",
      "        [0.2432, 0.2984, 0.2516, 0.2068],\n",
      "        [0.2432, 0.2984, 0.2516, 0.2068],\n",
      "        [0.2432, 0.2984, 0.2516, 0.2068],\n",
      "        [0.2432, 0.2984, 0.2516, 0.2068],\n",
      "        [0.2432, 0.2984, 0.2516, 0.2068],\n",
      "        [0.2432, 0.2984, 0.2516, 0.2068],\n",
      "        [0.2432, 0.2983, 0.2516, 0.2068],\n",
      "        [0.2432, 0.2984, 0.2516, 0.2068],\n",
      "        [0.2432, 0.2983, 0.2516, 0.2068],\n",
      "        [0.2432, 0.2984, 0.2516, 0.2068],\n",
      "        [0.2432, 0.2984, 0.2516, 0.2068],\n",
      "        [0.2432, 0.2984, 0.2517, 0.2068],\n",
      "        [0.2432, 0.2984, 0.2516, 0.2068],\n",
      "        [0.2432, 0.2984, 0.2516, 0.2068],\n",
      "        [0.2432, 0.2984, 0.2516, 0.2068],\n",
      "        [0.2432, 0.2984, 0.2517, 0.2068],\n",
      "        [0.2432, 0.2984, 0.2516, 0.2068]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3778, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(-0.0298, grad_fn=<AddBackward0>)\n",
      "batching here\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94: done 2 episodes, mean_reward=0.00, best_reward=2.00, speed=232.60\n",
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.4115, -1.2142, -1.3803, -1.5713],\n",
      "        [-1.4115, -1.2139, -1.3803, -1.5716],\n",
      "        [-1.4115, -1.2141, -1.3803, -1.5715],\n",
      "        [-1.4115, -1.2139, -1.3803, -1.5716],\n",
      "        [-1.4115, -1.2140, -1.3804, -1.5716],\n",
      "        [-1.4115, -1.2140, -1.3803, -1.5716],\n",
      "        [-1.4114, -1.2141, -1.3804, -1.5714],\n",
      "        [-1.4114, -1.2140, -1.3804, -1.5717],\n",
      "        [-1.4115, -1.2140, -1.3803, -1.5716],\n",
      "        [-1.4116, -1.2138, -1.3803, -1.5717],\n",
      "        [-1.4115, -1.2142, -1.3804, -1.5712],\n",
      "        [-1.4115, -1.2140, -1.3803, -1.5715],\n",
      "        [-1.4115, -1.2141, -1.3803, -1.5715],\n",
      "        [-1.4115, -1.2141, -1.3803, -1.5715],\n",
      "        [-1.4115, -1.2141, -1.3803, -1.5715],\n",
      "        [-1.4115, -1.2141, -1.3803, -1.5715],\n",
      "        [-1.4115, -1.2140, -1.3803, -1.5716],\n",
      "        [-1.4115, -1.2141, -1.3803, -1.5715],\n",
      "        [-1.4115, -1.2139, -1.3803, -1.5718],\n",
      "        [-1.4115, -1.2140, -1.3803, -1.5715],\n",
      "        [-1.4114, -1.2141, -1.3804, -1.5715],\n",
      "        [-1.4115, -1.2140, -1.3803, -1.5716],\n",
      "        [-1.4115, -1.2139, -1.3803, -1.5717],\n",
      "        [-1.4114, -1.2142, -1.3804, -1.5713],\n",
      "        [-1.4115, -1.2139, -1.3803, -1.5717],\n",
      "        [-1.4115, -1.2139, -1.3803, -1.5716],\n",
      "        [-1.4115, -1.2139, -1.3803, -1.5717],\n",
      "        [-1.4115, -1.2140, -1.3803, -1.5715],\n",
      "        [-1.4114, -1.2142, -1.3803, -1.5713],\n",
      "        [-1.4115, -1.2141, -1.3803, -1.5715],\n",
      "        [-1.4114, -1.2141, -1.3803, -1.5716],\n",
      "        [-1.4115, -1.2140, -1.3803, -1.5715],\n",
      "        [-1.4115, -1.2138, -1.3803, -1.5718],\n",
      "        [-1.4114, -1.2142, -1.3803, -1.5714],\n",
      "        [-1.4115, -1.2139, -1.3803, -1.5718],\n",
      "        [-1.4115, -1.2138, -1.3803, -1.5719],\n",
      "        [-1.4115, -1.2139, -1.3803, -1.5717],\n",
      "        [-1.4115, -1.2141, -1.3803, -1.5715],\n",
      "        [-1.4115, -1.2141, -1.3804, -1.5714],\n",
      "        [-1.4115, -1.2139, -1.3803, -1.5717],\n",
      "        [-1.4115, -1.2141, -1.3803, -1.5714],\n",
      "        [-1.4115, -1.2140, -1.3803, -1.5717],\n",
      "        [-1.4115, -1.2141, -1.3803, -1.5716],\n",
      "        [-1.4115, -1.2141, -1.3803, -1.5715],\n",
      "        [-1.4115, -1.2140, -1.3803, -1.5715],\n",
      "        [-1.4115, -1.2141, -1.3803, -1.5715],\n",
      "        [-1.4115, -1.2140, -1.3803, -1.5715],\n",
      "        [-1.4115, -1.2139, -1.3803, -1.5717],\n",
      "        [-1.4115, -1.2141, -1.3803, -1.5715],\n",
      "        [-1.4115, -1.2139, -1.3803, -1.5717],\n",
      "        [-1.4115, -1.2140, -1.3803, -1.5715],\n",
      "        [-1.4115, -1.2138, -1.3803, -1.5718],\n",
      "        [-1.4114, -1.2144, -1.3803, -1.5712],\n",
      "        [-1.4114, -1.2142, -1.3804, -1.5714],\n",
      "        [-1.4115, -1.2140, -1.3803, -1.5716],\n",
      "        [-1.4114, -1.2140, -1.3803, -1.5716],\n",
      "        [-1.4115, -1.2142, -1.3803, -1.5713],\n",
      "        [-1.4114, -1.2142, -1.3803, -1.5713],\n",
      "        [-1.4115, -1.2142, -1.3803, -1.5713],\n",
      "        [-1.4115, -1.2141, -1.3803, -1.5715],\n",
      "        [-1.4115, -1.2140, -1.3804, -1.5715],\n",
      "        [-1.4115, -1.2139, -1.3803, -1.5716],\n",
      "        [-1.4115, -1.2141, -1.3803, -1.5714],\n",
      "        [-1.4115, -1.2140, -1.3803, -1.5716]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([0.0046, 0.0033, 0.0027, 0.0013, 0.1262, 0.1133, 0.0973, 0.0011, 0.0038,\n",
      "        0.0034, 0.0018, 0.0010, 0.0039, 0.0033, 0.0022, 0.0010, 0.0040, 0.0041,\n",
      "        0.0024, 0.0012, 0.0050, 0.0036, 0.0024, 0.0009, 0.1110, 0.0976, 0.1135,\n",
      "        0.0012, 0.0039, 0.0038, 0.0024, 0.0017, 0.0045, 0.0026, 0.0026, 0.0013,\n",
      "        0.0045, 0.0029, 0.0024, 0.0011, 0.0036, 0.0034, 0.0019, 0.0011, 0.0044,\n",
      "        0.0029, 0.0025, 0.0011, 0.0041, 0.0037, 0.0020, 0.0011, 0.0034, 0.0029,\n",
      "        0.0023, 0.0012, 0.0036, 0.0033, 0.0018, 0.0011, 0.0045, 0.0034, 0.0019,\n",
      "        0.0012], grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(-0.0127, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2438, 0.2970, 0.2515, 0.2078],\n",
      "        [0.2438, 0.2970, 0.2515, 0.2077],\n",
      "        [0.2438, 0.2970, 0.2515, 0.2077],\n",
      "        [0.2438, 0.2970, 0.2515, 0.2077],\n",
      "        [0.2438, 0.2970, 0.2515, 0.2077],\n",
      "        [0.2438, 0.2970, 0.2515, 0.2077],\n",
      "        [0.2438, 0.2970, 0.2515, 0.2077],\n",
      "        [0.2438, 0.2970, 0.2515, 0.2077],\n",
      "        [0.2438, 0.2970, 0.2515, 0.2077],\n",
      "        [0.2438, 0.2971, 0.2515, 0.2077],\n",
      "        [0.2438, 0.2969, 0.2515, 0.2078],\n",
      "        [0.2438, 0.2970, 0.2515, 0.2077],\n",
      "        [0.2438, 0.2970, 0.2515, 0.2077],\n",
      "        [0.2438, 0.2970, 0.2515, 0.2077],\n",
      "        [0.2438, 0.2970, 0.2515, 0.2077],\n",
      "        [0.2438, 0.2970, 0.2515, 0.2077],\n",
      "        [0.2438, 0.2970, 0.2515, 0.2077],\n",
      "        [0.2438, 0.2970, 0.2515, 0.2077],\n",
      "        [0.2438, 0.2971, 0.2515, 0.2077],\n",
      "        [0.2438, 0.2970, 0.2515, 0.2077],\n",
      "        [0.2438, 0.2970, 0.2515, 0.2077],\n",
      "        [0.2438, 0.2970, 0.2515, 0.2077],\n",
      "        [0.2438, 0.2970, 0.2515, 0.2077],\n",
      "        [0.2438, 0.2969, 0.2515, 0.2078],\n",
      "        [0.2438, 0.2970, 0.2515, 0.2077],\n",
      "        [0.2438, 0.2970, 0.2515, 0.2077],\n",
      "        [0.2438, 0.2970, 0.2515, 0.2077],\n",
      "        [0.2438, 0.2970, 0.2515, 0.2077],\n",
      "        [0.2438, 0.2969, 0.2515, 0.2078],\n",
      "        [0.2438, 0.2970, 0.2515, 0.2077],\n",
      "        [0.2438, 0.2970, 0.2515, 0.2077],\n",
      "        [0.2438, 0.2970, 0.2515, 0.2077],\n",
      "        [0.2438, 0.2971, 0.2515, 0.2077],\n",
      "        [0.2438, 0.2970, 0.2515, 0.2077],\n",
      "        [0.2438, 0.2970, 0.2515, 0.2077],\n",
      "        [0.2438, 0.2971, 0.2515, 0.2077],\n",
      "        [0.2438, 0.2970, 0.2515, 0.2077],\n",
      "        [0.2438, 0.2970, 0.2515, 0.2077],\n",
      "        [0.2438, 0.2970, 0.2515, 0.2078],\n",
      "        [0.2438, 0.2970, 0.2515, 0.2077],\n",
      "        [0.2438, 0.2970, 0.2515, 0.2078],\n",
      "        [0.2438, 0.2970, 0.2515, 0.2077],\n",
      "        [0.2438, 0.2970, 0.2515, 0.2077],\n",
      "        [0.2438, 0.2970, 0.2515, 0.2077],\n",
      "        [0.2438, 0.2970, 0.2515, 0.2077],\n",
      "        [0.2438, 0.2970, 0.2515, 0.2077],\n",
      "        [0.2438, 0.2970, 0.2515, 0.2077],\n",
      "        [0.2438, 0.2970, 0.2515, 0.2077],\n",
      "        [0.2438, 0.2970, 0.2515, 0.2077],\n",
      "        [0.2438, 0.2970, 0.2515, 0.2077],\n",
      "        [0.2438, 0.2970, 0.2515, 0.2077],\n",
      "        [0.2438, 0.2971, 0.2515, 0.2077],\n",
      "        [0.2438, 0.2969, 0.2515, 0.2078],\n",
      "        [0.2438, 0.2970, 0.2515, 0.2078],\n",
      "        [0.2438, 0.2970, 0.2515, 0.2077],\n",
      "        [0.2438, 0.2970, 0.2515, 0.2077],\n",
      "        [0.2438, 0.2970, 0.2515, 0.2078],\n",
      "        [0.2438, 0.2969, 0.2515, 0.2078],\n",
      "        [0.2438, 0.2969, 0.2515, 0.2078],\n",
      "        [0.2438, 0.2970, 0.2515, 0.2077],\n",
      "        [0.2438, 0.2970, 0.2515, 0.2077],\n",
      "        [0.2438, 0.2970, 0.2515, 0.2077],\n",
      "        [0.2438, 0.2970, 0.2515, 0.2078],\n",
      "        [0.2438, 0.2970, 0.2515, 0.2077]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3783, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(-0.0259, grad_fn=<AddBackward0>)\n",
      "batching here\n",
      "95: done 2 episodes, mean_reward=0.00, best_reward=2.00, speed=231.87\n",
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(0.0607, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.4112, -1.2199, -1.3798, -1.5643],\n",
      "        [-1.4112, -1.2198, -1.3797, -1.5644],\n",
      "        [-1.4112, -1.2198, -1.3797, -1.5645],\n",
      "        [-1.4112, -1.2199, -1.3797, -1.5644],\n",
      "        [-1.4111, -1.2197, -1.3798, -1.5645],\n",
      "        [-1.4112, -1.2196, -1.3798, -1.5647],\n",
      "        [-1.4112, -1.2200, -1.3797, -1.5642],\n",
      "        [-1.4111, -1.2200, -1.3798, -1.5642],\n",
      "        [-1.4112, -1.2198, -1.3797, -1.5645],\n",
      "        [-1.4112, -1.2201, -1.3797, -1.5640],\n",
      "        [-1.4112, -1.2196, -1.3797, -1.5647],\n",
      "        [-1.4112, -1.2198, -1.3797, -1.5644],\n",
      "        [-1.4112, -1.2199, -1.3797, -1.5643],\n",
      "        [-1.4112, -1.2198, -1.3798, -1.5644],\n",
      "        [-1.4112, -1.2199, -1.3798, -1.5643],\n",
      "        [-1.4112, -1.2199, -1.3797, -1.5644],\n",
      "        [-1.4112, -1.2200, -1.3797, -1.5641],\n",
      "        [-1.4112, -1.2197, -1.3797, -1.5646],\n",
      "        [-1.4112, -1.2200, -1.3797, -1.5642],\n",
      "        [-1.4112, -1.2199, -1.3797, -1.5644],\n",
      "        [-1.4112, -1.2199, -1.3797, -1.5643],\n",
      "        [-1.4112, -1.2198, -1.3797, -1.5644],\n",
      "        [-1.4112, -1.2199, -1.3797, -1.5643],\n",
      "        [-1.4111, -1.2199, -1.3797, -1.5643],\n",
      "        [-1.4112, -1.2198, -1.3798, -1.5643],\n",
      "        [-1.4112, -1.2197, -1.3797, -1.5645],\n",
      "        [-1.4112, -1.2199, -1.3797, -1.5643],\n",
      "        [-1.4111, -1.2201, -1.3797, -1.5641],\n",
      "        [-1.4111, -1.2203, -1.3797, -1.5638],\n",
      "        [-1.4111, -1.2203, -1.3798, -1.5638],\n",
      "        [-1.4112, -1.2198, -1.3797, -1.5644],\n",
      "        [-1.4112, -1.2198, -1.3797, -1.5644],\n",
      "        [-1.4112, -1.2198, -1.3797, -1.5645],\n",
      "        [-1.4112, -1.2198, -1.3797, -1.5645],\n",
      "        [-1.4111, -1.2198, -1.3797, -1.5645],\n",
      "        [-1.4111, -1.2199, -1.3797, -1.5644],\n",
      "        [-1.4112, -1.2199, -1.3797, -1.5644],\n",
      "        [-1.4112, -1.2197, -1.3797, -1.5645],\n",
      "        [-1.4112, -1.2198, -1.3798, -1.5643],\n",
      "        [-1.4112, -1.2198, -1.3797, -1.5644],\n",
      "        [-1.4112, -1.2198, -1.3797, -1.5644],\n",
      "        [-1.4112, -1.2199, -1.3797, -1.5643],\n",
      "        [-1.4111, -1.2199, -1.3797, -1.5643],\n",
      "        [-1.4112, -1.2198, -1.3797, -1.5644],\n",
      "        [-1.4112, -1.2199, -1.3798, -1.5643],\n",
      "        [-1.4112, -1.2198, -1.3797, -1.5644],\n",
      "        [-1.4112, -1.2198, -1.3797, -1.5644],\n",
      "        [-1.4112, -1.2199, -1.3797, -1.5642],\n",
      "        [-1.4112, -1.2197, -1.3797, -1.5645],\n",
      "        [-1.4112, -1.2199, -1.3797, -1.5643],\n",
      "        [-1.4113, -1.2195, -1.3797, -1.5648],\n",
      "        [-1.4111, -1.2204, -1.3798, -1.5637],\n",
      "        [-1.4112, -1.2198, -1.3797, -1.5644],\n",
      "        [-1.4111, -1.2203, -1.3797, -1.5638],\n",
      "        [-1.4111, -1.2202, -1.3798, -1.5639],\n",
      "        [-1.4112, -1.2197, -1.3798, -1.5645],\n",
      "        [-1.4112, -1.2197, -1.3797, -1.5645],\n",
      "        [-1.4112, -1.2199, -1.3798, -1.5642],\n",
      "        [-1.4112, -1.2198, -1.3797, -1.5644],\n",
      "        [-1.4112, -1.2197, -1.3797, -1.5645],\n",
      "        [-1.4112, -1.2199, -1.3797, -1.5644],\n",
      "        [-1.4112, -1.2199, -1.3797, -1.5643],\n",
      "        [-1.4112, -1.2199, -1.3797, -1.5643],\n",
      "        [-1.4112, -1.2199, -1.3797, -1.5643]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([ 3.7465e-03,  3.1259e-03,  1.8875e-03,  1.0891e-03,  4.2421e-03,\n",
      "         2.6508e-03,  1.4783e-03,  6.7346e-04,  3.9964e-03,  2.4122e-03,\n",
      "         1.5821e-03,  6.6173e-04,  9.8063e-02,  3.1075e-03,  1.8372e-03,\n",
      "         1.0775e-03,  4.2022e-03,  2.8724e-03,  1.7970e-03,  9.8952e-04,\n",
      "         3.9113e-03,  3.4808e-03,  2.3255e-03,  9.5008e-04,  3.8453e-03,\n",
      "         3.3933e-03,  2.2987e-03,  9.3003e-04,  3.2114e-03,  2.7347e-03,\n",
      "         2.7251e-03,  1.2255e-03,  4.0084e-03,  2.9075e-03,  2.0561e-03,\n",
      "         9.7498e-04,  3.3247e-03,  2.8404e-03,  1.3949e-03,  1.0219e-03,\n",
      "         3.8144e-03,  3.0434e-03,  1.6067e-03,  1.1639e-03,  3.6884e-03,\n",
      "         2.8446e-03,  1.5782e-03,  7.5188e-04,  4.1031e-03,  3.0622e-03,\n",
      "         2.0392e-03,  5.2554e-04,  3.9206e-03,  2.4412e-03,  1.3024e-03,\n",
      "         1.1903e-03, -1.1799e+00, -1.1931e+00, -1.3950e+00, -1.4100e+00,\n",
      "         8.6772e-02,  9.8070e-02,  2.1246e-03,  9.8687e-04],\n",
      "       grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(0.0744, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2439, 0.2953, 0.2516, 0.2092],\n",
      "        [0.2439, 0.2953, 0.2516, 0.2092],\n",
      "        [0.2439, 0.2953, 0.2516, 0.2092],\n",
      "        [0.2439, 0.2953, 0.2517, 0.2092],\n",
      "        [0.2439, 0.2953, 0.2516, 0.2092],\n",
      "        [0.2439, 0.2953, 0.2516, 0.2092],\n",
      "        [0.2439, 0.2952, 0.2516, 0.2093],\n",
      "        [0.2439, 0.2952, 0.2516, 0.2093],\n",
      "        [0.2439, 0.2953, 0.2517, 0.2092],\n",
      "        [0.2438, 0.2952, 0.2516, 0.2093],\n",
      "        [0.2438, 0.2954, 0.2516, 0.2092],\n",
      "        [0.2438, 0.2953, 0.2516, 0.2092],\n",
      "        [0.2439, 0.2952, 0.2516, 0.2092],\n",
      "        [0.2439, 0.2953, 0.2516, 0.2092],\n",
      "        [0.2439, 0.2953, 0.2516, 0.2092],\n",
      "        [0.2439, 0.2953, 0.2516, 0.2092],\n",
      "        [0.2439, 0.2952, 0.2516, 0.2093],\n",
      "        [0.2438, 0.2953, 0.2517, 0.2092],\n",
      "        [0.2439, 0.2952, 0.2517, 0.2092],\n",
      "        [0.2439, 0.2953, 0.2517, 0.2092],\n",
      "        [0.2439, 0.2953, 0.2516, 0.2092],\n",
      "        [0.2439, 0.2953, 0.2517, 0.2092],\n",
      "        [0.2439, 0.2953, 0.2516, 0.2092],\n",
      "        [0.2439, 0.2952, 0.2516, 0.2092],\n",
      "        [0.2439, 0.2953, 0.2516, 0.2092],\n",
      "        [0.2438, 0.2953, 0.2516, 0.2092],\n",
      "        [0.2438, 0.2953, 0.2517, 0.2092],\n",
      "        [0.2439, 0.2952, 0.2516, 0.2093],\n",
      "        [0.2439, 0.2951, 0.2516, 0.2093],\n",
      "        [0.2439, 0.2951, 0.2516, 0.2093],\n",
      "        [0.2439, 0.2953, 0.2516, 0.2092],\n",
      "        [0.2439, 0.2953, 0.2516, 0.2092],\n",
      "        [0.2439, 0.2953, 0.2517, 0.2092],\n",
      "        [0.2439, 0.2953, 0.2517, 0.2092],\n",
      "        [0.2439, 0.2953, 0.2517, 0.2092],\n",
      "        [0.2439, 0.2953, 0.2516, 0.2092],\n",
      "        [0.2439, 0.2953, 0.2517, 0.2092],\n",
      "        [0.2438, 0.2953, 0.2517, 0.2092],\n",
      "        [0.2438, 0.2953, 0.2516, 0.2092],\n",
      "        [0.2438, 0.2953, 0.2516, 0.2092],\n",
      "        [0.2439, 0.2953, 0.2516, 0.2092],\n",
      "        [0.2439, 0.2953, 0.2516, 0.2092],\n",
      "        [0.2439, 0.2953, 0.2516, 0.2092],\n",
      "        [0.2439, 0.2953, 0.2516, 0.2092],\n",
      "        [0.2439, 0.2953, 0.2516, 0.2092],\n",
      "        [0.2439, 0.2953, 0.2516, 0.2092],\n",
      "        [0.2439, 0.2953, 0.2516, 0.2092],\n",
      "        [0.2439, 0.2952, 0.2516, 0.2092],\n",
      "        [0.2438, 0.2953, 0.2516, 0.2092],\n",
      "        [0.2439, 0.2953, 0.2516, 0.2092],\n",
      "        [0.2438, 0.2954, 0.2517, 0.2091],\n",
      "        [0.2439, 0.2951, 0.2516, 0.2094],\n",
      "        [0.2439, 0.2953, 0.2516, 0.2092],\n",
      "        [0.2439, 0.2951, 0.2516, 0.2093],\n",
      "        [0.2439, 0.2952, 0.2516, 0.2093],\n",
      "        [0.2439, 0.2953, 0.2516, 0.2092],\n",
      "        [0.2438, 0.2953, 0.2516, 0.2092],\n",
      "        [0.2439, 0.2953, 0.2516, 0.2093],\n",
      "        [0.2439, 0.2953, 0.2516, 0.2092],\n",
      "        [0.2439, 0.2953, 0.2517, 0.2092],\n",
      "        [0.2439, 0.2953, 0.2517, 0.2092],\n",
      "        [0.2439, 0.2953, 0.2516, 0.2092],\n",
      "        [0.2438, 0.2953, 0.2517, 0.2092],\n",
      "        [0.2439, 0.2953, 0.2517, 0.2092]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3788, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(0.1213, grad_fn=<AddBackward0>)\n",
      "batching here\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(7.8272e-06, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.4032, -1.1910, -1.3916, -1.6014],\n",
      "        [-1.4032, -1.1910, -1.3916, -1.6014],\n",
      "        [-1.4032, -1.1910, -1.3916, -1.6014],\n",
      "        [-1.4032, -1.1910, -1.3916, -1.6014],\n",
      "        [-1.4032, -1.1908, -1.3916, -1.6018],\n",
      "        [-1.4032, -1.1907, -1.3916, -1.6018],\n",
      "        [-1.4032, -1.1909, -1.3916, -1.6015],\n",
      "        [-1.4032, -1.1911, -1.3916, -1.6013],\n",
      "        [-1.4034, -1.1905, -1.3916, -1.6020],\n",
      "        [-1.4033, -1.1907, -1.3916, -1.6018],\n",
      "        [-1.4033, -1.1911, -1.3916, -1.6011],\n",
      "        [-1.4033, -1.1908, -1.3916, -1.6016],\n",
      "        [-1.4032, -1.1911, -1.3916, -1.6013],\n",
      "        [-1.4033, -1.1909, -1.3916, -1.6015],\n",
      "        [-1.4032, -1.1910, -1.3916, -1.6014],\n",
      "        [-1.4033, -1.1909, -1.3916, -1.6016],\n",
      "        [-1.4032, -1.1910, -1.3916, -1.6014],\n",
      "        [-1.4032, -1.1909, -1.3917, -1.6015],\n",
      "        [-1.4032, -1.1909, -1.3916, -1.6015],\n",
      "        [-1.4032, -1.1909, -1.3916, -1.6016],\n",
      "        [-1.4032, -1.1911, -1.3916, -1.6014],\n",
      "        [-1.4032, -1.1910, -1.3916, -1.6014],\n",
      "        [-1.4033, -1.1909, -1.3916, -1.6016],\n",
      "        [-1.4032, -1.1911, -1.3916, -1.6013],\n",
      "        [-1.4033, -1.1910, -1.3916, -1.6014],\n",
      "        [-1.4033, -1.1908, -1.3916, -1.6017],\n",
      "        [-1.4033, -1.1910, -1.3916, -1.6014],\n",
      "        [-1.4033, -1.1910, -1.3916, -1.6014],\n",
      "        [-1.4032, -1.1911, -1.3916, -1.6013],\n",
      "        [-1.4032, -1.1913, -1.3916, -1.6012],\n",
      "        [-1.4032, -1.1911, -1.3916, -1.6013],\n",
      "        [-1.4032, -1.1911, -1.3916, -1.6013],\n",
      "        [-1.4032, -1.1909, -1.3916, -1.6016],\n",
      "        [-1.4032, -1.1911, -1.3916, -1.6014],\n",
      "        [-1.4032, -1.1910, -1.3916, -1.6015],\n",
      "        [-1.4032, -1.1909, -1.3916, -1.6016],\n",
      "        [-1.4033, -1.1908, -1.3916, -1.6017],\n",
      "        [-1.4033, -1.1909, -1.3916, -1.6016],\n",
      "        [-1.4033, -1.1909, -1.3916, -1.6016],\n",
      "        [-1.4033, -1.1909, -1.3916, -1.6016],\n",
      "        [-1.4032, -1.1909, -1.3916, -1.6016],\n",
      "        [-1.4032, -1.1912, -1.3916, -1.6012],\n",
      "        [-1.4032, -1.1910, -1.3916, -1.6014],\n",
      "        [-1.4033, -1.1909, -1.3916, -1.6016],\n",
      "        [-1.4033, -1.1908, -1.3916, -1.6017],\n",
      "        [-1.4033, -1.1910, -1.3916, -1.6015],\n",
      "        [-1.4033, -1.1909, -1.3916, -1.6016],\n",
      "        [-1.4033, -1.1909, -1.3916, -1.6016],\n",
      "        [-1.4033, -1.1910, -1.3916, -1.6014],\n",
      "        [-1.4032, -1.1910, -1.3916, -1.6015],\n",
      "        [-1.4032, -1.1909, -1.3916, -1.6016],\n",
      "        [-1.4033, -1.1908, -1.3916, -1.6017],\n",
      "        [-1.4032, -1.1909, -1.3916, -1.6016],\n",
      "        [-1.4032, -1.1910, -1.3916, -1.6014],\n",
      "        [-1.4032, -1.1912, -1.3916, -1.6012],\n",
      "        [-1.4032, -1.1910, -1.3916, -1.6014],\n",
      "        [-1.4032, -1.1910, -1.3916, -1.6014],\n",
      "        [-1.4032, -1.1909, -1.3916, -1.6016],\n",
      "        [-1.4032, -1.1908, -1.3916, -1.6017],\n",
      "        [-1.4032, -1.1910, -1.3916, -1.6014],\n",
      "        [-1.4033, -1.1909, -1.3916, -1.6015],\n",
      "        [-1.4033, -1.1909, -1.3916, -1.6016],\n",
      "        [-1.4033, -1.1910, -1.3916, -1.6015],\n",
      "        [-1.4033, -1.1909, -1.3916, -1.6015]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([0.0049, 0.0050, 0.0033, 0.0012, 0.0050, 0.0039, 0.0029, 0.0014, 0.0059,\n",
      "        0.0051, 0.0020, 0.0015, 0.0069, 0.0048, 0.0031, 0.0017, 0.0055, 0.0036,\n",
      "        0.0028, 0.0012, 0.0047, 0.0042, 0.0029, 0.0014, 0.0054, 0.0043, 0.0032,\n",
      "        0.0015, 0.0064, 0.0041, 0.0029, 0.0014, 0.0055, 0.0046, 0.0028, 0.0014,\n",
      "        0.0059, 0.0044, 0.0034, 0.0015, 0.0049, 0.0038, 0.0027, 0.0012, 0.0058,\n",
      "        0.0036, 0.0032, 0.0013, 0.0063, 0.0050, 0.0025, 0.0016, 0.0050, 0.0042,\n",
      "        0.0027, 0.0015, 0.0049, 0.0044, 0.0030, 0.0015, 0.0056, 0.0043, 0.0028,\n",
      "        0.0012], grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(-0.0035, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2458, 0.3039, 0.2487, 0.2016],\n",
      "        [0.2458, 0.3039, 0.2487, 0.2016],\n",
      "        [0.2458, 0.3039, 0.2487, 0.2016],\n",
      "        [0.2458, 0.3039, 0.2487, 0.2016],\n",
      "        [0.2458, 0.3040, 0.2487, 0.2015],\n",
      "        [0.2458, 0.3040, 0.2487, 0.2015],\n",
      "        [0.2458, 0.3039, 0.2487, 0.2016],\n",
      "        [0.2458, 0.3039, 0.2487, 0.2016],\n",
      "        [0.2458, 0.3041, 0.2487, 0.2015],\n",
      "        [0.2458, 0.3040, 0.2487, 0.2015],\n",
      "        [0.2458, 0.3039, 0.2487, 0.2017],\n",
      "        [0.2458, 0.3040, 0.2487, 0.2016],\n",
      "        [0.2458, 0.3039, 0.2487, 0.2016],\n",
      "        [0.2458, 0.3039, 0.2487, 0.2016],\n",
      "        [0.2458, 0.3039, 0.2487, 0.2016],\n",
      "        [0.2458, 0.3040, 0.2487, 0.2016],\n",
      "        [0.2458, 0.3039, 0.2487, 0.2016],\n",
      "        [0.2458, 0.3039, 0.2487, 0.2016],\n",
      "        [0.2458, 0.3039, 0.2487, 0.2016],\n",
      "        [0.2458, 0.3039, 0.2487, 0.2016],\n",
      "        [0.2458, 0.3039, 0.2487, 0.2016],\n",
      "        [0.2458, 0.3039, 0.2487, 0.2016],\n",
      "        [0.2458, 0.3039, 0.2487, 0.2016],\n",
      "        [0.2458, 0.3039, 0.2487, 0.2016],\n",
      "        [0.2458, 0.3039, 0.2487, 0.2016],\n",
      "        [0.2458, 0.3040, 0.2487, 0.2016],\n",
      "        [0.2458, 0.3039, 0.2487, 0.2016],\n",
      "        [0.2458, 0.3039, 0.2487, 0.2016],\n",
      "        [0.2458, 0.3039, 0.2487, 0.2016],\n",
      "        [0.2458, 0.3038, 0.2487, 0.2017],\n",
      "        [0.2458, 0.3039, 0.2487, 0.2016],\n",
      "        [0.2458, 0.3039, 0.2487, 0.2016],\n",
      "        [0.2458, 0.3039, 0.2487, 0.2016],\n",
      "        [0.2458, 0.3039, 0.2487, 0.2016],\n",
      "        [0.2458, 0.3039, 0.2487, 0.2016],\n",
      "        [0.2458, 0.3039, 0.2487, 0.2016],\n",
      "        [0.2458, 0.3040, 0.2487, 0.2016],\n",
      "        [0.2458, 0.3040, 0.2487, 0.2016],\n",
      "        [0.2458, 0.3040, 0.2487, 0.2016],\n",
      "        [0.2458, 0.3040, 0.2487, 0.2016],\n",
      "        [0.2458, 0.3040, 0.2487, 0.2016],\n",
      "        [0.2458, 0.3039, 0.2487, 0.2017],\n",
      "        [0.2458, 0.3039, 0.2487, 0.2016],\n",
      "        [0.2458, 0.3039, 0.2487, 0.2016],\n",
      "        [0.2458, 0.3040, 0.2487, 0.2016],\n",
      "        [0.2458, 0.3039, 0.2487, 0.2016],\n",
      "        [0.2458, 0.3040, 0.2487, 0.2016],\n",
      "        [0.2458, 0.3039, 0.2487, 0.2016],\n",
      "        [0.2458, 0.3039, 0.2487, 0.2016],\n",
      "        [0.2458, 0.3039, 0.2487, 0.2016],\n",
      "        [0.2458, 0.3039, 0.2487, 0.2016],\n",
      "        [0.2458, 0.3040, 0.2487, 0.2016],\n",
      "        [0.2458, 0.3040, 0.2487, 0.2016],\n",
      "        [0.2458, 0.3039, 0.2487, 0.2016],\n",
      "        [0.2458, 0.3039, 0.2487, 0.2017],\n",
      "        [0.2458, 0.3039, 0.2487, 0.2016],\n",
      "        [0.2458, 0.3039, 0.2487, 0.2016],\n",
      "        [0.2458, 0.3039, 0.2487, 0.2016],\n",
      "        [0.2458, 0.3040, 0.2487, 0.2015],\n",
      "        [0.2458, 0.3039, 0.2487, 0.2016],\n",
      "        [0.2458, 0.3039, 0.2487, 0.2016],\n",
      "        [0.2458, 0.3040, 0.2487, 0.2016],\n",
      "        [0.2458, 0.3039, 0.2487, 0.2016],\n",
      "        [0.2458, 0.3039, 0.2487, 0.2016]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3758, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(-0.0173, grad_fn=<AddBackward0>)\n",
      "batching here\n",
      "97: done 2 episodes, mean_reward=0.00, best_reward=2.00, speed=228.81\n",
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.4027, -1.1939, -1.3917, -1.5978],\n",
      "        [-1.4027, -1.1939, -1.3917, -1.5978],\n",
      "        [-1.4027, -1.1938, -1.3917, -1.5978],\n",
      "        [-1.4027, -1.1937, -1.3917, -1.5980],\n",
      "        [-1.4026, -1.1938, -1.3917, -1.5978],\n",
      "        [-1.4026, -1.1938, -1.3917, -1.5979],\n",
      "        [-1.4027, -1.1936, -1.3917, -1.5981],\n",
      "        [-1.4027, -1.1937, -1.3917, -1.5979],\n",
      "        [-1.4027, -1.1936, -1.3918, -1.5980],\n",
      "        [-1.4028, -1.1935, -1.3917, -1.5982],\n",
      "        [-1.4027, -1.1935, -1.3917, -1.5982],\n",
      "        [-1.4027, -1.1937, -1.3916, -1.5980],\n",
      "        [-1.4026, -1.1940, -1.3917, -1.5975],\n",
      "        [-1.4027, -1.1936, -1.3917, -1.5980],\n",
      "        [-1.4027, -1.1939, -1.3917, -1.5977],\n",
      "        [-1.4027, -1.1937, -1.3917, -1.5980],\n",
      "        [-1.4027, -1.1937, -1.3917, -1.5980],\n",
      "        [-1.4027, -1.1937, -1.3917, -1.5980],\n",
      "        [-1.4027, -1.1938, -1.3917, -1.5979],\n",
      "        [-1.4027, -1.1937, -1.3917, -1.5979],\n",
      "        [-1.4027, -1.1938, -1.3917, -1.5979],\n",
      "        [-1.4027, -1.1938, -1.3917, -1.5978],\n",
      "        [-1.4027, -1.1939, -1.3917, -1.5977],\n",
      "        [-1.4027, -1.1935, -1.3917, -1.5982],\n",
      "        [-1.4027, -1.1937, -1.3917, -1.5979],\n",
      "        [-1.4027, -1.1937, -1.3916, -1.5980],\n",
      "        [-1.4027, -1.1936, -1.3916, -1.5981],\n",
      "        [-1.4027, -1.1938, -1.3917, -1.5978],\n",
      "        [-1.4027, -1.1939, -1.3916, -1.5977],\n",
      "        [-1.4027, -1.1939, -1.3916, -1.5978],\n",
      "        [-1.4026, -1.1940, -1.3916, -1.5977],\n",
      "        [-1.4026, -1.1939, -1.3916, -1.5978],\n",
      "        [-1.4027, -1.1937, -1.3917, -1.5980],\n",
      "        [-1.4027, -1.1936, -1.3917, -1.5981],\n",
      "        [-1.4027, -1.1936, -1.3917, -1.5981],\n",
      "        [-1.4027, -1.1936, -1.3917, -1.5981],\n",
      "        [-1.4027, -1.1937, -1.3917, -1.5980],\n",
      "        [-1.4027, -1.1938, -1.3916, -1.5978],\n",
      "        [-1.4027, -1.1938, -1.3917, -1.5978],\n",
      "        [-1.4027, -1.1938, -1.3917, -1.5978],\n",
      "        [-1.4027, -1.1937, -1.3917, -1.5979],\n",
      "        [-1.4027, -1.1938, -1.3917, -1.5978],\n",
      "        [-1.4027, -1.1938, -1.3916, -1.5978],\n",
      "        [-1.4027, -1.1938, -1.3917, -1.5978],\n",
      "        [-1.4027, -1.1937, -1.3917, -1.5980],\n",
      "        [-1.4027, -1.1938, -1.3917, -1.5978],\n",
      "        [-1.4027, -1.1938, -1.3917, -1.5978],\n",
      "        [-1.4027, -1.1936, -1.3917, -1.5980],\n",
      "        [-1.4027, -1.1939, -1.3916, -1.5978],\n",
      "        [-1.4027, -1.1939, -1.3916, -1.5977],\n",
      "        [-1.4027, -1.1940, -1.3917, -1.5976],\n",
      "        [-1.4027, -1.1939, -1.3917, -1.5978],\n",
      "        [-1.4027, -1.1938, -1.3917, -1.5978],\n",
      "        [-1.4027, -1.1938, -1.3917, -1.5978],\n",
      "        [-1.4027, -1.1938, -1.3917, -1.5979],\n",
      "        [-1.4027, -1.1939, -1.3917, -1.5978],\n",
      "        [-1.4026, -1.1941, -1.3916, -1.5975],\n",
      "        [-1.4027, -1.1939, -1.3917, -1.5976],\n",
      "        [-1.4026, -1.1939, -1.3917, -1.5978],\n",
      "        [-1.4027, -1.1939, -1.3916, -1.5977],\n",
      "        [-1.4027, -1.1937, -1.3917, -1.5979],\n",
      "        [-1.4027, -1.1939, -1.3917, -1.5977],\n",
      "        [-1.4026, -1.1939, -1.3917, -1.5977],\n",
      "        [-1.4027, -1.1937, -1.3917, -1.5979]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([0.1407, 0.1603, 0.1198, 0.0016, 0.0046, 0.0047, 0.0035, 0.0011, 0.0056,\n",
      "        0.0040, 0.0023, 0.0011, 0.0049, 0.0050, 0.0028, 0.0026, 0.0048, 0.0043,\n",
      "        0.0032, 0.0015, 0.0053, 0.0048, 0.0029, 0.0020, 0.0055, 0.0042, 0.0025,\n",
      "        0.0012, 0.0065, 0.0050, 0.0025, 0.0017, 0.1606, 0.0044, 0.0030, 0.0016,\n",
      "        0.0055, 0.0045, 0.0026, 0.0011, 0.0060, 0.0044, 0.0028, 0.0010, 0.0048,\n",
      "        0.0035, 0.0030, 0.0015, 0.0063, 0.0047, 0.0025, 0.0010, 0.0054, 0.0047,\n",
      "        0.0028, 0.0012, 0.0057, 0.0040, 0.0027, 0.0012, 0.0056, 0.0040, 0.0026,\n",
      "        0.0015], grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(-0.0123, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2459, 0.3030, 0.2487, 0.2023],\n",
      "        [0.2459, 0.3030, 0.2487, 0.2023],\n",
      "        [0.2459, 0.3031, 0.2487, 0.2023],\n",
      "        [0.2459, 0.3031, 0.2487, 0.2023],\n",
      "        [0.2459, 0.3031, 0.2486, 0.2023],\n",
      "        [0.2459, 0.3031, 0.2487, 0.2023],\n",
      "        [0.2459, 0.3031, 0.2487, 0.2023],\n",
      "        [0.2459, 0.3031, 0.2487, 0.2023],\n",
      "        [0.2459, 0.3031, 0.2486, 0.2023],\n",
      "        [0.2459, 0.3032, 0.2487, 0.2023],\n",
      "        [0.2459, 0.3032, 0.2487, 0.2023],\n",
      "        [0.2459, 0.3031, 0.2487, 0.2023],\n",
      "        [0.2459, 0.3030, 0.2487, 0.2024],\n",
      "        [0.2459, 0.3031, 0.2487, 0.2023],\n",
      "        [0.2459, 0.3030, 0.2487, 0.2024],\n",
      "        [0.2459, 0.3031, 0.2487, 0.2023],\n",
      "        [0.2459, 0.3031, 0.2487, 0.2023],\n",
      "        [0.2459, 0.3031, 0.2487, 0.2023],\n",
      "        [0.2459, 0.3031, 0.2487, 0.2023],\n",
      "        [0.2459, 0.3031, 0.2487, 0.2023],\n",
      "        [0.2459, 0.3031, 0.2487, 0.2023],\n",
      "        [0.2459, 0.3031, 0.2487, 0.2023],\n",
      "        [0.2459, 0.3030, 0.2487, 0.2024],\n",
      "        [0.2459, 0.3032, 0.2487, 0.2023],\n",
      "        [0.2459, 0.3031, 0.2487, 0.2023],\n",
      "        [0.2459, 0.3031, 0.2487, 0.2023],\n",
      "        [0.2459, 0.3031, 0.2487, 0.2023],\n",
      "        [0.2459, 0.3031, 0.2487, 0.2023],\n",
      "        [0.2459, 0.3030, 0.2487, 0.2024],\n",
      "        [0.2459, 0.3030, 0.2487, 0.2023],\n",
      "        [0.2459, 0.3030, 0.2487, 0.2024],\n",
      "        [0.2459, 0.3030, 0.2487, 0.2023],\n",
      "        [0.2459, 0.3031, 0.2487, 0.2023],\n",
      "        [0.2459, 0.3031, 0.2487, 0.2023],\n",
      "        [0.2459, 0.3031, 0.2487, 0.2023],\n",
      "        [0.2459, 0.3031, 0.2487, 0.2023],\n",
      "        [0.2459, 0.3031, 0.2487, 0.2023],\n",
      "        [0.2459, 0.3031, 0.2487, 0.2023],\n",
      "        [0.2459, 0.3031, 0.2487, 0.2023],\n",
      "        [0.2459, 0.3031, 0.2487, 0.2023],\n",
      "        [0.2459, 0.3031, 0.2487, 0.2023],\n",
      "        [0.2459, 0.3031, 0.2487, 0.2023],\n",
      "        [0.2459, 0.3031, 0.2487, 0.2023],\n",
      "        [0.2459, 0.3031, 0.2487, 0.2023],\n",
      "        [0.2459, 0.3031, 0.2487, 0.2023],\n",
      "        [0.2459, 0.3031, 0.2487, 0.2023],\n",
      "        [0.2459, 0.3031, 0.2487, 0.2023],\n",
      "        [0.2459, 0.3031, 0.2487, 0.2023],\n",
      "        [0.2459, 0.3031, 0.2487, 0.2023],\n",
      "        [0.2459, 0.3030, 0.2487, 0.2024],\n",
      "        [0.2459, 0.3030, 0.2487, 0.2024],\n",
      "        [0.2459, 0.3031, 0.2487, 0.2023],\n",
      "        [0.2459, 0.3031, 0.2487, 0.2023],\n",
      "        [0.2459, 0.3031, 0.2487, 0.2023],\n",
      "        [0.2459, 0.3031, 0.2487, 0.2023],\n",
      "        [0.2459, 0.3030, 0.2487, 0.2024],\n",
      "        [0.2460, 0.3030, 0.2487, 0.2024],\n",
      "        [0.2459, 0.3030, 0.2487, 0.2024],\n",
      "        [0.2459, 0.3030, 0.2487, 0.2024],\n",
      "        [0.2459, 0.3030, 0.2487, 0.2024],\n",
      "        [0.2459, 0.3031, 0.2487, 0.2023],\n",
      "        [0.2459, 0.3030, 0.2487, 0.2024],\n",
      "        [0.2459, 0.3030, 0.2487, 0.2024],\n",
      "        [0.2459, 0.3031, 0.2487, 0.2023]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3761, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(-0.0254, grad_fn=<AddBackward0>)\n",
      "batching here\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98: done 2 episodes, mean_reward=0.00, best_reward=2.00, speed=228.33\n",
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(0.0314, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.4023, -1.1950, -1.3892, -1.5995],\n",
      "        [-1.4023, -1.1951, -1.3892, -1.5994],\n",
      "        [-1.4022, -1.1950, -1.3893, -1.5994],\n",
      "        [-1.4023, -1.1950, -1.3892, -1.5994],\n",
      "        [-1.4022, -1.1951, -1.3893, -1.5994],\n",
      "        [-1.4022, -1.1949, -1.3893, -1.5997],\n",
      "        [-1.4022, -1.1950, -1.3892, -1.5996],\n",
      "        [-1.4022, -1.1950, -1.3893, -1.5995],\n",
      "        [-1.4023, -1.1949, -1.3893, -1.5996],\n",
      "        [-1.4023, -1.1949, -1.3893, -1.5996],\n",
      "        [-1.4023, -1.1947, -1.3893, -1.5998],\n",
      "        [-1.4023, -1.1948, -1.3893, -1.5997],\n",
      "        [-1.4022, -1.1956, -1.3893, -1.5987],\n",
      "        [-1.4023, -1.1950, -1.3893, -1.5995],\n",
      "        [-1.4023, -1.1952, -1.3892, -1.5993],\n",
      "        [-1.4023, -1.1951, -1.3892, -1.5994],\n",
      "        [-1.4023, -1.1951, -1.3892, -1.5993],\n",
      "        [-1.4023, -1.1951, -1.3892, -1.5993],\n",
      "        [-1.4023, -1.1950, -1.3893, -1.5994],\n",
      "        [-1.4023, -1.1950, -1.3893, -1.5995],\n",
      "        [-1.4022, -1.1957, -1.3893, -1.5986],\n",
      "        [-1.4023, -1.1952, -1.3892, -1.5993],\n",
      "        [-1.4023, -1.1951, -1.3892, -1.5994],\n",
      "        [-1.4022, -1.1951, -1.3892, -1.5995],\n",
      "        [-1.4023, -1.1950, -1.3892, -1.5995],\n",
      "        [-1.4023, -1.1950, -1.3893, -1.5994],\n",
      "        [-1.4023, -1.1951, -1.3892, -1.5994],\n",
      "        [-1.4022, -1.1952, -1.3893, -1.5993],\n",
      "        [-1.4022, -1.1954, -1.3892, -1.5990],\n",
      "        [-1.4022, -1.1952, -1.3893, -1.5992],\n",
      "        [-1.4022, -1.1954, -1.3892, -1.5990],\n",
      "        [-1.4023, -1.1950, -1.3892, -1.5995],\n",
      "        [-1.4024, -1.1951, -1.3892, -1.5993],\n",
      "        [-1.4022, -1.1953, -1.3893, -1.5991],\n",
      "        [-1.4023, -1.1950, -1.3892, -1.5995],\n",
      "        [-1.4023, -1.1950, -1.3892, -1.5995],\n",
      "        [-1.4023, -1.1950, -1.3893, -1.5994],\n",
      "        [-1.4023, -1.1950, -1.3892, -1.5994],\n",
      "        [-1.4023, -1.1951, -1.3892, -1.5994],\n",
      "        [-1.4023, -1.1951, -1.3892, -1.5993],\n",
      "        [-1.4023, -1.1948, -1.3892, -1.5997],\n",
      "        [-1.4023, -1.1951, -1.3893, -1.5994],\n",
      "        [-1.4022, -1.1950, -1.3893, -1.5994],\n",
      "        [-1.4022, -1.1951, -1.3893, -1.5993],\n",
      "        [-1.4023, -1.1950, -1.3892, -1.5995],\n",
      "        [-1.4023, -1.1951, -1.3892, -1.5993],\n",
      "        [-1.4023, -1.1951, -1.3893, -1.5993],\n",
      "        [-1.4023, -1.1951, -1.3892, -1.5993],\n",
      "        [-1.4022, -1.1952, -1.3893, -1.5993],\n",
      "        [-1.4023, -1.1951, -1.3892, -1.5994],\n",
      "        [-1.4023, -1.1948, -1.3893, -1.5998],\n",
      "        [-1.4022, -1.1950, -1.3893, -1.5995],\n",
      "        [-1.4022, -1.1952, -1.3893, -1.5993],\n",
      "        [-1.4022, -1.1953, -1.3893, -1.5991],\n",
      "        [-1.4022, -1.1952, -1.3893, -1.5992],\n",
      "        [-1.4022, -1.1953, -1.3893, -1.5991],\n",
      "        [-1.4022, -1.1952, -1.3892, -1.5992],\n",
      "        [-1.4023, -1.1953, -1.3892, -1.5991],\n",
      "        [-1.4022, -1.1954, -1.3892, -1.5991],\n",
      "        [-1.4022, -1.1957, -1.3892, -1.5987],\n",
      "        [-1.4023, -1.1950, -1.3893, -1.5995],\n",
      "        [-1.4022, -1.1953, -1.3893, -1.5991],\n",
      "        [-1.4023, -1.1952, -1.3892, -1.5993],\n",
      "        [-1.4023, -1.1951, -1.3892, -1.5994]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([ 5.0871e-03,  4.1266e-03,  1.9879e-03,  1.4195e-03,  5.7715e-03,\n",
      "         3.9527e-03,  2.5825e-03,  1.4056e-03, -1.1787e+00, -1.1916e+00,\n",
      "         2.5008e-03,  1.2580e-03,  4.6537e-03,  4.9404e-03,  2.2388e-03,\n",
      "         1.3659e-03,  1.2632e-01,  1.2752e-01,  1.2761e-01,  1.0874e-01,\n",
      "         4.2950e-03,  4.1904e-03,  2.5545e-03,  1.3466e-03,  4.4515e-03,\n",
      "         3.3059e-03,  2.5718e-03,  1.3638e-03,  3.9612e-03,  4.1301e-03,\n",
      "         2.4805e-03,  1.2386e-03,  5.2202e-03,  4.2270e-03,  2.3757e-03,\n",
      "         1.5209e-03,  1.0882e-01,  3.7604e-03,  2.5077e-03,  1.1060e-03,\n",
      "         6.1656e-03,  4.3760e-03,  2.6646e-03,  1.2945e-03,  5.6462e-03,\n",
      "         3.6039e-03,  2.3074e-03,  1.1889e-03,  5.7330e-03,  3.7894e-03,\n",
      "         2.9864e-03,  1.2035e-03,  5.6030e-03,  3.4900e-03,  2.0339e-03,\n",
      "         1.1021e-03,  4.4100e-03,  3.9181e-03,  2.6602e-03,  9.8488e-04,\n",
      "         4.3442e-03,  3.4605e-03,  2.4617e-03,  1.1242e-03],\n",
      "       grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(0.0249, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2460, 0.3027, 0.2493, 0.2020],\n",
      "        [0.2460, 0.3027, 0.2493, 0.2020],\n",
      "        [0.2460, 0.3027, 0.2492, 0.2020],\n",
      "        [0.2460, 0.3027, 0.2493, 0.2020],\n",
      "        [0.2460, 0.3027, 0.2493, 0.2020],\n",
      "        [0.2460, 0.3027, 0.2493, 0.2020],\n",
      "        [0.2460, 0.3027, 0.2493, 0.2020],\n",
      "        [0.2461, 0.3027, 0.2493, 0.2020],\n",
      "        [0.2460, 0.3027, 0.2493, 0.2020],\n",
      "        [0.2460, 0.3027, 0.2492, 0.2020],\n",
      "        [0.2460, 0.3028, 0.2493, 0.2019],\n",
      "        [0.2460, 0.3028, 0.2493, 0.2019],\n",
      "        [0.2461, 0.3025, 0.2493, 0.2022],\n",
      "        [0.2460, 0.3027, 0.2493, 0.2020],\n",
      "        [0.2460, 0.3026, 0.2493, 0.2020],\n",
      "        [0.2460, 0.3027, 0.2493, 0.2020],\n",
      "        [0.2460, 0.3027, 0.2493, 0.2020],\n",
      "        [0.2460, 0.3027, 0.2493, 0.2020],\n",
      "        [0.2460, 0.3027, 0.2493, 0.2020],\n",
      "        [0.2460, 0.3027, 0.2493, 0.2020],\n",
      "        [0.2461, 0.3025, 0.2493, 0.2022],\n",
      "        [0.2460, 0.3027, 0.2493, 0.2020],\n",
      "        [0.2460, 0.3027, 0.2493, 0.2020],\n",
      "        [0.2460, 0.3027, 0.2493, 0.2020],\n",
      "        [0.2460, 0.3027, 0.2493, 0.2020],\n",
      "        [0.2460, 0.3027, 0.2493, 0.2020],\n",
      "        [0.2460, 0.3027, 0.2493, 0.2020],\n",
      "        [0.2460, 0.3027, 0.2493, 0.2020],\n",
      "        [0.2461, 0.3026, 0.2493, 0.2021],\n",
      "        [0.2460, 0.3026, 0.2493, 0.2021],\n",
      "        [0.2460, 0.3026, 0.2493, 0.2021],\n",
      "        [0.2460, 0.3027, 0.2493, 0.2020],\n",
      "        [0.2460, 0.3027, 0.2493, 0.2020],\n",
      "        [0.2460, 0.3026, 0.2493, 0.2021],\n",
      "        [0.2460, 0.3027, 0.2493, 0.2020],\n",
      "        [0.2460, 0.3027, 0.2493, 0.2020],\n",
      "        [0.2460, 0.3027, 0.2493, 0.2020],\n",
      "        [0.2460, 0.3027, 0.2493, 0.2020],\n",
      "        [0.2460, 0.3027, 0.2493, 0.2020],\n",
      "        [0.2460, 0.3027, 0.2493, 0.2020],\n",
      "        [0.2460, 0.3028, 0.2493, 0.2020],\n",
      "        [0.2460, 0.3027, 0.2493, 0.2020],\n",
      "        [0.2460, 0.3027, 0.2493, 0.2020],\n",
      "        [0.2460, 0.3027, 0.2492, 0.2020],\n",
      "        [0.2460, 0.3027, 0.2493, 0.2020],\n",
      "        [0.2460, 0.3027, 0.2493, 0.2020],\n",
      "        [0.2460, 0.3027, 0.2493, 0.2020],\n",
      "        [0.2460, 0.3027, 0.2493, 0.2020],\n",
      "        [0.2461, 0.3027, 0.2493, 0.2020],\n",
      "        [0.2460, 0.3027, 0.2493, 0.2020],\n",
      "        [0.2460, 0.3028, 0.2493, 0.2019],\n",
      "        [0.2460, 0.3027, 0.2492, 0.2020],\n",
      "        [0.2460, 0.3026, 0.2493, 0.2020],\n",
      "        [0.2461, 0.3026, 0.2493, 0.2021],\n",
      "        [0.2460, 0.3026, 0.2493, 0.2021],\n",
      "        [0.2460, 0.3026, 0.2493, 0.2021],\n",
      "        [0.2460, 0.3026, 0.2493, 0.2021],\n",
      "        [0.2460, 0.3026, 0.2493, 0.2021],\n",
      "        [0.2461, 0.3026, 0.2493, 0.2021],\n",
      "        [0.2461, 0.3025, 0.2493, 0.2022],\n",
      "        [0.2460, 0.3027, 0.2493, 0.2020],\n",
      "        [0.2460, 0.3026, 0.2493, 0.2021],\n",
      "        [0.2460, 0.3027, 0.2493, 0.2020],\n",
      "        [0.2460, 0.3027, 0.2493, 0.2020]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3761, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(0.0426, grad_fn=<AddBackward0>)\n",
      "batching here\n",
      "99: done 1 episodes, mean_reward=0.00, best_reward=2.00, speed=226.90\n",
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(0.0162, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.4261, -1.1264, -1.4043, -1.6605],\n",
      "        [-1.4261, -1.1266, -1.4043, -1.6602],\n",
      "        [-1.4261, -1.1267, -1.4043, -1.6602],\n",
      "        [-1.4262, -1.1263, -1.4043, -1.6606],\n",
      "        [-1.4261, -1.1263, -1.4044, -1.6606],\n",
      "        [-1.4261, -1.1266, -1.4043, -1.6603],\n",
      "        [-1.4261, -1.1267, -1.4043, -1.6601],\n",
      "        [-1.4262, -1.1263, -1.4043, -1.6607],\n",
      "        [-1.4262, -1.1263, -1.4043, -1.6607],\n",
      "        [-1.4262, -1.1264, -1.4043, -1.6605],\n",
      "        [-1.4262, -1.1262, -1.4043, -1.6607],\n",
      "        [-1.4263, -1.1263, -1.4043, -1.6606],\n",
      "        [-1.4261, -1.1268, -1.4043, -1.6600],\n",
      "        [-1.4261, -1.1266, -1.4043, -1.6603],\n",
      "        [-1.4261, -1.1266, -1.4043, -1.6603],\n",
      "        [-1.4261, -1.1266, -1.4043, -1.6603],\n",
      "        [-1.4261, -1.1265, -1.4043, -1.6603],\n",
      "        [-1.4261, -1.1265, -1.4043, -1.6603],\n",
      "        [-1.4262, -1.1266, -1.4043, -1.6602],\n",
      "        [-1.4261, -1.1265, -1.4043, -1.6605],\n",
      "        [-1.4261, -1.1266, -1.4043, -1.6604],\n",
      "        [-1.4261, -1.1268, -1.4042, -1.6601],\n",
      "        [-1.4261, -1.1269, -1.4042, -1.6599],\n",
      "        [-1.4261, -1.1267, -1.4042, -1.6601],\n",
      "        [-1.4261, -1.1266, -1.4043, -1.6602],\n",
      "        [-1.4261, -1.1266, -1.4043, -1.6602],\n",
      "        [-1.4262, -1.1265, -1.4043, -1.6603],\n",
      "        [-1.4262, -1.1265, -1.4043, -1.6604],\n",
      "        [-1.4261, -1.1265, -1.4043, -1.6605],\n",
      "        [-1.4261, -1.1269, -1.4042, -1.6599],\n",
      "        [-1.4261, -1.1267, -1.4043, -1.6601],\n",
      "        [-1.4261, -1.1264, -1.4043, -1.6605],\n",
      "        [-1.4261, -1.1268, -1.4043, -1.6600],\n",
      "        [-1.4261, -1.1269, -1.4043, -1.6598],\n",
      "        [-1.4261, -1.1265, -1.4043, -1.6604],\n",
      "        [-1.4261, -1.1267, -1.4043, -1.6602],\n",
      "        [-1.4262, -1.1264, -1.4043, -1.6606],\n",
      "        [-1.4261, -1.1266, -1.4042, -1.6603],\n",
      "        [-1.4261, -1.1266, -1.4043, -1.6603],\n",
      "        [-1.4262, -1.1265, -1.4043, -1.6604],\n",
      "        [-1.4261, -1.1266, -1.4043, -1.6603],\n",
      "        [-1.4262, -1.1260, -1.4044, -1.6611],\n",
      "        [-1.4260, -1.1270, -1.4043, -1.6597],\n",
      "        [-1.4261, -1.1265, -1.4043, -1.6605],\n",
      "        [-1.4262, -1.1264, -1.4043, -1.6606],\n",
      "        [-1.4262, -1.1264, -1.4043, -1.6604],\n",
      "        [-1.4261, -1.1267, -1.4043, -1.6601],\n",
      "        [-1.4261, -1.1266, -1.4043, -1.6603],\n",
      "        [-1.4261, -1.1263, -1.4044, -1.6606],\n",
      "        [-1.4261, -1.1265, -1.4043, -1.6604],\n",
      "        [-1.4260, -1.1267, -1.4043, -1.6602],\n",
      "        [-1.4261, -1.1265, -1.4043, -1.6604],\n",
      "        [-1.4261, -1.1265, -1.4043, -1.6605],\n",
      "        [-1.4261, -1.1265, -1.4043, -1.6605],\n",
      "        [-1.4261, -1.1268, -1.4043, -1.6600],\n",
      "        [-1.4261, -1.1266, -1.4043, -1.6602],\n",
      "        [-1.4261, -1.1269, -1.4042, -1.6599],\n",
      "        [-1.4260, -1.1271, -1.4042, -1.6597],\n",
      "        [-1.4260, -1.1269, -1.4043, -1.6599],\n",
      "        [-1.4261, -1.1266, -1.4043, -1.6603],\n",
      "        [-1.4261, -1.1266, -1.4043, -1.6603],\n",
      "        [-1.4261, -1.1267, -1.4043, -1.6602],\n",
      "        [-1.4261, -1.1267, -1.4043, -1.6602],\n",
      "        [-1.4261, -1.1266, -1.4043, -1.6603]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([ 5.4352e-03,  3.7750e-03,  3.1308e-03,  2.1177e-03,  5.5786e-03,\n",
      "         6.0043e-03,  3.8990e-03,  2.0925e-03,  6.9311e-03,  4.0660e-03,\n",
      "         3.4424e-03,  2.1054e-03,  6.5175e-03,  5.0509e-03,  4.0616e-03,\n",
      "         1.3890e-03,  6.7099e-03,  4.1101e-03,  3.2856e-03,  2.2529e-03,\n",
      "         5.5797e-03,  5.0372e-03,  2.6489e-03,  2.2338e-03,  6.6398e-03,\n",
      "         3.8980e-03,  3.3328e-03,  1.3513e-03,  7.1063e-03,  5.8524e-03,\n",
      "         3.5716e-03,  2.2143e-03,  5.0377e-03,  3.5958e-03,  4.0175e-03,\n",
      "         1.6469e-03,  8.1629e-03,  5.9563e-03,  2.7924e-03,  1.3955e-03,\n",
      "         5.0589e-03,  5.5901e-03,  2.6555e-03,  1.7396e-03,  1.6882e-01,\n",
      "         1.6875e-01,  1.7102e-01,  1.3380e-03, -1.1208e+00,  5.9970e-03,\n",
      "         2.6679e-03,  1.9265e-03,  6.4126e-03,  3.8161e-03,  2.1924e-03,\n",
      "         1.0673e-03,  6.2301e-03,  4.3360e-03,  2.7510e-03,  1.6147e-03,\n",
      "         6.6532e-03,  3.8297e-03,  3.7166e-03,  1.8345e-03],\n",
      "       grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(0.0059, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2402, 0.3242, 0.2455, 0.1900],\n",
      "        [0.2402, 0.3241, 0.2455, 0.1901],\n",
      "        [0.2402, 0.3241, 0.2455, 0.1901],\n",
      "        [0.2402, 0.3242, 0.2455, 0.1900],\n",
      "        [0.2402, 0.3242, 0.2455, 0.1900],\n",
      "        [0.2402, 0.3241, 0.2455, 0.1901],\n",
      "        [0.2402, 0.3241, 0.2455, 0.1901],\n",
      "        [0.2402, 0.3242, 0.2455, 0.1900],\n",
      "        [0.2402, 0.3242, 0.2455, 0.1900],\n",
      "        [0.2402, 0.3242, 0.2455, 0.1900],\n",
      "        [0.2402, 0.3243, 0.2455, 0.1900],\n",
      "        [0.2402, 0.3242, 0.2455, 0.1900],\n",
      "        [0.2402, 0.3241, 0.2455, 0.1901],\n",
      "        [0.2402, 0.3241, 0.2455, 0.1901],\n",
      "        [0.2402, 0.3241, 0.2455, 0.1901],\n",
      "        [0.2402, 0.3241, 0.2455, 0.1901],\n",
      "        [0.2402, 0.3242, 0.2455, 0.1901],\n",
      "        [0.2402, 0.3242, 0.2455, 0.1901],\n",
      "        [0.2402, 0.3241, 0.2455, 0.1901],\n",
      "        [0.2402, 0.3242, 0.2455, 0.1900],\n",
      "        [0.2402, 0.3241, 0.2455, 0.1901],\n",
      "        [0.2402, 0.3241, 0.2456, 0.1901],\n",
      "        [0.2402, 0.3240, 0.2456, 0.1902],\n",
      "        [0.2402, 0.3241, 0.2456, 0.1901],\n",
      "        [0.2402, 0.3241, 0.2455, 0.1901],\n",
      "        [0.2402, 0.3241, 0.2455, 0.1901],\n",
      "        [0.2402, 0.3242, 0.2455, 0.1901],\n",
      "        [0.2402, 0.3242, 0.2455, 0.1901],\n",
      "        [0.2402, 0.3242, 0.2455, 0.1901],\n",
      "        [0.2402, 0.3240, 0.2456, 0.1902],\n",
      "        [0.2402, 0.3241, 0.2455, 0.1901],\n",
      "        [0.2402, 0.3242, 0.2455, 0.1900],\n",
      "        [0.2402, 0.3241, 0.2455, 0.1901],\n",
      "        [0.2402, 0.3240, 0.2455, 0.1902],\n",
      "        [0.2402, 0.3242, 0.2455, 0.1901],\n",
      "        [0.2403, 0.3241, 0.2455, 0.1901],\n",
      "        [0.2402, 0.3242, 0.2455, 0.1900],\n",
      "        [0.2402, 0.3241, 0.2456, 0.1901],\n",
      "        [0.2402, 0.3241, 0.2455, 0.1901],\n",
      "        [0.2402, 0.3242, 0.2455, 0.1901],\n",
      "        [0.2403, 0.3241, 0.2455, 0.1901],\n",
      "        [0.2402, 0.3243, 0.2455, 0.1899],\n",
      "        [0.2403, 0.3240, 0.2455, 0.1902],\n",
      "        [0.2402, 0.3242, 0.2455, 0.1901],\n",
      "        [0.2402, 0.3242, 0.2455, 0.1900],\n",
      "        [0.2402, 0.3242, 0.2455, 0.1901],\n",
      "        [0.2402, 0.3241, 0.2455, 0.1901],\n",
      "        [0.2402, 0.3241, 0.2455, 0.1901],\n",
      "        [0.2402, 0.3242, 0.2455, 0.1900],\n",
      "        [0.2402, 0.3242, 0.2455, 0.1901],\n",
      "        [0.2403, 0.3241, 0.2455, 0.1901],\n",
      "        [0.2402, 0.3241, 0.2455, 0.1901],\n",
      "        [0.2402, 0.3242, 0.2455, 0.1900],\n",
      "        [0.2402, 0.3242, 0.2455, 0.1901],\n",
      "        [0.2402, 0.3241, 0.2455, 0.1901],\n",
      "        [0.2402, 0.3241, 0.2455, 0.1901],\n",
      "        [0.2402, 0.3240, 0.2456, 0.1902],\n",
      "        [0.2403, 0.3240, 0.2456, 0.1902],\n",
      "        [0.2403, 0.3240, 0.2455, 0.1902],\n",
      "        [0.2403, 0.3241, 0.2455, 0.1901],\n",
      "        [0.2402, 0.3241, 0.2455, 0.1901],\n",
      "        [0.2402, 0.3241, 0.2455, 0.1901],\n",
      "        [0.2402, 0.3241, 0.2455, 0.1901],\n",
      "        [0.2402, 0.3241, 0.2455, 0.1901]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3682, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(0.0084, grad_fn=<AddBackward0>)\n",
      "batching here\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100: done 2 episodes, mean_reward=0.00, best_reward=2.00, speed=226.43\n",
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(0.1075, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.4444, -1.0773, -1.4226, -1.7011],\n",
      "        [-1.4444, -1.0774, -1.4226, -1.7009],\n",
      "        [-1.4443, -1.0774, -1.4226, -1.7009],\n",
      "        [-1.4443, -1.0774, -1.4226, -1.7009],\n",
      "        [-1.4443, -1.0774, -1.4226, -1.7009],\n",
      "        [-1.4444, -1.0772, -1.4226, -1.7011],\n",
      "        [-1.4443, -1.0775, -1.4226, -1.7007],\n",
      "        [-1.4444, -1.0772, -1.4226, -1.7011],\n",
      "        [-1.4445, -1.0771, -1.4226, -1.7011],\n",
      "        [-1.4446, -1.0769, -1.4227, -1.7014],\n",
      "        [-1.4446, -1.0770, -1.4226, -1.7012],\n",
      "        [-1.4444, -1.0773, -1.4226, -1.7010],\n",
      "        [-1.4444, -1.0774, -1.4226, -1.7008],\n",
      "        [-1.4443, -1.0774, -1.4226, -1.7008],\n",
      "        [-1.4443, -1.0774, -1.4226, -1.7008],\n",
      "        [-1.4443, -1.0774, -1.4226, -1.7008],\n",
      "        [-1.4444, -1.0775, -1.4225, -1.7008],\n",
      "        [-1.4444, -1.0774, -1.4226, -1.7009],\n",
      "        [-1.4444, -1.0773, -1.4226, -1.7009],\n",
      "        [-1.4444, -1.0775, -1.4225, -1.7007],\n",
      "        [-1.4444, -1.0775, -1.4225, -1.7007],\n",
      "        [-1.4443, -1.0776, -1.4225, -1.7006],\n",
      "        [-1.4444, -1.0774, -1.4225, -1.7008],\n",
      "        [-1.4445, -1.0771, -1.4226, -1.7012],\n",
      "        [-1.4444, -1.0772, -1.4226, -1.7010],\n",
      "        [-1.4444, -1.0772, -1.4226, -1.7010],\n",
      "        [-1.4444, -1.0773, -1.4226, -1.7009],\n",
      "        [-1.4444, -1.0774, -1.4226, -1.7008],\n",
      "        [-1.4443, -1.0778, -1.4225, -1.7004],\n",
      "        [-1.4443, -1.0776, -1.4225, -1.7006],\n",
      "        [-1.4443, -1.0775, -1.4226, -1.7006],\n",
      "        [-1.4443, -1.0776, -1.4225, -1.7006],\n",
      "        [-1.4444, -1.0773, -1.4226, -1.7010],\n",
      "        [-1.4444, -1.0774, -1.4226, -1.7008],\n",
      "        [-1.4444, -1.0773, -1.4226, -1.7010],\n",
      "        [-1.4444, -1.0773, -1.4226, -1.7009],\n",
      "        [-1.4444, -1.0774, -1.4226, -1.7008],\n",
      "        [-1.4444, -1.0773, -1.4226, -1.7009],\n",
      "        [-1.4444, -1.0771, -1.4226, -1.7012],\n",
      "        [-1.4444, -1.0771, -1.4226, -1.7011],\n",
      "        [-1.4443, -1.0772, -1.4227, -1.7011],\n",
      "        [-1.4445, -1.0768, -1.4227, -1.7016],\n",
      "        [-1.4444, -1.0772, -1.4226, -1.7011],\n",
      "        [-1.4444, -1.0773, -1.4226, -1.7009],\n",
      "        [-1.4444, -1.0774, -1.4226, -1.7008],\n",
      "        [-1.4444, -1.0773, -1.4226, -1.7009],\n",
      "        [-1.4444, -1.0775, -1.4225, -1.7007],\n",
      "        [-1.4444, -1.0773, -1.4226, -1.7010],\n",
      "        [-1.4443, -1.0773, -1.4226, -1.7010],\n",
      "        [-1.4444, -1.0774, -1.4226, -1.7008],\n",
      "        [-1.4444, -1.0772, -1.4226, -1.7010],\n",
      "        [-1.4444, -1.0771, -1.4226, -1.7012],\n",
      "        [-1.4445, -1.0770, -1.4226, -1.7013],\n",
      "        [-1.4443, -1.0776, -1.4226, -1.7006],\n",
      "        [-1.4444, -1.0774, -1.4226, -1.7009],\n",
      "        [-1.4444, -1.0773, -1.4226, -1.7009],\n",
      "        [-1.4443, -1.0773, -1.4226, -1.7010],\n",
      "        [-1.4443, -1.0775, -1.4226, -1.7007],\n",
      "        [-1.4443, -1.0777, -1.4226, -1.7005],\n",
      "        [-1.4442, -1.0779, -1.4225, -1.7002],\n",
      "        [-1.4444, -1.0773, -1.4226, -1.7010],\n",
      "        [-1.4443, -1.0775, -1.4226, -1.7007],\n",
      "        [-1.4444, -1.0774, -1.4226, -1.7009],\n",
      "        [-1.4444, -1.0774, -1.4226, -1.7008]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([ 7.9097e-03,  6.8128e-03,  4.4260e-03,  1.4764e-03,  7.6075e-03,\n",
      "         5.9996e-03,  2.9110e-03,  2.2285e-03,  7.4482e-03,  6.0453e-03,\n",
      "         3.7524e-03,  2.2847e-03,  1.9167e-01,  1.4518e-01,  1.9461e-01,\n",
      "         1.9461e-01,  7.6836e-03,  5.9401e-03,  4.0515e-03,  1.4669e-03,\n",
      "         5.5707e-03,  6.6498e-03,  2.8954e-03,  2.0921e-03,  1.9483e-01,\n",
      "         1.4529e-01,  2.9282e-03,  1.8412e-03, -1.0508e+00, -1.4025e+00,\n",
      "        -1.4188e+00,  1.9600e-03,  7.6728e-03,  4.1336e-03,  2.9225e-03,\n",
      "         1.3648e-03,  5.7450e-03,  5.8861e-03,  4.3405e-03,  2.7410e-03,\n",
      "         5.8887e-03,  6.2374e-03,  2.9920e-03,  1.4615e-03,  7.8158e-03,\n",
      "         5.8566e-03,  2.9092e-03,  2.1074e-03,  7.5101e-03,  5.8858e-03,\n",
      "         2.7917e-03,  2.2624e-03, -1.0391e+00, -1.4100e+00, -1.0635e+00,\n",
      "        -1.6985e+00,  7.9206e-03,  5.7129e-03,  4.4913e-03,  1.5537e-03,\n",
      "         7.8058e-03,  5.7589e-03,  3.0441e-03,  2.0586e-03],\n",
      "       grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(0.1217, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2359, 0.3405, 0.2411, 0.1825],\n",
      "        [0.2359, 0.3405, 0.2411, 0.1825],\n",
      "        [0.2359, 0.3405, 0.2411, 0.1825],\n",
      "        [0.2359, 0.3405, 0.2411, 0.1825],\n",
      "        [0.2359, 0.3405, 0.2411, 0.1825],\n",
      "        [0.2359, 0.3406, 0.2411, 0.1825],\n",
      "        [0.2359, 0.3405, 0.2411, 0.1826],\n",
      "        [0.2359, 0.3406, 0.2411, 0.1825],\n",
      "        [0.2359, 0.3406, 0.2411, 0.1825],\n",
      "        [0.2358, 0.3407, 0.2411, 0.1824],\n",
      "        [0.2358, 0.3406, 0.2411, 0.1825],\n",
      "        [0.2359, 0.3405, 0.2411, 0.1825],\n",
      "        [0.2359, 0.3405, 0.2411, 0.1825],\n",
      "        [0.2359, 0.3405, 0.2411, 0.1825],\n",
      "        [0.2359, 0.3405, 0.2411, 0.1825],\n",
      "        [0.2359, 0.3405, 0.2411, 0.1825],\n",
      "        [0.2359, 0.3405, 0.2411, 0.1825],\n",
      "        [0.2359, 0.3405, 0.2411, 0.1825],\n",
      "        [0.2359, 0.3405, 0.2411, 0.1825],\n",
      "        [0.2359, 0.3404, 0.2411, 0.1826],\n",
      "        [0.2359, 0.3404, 0.2411, 0.1826],\n",
      "        [0.2359, 0.3404, 0.2411, 0.1826],\n",
      "        [0.2359, 0.3405, 0.2411, 0.1825],\n",
      "        [0.2359, 0.3406, 0.2411, 0.1825],\n",
      "        [0.2359, 0.3405, 0.2411, 0.1825],\n",
      "        [0.2359, 0.3405, 0.2411, 0.1825],\n",
      "        [0.2359, 0.3405, 0.2411, 0.1825],\n",
      "        [0.2359, 0.3405, 0.2411, 0.1825],\n",
      "        [0.2359, 0.3404, 0.2411, 0.1826],\n",
      "        [0.2359, 0.3404, 0.2411, 0.1826],\n",
      "        [0.2359, 0.3404, 0.2411, 0.1826],\n",
      "        [0.2359, 0.3404, 0.2411, 0.1826],\n",
      "        [0.2359, 0.3405, 0.2411, 0.1825],\n",
      "        [0.2359, 0.3405, 0.2411, 0.1825],\n",
      "        [0.2359, 0.3405, 0.2411, 0.1825],\n",
      "        [0.2359, 0.3405, 0.2411, 0.1825],\n",
      "        [0.2359, 0.3405, 0.2411, 0.1825],\n",
      "        [0.2359, 0.3405, 0.2411, 0.1825],\n",
      "        [0.2359, 0.3406, 0.2411, 0.1825],\n",
      "        [0.2359, 0.3406, 0.2411, 0.1825],\n",
      "        [0.2359, 0.3405, 0.2411, 0.1825],\n",
      "        [0.2359, 0.3407, 0.2411, 0.1824],\n",
      "        [0.2359, 0.3406, 0.2411, 0.1825],\n",
      "        [0.2359, 0.3405, 0.2411, 0.1825],\n",
      "        [0.2359, 0.3405, 0.2411, 0.1825],\n",
      "        [0.2359, 0.3405, 0.2411, 0.1825],\n",
      "        [0.2359, 0.3404, 0.2411, 0.1826],\n",
      "        [0.2359, 0.3405, 0.2411, 0.1825],\n",
      "        [0.2359, 0.3405, 0.2411, 0.1825],\n",
      "        [0.2359, 0.3405, 0.2411, 0.1825],\n",
      "        [0.2359, 0.3405, 0.2411, 0.1825],\n",
      "        [0.2359, 0.3406, 0.2411, 0.1825],\n",
      "        [0.2359, 0.3406, 0.2411, 0.1824],\n",
      "        [0.2359, 0.3404, 0.2411, 0.1826],\n",
      "        [0.2359, 0.3405, 0.2411, 0.1825],\n",
      "        [0.2359, 0.3405, 0.2411, 0.1825],\n",
      "        [0.2359, 0.3405, 0.2411, 0.1825],\n",
      "        [0.2359, 0.3404, 0.2411, 0.1826],\n",
      "        [0.2359, 0.3404, 0.2411, 0.1826],\n",
      "        [0.2359, 0.3403, 0.2411, 0.1827],\n",
      "        [0.2359, 0.3405, 0.2411, 0.1825],\n",
      "        [0.2359, 0.3404, 0.2411, 0.1826],\n",
      "        [0.2359, 0.3405, 0.2411, 0.1825],\n",
      "        [0.2359, 0.3405, 0.2411, 0.1825]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3610, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(0.2156, grad_fn=<AddBackward0>)\n",
      "batching here\n",
      "training here\n",
      "loss_value_v torch.float32 torch.Size([]) tensor(0.0306, grad_fn=<MseLossBackward>)\n",
      "log_prob_v torch.float32 torch.Size([64, 4]) tensor([[-1.4621, -1.0494, -1.4231, -1.7308],\n",
      "        [-1.4621, -1.0493, -1.4231, -1.7310],\n",
      "        [-1.4621, -1.0494, -1.4231, -1.7309],\n",
      "        [-1.4621, -1.0493, -1.4231, -1.7310],\n",
      "        [-1.4620, -1.0495, -1.4231, -1.7308],\n",
      "        [-1.4622, -1.0492, -1.4231, -1.7311],\n",
      "        [-1.4621, -1.0493, -1.4231, -1.7311],\n",
      "        [-1.4622, -1.0492, -1.4231, -1.7312],\n",
      "        [-1.4622, -1.0493, -1.4230, -1.7309],\n",
      "        [-1.4622, -1.0494, -1.4230, -1.7309],\n",
      "        [-1.4622, -1.0495, -1.4230, -1.7307],\n",
      "        [-1.4622, -1.0492, -1.4231, -1.7311],\n",
      "        [-1.4621, -1.0494, -1.4230, -1.7309],\n",
      "        [-1.4621, -1.0494, -1.4230, -1.7309],\n",
      "        [-1.4621, -1.0494, -1.4230, -1.7309],\n",
      "        [-1.4621, -1.0494, -1.4230, -1.7309],\n",
      "        [-1.4621, -1.0495, -1.4230, -1.7307],\n",
      "        [-1.4621, -1.0494, -1.4231, -1.7309],\n",
      "        [-1.4621, -1.0493, -1.4231, -1.7310],\n",
      "        [-1.4620, -1.0497, -1.4230, -1.7305],\n",
      "        [-1.4621, -1.0495, -1.4230, -1.7307],\n",
      "        [-1.4622, -1.0493, -1.4231, -1.7310],\n",
      "        [-1.4621, -1.0495, -1.4230, -1.7308],\n",
      "        [-1.4620, -1.0496, -1.4230, -1.7307],\n",
      "        [-1.4622, -1.0493, -1.4231, -1.7310],\n",
      "        [-1.4622, -1.0494, -1.4230, -1.7308],\n",
      "        [-1.4621, -1.0493, -1.4231, -1.7310],\n",
      "        [-1.4621, -1.0494, -1.4230, -1.7308],\n",
      "        [-1.4621, -1.0494, -1.4231, -1.7308],\n",
      "        [-1.4621, -1.0494, -1.4230, -1.7309],\n",
      "        [-1.4622, -1.0493, -1.4231, -1.7310],\n",
      "        [-1.4621, -1.0497, -1.4230, -1.7305],\n",
      "        [-1.4621, -1.0493, -1.4231, -1.7310],\n",
      "        [-1.4621, -1.0494, -1.4231, -1.7308],\n",
      "        [-1.4621, -1.0494, -1.4231, -1.7309],\n",
      "        [-1.4621, -1.0494, -1.4231, -1.7309],\n",
      "        [-1.4622, -1.0494, -1.4230, -1.7308],\n",
      "        [-1.4621, -1.0494, -1.4231, -1.7309],\n",
      "        [-1.4623, -1.0491, -1.4231, -1.7312],\n",
      "        [-1.4622, -1.0492, -1.4231, -1.7311],\n",
      "        [-1.4622, -1.0494, -1.4230, -1.7308],\n",
      "        [-1.4621, -1.0493, -1.4231, -1.7310],\n",
      "        [-1.4622, -1.0493, -1.4231, -1.7309],\n",
      "        [-1.4621, -1.0494, -1.4230, -1.7309],\n",
      "        [-1.4621, -1.0494, -1.4230, -1.7308],\n",
      "        [-1.4622, -1.0492, -1.4231, -1.7312],\n",
      "        [-1.4621, -1.0496, -1.4230, -1.7305],\n",
      "        [-1.4622, -1.0492, -1.4231, -1.7311],\n",
      "        [-1.4622, -1.0492, -1.4230, -1.7312],\n",
      "        [-1.4621, -1.0493, -1.4230, -1.7311],\n",
      "        [-1.4621, -1.0494, -1.4230, -1.7310],\n",
      "        [-1.4621, -1.0494, -1.4230, -1.7309],\n",
      "        [-1.4621, -1.0495, -1.4230, -1.7308],\n",
      "        [-1.4621, -1.0493, -1.4231, -1.7310],\n",
      "        [-1.4622, -1.0493, -1.4231, -1.7310],\n",
      "        [-1.4622, -1.0491, -1.4231, -1.7311],\n",
      "        [-1.4620, -1.0496, -1.4230, -1.7307],\n",
      "        [-1.4621, -1.0496, -1.4230, -1.7307],\n",
      "        [-1.4621, -1.0495, -1.4230, -1.7309],\n",
      "        [-1.4621, -1.0495, -1.4230, -1.7308],\n",
      "        [-1.4620, -1.0496, -1.4230, -1.7306],\n",
      "        [-1.4623, -1.0490, -1.4231, -1.7314],\n",
      "        [-1.4619, -1.0501, -1.4230, -1.7300],\n",
      "        [-1.4621, -1.0495, -1.4230, -1.7307]], grad_fn=<LogSoftmaxBackward>)\n",
      "log_prob_actions_v torch.float32 torch.Size([64]) tensor([ 0.0126,  0.0058,  0.0052,  0.0032,  0.0105,  0.0059,  0.0039,  0.0027,\n",
      "         0.0105,  0.0077,  0.0034,  0.0017,  0.0110,  0.0099,  0.0066,  0.0020,\n",
      "         0.0103,  0.0059,  0.0066,  0.0017, -1.4368, -1.4540,  0.0053,  0.0026,\n",
      "         0.0105,  0.0056,  0.0053,  0.0023,  0.0127,  0.0082,  0.0051,  0.0018,\n",
      "         0.0109,  0.0081,  0.0039,  0.0020,  0.0127,  0.0081,  0.0057,  0.0031,\n",
      "         0.0074,  0.0057,  0.0036,  0.0024,  0.0105,  0.0061,  0.0051,  0.0030,\n",
      "         0.0110,  0.0083,  0.0041,  0.0022,  0.0080,  0.0085,  0.0070,  0.0037,\n",
      "         0.0105,  0.0057,  0.0064,  0.0026,  0.0075,  0.0061,  0.0033,  0.0026],\n",
      "       grad_fn=<MulBackward0>)\n",
      "loss_policy_v torch.float32 torch.Size([]) tensor(0.0392, grad_fn=<NegBackward>)\n",
      "prob_v torch.float32 torch.Size([64, 4]) tensor([[0.2318, 0.3501, 0.2410, 0.1771],\n",
      "        [0.2317, 0.3502, 0.2410, 0.1771],\n",
      "        [0.2318, 0.3501, 0.2410, 0.1771],\n",
      "        [0.2317, 0.3502, 0.2410, 0.1771],\n",
      "        [0.2318, 0.3501, 0.2410, 0.1771],\n",
      "        [0.2317, 0.3502, 0.2410, 0.1771],\n",
      "        [0.2317, 0.3502, 0.2410, 0.1771],\n",
      "        [0.2317, 0.3502, 0.2410, 0.1771],\n",
      "        [0.2317, 0.3502, 0.2410, 0.1771],\n",
      "        [0.2317, 0.3501, 0.2410, 0.1771],\n",
      "        [0.2317, 0.3501, 0.2410, 0.1772],\n",
      "        [0.2317, 0.3502, 0.2410, 0.1771],\n",
      "        [0.2317, 0.3501, 0.2410, 0.1771],\n",
      "        [0.2317, 0.3501, 0.2410, 0.1771],\n",
      "        [0.2317, 0.3502, 0.2410, 0.1771],\n",
      "        [0.2318, 0.3501, 0.2410, 0.1771],\n",
      "        [0.2318, 0.3501, 0.2410, 0.1772],\n",
      "        [0.2317, 0.3502, 0.2410, 0.1771],\n",
      "        [0.2317, 0.3502, 0.2410, 0.1771],\n",
      "        [0.2318, 0.3501, 0.2410, 0.1772],\n",
      "        [0.2317, 0.3501, 0.2410, 0.1772],\n",
      "        [0.2317, 0.3502, 0.2410, 0.1771],\n",
      "        [0.2317, 0.3501, 0.2410, 0.1771],\n",
      "        [0.2318, 0.3501, 0.2410, 0.1772],\n",
      "        [0.2317, 0.3502, 0.2410, 0.1771],\n",
      "        [0.2317, 0.3501, 0.2410, 0.1771],\n",
      "        [0.2317, 0.3502, 0.2410, 0.1771],\n",
      "        [0.2317, 0.3501, 0.2410, 0.1771],\n",
      "        [0.2318, 0.3501, 0.2410, 0.1771],\n",
      "        [0.2317, 0.3501, 0.2410, 0.1771],\n",
      "        [0.2317, 0.3502, 0.2410, 0.1771],\n",
      "        [0.2317, 0.3501, 0.2410, 0.1772],\n",
      "        [0.2317, 0.3502, 0.2410, 0.1771],\n",
      "        [0.2317, 0.3501, 0.2410, 0.1771],\n",
      "        [0.2317, 0.3502, 0.2410, 0.1771],\n",
      "        [0.2317, 0.3501, 0.2410, 0.1771],\n",
      "        [0.2317, 0.3501, 0.2410, 0.1771],\n",
      "        [0.2317, 0.3502, 0.2410, 0.1771],\n",
      "        [0.2317, 0.3502, 0.2410, 0.1771],\n",
      "        [0.2317, 0.3502, 0.2410, 0.1771],\n",
      "        [0.2317, 0.3502, 0.2410, 0.1771],\n",
      "        [0.2317, 0.3502, 0.2410, 0.1771],\n",
      "        [0.2317, 0.3502, 0.2410, 0.1771],\n",
      "        [0.2318, 0.3501, 0.2410, 0.1771],\n",
      "        [0.2317, 0.3501, 0.2410, 0.1771],\n",
      "        [0.2317, 0.3502, 0.2410, 0.1771],\n",
      "        [0.2318, 0.3501, 0.2410, 0.1772],\n",
      "        [0.2317, 0.3502, 0.2410, 0.1771],\n",
      "        [0.2317, 0.3502, 0.2410, 0.1771],\n",
      "        [0.2317, 0.3502, 0.2410, 0.1771],\n",
      "        [0.2317, 0.3501, 0.2410, 0.1771],\n",
      "        [0.2318, 0.3501, 0.2410, 0.1771],\n",
      "        [0.2318, 0.3501, 0.2410, 0.1771],\n",
      "        [0.2317, 0.3502, 0.2410, 0.1771],\n",
      "        [0.2317, 0.3502, 0.2410, 0.1771],\n",
      "        [0.2317, 0.3502, 0.2410, 0.1771],\n",
      "        [0.2318, 0.3501, 0.2410, 0.1772],\n",
      "        [0.2318, 0.3501, 0.2410, 0.1772],\n",
      "        [0.2318, 0.3501, 0.2410, 0.1771],\n",
      "        [0.2318, 0.3501, 0.2410, 0.1771],\n",
      "        [0.2318, 0.3501, 0.2410, 0.1772],\n",
      "        [0.2317, 0.3503, 0.2410, 0.1770],\n",
      "        [0.2318, 0.3499, 0.2410, 0.1773],\n",
      "        [0.2317, 0.3501, 0.2410, 0.1772]], grad_fn=<SoftmaxBackward>)\n",
      "entropy_loss_v torch.float32 torch.Size([]) tensor(-1.3558, grad_fn=<MeanBackward0>)\n",
      "loss_v torch.float32 torch.Size([]) tensor(0.0562, grad_fn=<AddBackward0>)\n",
      "batching here\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-88e95fe9a94b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mRewardTracker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_reward\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m18\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtracker\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mptan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTBMeanTracker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtb_tracker\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mmb_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmb_rewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmb_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmb_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone_rewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone_steps\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterate_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdone_rewards\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m                     \u001b[0mtotal_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdone_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-88e95fe9a94b>\u001b[0m in \u001b[0;36miterate_batches\u001b[0;34m(envs, net, device)\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mobs_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mptan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_states_preprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mmb_obs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobs_v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mlogits_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs_v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m             \u001b[0mprobs_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprobs_v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-88e95fe9a94b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mfx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mconv_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    418\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    419\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 420\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import gym\n",
    "import ptan\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import argparse\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils as nn_utils\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "#from lib import common\n",
    "\n",
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 5e-4\n",
    "ENTROPY_BETA = 0.01\n",
    "NUM_ENVS = 16\n",
    "\n",
    "REWARD_STEPS = 4\n",
    "CLIP_GRAD = 0.1\n",
    "IMG_SHAPE = (4, 84, 84)\n",
    "\n",
    "\n",
    "class AtariA2C(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(AtariA2C, self).__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "        self.policy = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "\n",
    "        self.value = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "\n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        fx = x.float() / 256\n",
    "        conv_out = self.conv(fx).view(fx.size()[0], -1)\n",
    "        return self.policy(conv_out), self.value(conv_out)\n",
    "\n",
    "\n",
    "def discount_with_dones(rewards, dones, gamma):\n",
    "    discounted = []\n",
    "    r = 0\n",
    "    for reward, done in zip(rewards[::-1], dones[::-1]):\n",
    "        r = reward + gamma*r*(1.-done)\n",
    "        discounted.append(r)\n",
    "    return discounted[::-1]\n",
    "\n",
    "\n",
    "def iterate_batches(envs, net, device=\"cpu\"):\n",
    "    n_actions = envs[0].action_space.n\n",
    "    act_selector = ptan.actions.ProbabilityActionSelector()\n",
    "    obs = [e.reset() for e in envs]\n",
    "    batch_dones = [[False] for _ in range(NUM_ENVS)]\n",
    "    total_reward = [0.0] * NUM_ENVS\n",
    "    total_steps = [0] * NUM_ENVS\n",
    "    mb_obs = np.zeros((NUM_ENVS, REWARD_STEPS) + IMG_SHAPE, dtype=np.uint8)\n",
    "    mb_rewards = np.zeros((NUM_ENVS, REWARD_STEPS), dtype=np.float32)\n",
    "    mb_values = np.zeros((NUM_ENVS, REWARD_STEPS), dtype=np.float32)\n",
    "    mb_actions = np.zeros((NUM_ENVS, REWARD_STEPS), dtype=np.int32)\n",
    "    mb_probs = np.zeros((NUM_ENVS, REWARD_STEPS, n_actions), dtype=np.float32)\n",
    "\n",
    "    while True:\n",
    "        print(\"batching here\")\n",
    "        batch_dones = [[dones[-1]] for dones in batch_dones]\n",
    "        done_rewards = []\n",
    "        done_steps = []\n",
    "        for n in range(REWARD_STEPS):\n",
    "            obs_v = ptan.agent.default_states_preprocessor(obs).to(device)\n",
    "            mb_obs[:, n] = obs_v.data.cpu().numpy()\n",
    "            logits_v, values_v = net(obs_v)\n",
    "            probs_v = F.softmax(logits_v, dim=1)\n",
    "            probs = probs_v.data.cpu().numpy()\n",
    "            actions = act_selector(probs)\n",
    "            mb_probs[:, n] = probs\n",
    "            mb_actions[:, n] = actions\n",
    "            mb_values[:, n] = values_v.squeeze().data.cpu().numpy()\n",
    "            for e_idx, e in enumerate(envs):\n",
    "                o, r, done, _ = e.step(actions[e_idx])\n",
    "                total_reward[e_idx] += r\n",
    "                total_steps[e_idx] += 1\n",
    "                if done:\n",
    "                    o = e.reset()\n",
    "                    done_rewards.append(total_reward[e_idx])\n",
    "                    done_steps.append(total_steps[e_idx])\n",
    "                    total_reward[e_idx] = 0.0\n",
    "                    total_steps[e_idx] = 0\n",
    "                obs[e_idx] = o\n",
    "                mb_rewards[e_idx, n] = r\n",
    "                batch_dones[e_idx].append(done)\n",
    "        # obtain values for the last observation\n",
    "        obs_v = ptan.agent.default_states_preprocessor(obs).to(device)\n",
    "        _, values_v = net(obs_v)\n",
    "        values_last = values_v.squeeze().data.cpu().numpy()\n",
    "\n",
    "        for e_idx, (rewards, dones, value) in enumerate(zip(mb_rewards, batch_dones, values_last)):\n",
    "            rewards = rewards.tolist()\n",
    "            if not dones[-1]:\n",
    "                rewards = discount_with_dones(rewards + [value], dones[1:] + [False], GAMMA)[:-1]\n",
    "            else:\n",
    "                rewards = discount_with_dones(rewards, dones[1:], GAMMA)\n",
    "            mb_rewards[e_idx] = rewards\n",
    "\n",
    "        out_mb_obs = mb_obs.reshape((-1,) + IMG_SHAPE)\n",
    "        out_mb_rewards = mb_rewards.flatten()\n",
    "        out_mb_actions = mb_actions.flatten()\n",
    "        out_mb_values = mb_values.flatten()\n",
    "        out_mb_probs = mb_probs.flatten()\n",
    "        yield out_mb_obs, out_mb_rewards, out_mb_actions, out_mb_values, out_mb_probs, \\\n",
    "              np.array(done_rewards), np.array(done_steps)\n",
    "\n",
    "\n",
    "def train_a2c(net, mb_obs, mb_rewards, mb_actions, mb_values, optimizer, tb_tracker, step_idx, device=\"cpu\"):\n",
    "    print(\"training here\")\n",
    "    optimizer.zero_grad()\n",
    "    mb_adv = mb_rewards - mb_values\n",
    "\n",
    "    adv_v = torch.FloatTensor(mb_adv).to(device)\n",
    "    obs_v = torch.FloatTensor(mb_obs).to(device)\n",
    "    rewards_v = torch.FloatTensor(mb_rewards).to(device)\n",
    "    actions_t = torch.LongTensor(mb_actions).to(device)\n",
    "    logits_v, values_v = net(obs_v)\n",
    "\n",
    "    loss_value_v = F.mse_loss(values_v.squeeze(-1), rewards_v)\n",
    "    print('loss_value_v', loss_value_v.dtype, loss_value_v.shape, loss_value_v)\n",
    "    \n",
    "    log_prob_v = F.log_softmax(logits_v, dim=1)\n",
    "    print('log_prob_v', log_prob_v.dtype, log_prob_v.shape, log_prob_v)\n",
    "    log_prob_actions_v = adv_v * log_prob_v[range(len(mb_actions)), actions_t]\n",
    "    print('log_prob_actions_v', log_prob_actions_v.dtype, log_prob_actions_v.shape, log_prob_actions_v)\n",
    "    loss_policy_v = -log_prob_actions_v.mean()\n",
    "    print('loss_policy_v', loss_policy_v.dtype, loss_policy_v.shape, loss_policy_v)\n",
    "\n",
    "    prob_v = F.softmax(logits_v, dim=1)\n",
    "    print('prob_v', prob_v.dtype, prob_v.shape, prob_v)\n",
    "    entropy_loss_v = (prob_v * log_prob_v).sum(dim=1).mean()\n",
    "    print('entropy_loss_v', entropy_loss_v.dtype, entropy_loss_v.shape, entropy_loss_v)\n",
    "    loss_v = ENTROPY_BETA * entropy_loss_v + loss_value_v + loss_policy_v\n",
    "    print('loss_v', loss_v.dtype, loss_v.shape, loss_v)\n",
    "    loss_v.backward()\n",
    "    nn_utils.clip_grad_norm_(net.parameters(), CLIP_GRAD)\n",
    "    optimizer.step()\n",
    "\n",
    "    return obs_v\n",
    "\n",
    "\n",
    "def set_seed(seed, envs=None, cuda=False):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed(seed)\n",
    "\n",
    "    if envs:\n",
    "        for idx, env in enumerate(envs):\n",
    "            env.seed(seed + idx)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "    make_env = lambda: ptan.common.wrappers.wrap_dqn(gym.make(\"BreakoutNoFrameskip-v4\"))\n",
    "    envs = [make_env() for _ in range(NUM_ENVS)]\n",
    "    writer = SummaryWriter(comment=\"-pong-a2c-r2_\")\n",
    "    set_seed(20, envs)\n",
    "\n",
    "    net = AtariA2C(envs[0].observation_space.shape, envs[0].action_space.n).to(device)\n",
    "    #print(net)\n",
    "\n",
    "    optimizer = optim.RMSprop(net.parameters(), lr=LEARNING_RATE, eps=1e-5)\n",
    "\n",
    "    step_idx = 0\n",
    "    total_steps = 0\n",
    "    best_reward = None\n",
    "    ts_start = time.time()\n",
    "\n",
    "    with RewardTracker(writer, stop_reward=18) as tracker:\n",
    "        with ptan.common.utils.TBMeanTracker(writer, batch_size=10) as tb_tracker:\n",
    "            for mb_obs, mb_rewards, mb_actions, mb_values, _, done_rewards, done_steps in iterate_batches(envs, net, device=device):\n",
    "                if len(done_rewards) > 0:\n",
    "                    total_steps += sum(done_steps)\n",
    "                    speed = total_steps / (time.time() - ts_start)\n",
    "                    if best_reward is None:\n",
    "                        best_reward = done_rewards.max()\n",
    "                    elif best_reward < done_rewards.max():\n",
    "                        best_reward = done_rewards.max()\n",
    "                    tb_tracker.track(\"total_reward_max\", best_reward, step_idx)\n",
    "                    tb_tracker.track(\"total_reward\", done_rewards, step_idx)\n",
    "                    tb_tracker.track(\"total_steps\", done_steps, step_idx)\n",
    "                    print(\"%d: done %d episodes, mean_reward=%.2f, best_reward=%.2f, speed=%.2f\" % (\n",
    "                        step_idx, len(done_rewards), done_rewards.mean(), best_reward, speed))\n",
    "\n",
    "                train_a2c(net, mb_obs, mb_rewards, mb_actions, mb_values,\n",
    "                          optimizer, tb_tracker, step_idx, device=device)\n",
    "                step_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb_actions = np.zeros((6, 3), dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = -36.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "new(): data must be a sequence (got float)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-6604db1d8673>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: new(): data must be a sequence (got float)"
     ]
    }
   ],
   "source": [
    "test1 = torch.FloatTensor(test).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1784, 0.1803, 0.1821, 0.1839, 0.1785, 0.1803, 0.1821, 0.1839, 0.1786,\n",
       "        0.1804, 0.1823, 0.1841, 0.1783, 0.1801, 0.1819, 0.1837, 0.1783, 0.1801,\n",
       "        0.1820, 0.1838, 1.1684, 1.1802, 0.1820, 0.1838, 0.1786, 0.1804, 0.1822,\n",
       "        0.1840, 0.1784, 0.1802, 0.1820, 0.1838, 0.1784, 0.1802, 0.1820, 0.1838,\n",
       "        0.1783, 0.1801, 0.1820, 0.1838, 0.1787, 0.1805, 0.1823, 0.1842, 0.1783,\n",
       "        0.1801, 0.1819, 0.1837, 0.1783, 0.1801, 0.1819, 0.1838, 0.1782, 0.1800,\n",
       "        0.1818, 0.1836, 0.1784, 0.1802, 0.1820, 0.1839, 0.1784, 0.1802, 0.1821,\n",
       "        0.1839])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_v = torch.tensor(test).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adv_v.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-36)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adv_v.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-1.6000)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test2 = test1 + 35\n",
    "test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
