{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class RewardTracker:\n",
    "    def __init__(self, writer, stop_reward):\n",
    "        self.writer = writer\n",
    "        self.stop_reward = stop_reward\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.ts = time.time()\n",
    "        self.ts_frame = 0\n",
    "        self.total_rewards = []\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *args):\n",
    "        self.writer.close()\n",
    "\n",
    "    def reward(self, reward, frame, epsilon=None):\n",
    "        self.total_rewards.append(reward)\n",
    "        speed = (frame - self.ts_frame) / (time.time() - self.ts)\n",
    "        self.ts_frame = frame\n",
    "        self.ts = time.time()\n",
    "        mean_reward = np.mean(self.total_rewards[-100:])\n",
    "        epsilon_str = \"\" if epsilon is None else \", eps %.2f\" % epsilon\n",
    "        print(\"%d: done %d games, mean reward %.3f, speed %.2f f/s%s\" % (\n",
    "            frame, len(self.total_rewards), mean_reward, speed, epsilon_str\n",
    "        ))\n",
    "        sys.stdout.flush()\n",
    "        if epsilon is not None:\n",
    "            self.writer.add_scalar(\"epsilon\", epsilon, frame)\n",
    "        self.writer.add_scalar(\"speed\", speed, frame)\n",
    "        self.writer.add_scalar(\"reward_100\", mean_reward, frame)\n",
    "        self.writer.add_scalar(\"reward\", reward, frame)\n",
    "        if mean_reward > self.stop_reward:\n",
    "            print(\"Solved in %d frames!\" % frame)\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "\n",
    "class AtariPGN(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(AtariPGN, self).__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "\n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        fx = x.float() / 256\n",
    "        conv_out = self.conv(fx).view(fx.size()[0], -1)\n",
    "        return self.fc(conv_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batching here\n",
      "training here\n",
      "end\n",
      "prob_v torch.float32\n",
      "entropy_loss_v torch.float32\n",
      "loss_v torch.float32\n",
      "batching here\n",
      "training here\n",
      "end\n",
      "prob_v torch.float32\n",
      "entropy_loss_v torch.float32\n",
      "loss_v torch.float32\n",
      "batching here\n",
      "training here\n",
      "end\n",
      "prob_v torch.float32\n",
      "entropy_loss_v torch.float32\n",
      "loss_v torch.float32\n",
      "batching here\n",
      "training here\n",
      "end\n",
      "prob_v torch.float32\n",
      "entropy_loss_v torch.float32\n",
      "loss_v torch.float32\n",
      "batching here\n",
      "training here\n",
      "end\n",
      "prob_v torch.float32\n",
      "entropy_loss_v torch.float32\n",
      "loss_v torch.float32\n",
      "batching here\n",
      "5: done 13 episodes, mean_reward=0.00, best_reward=0.00, speed=213.95\n",
      "training here\n",
      "end\n",
      "prob_v torch.float32\n",
      "entropy_loss_v torch.float32\n",
      "loss_v torch.float32\n",
      "batching here\n",
      "training here\n",
      "end\n",
      "prob_v torch.float32\n",
      "entropy_loss_v torch.float32\n",
      "loss_v torch.float32\n",
      "batching here\n",
      "training here\n",
      "end\n",
      "prob_v torch.float32\n",
      "entropy_loss_v torch.float32\n",
      "loss_v torch.float32\n",
      "batching here\n",
      "training here\n",
      "end\n",
      "prob_v torch.float32\n",
      "entropy_loss_v torch.float32\n",
      "loss_v torch.float32\n",
      "batching here\n",
      "training here\n",
      "end\n",
      "prob_v torch.float32\n",
      "entropy_loss_v torch.float32\n",
      "loss_v torch.float32\n",
      "batching here\n",
      "training here\n",
      "end\n",
      "prob_v torch.float32\n",
      "entropy_loss_v torch.float32\n",
      "loss_v torch.float32\n",
      "batching here\n",
      "11: done 11 episodes, mean_reward=0.00, best_reward=0.00, speed=201.43\n",
      "training here\n",
      "end\n",
      "prob_v torch.float32\n",
      "entropy_loss_v torch.float32\n",
      "loss_v torch.float32\n",
      "batching here\n",
      "12: done 2 episodes, mean_reward=1.00, best_reward=1.00, speed=220.16\n",
      "training here\n",
      "end\n",
      "prob_v torch.float32\n",
      "entropy_loss_v torch.float32\n",
      "loss_v torch.float32\n",
      "batching here\n",
      "training here\n",
      "end\n",
      "prob_v torch.float32\n",
      "entropy_loss_v torch.float32\n",
      "loss_v torch.float32\n",
      "batching here\n",
      "training here\n",
      "end\n",
      "prob_v torch.float32\n",
      "entropy_loss_v torch.float32\n",
      "loss_v torch.float32\n",
      "batching here\n",
      "training here\n",
      "end\n",
      "prob_v torch.float32\n",
      "entropy_loss_v torch.float32\n",
      "loss_v torch.float32\n",
      "batching here\n",
      "training here\n",
      "end\n",
      "prob_v torch.float32\n",
      "entropy_loss_v torch.float32\n",
      "loss_v torch.float32\n",
      "batching here\n",
      "17: done 8 episodes, mean_reward=0.00, best_reward=1.00, speed=204.36\n",
      "training here\n",
      "end\n",
      "prob_v torch.float32\n",
      "entropy_loss_v torch.float32\n",
      "loss_v torch.float32\n",
      "batching here\n",
      "18: done 1 episodes, mean_reward=1.00, best_reward=1.00, speed=205.53\n",
      "training here\n",
      "end\n",
      "prob_v torch.float32\n",
      "entropy_loss_v torch.float32\n",
      "loss_v torch.float32\n",
      "batching here\n",
      "training here\n",
      "end\n",
      "prob_v torch.float32\n",
      "entropy_loss_v torch.float32\n",
      "loss_v torch.float32\n",
      "batching here\n",
      "training here\n",
      "end\n",
      "prob_v torch.float32\n",
      "entropy_loss_v torch.float32\n",
      "loss_v torch.float32\n",
      "batching here\n",
      "training here\n",
      "end\n",
      "prob_v torch.float32\n",
      "entropy_loss_v torch.float32\n",
      "loss_v torch.float32\n",
      "batching here\n",
      "22: done 7 episodes, mean_reward=0.00, best_reward=1.00, speed=200.94\n",
      "training here\n",
      "end\n",
      "prob_v torch.float32\n",
      "entropy_loss_v torch.float32\n",
      "loss_v torch.float32\n",
      "batching here\n",
      "training here\n",
      "end\n",
      "prob_v torch.float32\n",
      "entropy_loss_v torch.float32\n",
      "loss_v torch.float32\n",
      "batching here\n",
      "24: done 4 episodes, mean_reward=1.00, best_reward=2.00, speed=223.95\n",
      "training here\n",
      "end\n",
      "prob_v torch.float32\n",
      "entropy_loss_v torch.float32\n",
      "loss_v torch.float32\n",
      "batching here\n",
      "training here\n",
      "end\n",
      "prob_v torch.float32\n",
      "entropy_loss_v torch.float32\n",
      "loss_v torch.float32\n",
      "batching here\n",
      "training here\n",
      "end\n",
      "prob_v torch.float32\n",
      "entropy_loss_v torch.float32\n",
      "loss_v torch.float32\n",
      "batching here\n",
      "training here\n",
      "end\n",
      "prob_v torch.float32\n",
      "entropy_loss_v torch.float32\n",
      "loss_v torch.float32\n",
      "batching here\n",
      "28: done 6 episodes, mean_reward=0.17, best_reward=2.00, speed=219.84\n",
      "training here\n",
      "end\n",
      "prob_v torch.float32\n",
      "entropy_loss_v torch.float32\n",
      "loss_v torch.float32\n",
      "batching here\n",
      "29: done 3 episodes, mean_reward=0.00, best_reward=2.00, speed=222.56\n",
      "training here\n",
      "end\n",
      "prob_v torch.float32\n",
      "entropy_loss_v torch.float32\n",
      "loss_v torch.float32\n",
      "batching here\n",
      "30: done 2 episodes, mean_reward=1.00, best_reward=2.00, speed=232.38\n",
      "training here\n",
      "end\n",
      "prob_v torch.float32\n",
      "entropy_loss_v torch.float32\n",
      "loss_v torch.float32\n",
      "batching here\n",
      "training here\n",
      "end\n",
      "prob_v torch.float32\n",
      "entropy_loss_v torch.float32\n",
      "loss_v torch.float32\n",
      "batching here\n",
      "training here\n",
      "end\n",
      "prob_v torch.float32\n",
      "entropy_loss_v torch.float32\n",
      "loss_v torch.float32\n",
      "batching here\n",
      "training here\n",
      "end\n",
      "prob_v torch.float32\n",
      "entropy_loss_v torch.float32\n",
      "loss_v torch.float32\n",
      "batching here\n",
      "34: done 6 episodes, mean_reward=0.17, best_reward=2.00, speed=228.71\n",
      "training here\n",
      "end\n",
      "prob_v torch.float32\n",
      "entropy_loss_v torch.float32\n",
      "loss_v torch.float32\n",
      "batching here\n",
      "35: done 5 episodes, mean_reward=0.20, best_reward=2.00, speed=238.68\n",
      "training here\n",
      "end\n",
      "prob_v torch.float32\n",
      "entropy_loss_v torch.float32\n",
      "loss_v torch.float32\n",
      "batching here\n",
      "training here\n",
      "end\n",
      "prob_v torch.float32\n",
      "entropy_loss_v torch.float32\n",
      "loss_v torch.float32\n",
      "batching here\n",
      "37: done 1 episodes, mean_reward=2.00, best_reward=2.00, speed=237.69\n",
      "training here\n",
      "end\n",
      "prob_v torch.float32\n",
      "entropy_loss_v torch.float32\n",
      "loss_v torch.float32\n",
      "batching here\n",
      "training here\n",
      "end\n",
      "prob_v torch.float32\n",
      "entropy_loss_v torch.float32\n",
      "loss_v torch.float32\n",
      "batching here\n",
      "training here\n",
      "end\n",
      "prob_v torch.float32\n",
      "entropy_loss_v torch.float32\n",
      "loss_v torch.float32\n",
      "batching here\n",
      "40: done 7 episodes, mean_reward=0.14, best_reward=2.00, speed=241.64\n",
      "training here\n",
      "end\n",
      "prob_v torch.float32\n",
      "entropy_loss_v torch.float32\n",
      "loss_v torch.float32\n",
      "batching here\n",
      "41: done 4 episodes, mean_reward=0.00, best_reward=2.00, speed=245.48\n",
      "training here\n",
      "end\n",
      "prob_v torch.float32\n",
      "entropy_loss_v torch.float32\n",
      "loss_v torch.float32\n",
      "batching here\n",
      "42: done 2 episodes, mean_reward=1.00, best_reward=2.00, speed=253.63\n",
      "training here\n",
      "end\n",
      "prob_v torch.float32\n",
      "entropy_loss_v torch.float32\n",
      "loss_v torch.float32\n",
      "batching here\n",
      "training here\n",
      "end\n",
      "prob_v torch.float32\n",
      "entropy_loss_v torch.float32\n",
      "loss_v torch.float32\n",
      "batching here\n",
      "training here\n",
      "end\n",
      "prob_v torch.float32\n",
      "entropy_loss_v torch.float32\n",
      "loss_v torch.float32\n",
      "batching here\n",
      "45: done 4 episodes, mean_reward=0.00, best_reward=2.00, speed=245.87\n",
      "training here\n",
      "end\n",
      "prob_v torch.float32\n",
      "entropy_loss_v torch.float32\n",
      "loss_v torch.float32\n",
      "batching here\n",
      "46: done 1 episodes, mean_reward=0.00, best_reward=2.00, speed=242.94\n",
      "training here\n",
      "end\n",
      "prob_v torch.float32\n",
      "entropy_loss_v torch.float32\n",
      "loss_v torch.float32\n",
      "batching here\n",
      "47: done 4 episodes, mean_reward=0.00, best_reward=2.00, speed=245.89\n",
      "training here\n",
      "end\n",
      "prob_v torch.float32\n",
      "entropy_loss_v torch.float32\n",
      "loss_v torch.float32\n",
      "batching here\n",
      "48: done 1 episodes, mean_reward=0.00, best_reward=2.00, speed=242.96\n",
      "training here\n",
      "end\n",
      "prob_v torch.float32\n",
      "entropy_loss_v torch.float32\n",
      "loss_v torch.float32\n",
      "batching here\n",
      "training here\n",
      "end\n",
      "prob_v torch.float32\n",
      "entropy_loss_v torch.float32\n",
      "loss_v torch.float32\n",
      "batching here\n",
      "training here\n",
      "end\n",
      "prob_v torch.float32\n",
      "entropy_loss_v torch.float32\n",
      "loss_v torch.float32\n",
      "batching here\n",
      "51: done 3 episodes, mean_reward=0.00, best_reward=2.00, speed=234.85\n",
      "training here\n",
      "end\n",
      "prob_v torch.float32\n",
      "entropy_loss_v torch.float32\n",
      "loss_v torch.float32\n",
      "batching here\n",
      "52: done 3 episodes, mean_reward=0.00, best_reward=2.00, speed=235.85\n",
      "training here\n",
      "end\n",
      "prob_v torch.float32\n",
      "entropy_loss_v torch.float32\n",
      "loss_v torch.float32\n",
      "batching here\n",
      "53: done 1 episodes, mean_reward=0.00, best_reward=2.00, speed=233.48\n",
      "training here\n",
      "end\n",
      "prob_v torch.float32\n",
      "entropy_loss_v torch.float32\n",
      "loss_v torch.float32\n",
      "batching here\n",
      "54: done 1 episodes, mean_reward=1.00, best_reward=2.00, speed=233.35\n",
      "training here\n",
      "end\n",
      "prob_v torch.float32\n",
      "entropy_loss_v torch.float32\n",
      "loss_v torch.float32\n",
      "batching here\n",
      "training here\n",
      "end\n",
      "prob_v torch.float32\n",
      "entropy_loss_v torch.float32\n",
      "loss_v torch.float32\n",
      "batching here\n",
      "training here\n",
      "end\n",
      "prob_v torch.float32\n",
      "entropy_loss_v torch.float32\n",
      "loss_v torch.float32\n",
      "batching here\n",
      "57: done 4 episodes, mean_reward=0.25, best_reward=2.00, speed=231.43\n",
      "training here\n",
      "end\n",
      "prob_v torch.float32\n",
      "entropy_loss_v torch.float32\n",
      "loss_v torch.float32\n",
      "batching here\n",
      "58: done 4 episodes, mean_reward=0.50, best_reward=2.00, speed=241.13\n",
      "training here\n",
      "end\n",
      "prob_v torch.float32\n",
      "entropy_loss_v torch.float32\n",
      "loss_v torch.float32\n",
      "batching here\n",
      "training here\n",
      "end\n",
      "prob_v torch.float32\n",
      "entropy_loss_v torch.float32\n",
      "loss_v torch.float32\n",
      "batching here\n",
      "60: done 4 episodes, mean_reward=1.50, best_reward=2.00, speed=256.25\n",
      "training here\n",
      "end\n",
      "prob_v torch.float32\n",
      "entropy_loss_v torch.float32\n",
      "loss_v torch.float32\n",
      "batching here\n",
      "training here\n",
      "end\n",
      "prob_v torch.float32\n",
      "entropy_loss_v torch.float32\n",
      "loss_v torch.float32\n",
      "batching here\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training here\n",
      "end\n",
      "prob_v torch.float32\n",
      "entropy_loss_v torch.float32\n",
      "loss_v torch.float32\n",
      "batching here\n",
      "63: done 4 episodes, mean_reward=0.00, best_reward=2.00, speed=250.63\n",
      "training here\n",
      "end\n",
      "prob_v torch.float32\n",
      "entropy_loss_v torch.float32\n",
      "loss_v torch.float32\n",
      "batching here\n",
      "64: done 3 episodes, mean_reward=0.00, best_reward=2.00, speed=251.14\n",
      "training here\n",
      "end\n",
      "prob_v torch.float32\n",
      "entropy_loss_v torch.float32\n",
      "loss_v torch.float32\n",
      "batching here\n",
      "65: done 1 episodes, mean_reward=0.00, best_reward=2.00, speed=248.92\n",
      "training here\n",
      "end\n",
      "prob_v torch.float32\n",
      "entropy_loss_v torch.float32\n",
      "loss_v torch.float32\n",
      "batching here\n",
      "66: done 3 episodes, mean_reward=0.33, best_reward=2.00, speed=252.61\n",
      "training here\n",
      "end\n",
      "prob_v torch.float32\n",
      "entropy_loss_v torch.float32\n",
      "loss_v torch.float32\n",
      "batching here\n",
      "training here\n",
      "end\n",
      "prob_v torch.float32\n",
      "entropy_loss_v torch.float32\n",
      "loss_v torch.float32\n",
      "batching here\n",
      "68: done 3 episodes, mean_reward=0.33, best_reward=2.00, speed=252.15\n",
      "training here\n",
      "end\n",
      "prob_v torch.float32\n",
      "entropy_loss_v torch.float32\n",
      "loss_v torch.float32\n",
      "batching here\n",
      "69: done 2 episodes, mean_reward=0.00, best_reward=2.00, speed=251.40\n",
      "training here\n",
      "end\n",
      "prob_v torch.float32\n",
      "entropy_loss_v torch.float32\n",
      "loss_v torch.float32\n",
      "batching here\n",
      "70: done 3 episodes, mean_reward=0.67, best_reward=2.00, speed=256.58\n",
      "training here\n",
      "end\n",
      "prob_v torch.float32\n",
      "entropy_loss_v torch.float32\n",
      "loss_v torch.float32\n",
      "batching here\n",
      "71: done 2 episodes, mean_reward=1.00, best_reward=2.00, speed=260.02\n",
      "training here\n",
      "end\n",
      "prob_v torch.float32\n",
      "entropy_loss_v torch.float32\n",
      "loss_v torch.float32\n",
      "batching here\n",
      "72: done 2 episodes, mean_reward=0.00, best_reward=2.00, speed=259.04\n",
      "training here\n",
      "end\n",
      "prob_v torch.float32\n",
      "entropy_loss_v torch.float32\n",
      "loss_v torch.float32\n",
      "batching here\n",
      "training here\n",
      "end\n",
      "prob_v torch.float32\n",
      "entropy_loss_v torch.float32\n",
      "loss_v torch.float32\n",
      "batching here\n",
      "74: done 2 episodes, mean_reward=0.00, best_reward=2.00, speed=254.44\n",
      "training here\n",
      "end\n",
      "prob_v torch.float32\n",
      "entropy_loss_v torch.float32\n",
      "loss_v torch.float32\n",
      "batching here\n",
      "75: done 3 episodes, mean_reward=0.00, best_reward=2.00, speed=254.78\n",
      "training here\n",
      "end\n",
      "prob_v torch.float32\n",
      "entropy_loss_v torch.float32\n",
      "loss_v torch.float32\n",
      "batching here\n",
      "76: done 1 episodes, mean_reward=0.00, best_reward=2.00, speed=252.81\n",
      "training here\n",
      "end\n",
      "prob_v torch.float32\n",
      "entropy_loss_v torch.float32\n",
      "loss_v torch.float32\n",
      "batching here\n",
      "77: done 2 episodes, mean_reward=0.00, best_reward=2.00, speed=252.16\n",
      "training here\n",
      "end\n",
      "prob_v torch.float32\n",
      "entropy_loss_v torch.float32\n",
      "loss_v torch.float32\n",
      "batching here\n",
      "78: done 2 episodes, mean_reward=0.00, best_reward=2.00, speed=251.41\n",
      "training here\n",
      "end\n",
      "prob_v torch.float32\n",
      "entropy_loss_v torch.float32\n",
      "loss_v torch.float32\n",
      "batching here\n",
      "training here\n",
      "end\n",
      "prob_v torch.float32\n",
      "entropy_loss_v torch.float32\n",
      "loss_v torch.float32\n",
      "batching here\n",
      "80: done 4 episodes, mean_reward=0.25, best_reward=2.00, speed=252.34\n",
      "training here\n",
      "end\n",
      "prob_v torch.float32\n",
      "entropy_loss_v torch.float32\n",
      "loss_v torch.float32\n",
      "batching here\n",
      "81: done 3 episodes, mean_reward=0.33, best_reward=2.00, speed=254.07\n",
      "training here\n",
      "end\n",
      "prob_v torch.float32\n",
      "entropy_loss_v torch.float32\n",
      "loss_v torch.float32\n",
      "batching here\n",
      "training here\n",
      "end\n",
      "prob_v torch.float32\n",
      "entropy_loss_v torch.float32\n",
      "loss_v torch.float32\n",
      "batching here\n",
      "83: done 5 episodes, mean_reward=0.20, best_reward=2.00, speed=255.58\n",
      "training here\n",
      "end\n",
      "prob_v torch.float32\n",
      "entropy_loss_v torch.float32\n",
      "loss_v torch.float32\n",
      "batching here\n",
      "training here\n",
      "end\n",
      "prob_v torch.float32\n",
      "entropy_loss_v torch.float32\n",
      "loss_v torch.float32\n",
      "batching here\n",
      "85: done 1 episodes, mean_reward=2.00, best_reward=2.00, speed=254.41\n",
      "training here\n",
      "end\n",
      "prob_v torch.float32\n",
      "entropy_loss_v torch.float32\n",
      "loss_v torch.float32\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-788c5266c6c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m                 train_a2c(net, mb_obs, mb_rewards, mb_actions, mb_values,\n\u001b[0;32m--> 215\u001b[0;31m                           optimizer, tb_tracker, step_idx, device=device)\n\u001b[0m\u001b[1;32m    216\u001b[0m                 \u001b[0mstep_idx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-788c5266c6c6>\u001b[0m in \u001b[0;36mtrain_a2c\u001b[0;34m(net, mb_obs, mb_rewards, mb_actions, mb_values, optimizer, tb_tracker, step_idx, device)\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0mloss_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mENTROPY_BETA\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mentropy_loss_v\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_value_v\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_policy_v\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loss_v\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m     \u001b[0mloss_v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m     \u001b[0mnn_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCLIP_GRAD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import gym\n",
    "import ptan\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import argparse\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils as nn_utils\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "#from lib import common\n",
    "\n",
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 5e-4\n",
    "ENTROPY_BETA = 0.01\n",
    "NUM_ENVS = 16\n",
    "\n",
    "REWARD_STEPS = 4\n",
    "CLIP_GRAD = 0.1\n",
    "IMG_SHAPE = (4, 84, 84)\n",
    "\n",
    "\n",
    "class AtariA2C(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(AtariA2C, self).__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "        self.policy = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "\n",
    "        self.value = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "\n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        fx = x.float() / 256\n",
    "        conv_out = self.conv(fx).view(fx.size()[0], -1)\n",
    "        return self.policy(conv_out), self.value(conv_out)\n",
    "\n",
    "\n",
    "def discount_with_dones(rewards, dones, gamma):\n",
    "    discounted = []\n",
    "    r = 0\n",
    "    for reward, done in zip(rewards[::-1], dones[::-1]):\n",
    "        r = reward + gamma*r*(1.-done)\n",
    "        discounted.append(r)\n",
    "    return discounted[::-1]\n",
    "\n",
    "\n",
    "def iterate_batches(envs, net, device=\"cpu\"):\n",
    "    n_actions = envs[0].action_space.n\n",
    "    act_selector = ptan.actions.ProbabilityActionSelector()\n",
    "    obs = [e.reset() for e in envs]\n",
    "    batch_dones = [[False] for _ in range(NUM_ENVS)]\n",
    "    total_reward = [0.0] * NUM_ENVS\n",
    "    total_steps = [0] * NUM_ENVS\n",
    "    mb_obs = np.zeros((NUM_ENVS, REWARD_STEPS) + IMG_SHAPE, dtype=np.uint8)\n",
    "    mb_rewards = np.zeros((NUM_ENVS, REWARD_STEPS), dtype=np.float32)\n",
    "    mb_values = np.zeros((NUM_ENVS, REWARD_STEPS), dtype=np.float32)\n",
    "    mb_actions = np.zeros((NUM_ENVS, REWARD_STEPS), dtype=np.int32)\n",
    "    mb_probs = np.zeros((NUM_ENVS, REWARD_STEPS, n_actions), dtype=np.float32)\n",
    "\n",
    "    while True:\n",
    "        print(\"batching here\")\n",
    "        batch_dones = [[dones[-1]] for dones in batch_dones]\n",
    "        done_rewards = []\n",
    "        done_steps = []\n",
    "        for n in range(REWARD_STEPS):\n",
    "            obs_v = ptan.agent.default_states_preprocessor(obs).to(device)\n",
    "            mb_obs[:, n] = obs_v.data.cpu().numpy()\n",
    "            logits_v, values_v = net(obs_v)\n",
    "            probs_v = F.softmax(logits_v, dim=1)\n",
    "            probs = probs_v.data.cpu().numpy()\n",
    "            actions = act_selector(probs)\n",
    "            mb_probs[:, n] = probs\n",
    "            mb_actions[:, n] = actions\n",
    "            mb_values[:, n] = values_v.squeeze().data.cpu().numpy()\n",
    "            for e_idx, e in enumerate(envs):\n",
    "                o, r, done, _ = e.step(actions[e_idx])\n",
    "                total_reward[e_idx] += r\n",
    "                total_steps[e_idx] += 1\n",
    "                if done:\n",
    "                    o = e.reset()\n",
    "                    done_rewards.append(total_reward[e_idx])\n",
    "                    done_steps.append(total_steps[e_idx])\n",
    "                    total_reward[e_idx] = 0.0\n",
    "                    total_steps[e_idx] = 0\n",
    "                obs[e_idx] = o\n",
    "                mb_rewards[e_idx, n] = r\n",
    "                batch_dones[e_idx].append(done)\n",
    "        # obtain values for the last observation\n",
    "        obs_v = ptan.agent.default_states_preprocessor(obs).to(device)\n",
    "        _, values_v = net(obs_v)\n",
    "        values_last = values_v.squeeze().data.cpu().numpy()\n",
    "\n",
    "        for e_idx, (rewards, dones, value) in enumerate(zip(mb_rewards, batch_dones, values_last)):\n",
    "            rewards = rewards.tolist()\n",
    "            if not dones[-1]:\n",
    "                rewards = discount_with_dones(rewards + [value], dones[1:] + [False], GAMMA)[:-1]\n",
    "            else:\n",
    "                rewards = discount_with_dones(rewards, dones[1:], GAMMA)\n",
    "            mb_rewards[e_idx] = rewards\n",
    "\n",
    "        out_mb_obs = mb_obs.reshape((-1,) + IMG_SHAPE)\n",
    "        out_mb_rewards = mb_rewards.flatten()\n",
    "        out_mb_actions = mb_actions.flatten()\n",
    "        out_mb_values = mb_values.flatten()\n",
    "        out_mb_probs = mb_probs.flatten()\n",
    "        yield out_mb_obs, out_mb_rewards, out_mb_actions, out_mb_values, out_mb_probs, \\\n",
    "              np.array(done_rewards), np.array(done_steps)\n",
    "\n",
    "\n",
    "def train_a2c(net, mb_obs, mb_rewards, mb_actions, mb_values, optimizer, tb_tracker, step_idx, device=\"cpu\"):\n",
    "    print(\"training here\")\n",
    "    optimizer.zero_grad()\n",
    "    mb_adv = mb_rewards - mb_values\n",
    "\n",
    "    adv_v = torch.FloatTensor(mb_adv).to(device)\n",
    "    obs_v = torch.FloatTensor(mb_obs).to(device)\n",
    "    rewards_v = torch.FloatTensor(mb_rewards).to(device)\n",
    "    actions_t = torch.LongTensor(mb_actions).to(device)\n",
    "    logits_v, values_v = net(obs_v)\n",
    "\n",
    "    loss_value_v = F.mse_loss(values_v.squeeze(-1), rewards_v)\n",
    "\n",
    "    log_prob_v = F.log_softmax(logits_v, dim=1)\n",
    "    print('end')\n",
    "    log_prob_actions_v = adv_v * log_prob_v[range(len(mb_actions)), actions_t]\n",
    "    loss_policy_v = -log_prob_actions_v.mean()\n",
    "\n",
    "    prob_v = F.softmax(logits_v, dim=1)\n",
    "    print('prob_v', prob_v.dtype)\n",
    "    entropy_loss_v = (prob_v * log_prob_v).sum(dim=1).mean()\n",
    "    print('entropy_loss_v', entropy_loss_v.dtype)\n",
    "    loss_v = ENTROPY_BETA * entropy_loss_v + loss_value_v + loss_policy_v\n",
    "    print(\"loss_v\", loss_v.dtype)\n",
    "    loss_v.backward()\n",
    "    nn_utils.clip_grad_norm_(net.parameters(), CLIP_GRAD)\n",
    "    optimizer.step()\n",
    "\n",
    "    return obs_v\n",
    "\n",
    "\n",
    "def set_seed(seed, envs=None, cuda=False):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed(seed)\n",
    "\n",
    "    if envs:\n",
    "        for idx, env in enumerate(envs):\n",
    "            env.seed(seed + idx)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "    make_env = lambda: ptan.common.wrappers.wrap_dqn(gym.make(\"BreakoutNoFrameskip-v4\"))\n",
    "    envs = [make_env() for _ in range(NUM_ENVS)]\n",
    "    writer = SummaryWriter(comment=\"-pong-a2c-r2_\")\n",
    "    set_seed(20, envs)\n",
    "\n",
    "    net = AtariA2C(envs[0].observation_space.shape, envs[0].action_space.n).to(device)\n",
    "    #print(net)\n",
    "\n",
    "    optimizer = optim.RMSprop(net.parameters(), lr=LEARNING_RATE, eps=1e-5)\n",
    "\n",
    "    step_idx = 0\n",
    "    total_steps = 0\n",
    "    best_reward = None\n",
    "    ts_start = time.time()\n",
    "\n",
    "    with RewardTracker(writer, stop_reward=18) as tracker:\n",
    "        with ptan.common.utils.TBMeanTracker(writer, batch_size=10) as tb_tracker:\n",
    "            for mb_obs, mb_rewards, mb_actions, mb_values, _, done_rewards, done_steps in iterate_batches(envs, net, device=device):\n",
    "                if len(done_rewards) > 0:\n",
    "                    total_steps += sum(done_steps)\n",
    "                    speed = total_steps / (time.time() - ts_start)\n",
    "                    if best_reward is None:\n",
    "                        best_reward = done_rewards.max()\n",
    "                    elif best_reward < done_rewards.max():\n",
    "                        best_reward = done_rewards.max()\n",
    "                    tb_tracker.track(\"total_reward_max\", best_reward, step_idx)\n",
    "                    tb_tracker.track(\"total_reward\", done_rewards, step_idx)\n",
    "                    tb_tracker.track(\"total_steps\", done_steps, step_idx)\n",
    "                    print(\"%d: done %d episodes, mean_reward=%.2f, best_reward=%.2f, speed=%.2f\" % (\n",
    "                        step_idx, len(done_rewards), done_rewards.mean(), best_reward, speed))\n",
    "\n",
    "                train_a2c(net, mb_obs, mb_rewards, mb_actions, mb_values,\n",
    "                          optimizer, tb_tracker, step_idx, device=device)\n",
    "                step_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb_actions = np.zeros((6, 3), dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = -3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = torch.tensor(test).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_v = torch.tensor(test).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_v.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_v.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
