{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class RewardTracker:\n",
    "    def __init__(self, writer, stop_reward):\n",
    "        self.writer = writer\n",
    "        self.stop_reward = stop_reward\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.ts = time.time()\n",
    "        self.ts_frame = 0\n",
    "        self.total_rewards = []\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *args):\n",
    "        self.writer.close()\n",
    "\n",
    "    def reward(self, reward, frame, epsilon=None):\n",
    "        self.total_rewards.append(reward)\n",
    "        speed = (frame - self.ts_frame) / (time.time() - self.ts)\n",
    "        self.ts_frame = frame\n",
    "        self.ts = time.time()\n",
    "        mean_reward = np.mean(self.total_rewards[-100:])\n",
    "        epsilon_str = \"\" if epsilon is None else \", eps %.2f\" % epsilon\n",
    "        print(\"%d: done %d games, mean reward %.3f, speed %.2f f/s%s\" % (\n",
    "            frame, len(self.total_rewards), mean_reward, speed, epsilon_str\n",
    "        ))\n",
    "        sys.stdout.flush()\n",
    "        if epsilon is not None:\n",
    "            self.writer.add_scalar(\"epsilon\", epsilon, frame)\n",
    "        self.writer.add_scalar(\"speed\", speed, frame)\n",
    "        self.writer.add_scalar(\"reward_100\", mean_reward, frame)\n",
    "        self.writer.add_scalar(\"reward\", reward, frame)\n",
    "        if mean_reward > self.stop_reward:\n",
    "            print(\"Solved in %d frames!\" % frame)\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "\n",
    "class AtariPGN(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(AtariPGN, self).__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "\n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        fx = x.float() / 256\n",
    "        conv_out = self.conv(fx).view(fx.size()[0], -1)\n",
    "        return self.fc(conv_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import gym\n",
    "import ptan\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import argparse\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils as nn_utils\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "#from lib import common\n",
    "\n",
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 5e-4\n",
    "ENTROPY_BETA = 0.01\n",
    "NUM_ENVS = 16\n",
    "\n",
    "REWARD_STEPS = 4\n",
    "CLIP_GRAD = 0.1\n",
    "IMG_SHAPE = (4, 84, 84)\n",
    "\n",
    "\n",
    "class AtariA2C(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(AtariA2C, self).__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "        self.policy = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "\n",
    "        self.value = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "\n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        fx = x.float() / 256\n",
    "        conv_out = self.conv(fx).view(fx.size()[0], -1)\n",
    "        return self.policy(conv_out), self.value(conv_out)\n",
    "\n",
    "\n",
    "def discount_with_dones(rewards, dones, gamma):\n",
    "    discounted = []\n",
    "    r = 0\n",
    "    for reward, done in zip(rewards[::-1], dones[::-1]):\n",
    "        r = reward + gamma*r*(1.-done)\n",
    "        discounted.append(r)\n",
    "    return discounted[::-1]\n",
    "\n",
    "\n",
    "def iterate_batches(envs, net, device=\"cpu\"):\n",
    "    n_actions = envs[0].action_space.n\n",
    "    act_selector = ptan.actions.ProbabilityActionSelector()\n",
    "    obs = [e.reset() for e in envs]\n",
    "    batch_dones = [[False] for _ in range(NUM_ENVS)]\n",
    "    total_reward = [0.0] * NUM_ENVS\n",
    "    total_steps = [0] * NUM_ENVS\n",
    "    mb_obs = np.zeros((NUM_ENVS, REWARD_STEPS) + IMG_SHAPE, dtype=np.uint8)\n",
    "    mb_rewards = np.zeros((NUM_ENVS, REWARD_STEPS), dtype=np.float32)\n",
    "    mb_values = np.zeros((NUM_ENVS, REWARD_STEPS), dtype=np.float32)\n",
    "    mb_actions = np.zeros((NUM_ENVS, REWARD_STEPS), dtype=np.int32)\n",
    "    mb_probs = np.zeros((NUM_ENVS, REWARD_STEPS, n_actions), dtype=np.float32)\n",
    "\n",
    "    while True:\n",
    "        print(\"batching here\")\n",
    "        batch_dones = [[dones[-1]] for dones in batch_dones]\n",
    "        done_rewards = []\n",
    "        done_steps = []\n",
    "        for n in range(REWARD_STEPS):\n",
    "            obs_v = ptan.agent.default_states_preprocessor(obs).to(device)\n",
    "            mb_obs[:, n] = obs_v.data.cpu().numpy()\n",
    "            logits_v, values_v = net(obs_v)\n",
    "            probs_v = F.softmax(logits_v, dim=1)\n",
    "            probs = probs_v.data.cpu().numpy()\n",
    "            actions = act_selector(probs)\n",
    "            mb_probs[:, n] = probs\n",
    "            mb_actions[:, n] = actions\n",
    "            mb_values[:, n] = values_v.squeeze().data.cpu().numpy()\n",
    "            for e_idx, e in enumerate(envs):\n",
    "                o, r, done, _ = e.step(actions[e_idx])\n",
    "                total_reward[e_idx] += r\n",
    "                total_steps[e_idx] += 1\n",
    "                if done:\n",
    "                    o = e.reset()\n",
    "                    done_rewards.append(total_reward[e_idx])\n",
    "                    done_steps.append(total_steps[e_idx])\n",
    "                    total_reward[e_idx] = 0.0\n",
    "                    total_steps[e_idx] = 0\n",
    "                obs[e_idx] = o\n",
    "                mb_rewards[e_idx, n] = r\n",
    "                batch_dones[e_idx].append(done)\n",
    "        # obtain values for the last observation\n",
    "        obs_v = ptan.agent.default_states_preprocessor(obs).to(device)\n",
    "        _, values_v = net(obs_v)\n",
    "        values_last = values_v.squeeze().data.cpu().numpy()\n",
    "\n",
    "        for e_idx, (rewards, dones, value) in enumerate(zip(mb_rewards, batch_dones, values_last)):\n",
    "            rewards = rewards.tolist()\n",
    "            if not dones[-1]:\n",
    "                rewards = discount_with_dones(rewards + [value], dones[1:] + [False], GAMMA)[:-1]\n",
    "            else:\n",
    "                rewards = discount_with_dones(rewards, dones[1:], GAMMA)\n",
    "            mb_rewards[e_idx] = rewards\n",
    "\n",
    "        out_mb_obs = mb_obs.reshape((-1,) + IMG_SHAPE)\n",
    "        out_mb_rewards = mb_rewards.flatten()\n",
    "        out_mb_actions = mb_actions.flatten()\n",
    "        out_mb_values = mb_values.flatten()\n",
    "        out_mb_probs = mb_probs.flatten()\n",
    "        yield out_mb_obs, out_mb_rewards, out_mb_actions, out_mb_values, out_mb_probs, \\\n",
    "              np.array(done_rewards), np.array(done_steps)\n",
    "\n",
    "\n",
    "def train_a2c(net, mb_obs, mb_rewards, mb_actions, mb_values, optimizer, tb_tracker, step_idx, device=\"cpu\"):\n",
    "    print(\"training here\")\n",
    "    optimizer.zero_grad()\n",
    "    mb_adv = mb_rewards - mb_values\n",
    "\n",
    "    adv_v = torch.FloatTensor(mb_adv).to(device)\n",
    "    obs_v = torch.FloatTensor(mb_obs).to(device)\n",
    "    rewards_v = torch.FloatTensor(mb_rewards).to(device)\n",
    "    actions_t = torch.LongTensor(mb_actions).to(device)\n",
    "    logits_v, values_v = net(obs_v)\n",
    "\n",
    "    loss_value_v = F.mse_loss(values_v.squeeze(-1), rewards_v)\n",
    "    print('loss_value_v', loss_value_v.dtype, loss_value_v.shape, loss_value_v)\n",
    "    \n",
    "    log_prob_v = F.log_softmax(logits_v, dim=1)\n",
    "    print('log_prob_v', log_prob_v.dtype, log_prob_v.shape, log_prob_v)\n",
    "    log_prob_actions_v = adv_v * log_prob_v[range(len(mb_actions)), actions_t]\n",
    "    print('log_prob_actions_v', log_prob_actions_v.dtype, log_prob_actions_v.shape, log_prob_actions_v)\n",
    "    loss_policy_v = -log_prob_actions_v.mean()\n",
    "    print('loss_policy_v', loss_policy_v.dtype, loss_policy_v.shape, loss_policy_v)\n",
    "\n",
    "    prob_v = F.softmax(logits_v, dim=1)\n",
    "    print('prob_v', prob_v.dtype, prob_v.shape, prob_v)\n",
    "    entropy_loss_v = (prob_v * log_prob_v).sum(dim=1).mean()\n",
    "    print('entropy_loss_v', entropy_loss_v.dtype, entropy_loss_v.shape, entropy_loss_v)\n",
    "    loss_v = ENTROPY_BETA * entropy_loss_v + loss_value_v + loss_policy_v\n",
    "    print('loss_v', loss_v.dtype, loss_v.shape, loss_v)\n",
    "    loss_v.backward()\n",
    "    nn_utils.clip_grad_norm_(net.parameters(), CLIP_GRAD)\n",
    "    optimizer.step()\n",
    "\n",
    "    return obs_v\n",
    "\n",
    "\n",
    "def set_seed(seed, envs=None, cuda=False):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed(seed)\n",
    "\n",
    "    if envs:\n",
    "        for idx, env in enumerate(envs):\n",
    "            env.seed(seed + idx)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "    make_env = lambda: ptan.common.wrappers.wrap_dqn(gym.make(\"BreakoutNoFrameskip-v4\"))\n",
    "    envs = [make_env() for _ in range(NUM_ENVS)]\n",
    "    writer = SummaryWriter(comment=\"-pong-a2c-r2_\")\n",
    "    set_seed(20, envs)\n",
    "\n",
    "    net = AtariA2C(envs[0].observation_space.shape, envs[0].action_space.n).to(device)\n",
    "    #print(net)\n",
    "\n",
    "    optimizer = optim.RMSprop(net.parameters(), lr=LEARNING_RATE, eps=1e-5)\n",
    "\n",
    "    step_idx = 0\n",
    "    total_steps = 0\n",
    "    best_reward = None\n",
    "    ts_start = time.time()\n",
    "\n",
    "    with RewardTracker(writer, stop_reward=18) as tracker:\n",
    "        with ptan.common.utils.TBMeanTracker(writer, batch_size=10) as tb_tracker:\n",
    "            for mb_obs, mb_rewards, mb_actions, mb_values, _, done_rewards, done_steps in iterate_batches(envs, net, device=device):\n",
    "                if len(done_rewards) > 0:\n",
    "                    total_steps += sum(done_steps)\n",
    "                    speed = total_steps / (time.time() - ts_start)\n",
    "                    if best_reward is None:\n",
    "                        best_reward = done_rewards.max()\n",
    "                    elif best_reward < done_rewards.max():\n",
    "                        best_reward = done_rewards.max()\n",
    "                    tb_tracker.track(\"total_reward_max\", best_reward, step_idx)\n",
    "                    tb_tracker.track(\"total_reward\", done_rewards, step_idx)\n",
    "                    tb_tracker.track(\"total_steps\", done_steps, step_idx)\n",
    "                    print(\"%d: done %d episodes, mean_reward=%.2f, best_reward=%.2f, speed=%.2f\" % (\n",
    "                        step_idx, len(done_rewards), done_rewards.mean(), best_reward, speed))\n",
    "\n",
    "                train_a2c(net, mb_obs, mb_rewards, mb_actions, mb_values,\n",
    "                          optimizer, tb_tracker, step_idx, device=device)\n",
    "                step_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb_actions = np.zeros((6, 3), dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = -36.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = torch.FloatTensor(test).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_v = torch.tensor(test).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_v.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_v.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2 = test1 + 35\n",
    "test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
