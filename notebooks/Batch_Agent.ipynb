{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ale_py/roms/utils.py:90: DeprecationWarning: SelectableGroups dict interface is deprecated. Use select.\n",
      "  for external in metadata.entry_points().get(self.group, []):\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import ptan\n",
    "import numpy as np\n",
    "import argparse\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils as nn_utils\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "gamma = 0.99\n",
    "batch_size = 11\n",
    "num_envs = 6\n",
    "reward_steps = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_shape[0]*input_shape[1], 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128) \n",
    "        )\n",
    "        \n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(128, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(128, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # flatten the observation space Box to linear tensor\n",
    "        x_flat = torch.flatten(x, 1,2).to(torch.float32)\n",
    "        #print('x_flat', x_flat.size(), x_flat)\n",
    "        init_out = self.net(x_flat)\n",
    "        return self.actor(init_out), self.critic(init_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack_batch(batch, model, device='cpu'):\n",
    "\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    not_done_idx = []\n",
    "    last_states = []\n",
    "    #create lists of the states, actions and rewards\n",
    "    for idx, exp in enumerate(batch):\n",
    "        states.append(np.array(exp.state, copy=False))\n",
    "        actions.append(int(exp.action))\n",
    "        rewards.append(exp.reward)\n",
    "        #separate out the last states to be able to calculate the rewards\n",
    "        if exp.last_state is not None:\n",
    "            not_done_idx.append(idx)\n",
    "            last_states.append(np.array(exp.last_state, copy=False))\n",
    "\n",
    "    #convert to tensors for calculations\n",
    "    states = torch.FloatTensor(\n",
    "        np.array(states, copy=False)).to(device)\n",
    "    actions = torch.LongTensor(actions).to(device)\n",
    "\n",
    "    # handle rewards\n",
    "    rewards_np = np.array(rewards, dtype=np.float32)\n",
    "    if not_done_idx:\n",
    "        last_states = torch.FloatTensor(np.array(last_states, copy=False)).to(device)\n",
    "        last_vals = model(last_states)[1]\n",
    "        last_vals_np = last_vals.data.cpu().numpy()[:, 0]\n",
    "        last_vals_np *= gamma ** reward_steps\n",
    "        rewards_np[not_done_idx] += last_vals_np\n",
    "\n",
    "    rewards = torch.FloatTensor(rewards_np).to(device)\n",
    "\n",
    "    return states, actions, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SchedulerEnv(gym.Env):\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        #starting parameters\n",
    "        num_gps = 100\n",
    "        num_slots = 32\n",
    "        num_pre_booked = 15\n",
    "        to_book = [2,1,1,1,1,1,1]\n",
    "        num_to_book = len(to_book)\n",
    "        agent_pos = [0,0]\n",
    "        reward_decay = 0.95\n",
    "        \n",
    "        #set parameters for the day\n",
    "        self.num_gps = num_gps\n",
    "        self.num_slots = num_slots\n",
    "        self.num_pre_booked = num_pre_booked\n",
    "        self.to_book = to_book\n",
    "        self.num_to_book = num_to_book\n",
    "        self.diary_slots = num_gps*num_slots\n",
    "        self.agent_pos = agent_pos\n",
    "        self.reward_decay = reward_decay\n",
    "\n",
    "        #set action space to move around the grid\n",
    "        self.action_space = gym.spaces.Discrete(4) #up, down, left, right\n",
    "        \n",
    "        #set observation space \n",
    "        self.observation_space = gym.spaces.Box(low=0, high=1, shape=(self.num_slots, self.num_gps), dtype=np.int32)\n",
    "   \n",
    "    #creates daily diary for each gp, randomly populates prebooked appointments and resets parameters\n",
    "    def reset(self):\n",
    "\n",
    "        #creates zero filled dataframe with row per time slot and column per gp\n",
    "        self.state = np.zeros((self.num_slots, self.num_gps),dtype=float)\n",
    "\n",
    "        #randomly enters a 1 for each pre booked appointments\n",
    "        pre_booked = self.num_pre_booked\n",
    "        while pre_booked>0:\n",
    "            pre_booked -= 1\n",
    "            self.state[np.random.randint(self.num_slots), np.random.randint(self.num_gps)] = 1\n",
    "            \n",
    "        #randomly sets the agent start space\n",
    "        self.agent_pos = [np.random.randint(self.num_slots), np.random.randint(self.num_gps)]\n",
    "\n",
    "        #resets parameters for new episode\n",
    "        self.done = False\n",
    "        self.reward = 0\n",
    "        self.appt_idx = 0\n",
    "        self.decay_steps = 1\n",
    "        \n",
    "        #print('starting state', self.state.sum(), self.state)\n",
    "\n",
    "        return self.state\n",
    "    \n",
    "    #calculates new position of the agent based on the action\n",
    "    def move_agent(self, action):\n",
    "\n",
    "        #set boundaries for the grid\n",
    "        max_row = self.num_slots - 1\n",
    "        max_col = self.num_gps - 1\n",
    "\n",
    "        #setting new co-ordinates for the agent\n",
    "        new_row = self.agent_pos[0]\n",
    "        new_col = self.agent_pos[1]\n",
    "\n",
    "        #calculate what the new position may be based on the action without going out the grid\n",
    "        if action == 0:\n",
    "            #print('up')\n",
    "            new_row = max(self.agent_pos[0] - 1, 0)\n",
    "        if action == 1:\n",
    "            #print('down')\n",
    "            new_row = min(self.agent_pos[0] + 1, max_row)\n",
    "        if action == 2:\n",
    "            #print('left')\n",
    "            new_col = max(self.agent_pos[1] - 1, 0)\n",
    "        if action == 3:\n",
    "            #print('right')\n",
    "            new_col = min(self.agent_pos[1] + 1, max_col)\n",
    "\n",
    "        new_pos = [new_row, new_col]\n",
    "        #print('new pos', new_pos)\n",
    "\n",
    "        return new_pos\n",
    "\n",
    "    #checks if we can look to book appointment starting here\n",
    "    def check_bookable(self):\n",
    "        return self.state[self.agent_pos[0], self.agent_pos[1]] == 0.0\n",
    "    \n",
    "    #action if we can't book the appointment\n",
    "    def invalid_booking(self):\n",
    "        #print('cant book')\n",
    "        self.decay_steps += 1\n",
    "        self.reward = -1\n",
    "        \n",
    "    #action if we can book the appointment\n",
    "    def valid_booking(self):\n",
    "        #print('go ahead and book')\n",
    "        self.appt_idx += 1\n",
    "        self.decay_steps = 1\n",
    "        self.reward = 1\n",
    "    \n",
    "    #checks if the appointment fits\n",
    "    def check_and_book(self):\n",
    "        \n",
    "        max_row = self.num_slots - 1\n",
    "        cells_to_check = self.to_book[self.appt_idx]\n",
    "        \n",
    "        if cells_to_check==1:\n",
    "            #print('good to check for single')\n",
    "            if self.state[self.agent_pos[0], self.agent_pos[1]] == 0:\n",
    "                self.state[self.agent_pos[0], self.agent_pos[1]] = 1\n",
    "                self.valid_booking()\n",
    "            else:\n",
    "                #print('single taken')\n",
    "                self.invalid_booking()\n",
    "\n",
    "        if cells_to_check==2:\n",
    "            #check we're not at the bottom of the grid\n",
    "            if self.agent_pos[0]<max_row:\n",
    "                #check the next cells is also 0.0\n",
    "                #print('good to check for double')\n",
    "                if self.state[self.agent_pos[0], self.agent_pos[1]] == 0 and \\\n",
    "                self.state[(self.agent_pos[0]+1), self.agent_pos[1]] == 0:\n",
    "                    self.state[self.agent_pos[0], self.agent_pos[1]] = 1\n",
    "                    self.state[(self.agent_pos[0]+1), self.agent_pos[1]] = 1\n",
    "                    self.valid_booking()\n",
    "                    self.agent_pos = [(self.agent_pos[0]+1), self.agent_pos[1]]\n",
    "                    #print('after booking', self.agent_pos)\n",
    "                else:\n",
    "                    #print('double taken')\n",
    "                    self.invalid_booking()\n",
    "            else:\n",
    "                #print('not for double')\n",
    "                self.invalid_booking()\n",
    "                \n",
    "        if cells_to_check==3:\n",
    "            #check we're not at the bottom of the grid\n",
    "            if self.agent_pos[0]+1<max_row:\n",
    "                #print('good to check for treble')\n",
    "                if self.state[self.agent_pos[0], self.agent_pos[1]] == 0 and \\\n",
    "                self.state[(self.agent_pos[0]+1), self.agent_pos[1]] == 0 \\\n",
    "                 and self.state[(self.agent_pos[0]+2), self.agent_pos[1]] == 0:\n",
    "                    self.state[self.agent_pos[0], self.agent_pos[1]] = 1\n",
    "                    self.state[(self.agent_pos[0]+1), self.agent_pos[1]] = 1\n",
    "                    self.state[(self.agent_pos[0]+2), self.agent_pos[1]] = 1\n",
    "                    self.valid_booking()\n",
    "                    self.agent_pos = [(self.agent_pos[0]+2), self.agent_pos[1]]\n",
    "                else:\n",
    "                    #print('treble taken')\n",
    "                    self.invalid_booking()\n",
    "            else:\n",
    "                #print('not for treble')\n",
    "                self.invalid_booking()\n",
    "                \n",
    "        if cells_to_check==4:\n",
    "            #check we're not at the bottom of the grid\n",
    "            if self.agent_pos[0]+2<max_row:\n",
    "                #check the next cells is also 0.0\n",
    "                #print('good for quad')\n",
    "                if self.state[self.agent_pos[0], self.agent_pos[1]] == 0 and \\\n",
    "                self.state[(self.agent_pos[0]+1), self.agent_pos[1]] == 0 \\\n",
    "                 and self.state[(self.agent_pos[0]+2), self.agent_pos[1]] == 0 and \\\n",
    "                self.state[(self.agent_pos[0]+3), self.agent_pos[1]] == 0:\n",
    "                    self.state[self.agent_pos[0], self.agent_pos[1]] = 1\n",
    "                    self.state[(self.agent_pos[0]+1), self.agent_pos[1]] = 1\n",
    "                    self.state[(self.agent_pos[0]+2), self.agent_pos[1]] = 1\n",
    "                    self.state[(self.agent_pos[0]+3), self.agent_pos[1]] = 1\n",
    "                    self.valid_booking()\n",
    "                    self.agent_pos = [(self.agent_pos[0]+3), self.agent_pos[1]]\n",
    "                else:\n",
    "                    #print('quad taken')\n",
    "                    self.invalid_booking()\n",
    "            else:\n",
    "                #print('not for quad')\n",
    "                self.invalid_booking()\n",
    "\n",
    "        next_state = self.state\n",
    "\n",
    "        return next_state\n",
    "\n",
    "    def step(self, action):\n",
    "\n",
    "        #print('start step' , self.decay_steps)\n",
    "        #get new position of agent based on action\n",
    "        new_agent_pos = self.move_agent(action)\n",
    "        #print('new and old pos', new_agent_pos, self.agent_pos)\n",
    "        \n",
    "        #if the agent is stuck on an edge then move to a new position\n",
    "        if new_agent_pos == self.agent_pos:\n",
    "            self.agent_pos = [np.random.randint(self.num_slots), np.random.randint(self.num_gps)]\n",
    "            #print('here1', self.agent_pos)\n",
    "        else:\n",
    "            self.agent_pos = new_agent_pos\n",
    "            #print('here2', self.agent_pos)\n",
    "        \n",
    "        #print('trying to book', self.to_book, self.appt_idx)\n",
    "        \n",
    "        #check if it's possible to book then book\n",
    "        if self.check_bookable():\n",
    "            #print('checked here')\n",
    "            self.state = self.check_and_book()\n",
    "        else:\n",
    "            #print('not bookable')\n",
    "            self.invalid_booking()\n",
    "        \n",
    "        #work out if episode complete\n",
    "        if self.appt_idx == len(self.to_book):\n",
    "            #print('all booked')\n",
    "            self.done = True\n",
    "  \n",
    "        #work out rewards\n",
    "        #self.reward = (1 - (self.reward_decay**self.decay_steps))\n",
    "        \n",
    "        #print('step', self.decay_steps, self.reward)\n",
    "        #print('end step')\n",
    "\n",
    "        info = {}\n",
    "        return self.state, self.reward, self.done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ave_batch_reward 1.3181218835525215 step 120\n",
      "ave_batch_loss 4.594318103790283 step 120\n",
      "ave_batch_reward 1.6223404937320285 step 230\n",
      "ave_batch_loss 3.553301705254449 step 230\n",
      "ave_batch_reward 1.8634349637561374 step 340\n",
      "ave_batch_loss 3.211408151520623 step 340\n",
      "ave_batch_reward 2.1520151098569236 step 450\n",
      "ave_batch_loss 4.357073214319017 step 450\n",
      "ave_batch_reward 2.786999444166819 step 560\n",
      "ave_batch_loss 4.680599914656745 step 560\n",
      "ave_batch_reward 3.4760952525668674 step 670\n",
      "ave_batch_loss 4.85870557361179 step 670\n",
      "ave_batch_reward 4.065450721316868 step 780\n",
      "ave_batch_loss 6.00934714741177 step 780\n",
      "ave_batch_reward 4.374369117948744 step 890\n",
      "ave_batch_loss 9.269775284661186 step 890\n",
      "ave_batch_reward 4.9022504223717585 step 1000\n",
      "ave_batch_loss 9.058591683705648 step 1000\n",
      "ave_batch_reward 4.901172743903266 step 1110\n",
      "ave_batch_loss 9.796756638420952 step 1110\n",
      "ave_batch_reward 5.318439324696858 step 1220\n",
      "ave_batch_loss 11.262685669793022 step 1220\n",
      "ave_batch_reward 5.078473461998834 step 1330\n",
      "ave_batch_loss 10.332385592990452 step 1330\n",
      "ave_batch_reward 5.150258170233832 step 1440\n",
      "ave_batch_loss 10.300681538052029 step 1440\n",
      "ave_batch_reward 5.177748097313775 step 1550\n",
      "ave_batch_loss 10.589453591240776 step 1550\n",
      "ave_batch_reward 5.092187404632568 step 1660\n",
      "ave_batch_loss 10.230302121904161 step 1660\n",
      "ave_batch_reward 5.39045778910319 step 1770\n",
      "ave_batch_loss 11.357831637064615 step 1770\n",
      "ave_batch_reward 5.220581637488471 step 1880\n",
      "ave_batch_loss 10.615387280782064 step 1880\n",
      "ave_batch_reward 5.240604559580485 step 1990\n",
      "ave_batch_loss 10.91156726413303 step 1990\n",
      "ave_batch_reward 5.187226719326443 step 2100\n",
      "ave_batch_loss 11.033216688368055 step 2100\n",
      "ave_batch_reward 5.383250130547418 step 2210\n",
      "ave_batch_loss 10.515552202860514 step 2210\n",
      "ave_batch_reward 5.267524931165907 step 2320\n",
      "ave_batch_loss 10.989360809326172 step 2320\n",
      "ave_batch_reward 5.15480571322971 step 2430\n",
      "ave_batch_loss 11.141779687669542 step 2430\n",
      "ave_batch_reward 5.189050250583225 step 2540\n",
      "ave_batch_loss 10.37239827050103 step 2540\n",
      "ave_batch_reward 5.1432022253672285 step 2650\n",
      "ave_batch_loss 11.285751554701063 step 2650\n",
      "ave_batch_reward 5.42932571305169 step 2760\n",
      "ave_batch_loss 11.19361760881212 step 2760\n",
      "ave_batch_reward 4.956185075971815 step 2870\n",
      "ave_batch_loss 10.30667241414388 step 2870\n",
      "ave_batch_reward 5.192323154873318 step 2980\n",
      "ave_batch_loss 10.952526410420736 step 2980\n",
      "ave_batch_reward 5.151105191972521 step 3090\n",
      "ave_batch_loss 11.015003522237143 step 3090\n",
      "ave_batch_reward 5.175876008139716 step 3200\n",
      "ave_batch_loss 10.507996453179253 step 3200\n",
      "ave_batch_reward 5.376548237270779 step 3310\n",
      "ave_batch_loss 11.041405783759224 step 3310\n",
      "ave_batch_reward 5.260927571190728 step 3420\n",
      "ave_batch_loss 10.883479012383354 step 3420\n",
      "ave_batch_reward 5.26698997285631 step 3530\n",
      "ave_batch_loss 10.289865493774414 step 3530\n",
      "ave_batch_reward 5.068927420510186 step 3640\n",
      "ave_batch_loss 10.588457955254448 step 3640\n",
      "ave_batch_reward 5.335743427276611 step 3750\n",
      "ave_batch_loss 11.381826612684462 step 3750\n",
      "ave_batch_reward 5.12207751803928 step 3860\n",
      "ave_batch_loss 11.006982379489475 step 3860\n",
      "ave_batch_reward 5.077750470903185 step 3970\n",
      "ave_batch_loss 10.522901323106554 step 3970\n",
      "ave_batch_reward 5.179831027984619 step 4080\n",
      "ave_batch_loss 10.087892426384819 step 4080\n",
      "ave_batch_reward 5.280924293729994 step 4190\n",
      "ave_batch_loss 11.048334545559353 step 4190\n",
      "ave_batch_reward 5.553069591522217 step 4300\n",
      "ave_batch_loss 11.97937658098009 step 4300\n",
      "ave_batch_reward 5.000551753573948 step 4410\n",
      "ave_batch_loss 10.649824142456055 step 4410\n",
      "ave_batch_reward 5.203832149505615 step 4520\n",
      "ave_batch_loss 11.001169628567165 step 4520\n",
      "ave_batch_reward 5.031393077638414 step 4630\n",
      "ave_batch_loss 10.888486120435926 step 4630\n",
      "ave_batch_reward 5.184634659025404 step 4740\n",
      "ave_batch_loss 10.903690232170952 step 4740\n",
      "ave_batch_reward 5.497167454825507 step 4850\n",
      "ave_batch_loss 11.259203063117134 step 4850\n",
      "ave_batch_reward 5.01528975698683 step 4960\n",
      "ave_batch_loss 10.472434255811903 step 4960\n",
      "ave_batch_reward 5.350919485092163 step 5070\n",
      "ave_batch_loss 10.901377465989855 step 5070\n",
      "ave_batch_reward 5.149440288543701 step 5180\n",
      "ave_batch_loss 11.164043532477486 step 5180\n",
      "ave_batch_reward 5.115264548195733 step 5290\n",
      "ave_batch_loss 10.442410786946615 step 5290\n",
      "ave_batch_reward 5.418703979916042 step 5400\n",
      "ave_batch_loss 10.520825174119738 step 5400\n",
      "ave_batch_reward 5.1114678382873535 step 5510\n",
      "ave_batch_loss 11.042223930358887 step 5510\n",
      "ave_batch_reward 5.4227205117543535 step 5620\n",
      "ave_batch_loss 10.927066697014702 step 5620\n",
      "ave_batch_reward 5.138747851053874 step 5730\n",
      "ave_batch_loss 11.111431121826172 step 5730\n",
      "ave_batch_reward 5.13146169980367 step 5840\n",
      "ave_batch_loss 11.048137876722548 step 5840\n",
      "ave_batch_reward 5.520968596140544 step 5950\n",
      "ave_batch_loss 11.264468510945639 step 5950\n",
      "ave_batch_reward 5.091515170203315 step 6060\n",
      "ave_batch_loss 11.067483160230848 step 6060\n",
      "ave_batch_reward 5.287152502271864 step 6170\n",
      "ave_batch_loss 11.049588203430176 step 6170\n",
      "ave_batch_reward 5.270737648010254 step 6280\n",
      "ave_batch_loss 10.98216438293457 step 6280\n",
      "ave_batch_reward 5.21228994263543 step 6390\n",
      "ave_batch_loss 10.830002148946127 step 6390\n",
      "ave_batch_reward 5.337732368045383 step 6500\n",
      "ave_batch_loss 10.90376059214274 step 6500\n",
      "ave_batch_reward 5.12631188498603 step 6610\n",
      "ave_batch_loss 10.84956063164605 step 6610\n",
      "ave_batch_reward 5.428920798831516 step 6720\n",
      "ave_batch_loss 11.06341224246555 step 6720\n",
      "ave_batch_reward 5.251808537377252 step 6830\n",
      "ave_batch_loss 11.048084576924643 step 6830\n",
      "ave_batch_reward 5.328976472218831 step 6940\n",
      "ave_batch_loss 10.7994351916843 step 6940\n",
      "ave_batch_reward 5.255167696211073 step 7050\n",
      "ave_batch_loss 11.434854825337728 step 7050\n",
      "ave_batch_reward 5.035229497485691 step 7160\n",
      "ave_batch_loss 10.637556076049805 step 7160\n",
      "ave_batch_reward 5.292956511179606 step 7270\n",
      "ave_batch_loss 10.828340424431694 step 7270\n",
      "ave_batch_reward 5.25918526119656 step 7380\n",
      "ave_batch_loss 10.712112214830187 step 7380\n",
      "ave_batch_reward 5.450643380482991 step 7490\n",
      "ave_batch_loss 10.853415807088217 step 7490\n",
      "ave_batch_reward 5.275266647338867 step 7600\n",
      "ave_batch_loss 11.128737343682182 step 7600\n",
      "ave_batch_reward 5.320204681820339 step 7710\n",
      "ave_batch_loss 11.310520384046766 step 7710\n",
      "ave_batch_reward 5.214610417683919 step 7820\n",
      "ave_batch_loss 10.64909659491645 step 7820\n",
      "ave_batch_reward 5.172522438897027 step 7930\n",
      "ave_batch_loss 10.57077333662245 step 7930\n",
      "ave_batch_reward 5.285523308648004 step 8040\n",
      "ave_batch_loss 11.00001663631863 step 8040\n",
      "ave_batch_reward 5.20394574271308 step 8150\n",
      "ave_batch_loss 10.91920640733507 step 8150\n",
      "ave_batch_reward 5.32253180609809 step 8260\n",
      "ave_batch_loss 11.282455656263563 step 8260\n",
      "ave_batch_reward 5.220490429136488 step 8370\n",
      "ave_batch_loss 11.155125617980957 step 8370\n",
      "ave_batch_reward 5.293040487501356 step 8480\n",
      "ave_batch_loss 11.08134322696262 step 8480\n",
      "ave_batch_reward 5.417276064554851 step 8590\n",
      "ave_batch_loss 11.35862594180637 step 8590\n",
      "ave_batch_reward 5.324463897281223 step 8700\n",
      "ave_batch_loss 10.859105004204643 step 8700\n",
      "ave_batch_reward 5.323775132497151 step 8810\n",
      "ave_batch_loss 11.119807031419542 step 8810\n",
      "ave_batch_reward 5.361149099138048 step 8920\n",
      "ave_batch_loss 11.291222784254286 step 8920\n",
      "ave_batch_reward 5.295653290218777 step 9030\n",
      "ave_batch_loss 11.069592581854927 step 9030\n",
      "ave_batch_reward 5.42308685514662 step 9140\n",
      "ave_batch_loss 10.9230989880032 step 9140\n",
      "ave_batch_reward 5.289008405473497 step 9250\n",
      "ave_batch_loss 11.066947407192654 step 9250\n",
      "ave_batch_reward 5.118145571814643 step 9360\n",
      "ave_batch_loss 10.788564893934462 step 9360\n",
      "ave_batch_reward 5.235605133904351 step 9470\n",
      "ave_batch_loss 11.042380862765842 step 9470\n",
      "ave_batch_reward 5.078546735975477 step 9580\n",
      "ave_batch_loss 11.213275273640951 step 9580\n",
      "ave_batch_reward 5.335727055867513 step 9690\n",
      "ave_batch_loss 11.114155133565268 step 9690\n",
      "ave_batch_reward 5.14161557621426 step 9800\n",
      "ave_batch_loss 11.681187205844456 step 9800\n",
      "ave_batch_reward 5.315131611294216 step 9910\n",
      "ave_batch_loss 10.476102193196615 step 9910\n",
      "ave_batch_reward 5.406038337283665 step 10020\n",
      "ave_batch_loss 10.488757345411512 step 10020\n",
      "ave_batch_reward 5.226099756028917 step 10130\n",
      "ave_batch_loss 11.148758676317003 step 10130\n",
      "ave_batch_reward 5.123207516140408 step 10240\n",
      "ave_batch_loss 10.889510154724121 step 10240\n",
      "ave_batch_reward 5.202856699625651 step 10350\n",
      "ave_batch_loss 10.736651738484701 step 10350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ave_batch_reward 5.301776780022515 step 10460\n",
      "ave_batch_loss 11.115466117858887 step 10460\n",
      "ave_batch_reward 5.303338845570882 step 10570\n",
      "ave_batch_loss 11.120767487419975 step 10570\n",
      "ave_batch_reward 5.260269112057156 step 10680\n",
      "ave_batch_loss 11.0654083887736 step 10680\n",
      "ave_batch_reward 5.131660461425781 step 10790\n",
      "ave_batch_loss 11.085326936509874 step 10790\n",
      "ave_batch_reward 5.209829330444336 step 10900\n",
      "ave_batch_loss 10.833916346232096 step 10900\n",
      "ave_batch_reward 5.227771997451782 step 11010\n",
      "ave_batch_loss 11.205587493048775 step 11010\n",
      "ave_batch_reward 5.1779919200473365 step 11120\n",
      "ave_batch_loss 10.78127023908827 step 11120\n",
      "ave_batch_reward 4.990943193435669 step 11230\n",
      "ave_batch_loss 10.512361314561632 step 11230\n",
      "ave_batch_reward 5.205412228902181 step 11340\n",
      "ave_batch_loss 10.565112855699327 step 11340\n",
      "ave_batch_reward 5.239496813880073 step 11450\n",
      "ave_batch_loss 11.294324133131239 step 11450\n",
      "ave_batch_reward 5.404439979129368 step 11560\n",
      "ave_batch_loss 11.062705463833279 step 11560\n",
      "ave_batch_reward 5.272103733486599 step 11670\n",
      "ave_batch_loss 11.152610884772407 step 11670\n",
      "ave_batch_reward 5.246178679996067 step 11780\n",
      "ave_batch_loss 10.932344330681694 step 11780\n",
      "ave_batch_reward 5.035604265001085 step 11890\n",
      "ave_batch_loss 11.216434372795952 step 11890\n",
      "ave_batch_reward 5.357968330383301 step 12000\n",
      "ave_batch_loss 11.263731426662869 step 12000\n",
      "ave_batch_reward 5.318225728140937 step 12110\n",
      "ave_batch_loss 10.735494083828396 step 12110\n",
      "ave_batch_reward 5.427238941192627 step 12220\n",
      "ave_batch_loss 10.954288482666016 step 12220\n",
      "ave_batch_reward 5.2231005562676325 step 12330\n",
      "ave_batch_loss 11.181923336452908 step 12330\n",
      "ave_batch_reward 5.208050701353285 step 12440\n",
      "ave_batch_loss 11.11494223276774 step 12440\n",
      "ave_batch_reward 5.166454977459377 step 12550\n",
      "ave_batch_loss 10.563641548156738 step 12550\n",
      "ave_batch_reward 5.249651008182102 step 12660\n",
      "ave_batch_loss 11.081365797254774 step 12660\n",
      "ave_batch_reward 5.280285411410862 step 12770\n",
      "ave_batch_loss 11.024914317660862 step 12770\n",
      "ave_batch_reward 5.4357176886664496 step 12880\n",
      "ave_batch_loss 11.018662028842503 step 12880\n",
      "ave_batch_reward 5.279287020365397 step 12990\n",
      "ave_batch_loss 11.023362477620443 step 12990\n",
      "ave_batch_reward 5.119160599178738 step 13100\n",
      "ave_batch_loss 11.276007228427464 step 13100\n",
      "ave_batch_reward 5.2979242536756725 step 13210\n",
      "ave_batch_loss 11.080998632642958 step 13210\n",
      "ave_batch_reward 5.358976576063368 step 13320\n",
      "ave_batch_loss 10.963422139485678 step 13320\n",
      "ave_batch_reward 5.1490481164720325 step 13430\n",
      "ave_batch_loss 11.695656458536783 step 13430\n",
      "ave_batch_reward 5.183907084994846 step 13540\n",
      "ave_batch_loss 10.640446768866646 step 13540\n",
      "ave_batch_reward 5.232907030317518 step 13650\n",
      "ave_batch_loss 10.704429626464844 step 13650\n",
      "ave_batch_reward 5.3794301350911455 step 13760\n",
      "ave_batch_loss 10.747210290696886 step 13760\n",
      "ave_batch_reward 5.222742822435167 step 13870\n",
      "ave_batch_loss 11.166521178351509 step 13870\n",
      "ave_batch_reward 5.500495009952122 step 13980\n",
      "ave_batch_loss 11.034580760531956 step 13980\n",
      "ave_batch_reward 5.175035211775038 step 14090\n",
      "ave_batch_loss 9.732144037882486 step 14090\n",
      "ave_batch_reward 5.014424800872803 step 14200\n",
      "ave_batch_loss 10.1712097591824 step 14200\n",
      "ave_batch_reward 4.838787608676487 step 14310\n",
      "ave_batch_loss 9.459009064568413 step 14310\n",
      "ave_batch_reward 5.034001323911879 step 14420\n",
      "ave_batch_loss 9.697486771477593 step 14420\n",
      "ave_batch_reward 4.980945852067736 step 14530\n",
      "ave_batch_loss 10.122810734642876 step 14530\n",
      "ave_batch_reward 4.977340512805515 step 14640\n",
      "ave_batch_loss 10.142985661824545 step 14640\n",
      "ave_batch_reward 4.888908942540486 step 14750\n",
      "ave_batch_loss 9.951505872938368 step 14750\n",
      "ave_batch_reward 5.18018462922838 step 14860\n",
      "ave_batch_loss 10.605962011549208 step 14860\n",
      "ave_batch_reward 5.215444405873616 step 14970\n",
      "ave_batch_loss 10.598676787482368 step 14970\n",
      "ave_batch_reward 5.15386962890625 step 15080\n",
      "ave_batch_loss 11.146613756815592 step 15080\n",
      "ave_batch_reward 5.570674737294515 step 15190\n",
      "ave_batch_loss 11.426246113247341 step 15190\n",
      "ave_batch_reward 5.076964484320746 step 15300\n",
      "ave_batch_loss 11.252481990390354 step 15300\n",
      "ave_batch_reward 5.292749934726292 step 15410\n",
      "ave_batch_loss 11.306814405653212 step 15410\n",
      "ave_batch_reward 5.52451835738288 step 15520\n",
      "ave_batch_loss 11.168863296508789 step 15520\n",
      "ave_batch_reward 5.0474117332034645 step 15630\n",
      "ave_batch_loss 11.025246938069662 step 15630\n",
      "ave_batch_reward 5.382070091035631 step 15740\n",
      "ave_batch_loss 11.07491789923774 step 15740\n",
      "ave_batch_reward 5.073417027791341 step 15850\n",
      "ave_batch_loss 10.905675570170084 step 15850\n",
      "ave_batch_reward 4.929543998506334 step 15960\n",
      "ave_batch_loss 10.273914125230577 step 15960\n",
      "ave_batch_reward 5.290230909983317 step 16070\n",
      "ave_batch_loss 11.016305923461914 step 16070\n",
      "ave_batch_reward 5.329339239332411 step 16180\n",
      "ave_batch_loss 11.328470547993978 step 16180\n",
      "ave_batch_reward 5.383805274963379 step 16290\n",
      "ave_batch_loss 11.066266589694553 step 16290\n",
      "ave_batch_reward 5.117748631371392 step 16400\n",
      "ave_batch_loss 10.718579610188803 step 16400\n",
      "ave_batch_reward 5.071522686216566 step 16510\n",
      "ave_batch_loss 10.905147870381674 step 16510\n",
      "ave_batch_reward 5.273690170711941 step 16620\n",
      "ave_batch_loss 10.977577421400282 step 16620\n",
      "ave_batch_reward 5.213311248355442 step 16730\n",
      "ave_batch_loss 10.732372177971733 step 16730\n",
      "ave_batch_reward 5.492791652679443 step 16840\n",
      "ave_batch_loss 11.39188618130154 step 16840\n",
      "ave_batch_reward 5.219516648186578 step 16950\n",
      "ave_batch_loss 11.194026205274794 step 16950\n",
      "ave_batch_reward 5.256303893195258 step 17060\n",
      "ave_batch_loss 11.050975693596733 step 17060\n",
      "ave_batch_reward 5.183080620235867 step 17170\n",
      "ave_batch_loss 10.556461758083767 step 17170\n",
      "ave_batch_reward 5.192872842152913 step 17280\n",
      "ave_batch_loss 11.230571640862358 step 17280\n",
      "ave_batch_reward 5.522519005669488 step 17390\n",
      "ave_batch_loss 11.255660163031685 step 17390\n",
      "ave_batch_reward 4.985834280649821 step 17500\n",
      "ave_batch_loss 10.606192376878527 step 17500\n",
      "ave_batch_reward 5.306410100724962 step 17610\n",
      "ave_batch_loss 11.031086285909018 step 17610\n",
      "ave_batch_reward 5.226400004492866 step 17720\n",
      "ave_batch_loss 10.228008588155111 step 17720\n",
      "ave_batch_reward 5.273145357767741 step 17830\n",
      "ave_batch_loss 10.932809193929037 step 17830\n",
      "ave_batch_reward 5.3290773497687445 step 17940\n",
      "ave_batch_loss 11.304408603244358 step 17940\n",
      "ave_batch_reward 5.125301837921143 step 18050\n",
      "ave_batch_loss 10.96689870622423 step 18050\n",
      "ave_batch_reward 5.500374661551581 step 18160\n",
      "ave_batch_loss 11.150602340698242 step 18160\n",
      "ave_batch_reward 5.189879576365153 step 18270\n",
      "ave_batch_loss 11.466995557149252 step 18270\n",
      "ave_batch_reward 4.963740958107842 step 18380\n",
      "ave_batch_loss 10.869103643629286 step 18380\n",
      "ave_batch_reward 5.198966052797106 step 18490\n",
      "ave_batch_loss 10.413172721862793 step 18490\n",
      "ave_batch_reward 4.939550585216946 step 18600\n",
      "ave_batch_loss 10.264260821872288 step 18600\n",
      "ave_batch_reward 5.389606396357219 step 18710\n",
      "ave_batch_loss 10.433439042833117 step 18710\n",
      "ave_batch_reward 5.16457478205363 step 18820\n",
      "ave_batch_loss 10.840484301249186 step 18820\n",
      "ave_batch_reward 4.904489835103353 step 18930\n",
      "ave_batch_loss 10.734657605489096 step 18930\n",
      "ave_batch_reward 4.994304021199544 step 19040\n",
      "ave_batch_loss 9.640403429667154 step 19040\n",
      "ave_batch_reward 4.947957807117039 step 19150\n",
      "ave_batch_loss 10.555201000637478 step 19150\n",
      "ave_batch_reward 5.11833651860555 step 19260\n",
      "ave_batch_loss 10.17026752895779 step 19260\n",
      "ave_batch_reward 5.268131203121609 step 19370\n",
      "ave_batch_loss 11.252159118652344 step 19370\n",
      "ave_batch_reward 5.252730952368842 step 19480\n",
      "ave_batch_loss 10.858107778761122 step 19480\n",
      "ave_batch_reward 5.1460632748074 step 19590\n",
      "ave_batch_loss 11.684143596225315 step 19590\n",
      "ave_batch_reward 5.265360037485759 step 19700\n",
      "ave_batch_loss 10.410734388563368 step 19700\n",
      "ave_batch_reward 5.254494667053223 step 19810\n",
      "ave_batch_loss 10.805983225504557 step 19810\n",
      "ave_batch_reward 5.290508959028456 step 19920\n",
      "ave_batch_loss 11.05410353342692 step 19920\n",
      "ave_batch_reward 5.297452502780491 step 20030\n",
      "ave_batch_loss 11.110870361328125 step 20030\n",
      "ave_batch_reward 5.24802303314209 step 20140\n",
      "ave_batch_loss 10.832030084398058 step 20140\n",
      "ave_batch_reward 5.071124924553765 step 20250\n",
      "ave_batch_loss 10.684083938598633 step 20250\n",
      "ave_batch_reward 5.386427296532525 step 20360\n",
      "ave_batch_loss 11.126911799112955 step 20360\n",
      "ave_batch_reward 5.301024966769749 step 20470\n",
      "ave_batch_loss 11.11943499247233 step 20470\n",
      "ave_batch_reward 5.295626746283637 step 20580\n",
      "ave_batch_loss 11.100428899129232 step 20580\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ave_batch_reward 5.324835671318902 step 20690\n",
      "ave_batch_loss 11.286741786532932 step 20690\n",
      "ave_batch_reward 5.2698089811537 step 20800\n",
      "ave_batch_loss 10.94198120964898 step 20800\n",
      "ave_batch_reward 5.379720687866211 step 20910\n",
      "ave_batch_loss 11.16559558444553 step 20910\n",
      "ave_batch_reward 5.320831245846218 step 21020\n",
      "ave_batch_loss 11.25664922926161 step 21020\n",
      "ave_batch_reward 5.288255373636882 step 21130\n",
      "ave_batch_loss 11.011272748311361 step 21130\n",
      "ave_batch_reward 5.303782569037543 step 21240\n",
      "ave_batch_loss 11.164710680643717 step 21240\n",
      "ave_batch_reward 5.160009860992432 step 21350\n",
      "ave_batch_loss 11.062032699584961 step 21350\n",
      "ave_batch_reward 5.263107670678033 step 21460\n",
      "ave_batch_loss 10.920257886250814 step 21460\n",
      "ave_batch_reward 5.396144125196669 step 21570\n",
      "ave_batch_loss 10.885385195414225 step 21570\n",
      "ave_batch_reward 5.286918428209093 step 21680\n",
      "ave_batch_loss 11.02766587999132 step 21680\n",
      "ave_batch_reward 5.276480250888401 step 21790\n",
      "ave_batch_loss 10.960535367329916 step 21790\n",
      "ave_batch_reward 5.160644637213813 step 21900\n",
      "ave_batch_loss 10.943418396843803 step 21900\n",
      "ave_batch_reward 5.296130869123671 step 22010\n",
      "ave_batch_loss 10.897583431667751 step 22010\n",
      "ave_batch_reward 5.510683006710476 step 22120\n",
      "ave_batch_loss 11.195318433973524 step 22120\n",
      "ave_batch_reward 5.223448276519775 step 22230\n",
      "ave_batch_loss 11.163233757019043 step 22230\n",
      "ave_batch_reward 5.3135008282131615 step 22340\n",
      "ave_batch_loss 11.214769787258572 step 22340\n",
      "ave_batch_reward 5.152724424997966 step 22450\n",
      "ave_batch_loss 11.13718711005317 step 22450\n",
      "ave_batch_reward 5.157542679044935 step 22560\n",
      "ave_batch_loss 10.159196429782444 step 22560\n",
      "ave_batch_reward 5.262673695882161 step 22670\n",
      "ave_batch_loss 10.858815511067709 step 22670\n",
      "ave_batch_reward 5.362921635309855 step 22780\n",
      "ave_batch_loss 11.117624388800728 step 22780\n",
      "ave_batch_reward 5.336105558607313 step 22890\n",
      "ave_batch_loss 10.724062866634792 step 22890\n",
      "ave_batch_reward 5.134096145629883 step 23000\n",
      "ave_batch_loss 10.703452534145779 step 23000\n",
      "ave_batch_reward 5.221627155939738 step 23110\n",
      "ave_batch_loss 10.872879770067003 step 23110\n",
      "ave_batch_reward 5.134729782740275 step 23220\n",
      "ave_batch_loss 11.060034645928276 step 23220\n"
     ]
    }
   ],
   "source": [
    "#device = \"cuda\"\n",
    "device = \"cpu\"\n",
    "\n",
    "#create multiple environments for multiprocessing\n",
    "make_env = lambda: SchedulerEnv()\n",
    "envs = [make_env() for _ in range(num_envs)]\n",
    "\n",
    "#start writing to tensorboard\n",
    "writer = SummaryWriter(comment=\"Scheduler\")\n",
    "\n",
    "#initialise model, agent and run through episodes to get experience\n",
    "model = Model(envs[0].observation_space.shape, envs[0].action_space.n).to(device)\n",
    "agent = ptan.agent.PolicyAgent(lambda x: model(x)[0], apply_softmax=True, device=device)\n",
    "exp_source = ptan.experience.ExperienceSourceFirstLast(envs, agent, gamma=gamma, steps_count=reward_steps)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, eps=1e-3)\n",
    "\n",
    "#create list to capture batches\n",
    "batch = []\n",
    "\n",
    "#create lists to be used to record values for tracking averages\n",
    "reward_stack = []\n",
    "loss_stack = []\n",
    "\n",
    "#work through each experience source to capture state, actions etc\n",
    "for step_idx, exp in enumerate(exp_source):\n",
    "    batch.append(exp)\n",
    "\n",
    "    if len(batch) < batch_size:\n",
    "        continue\n",
    "\n",
    "    states, actions, rewards = unpack_batch(batch, model, device=device)\n",
    "    batch.clear()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # using the network to give actions and state_value\n",
    "    actor_val, critic_val = model(states)\n",
    "    # [CRITIC] calculate the loss between value_state (just predicted now) and reward from the batch\n",
    "    critic_loss = F.mse_loss(critic_val.squeeze(-1), rewards)\n",
    "\n",
    "    # Runs the log_softmax against actor output (just predicted now)\n",
    "    log_prob = F.log_softmax(actor_val, dim=1)\n",
    "    # Advantage equals reward from the batch (size:[batch_size]) minus the value_state (just predicted now)\n",
    "    advantage = rewards - critic_val.detach()\n",
    "\n",
    "    # multiples the advantage at each step by the log probability of the chosen action for that step\n",
    "    log_prob_actions = advantage * log_prob[range(batch_size), actions]\n",
    "    # calculate the policy gradient adjustment to make (negated to move toward policy improvement)\n",
    "    actor_loss = -log_prob_actions.mean()\n",
    "\n",
    "    # perform softmax on action estimates (from ACTOR) (just predicted now)\n",
    "    prob_val = F.softmax(actor_val, dim=1)\n",
    "    # calculating the action entropy \n",
    "    entropy_loss = 0.01 * (prob_val * log_prob).sum(dim=1).mean()\n",
    "\n",
    "    # calculate policy gradients only\n",
    "\n",
    "    # [ACTOR] backpropogate\n",
    "    actor_loss.backward(retain_graph=True)\n",
    "\n",
    "    # apply entropy and value gradients\n",
    "    # [CRITIC] backpropagate and apply entropy\n",
    "    loss = entropy_loss + critic_loss\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    #send average loss and rewards to tensorboard\n",
    "    if len(reward_stack) > 0 and step_idx % 10 == 0:\n",
    "        #print(step_idx)\n",
    "        avg_rewards = np.mean(reward_stack)\n",
    "        avg_loss = np.mean(loss_stack)\n",
    "        writer.add_scalar('ave_batch_reward', avg_rewards, step_idx)\n",
    "        writer.add_scalar('ave_batch_loss', avg_loss, step_idx)\n",
    "        print('ave_batch_reward', avg_rewards, 'step', step_idx)\n",
    "        print('ave_batch_loss', avg_loss, 'step', step_idx)\n",
    "        reward_stack.clear()\n",
    "        loss_stack.clear()\n",
    "    else:\n",
    "        reward_stack.append(torch.mean(rewards).item())\n",
    "        loss_stack.append(torch.mean(critic_loss).item())\n",
    "        \n",
    "    if step_idx > 100000:\n",
    "        break\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
