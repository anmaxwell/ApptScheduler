{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def __init__(self, arg1, arg2, ...):\n",
    "    super(CustomEnv, self).__init__()\n",
    "\n",
    "    self.action_space = \n",
    "    self.observation_space = \n",
    "    \n",
    "    \n",
    "    \n",
    "def __init__(self):\n",
    "        self.action_space = gym.spaces.Discrete(5)\n",
    "        self.observation_space = gym.spaces.Discrete(2)\n",
    "        \n",
    "        \n",
    "def __init__(self):\n",
    "    # There are two actions, first will get reward of 1, second reward of -1. \n",
    "    self.action_space = gym.spaces.Discrete(5)\n",
    "    self.observation_space = gym.spaces.Discrete(2)\n",
    "\n",
    "\n",
    "def __init__(self, n, mt, mr):\n",
    "    \"\"\"\n",
    "    Simulate a job scheduling environment without preemption and two job\n",
    "    instance types with variable amount of resource instances, job numbers\n",
    "    and max job time.\n",
    "    @param n number of jobs\n",
    "    @param mt max burst time\n",
    "    @param mr max resource instance requirement\n",
    "    There are two resources\n",
    "    \"\"\"\n",
    "    self.state = 'INVALID' #Call make() to make an environment\n",
    "    self.action_sequence = [] #No actions taken yet\n",
    "    self.invalid_action_reward = -100\n",
    "    self.valid_action_reward = 0 #Try changing this\n",
    "    self.n = n\n",
    "    self.mt = mt\n",
    "    self.mr = mr\n",
    "    self.action_space = [i for i in range(1, mt+1)]\n",
    "    self.action_space_n = len(self.action_space)\n",
    "    self.observation_space = self.get_observation_space()\n",
    "    self.symbol = [str(i) for i in range(1, mt+1)]\n",
    "    self.rows = n * mt\n",
    "    self.columns = mr\n",
    "    self.reset()\n",
    "    \n",
    "    \n",
    " def __init__(self):\n",
    "    super(ChopperScape, self).__init__()\n",
    "\n",
    "    # Define a 2-D observation space\n",
    "    self.observation_shape = (600, 800, 3)\n",
    "    self.observation_space = spaces.Box(low = np.zeros(self.observation_shape), \n",
    "                                        high = np.ones(self.observation_shape),\n",
    "                                        dtype = np.float16)\n",
    "\n",
    "\n",
    "    # Define an action space ranging from 0 to 4\n",
    "    self.action_space = spaces.Discrete(6,)\n",
    "\n",
    "    # Create a canvas to render the environment images upon \n",
    "    self.canvas = np.ones(self.observation_shape) * 1\n",
    "\n",
    "    # Define elements present inside the environment\n",
    "    self.elements = []\n",
    "\n",
    "    # Maximum fuel chopper can take at once\n",
    "    self.max_fuel = 1000\n",
    "\n",
    "    # Permissible area of helicper to be \n",
    "    self.y_min = int (self.observation_shape[0] * 0.1)\n",
    "    self.x_min = 0\n",
    "    self.y_max = int (self.observation_shape[0] * 0.9)\n",
    "    self.x_max = self.observation_shape[1]\n",
    "    \n",
    "def __init__(self, df):\n",
    "    super(StockTradingEnv, self).__init__()\n",
    "    self.df = df\n",
    "    self.reward_range = (0, MAX_ACCOUNT_BALANCE) \n",
    "    # Actions of the format Buy x%, Sell x%, Hold, etc.\n",
    "    self.action_space = spaces.Box(\n",
    "      low=np.array([0, 0]), high=np.array([3, 1]), dtype=np.float16)\n",
    "    # Prices contains the OHCL values for the last five prices\n",
    "    self.observation_space = spaces.Box(\n",
    "      low=0, high=1, shape=(6, 6), dtype=np.float16)\n",
    "      \n",
    "      \n",
    "def __init__(self):\n",
    "    self.gravity = 9.8\n",
    "    self.masscart = 1.0\n",
    "    self.masspole = 0.1\n",
    "    self.total_mass = self.masspole + self.masscart\n",
    "    self.length = 0.5  # actually half the pole's length\n",
    "    self.polemass_length = self.masspole * self.length\n",
    "    self.force_mag = 10.0\n",
    "    self.tau = 0.02  # seconds between state updates\n",
    "    self.kinematics_integrator = \"euler\"\n",
    "\n",
    "    # Angle at which to fail the episode\n",
    "    self.theta_threshold_radians = 12 * 2 * math.pi / 360\n",
    "    self.x_threshold = 2.4\n",
    "\n",
    "    # Angle limit set to 2 * theta_threshold_radians so failing observation\n",
    "    # is still within bounds.\n",
    "    high = np.array(\n",
    "        [\n",
    "            self.x_threshold * 2,\n",
    "            np.finfo(np.float32).max,\n",
    "            self.theta_threshold_radians * 2,\n",
    "            np.finfo(np.float32).max,\n",
    "        ],\n",
    "        dtype=np.float32,\n",
    "    )\n",
    "\n",
    "    self.action_space = spaces.Discrete(2)\n",
    "    self.observation_space = spaces.Box(-high, high, dtype=np.float32)\n",
    "\n",
    "    self.viewer = None\n",
    "    self.state = None\n",
    "\n",
    "    self.steps_beyond_done = None\n",
    "    \n",
    "    \n",
    "    \n",
    "def __init__(self, goal_velocity=0):\n",
    "    self.min_position = -1.2\n",
    "    self.max_position = 0.6\n",
    "    self.max_speed = 0.07\n",
    "    self.goal_position = 0.5\n",
    "    self.goal_velocity = goal_velocity\n",
    "\n",
    "    self.force = 0.001\n",
    "    self.gravity = 0.0025\n",
    "\n",
    "    self.low = np.array([self.min_position, -self.max_speed], dtype=np.float32)\n",
    "    self.high = np.array([self.max_position, self.max_speed], dtype=np.float32)\n",
    "\n",
    "    self.viewer = None\n",
    "\n",
    "    self.action_space = spaces.Discrete(3)\n",
    "    self.observation_space = spaces.Box(self.low, self.high, dtype=np.float32)\n",
    "    \n",
    "def __init__(self, natural=False, sab=False):\n",
    "    self.action_space = spaces.Discrete(2)\n",
    "    self.observation_space = spaces.Tuple(\n",
    "        (spaces.Discrete(32), spaces.Discrete(11), spaces.Discrete(2))\n",
    "    )\n",
    "\n",
    "    # Flag to payout 1.5 on a \"natural\" blackjack win, like casino rules\n",
    "    # Ref: http://www.bicyclecards.com/how-to-play/blackjack/\n",
    "    self.natural = natural\n",
    "\n",
    "    # Flag for full agreement with the (Sutton and Barto, 2018) definition. Overrides self.natural\n",
    "    self.sab = sab\n",
    "    \n",
    "    \n",
    "def __init__(self):\n",
    "    self.desc = np.asarray(MAP, dtype=\"c\")\n",
    "\n",
    "    self.locs = locs = [(0, 0), (0, 4), (4, 0), (4, 3)]\n",
    "\n",
    "    num_states = 500\n",
    "    num_rows = 5\n",
    "    num_columns = 5\n",
    "    max_row = num_rows - 1\n",
    "    max_col = num_columns - 1\n",
    "    self.initial_state_distrib = np.zeros(num_states)\n",
    "    num_actions = 6\n",
    "    self.P = {\n",
    "        state: {action: [] for action in range(num_actions)}\n",
    "        for state in range(num_states)\n",
    "    }\n",
    "    for row in range(num_rows):\n",
    "        for col in range(num_columns):\n",
    "            for pass_idx in range(len(locs) + 1):  # +1 for being inside taxi\n",
    "                for dest_idx in range(len(locs)):\n",
    "                    state = self.encode(row, col, pass_idx, dest_idx)\n",
    "                    if pass_idx < 4 and pass_idx != dest_idx:\n",
    "                        self.initial_state_distrib[state] += 1\n",
    "                    for action in range(num_actions):\n",
    "                        # defaults\n",
    "                        new_row, new_col, new_pass_idx = row, col, pass_idx\n",
    "                        reward = (\n",
    "                            -1\n",
    "                        )  # default reward when there is no pickup/dropoff\n",
    "                        done = False\n",
    "                        taxi_loc = (row, col)\n",
    "\n",
    "                        if action == 0:\n",
    "                            new_row = min(row + 1, max_row)\n",
    "                        elif action == 1:\n",
    "                            new_row = max(row - 1, 0)\n",
    "                        if action == 2 and self.desc[1 + row, 2 * col + 2] == b\":\":\n",
    "                            new_col = min(col + 1, max_col)\n",
    "                        elif action == 3 and self.desc[1 + row, 2 * col] == b\":\":\n",
    "                            new_col = max(col - 1, 0)\n",
    "                        elif action == 4:  # pickup\n",
    "                            if pass_idx < 4 and taxi_loc == locs[pass_idx]:\n",
    "                                new_pass_idx = 4\n",
    "                            else:  # passenger not at location\n",
    "                                reward = -10\n",
    "                        elif action == 5:  # dropoff\n",
    "                            if (taxi_loc == locs[dest_idx]) and pass_idx == 4:\n",
    "                                new_pass_idx = dest_idx\n",
    "                                done = True\n",
    "                                reward = 20\n",
    "                            elif (taxi_loc in locs) and pass_idx == 4:\n",
    "                                new_pass_idx = locs.index(taxi_loc)\n",
    "                            else:  # dropoff at wrong location\n",
    "                                reward = -10\n",
    "                        new_state = self.encode(\n",
    "                            new_row, new_col, new_pass_idx, dest_idx\n",
    "                        )\n",
    "                        self.P[state][action].append((1.0, new_state, reward, done))\n",
    "    self.initial_state_distrib /= self.initial_state_distrib.sum()\n",
    "    self.action_space = spaces.Discrete(num_actions)\n",
    "    self.observation_space = spaces.Discrete(num_states)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
